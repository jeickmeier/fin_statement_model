
================================================================================
File: examples/scripts/clean_api_demo.py
================================================================================

"""Clean API Demonstration.

This example shows how the library uses centralized configuration
to provide a clean, simple API without requiring constant parameter passing.
"""

import logging
from fin_statement_model.config import get_config, update_config
from fin_statement_model.io import read_data
from fin_statement_model.io.validation import UnifiedNodeValidator
from fin_statement_model.statements import create_statement_dataframe
from fin_statement_model.core.nodes import FixedGrowthForecastNode

logger = logging.getLogger(__name__)


def main():
    """Demonstrate the clean API with automatic config usage."""
    print("=" * 80)
    print("CLEAN API DEMONSTRATION")
    print("Using centralized configuration for clean, simple code")
    print("=" * 80)

    # Set up some configuration
    update_config(
        {
            "display": {
                "default_units": "USD Millions",
                "scale_factor": 0.000001,
                "hide_zero_rows": True,
            },
            "validation": {"strict_mode": False, "auto_standardize_names": True},
            "forecasting": {"default_growth_rate": 0.05},  # 5% default growth
        }
    )

    config = get_config()
    print("\nCurrent Configuration:")
    print(f"  Display units: {config.display.default_units}")
    print(f"  Hide zero rows: {config.display.hide_zero_rows}")
    print(f"  Validation strict mode: {config.validation.strict_mode}")
    print(f"  Default growth rate: {config.forecasting.default_growth_rate}")

    # Sample data
    financial_data = {
        "revenue": {"2021": 100000000, "2022": 110000000, "2023": 121000000},
        "expenses": {"2021": 60000000, "2022": 65000000, "2023": 70000000},
        "net_income": {"2021": 40000000, "2022": 45000000, "2023": 51000000},
    }

    print("\n" + "=" * 60)
    print("AFTER: Clean API with automatic config usage")
    print("=" * 60)

    # The NEW way - clean and simple!

    # 1. Validation - automatically uses config defaults
    print("\n1. Node Validation (auto-uses config):")
    # Demo only: direct use of UnifiedNodeValidator; production code should use StatementConfig/StatementStructureBuilder for validation
    validator = UnifiedNodeValidator()  # That's it!

    result = validator.validate("gross revenue")
    print(f"   Validated 'gross revenue' -> '{result.standardized_name}'")
    print(
        f"   (Used config: strict_mode={validator.strict_mode}, "
        + f"auto_standardize={validator.auto_standardize})"
    )

    # 2. Data loading
    print("\n2. Loading Data:")
    graph = read_data(format_type="dict", source=financial_data)
    print(f"   Loaded {len(graph.nodes)} nodes")

    # 3. Forecasting - uses default growth rate
    print("\n3. Forecasting (auto-uses default growth rate):")
    revenue_node = graph.get_node("revenue")
    forecast = FixedGrowthForecastNode(
        revenue_node,
        "2023",
        ["2024", "2025"],
        # No growth_rate needed - uses config default!
    )
    print(f"   Created forecast with {forecast.growth_rate:.0%} growth (from config)")
    print(f"   2024 forecast: ${forecast.calculate('2024'):,.0f}")

    # 4. Statement formatting - automatically uses display config
    print("\n4. Statement Formatting (auto-uses display config):")

    # Define in-memory statement configuration
    statement_config = {
        "simple_income": {
            "id": "simple_income",
            "name": "Simple Income Statement",
            "sections": [
                {
                    "id": "main",
                    "name": "Income",
                    "items": [
                        {
                            "type": "line_item",
                            "id": "revenue",
                            "name": "Revenue",
                            "node_id": "revenue",
                        },
                        {
                            "type": "line_item",
                            "id": "expenses",
                            "name": "Expenses",
                            "node_id": "expenses",
                        },
                        {
                            "type": "line_item",
                            "id": "net_income",
                            "name": "Net Income",
                            "node_id": "net_income",
                        },
                    ],
                }
            ],
        }
    }

    # Create statement using in-memory config
    df_map = create_statement_dataframe(
        graph=graph,
        raw_configs=statement_config,
        format_kwargs={
            "should_apply_signs": True,
            "number_format": None,
        },
    )
    df = df_map["simple_income"]

    print("   Statement created with automatic config usage:")
    print(
        f"   - Scale factor: {config.display.scale_factor} (values in {config.display.default_units})"
    )
    print(f"   - Hide zero rows: {config.display.hide_zero_rows}")
    print(f"   - Number format: {config.display.default_number_format}")

    print("\n   Statement Output:")
    print(df.to_string(index=False))

    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)
    print(
        """
The library now provides a much cleaner API by:

1. Using configuration defaults automatically
2. Only requiring overrides when needed
3. Reducing boilerplate code significantly
4. Making the code more readable and maintainable

Benefits:
✓ Less code to write and maintain
✓ Consistent behavior across the application
✓ Easy to change defaults globally
✓ Still flexible when you need overrides
"""
    )


if __name__ == "__main__":
    main()



================================================================================
File: examples/scripts/config_demo.py
================================================================================

"""Configuration System Demonstration.

This example shows how to use the centralized configuration system
in the fin_statement_model library.
"""

import logging
import os
from pathlib import Path
import tempfile

from fin_statement_model.config import get_config, update_config, reset_config, cfg
from fin_statement_model.io import read_data
from fin_statement_model.config.manager import generate_env_mappings

logger = logging.getLogger(__name__)


def demo_config_basics() -> None:
    """Demonstrate basic configuration usage."""
    print("=" * 60)
    print("CONFIGURATION BASICS")
    print("=" * 60)

    # Get current configuration
    config = get_config()

    print("Current Configuration Sources:")
    print(f"  Loaded from: {getattr(config, '_source', 'defaults')}")

    print("\nLogging Configuration:")
    print(f"  Level: {cfg('logging.level')}")
    print(f"  Format: {cfg('logging.format')[:50]}...")
    print(f"  Detailed: {cfg('logging.detailed')}")

    print("\nDisplay Configuration:")
    print(f"  Units: {cfg('display.default_units')}")
    print(f"  Currency Format: {cfg('display.default_currency_format')}")
    print(f"  Scale Factor: {cfg('display.scale_factor')}")

    print("\nValidation Configuration:")
    print(f"  Strict Mode: {cfg('validation.strict_mode')}")
    print(f"  Auto Standardize: {cfg('validation.auto_standardize_names')}")


def demo_runtime_updates() -> None:
    """Show how to update configuration at runtime."""
    print("\n" + "=" * 60)
    print("RUNTIME CONFIGURATION UPDATES")
    print("=" * 60)

    # Save original config
    original_units = cfg("display.default_units")

    # Update configuration
    print("Updating display configuration...")
    update_config(
        {
            "display": {
                "default_units": "EUR Thousands",
                "scale_factor": 0.001,
                "default_currency_format": ",.2f",
            }
        }
    )

    print("\nUpdated Display Config:")
    print(f"  Units: {cfg('display.default_units')}")
    print(f"  Scale Factor: {cfg('display.scale_factor')}")
    print(f"  Format: {cfg('display.default_currency_format')}")

    # Demonstrate effect on data display
    sample_value = 1234567.89
    scaled_value = sample_value * cfg("display.scale_factor")
    print("\nExample value formatting:")
    print(f"  Original: ${sample_value:,.2f}")
    print(
        f"  Scaled: {scaled_value:{cfg('display.default_currency_format')}} {cfg('display.default_units')}"
    )

    # Reset to original
    update_config({"display": {"default_units": original_units}})


def demo_environment_variables() -> None:
    """Show how environment variables affect configuration."""
    print("\n" + "=" * 60)
    print("ENVIRONMENT VARIABLE CONFIGURATION")
    print("=" * 60)

    print("Supported environment variables (sample):")
    env_mappings = generate_env_mappings(get_config().__class__)
    for env_key, path in sorted(env_mappings.items())[:8]:
        print(f"  {env_key} -> {'.'.join(path)}")

    # Demonstrate setting an environment variable
    print("\nDemo: Setting FSM_DISPLAY_UNITS...")
    os.environ["FSM_DISPLAY_UNITS"] = "GBP Millions"

    # Reset config to pick up environment variable
    reset_config()
    config = get_config()

    print(f"Display units after reset: {config.display.default_units}")

    # Clean up
    del os.environ["FSM_DISPLAY_UNITS"]
    reset_config()


def demo_config_file() -> None:
    """Show how to use a configuration file."""
    print("\n" + "=" * 60)
    print("CONFIGURATION FILE USAGE")
    print("=" * 60)

    # Create a sample config file
    config_content = """
# Example fin_statement_model.config settings
logging:
  level: INFO
  detailed: true

display:
  default_units: "USD Millions"
  scale_factor: 0.000001
  default_currency_format: ",.1f"
  default_percentage_format: ".1%"
  hide_zero_rows: true

validation:
  strict_mode: true
  auto_standardize_names: true
  check_balance_sheet_balance: true
  balance_tolerance: 0.01

io:
  default_excel_sheet: "FinancialData"
  skip_invalid_rows: false
  strict_validation: true

forecasting:
  default_method: "historical_growth"
  default_periods: 5
  default_growth_rate: 0.05
"""

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        f.write(config_content)
        config_path = f.name

    print(f"Created config file: {config_path}")
    print("\nConfig file content (excerpt):")
    print(config_content[:300] + "...")

    # Load from file
    from fin_statement_model.config import Config

    loaded_config = Config.from_file(Path(config_path))
    file_config = loaded_config.to_dict()

    print("\nLoaded configuration sections:")
    for section in file_config:
        print(f"  - {section}")

    # Apply the file config
    update_config(file_config)

    print("\nApplied configuration:")
    print(f"  Display units: {cfg('display.default_units')}")
    print(f"  Validation strict mode: {cfg('validation.strict_mode')}")
    print(f"  Default forecast method: {cfg('forecasting.default_method')}")

    # Clean up
    os.unlink(config_path)
    reset_config()


def demo_config_in_action() -> None:
    """Demonstrate config affecting actual operations."""
    print("\n" + "=" * 60)
    print("CONFIGURATION IN ACTION")
    print("=" * 60)

    # Create sample data
    sample_data = {
        "revenue": {"2022": 1000000, "2023": 1200000},
        "expenses": {"2022": 600000, "2023": 700000},
        "net_income": {"2022": 400000, "2023": 500000},
    }

    # Test 1: Default configuration
    print("Test 1: Default Configuration")
    print(f"  Units: {cfg('display.default_units')}")

    graph = read_data(format_type="dict", source=sample_data)
    revenue_node = graph.get_node("revenue")
    if revenue_node is None:
        print("  Error: Revenue node not found")
        return
    revenue_2023 = revenue_node.get_value("2023")
    print(f"  Revenue 2023: ${revenue_2023:,.2f}")

    # Test 2: European configuration
    print("\nTest 2: European Configuration")
    update_config(
        {
            "display": {
                "default_units": "EUR Thousands",
                "scale_factor": 0.001,
                "default_currency_format": ",.2f",
                "thousands_separator": ".",
                "decimal_separator": ",",
            }
        }
    )

    scaled_revenue = revenue_2023 * cfg("display.scale_factor")
    print(f"  Revenue 2023: {scaled_revenue:,.2f} {cfg('display.default_units')}")

    # Test 3: Validation configuration
    print("\nTest 3: Validation Configuration")
    update_config({"validation": {"strict_mode": True, "warn_on_non_standard": True}})

    # Try to add a node with non-standard name
    from fin_statement_model.io.validation import UnifiedNodeValidator

    # Demo only: direct use of UnifiedNodeValidator; production code should use StatementConfig/StatementStructureBuilder for validation
    validator = UnifiedNodeValidator(
        strict_mode=cfg("validation.strict_mode"),
        warn_on_non_standard=cfg("validation.warn_on_non_standard"),
    )

    test_names = ["Revenue 2023!", "net_income", "EBITDA"]
    print(f"  Validating node names with strict={cfg('validation.strict_mode')}:")
    for name in test_names:
        result = validator.validate(name)
        print(f"    {name}: {'✓' if result.is_valid else '✗'} {result.message}")

    # Reset
    reset_config()


def demo_config_serialization() -> None:
    """Show how to save and share configurations."""
    print("\n" + "=" * 60)
    print("CONFIGURATION SERIALIZATION")
    print("=" * 60)

    # Create a custom configuration
    update_config(
        {
            "display": {
                "default_units": "JPY Millions",
                "scale_factor": 0.000001,
                "default_currency_format": ",.0f",
            },
            "validation": {"strict_mode": False},
        }
    )

    config = get_config()

    # Export to dict
    config_dict = config.to_dict()
    print("Configuration as dictionary:")
    print(f"  Keys: {list(config_dict.keys())}")

    # Export to YAML
    yaml_str = config.to_yaml()
    print("\nConfiguration as YAML (first 200 chars):")
    print(yaml_str[:200] + "...")

    # Save to file
    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        f.write(yaml_str)
        saved_path = f.name

    print(f"\nSaved configuration to: {saved_path}")

    # Load it back
    from fin_statement_model.config import Config

    loaded_config = Config.from_file(Path(saved_path))

    print("\nLoaded configuration:")
    print(f"  Display units: {loaded_config.display.default_units}")

    # Clean up
    os.unlink(saved_path)
    reset_config()

    print("• Configurations can be serialized and shared")
    print("• Slim config API under fin_statement_model.config for concise access")


def main() -> None:
    """Run all configuration demonstrations."""
    print("FINANCIAL STATEMENT MODEL - CONFIGURATION SYSTEM DEMO")
    print("=" * 80)

    # Run demonstrations
    demo_config_basics()
    demo_runtime_updates()
    demo_environment_variables()
    demo_config_file()
    demo_config_in_action()
    demo_config_serialization()

    print("\n" + "=" * 80)
    print("CONFIGURATION DEMO COMPLETE")
    print("=" * 80)
    print("\nKey Takeaways:")
    print("• Configuration can be loaded from multiple sources with clear precedence")
    print("• Runtime updates allow dynamic behavior changes")
    print("• Environment variables provide deployment flexibility")
    print("• Configuration files enable reproducible setups")
    print("• All subsystems respect the centralized configuration")
    print("• Configurations can be serialized and shared")
    print("• Slim config API under fin_statement_model.config for concise access")


if __name__ == "__main__":
    main()



================================================================================
File: examples/scripts/declarative_adjustment_demo.py
================================================================================

#!/usr/bin/env python3
"""Demonstration of Declarative Adjustment Handling in Statement Configurations.

This script shows how to use the new declarative adjustment handling feature
that allows specifying default adjustment filters directly in statement
configuration files.
"""

import sys
from typing import Any
import pandas as pd
import logging

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.adjustments.models import (
    AdjustmentFilter,
    AdjustmentType,
)
from fin_statement_model.statements import (
    StatementStructureBuilder,
    StatementConfig,
    StatementFormatter,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def create_sample_graph_with_adjustments() -> Graph:
    """Create a sample graph with nodes and adjustments for demonstration."""
    graph = Graph()

    # Add base data nodes
    graph.add_financial_statement_item(
        "revenue_gross", {"2023Q1": 1000, "2023Q2": 1100, "2023Q3": 1200}
    )
    graph.add_financial_statement_item(
        "revenue_deductions", {"2023Q1": 50, "2023Q2": 55, "2023Q3": 60}
    )
    graph.add_financial_statement_item(
        "cogs_materials", {"2023Q1": 300, "2023Q2": 330, "2023Q3": 360}
    )
    graph.add_financial_statement_item(
        "cogs_labor", {"2023Q1": 200, "2023Q2": 220, "2023Q3": 240}
    )
    graph.add_financial_statement_item(
        "opex_salaries", {"2023Q1": 150, "2023Q2": 155, "2023Q3": 160}
    )

    # Add some adjustments
    # Budget adjustments for revenue
    graph.add_adjustment(
        node_name="revenue_gross",
        period="2023Q1",
        value=50,
        adj_type=AdjustmentType.ADDITIVE,
        tags={"budget", "forecast"},
        reason="Budget increase for Q1",
    )
    graph.add_adjustment(
        node_name="revenue_gross",
        period="2023Q2",
        value=75,
        adj_type=AdjustmentType.ADDITIVE,
        tags={"budget", "forecast"},
        reason="Budget increase for Q2",
    )
    # Management estimates for labor
    graph.add_adjustment(
        node_name="cogs_labor",
        period="2023Q1",
        value=20,
        adj_type=AdjustmentType.ADDITIVE,
        tags={"management", "estimate"},
        reason="Management labor cost estimate",
    )
    # Approved adjustments for materials
    graph.add_adjustment(
        node_name="cogs_materials",
        period="2023Q1",
        value=-10,
        adj_type=AdjustmentType.ADDITIVE,
        tags={"approved"},
        reason="Approved material cost reduction",
    )
    # Preliminary adjustment (should be excluded in some views)
    graph.add_adjustment(
        node_name="revenue_deductions",
        period="2023Q1",
        value=5,
        adj_type=AdjustmentType.ADDITIVE,
        tags={"preliminary"},
        reason="Preliminary adjustment",
    )

    return graph


def create_sample_config() -> dict[str, Any]:
    """Create a sample statement configuration with declarative adjustment filters."""
    config_data = {
        "id": "demo_income_statement",
        "name": "Demo Income Statement with Declarative Adjustments",
        "description": "Demonstrates declarative adjustment handling",
        "sections": [
            {
                "id": "revenue_section",
                "name": "Revenue",
                "description": "Revenue items with budget/forecast adjustments",
                # Section-level filter: show budget and forecast adjustments
                "default_adjustment_filter": {
                    "include_tags": ["budget", "forecast"],
                    "exclude_tags": ["preliminary"],
                },
                "items": [
                    {
                        "type": "line_item",
                        "id": "gross_revenue",
                        "name": "Gross Revenue",
                        "node_id": "revenue_gross",
                        "description": "Uses section filter: budget/forecast, excludes preliminary",
                        # Inherits section filter
                    },
                    {
                        "type": "line_item",
                        "id": "revenue_deductions",
                        "name": "Revenue Deductions",
                        "node_id": "revenue_deductions",
                        "sign_convention": -1,
                        "description": "Override to show no adjustments (actuals only)",
                        # Override section filter - show actuals only
                        "default_adjustment_filter": [],
                    },
                    {
                        "type": "calculated",
                        "id": "net_revenue",
                        "name": "Net Revenue",
                        "description": "Calculated item using section filter",
                        "calculation": {
                            "type": "subtraction",
                            "inputs": ["gross_revenue", "revenue_deductions"],
                        },
                        # Uses section filter by inheritance
                    },
                ],
            },
            {
                "id": "cost_section",
                "name": "Cost of Goods Sold",
                "description": "Cost items with different adjustment strategies",
                # Section-level filter: approved adjustments only
                "default_adjustment_filter": {"include_tags": ["approved"]},
                "items": [
                    {
                        "type": "line_item",
                        "id": "material_costs",
                        "name": "Material Costs",
                        "node_id": "cogs_materials",
                        "description": "Uses section filter: approved adjustments only",
                        # Inherits section filter: approved only
                    },
                    {
                        "type": "line_item",
                        "id": "labor_costs",
                        "name": "Direct Labor",
                        "node_id": "cogs_labor",
                        "description": "Override to show management estimates",
                        # Override section filter - show management estimates
                        "default_adjustment_filter": ["management", "estimate"],
                    },
                    {
                        "type": "calculated",
                        "id": "total_cogs",
                        "name": "Total COGS",
                        "description": "Sum of materials and labor",
                        "calculation": {
                            "type": "addition",
                            "inputs": ["material_costs", "labor_costs"],
                        },
                        # Uses section filter: approved only
                    },
                ],
            },
            {
                "id": "other_section",
                "name": "Other Items",
                "description": "Items with no section-level filter",
                # No section-level filter specified
                "items": [
                    {
                        "type": "line_item",
                        "id": "salaries",
                        "name": "Salaries",
                        "node_id": "opex_salaries",
                        "description": "No adjustments - shows actuals only",
                        # No item or section filter - shows raw data
                    }
                ],
            },
        ],
    }

    return config_data


def demonstrate_declarative_adjustments():
    """Demonstrate the declarative adjustment handling feature."""
    logger.info("=" * 80)
    logger.info("Declarative Adjustment Handling Demonstration")
    logger.info("=" * 80)

    # Create sample data
    graph = create_sample_graph_with_adjustments()
    config_data = create_sample_config()

    logger.info("\n1. CONFIGURATION OVERVIEW")
    logger.info("-" * 40)
    logger.info(f"Statement: {config_data['name']}")
    logger.info(f"Sections: {len(config_data['sections'])}")

    for section in config_data["sections"]:
        logger.info(f"\n  Section: {section['name']}")
        section_filter = section.get("default_adjustment_filter")
        if section_filter:
            logger.info(f"    Section Filter: {section_filter}")
        else:
            logger.info("    Section Filter: None")

        for item in section["items"]:
            item_filter = item.get("default_adjustment_filter")
            logger.info(
                f"    - {item['name']}: {item_filter if item_filter is not None else 'Inherits section filter'}"
            )

    # Build statement structure
    config = StatementConfig(config_data)
    validation_errors = config.validate_config()

    if validation_errors:
        logger.info(f"\nConfiguration validation errors: {validation_errors}")
        return

    builder = StatementStructureBuilder()
    statement = builder.build(config)
    formatter = StatementFormatter(statement)

    logger.info("\n2. DATA FETCHING WITH DEFAULT FILTERS")
    logger.info("-" * 40)

    # Generate dataframe using default filters from configuration
    df_with_defaults = formatter.generate_dataframe(
        graph=graph, include_empty_items=True, include_metadata_cols=True
    )

    logger.info("Generated DataFrame with default adjustment filters:")
    logger.info(df_with_defaults.to_string())

    logger.info("\n3. OVERRIDE WITH GLOBAL FILTER")
    logger.info("-" * 40)

    # Override all default filters with a global filter
    global_filter = AdjustmentFilter(include_tags={"management"})
    df_with_override = formatter.generate_dataframe(
        graph=graph,
        adjustment_filter=global_filter,  # Overrides all defaults
        include_empty_items=True,
        include_metadata_cols=True,
    )

    logger.info("Generated DataFrame with global filter override (management only):")
    logger.info(df_with_override.to_string())

    logger.info("\n4. RAW DATA (NO ADJUSTMENTS)")
    logger.info("-" * 40)

    # Show raw data without any adjustments
    empty_filter = AdjustmentFilter()  # Empty filter
    df_raw = formatter.generate_dataframe(
        graph=graph,
        adjustment_filter=empty_filter,
        include_empty_items=True,
        include_metadata_cols=True,
    )

    logger.info("Generated DataFrame with no adjustments (raw data):")
    logger.info(df_raw.to_string())

    logger.info("\n5. FILTER ANALYSIS")
    logger.info("-" * 40)

    logger.info("Examining default filters in the built statement structure:")

    for section in statement.sections:
        logger.info(f"\nSection '{section.name}':")
        logger.info(f"  Default filter: {section.default_adjustment_filter}")

        for item in section.items:
            if hasattr(item, "default_adjustment_filter"):
                logger.info(f"  Item '{item.name}': {item.default_adjustment_filter}")

    logger.info("\n6. COMPARISON SUMMARY")
    logger.info("-" * 40)

    # Compare key values across different filter scenarios
    comparison_data = [
        {
            "Period": period,
            "Gross Revenue (Default)": (
                df_with_defaults.loc[
                    df_with_defaults["ID"] == "gross_revenue", period
                ].iloc[0]
                if len(df_with_defaults.loc[df_with_defaults["ID"] == "gross_revenue"])
                > 0
                else "N/A"
            ),
            "Gross Revenue (Raw)": (
                df_raw.loc[df_raw["ID"] == "gross_revenue", period].iloc[0]
                if len(df_raw.loc[df_raw["ID"] == "gross_revenue"]) > 0
                else "N/A"
            ),
            "Labor Costs (Default)": (
                df_with_defaults.loc[
                    df_with_defaults["ID"] == "labor_costs", period
                ].iloc[0]
                if len(df_with_defaults.loc[df_with_defaults["ID"] == "labor_costs"])
                > 0
                else "N/A"
            ),
            "Labor Costs (Management)": (
                df_with_override.loc[
                    df_with_override["ID"] == "labor_costs", period
                ].iloc[0]
                if len(df_with_override.loc[df_with_override["ID"] == "labor_costs"])
                > 0
                else "N/A"
            ),
        }
        for period in ["2023Q1", "2023Q2", "2023Q3"]
        if period in df_with_defaults.columns
    ]

    comparison_df = pd.DataFrame(comparison_data)
    logger.info("Value Comparison Across Filter Scenarios:")
    logger.info(comparison_df.to_string(index=False))

    logger.info("\n" + "=" * 80)
    logger.info("Demonstration Complete!")
    logger.info("\nKey Takeaways:")
    logger.info("- Each item can have its own default adjustment filter")
    logger.info("- Section-level filters apply to all items that don't override")
    logger.info("- Global filters passed to generate_dataframe() override all defaults")
    logger.info("- Filter precedence: Global > Item > Section > None")
    logger.info("=" * 80)


if __name__ == "__main__":
    try:
        demonstrate_declarative_adjustments()
    except Exception:
        logger.exception("Demonstration failed")
        sys.exit(1)



================================================================================
File: examples/scripts/example_statement_with_adjustments.py
================================================================================

"""This example demonstrates using the detailed test_statement.yaml configuration.

and applying adjustments before generating the final statement.

It includes:
- Loading the complex statement configuration.
- Populating a graph with corresponding sample data.
- Running forecasts.
- Adding adjustments to specific nodes/periods.
- Generating the statement DataFrame showing the impact of adjustments.
"""

import sys
import logging
from pathlib import Path
import yaml

from fin_statement_model.core.errors import FinancialModelError
from fin_statement_model.core.adjustments.models import AdjustmentType

from fin_statement_model.io import read_data
from fin_statement_model.statements import create_statement_dataframe
from fin_statement_model.forecasting.forecaster import StatementForecaster
from fin_statement_model.statements.orchestration import export_statements_to_excel

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- 1. Setup ---

# Use relative paths assuming script is run from workspace root
WORKSPACE_ROOT = Path(__file__).resolve().parents[2]  # Adjust depth if needed
CONFIG_PATH = WORKSPACE_ROOT / "examples/scripts/configs/test_statement.yaml"
MD_OUTPUT_PATH = (
    WORKSPACE_ROOT / "examples/scripts/output/statement_with_adjustments.md"
)

if not CONFIG_PATH.is_file():
    logger.error(f"Configuration file not found: {CONFIG_PATH}")
    sys.exit(1)

MD_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

# --- 2. Sample Data ---

# Sample historical data matching node_ids in test_statement.yaml
historical_data = {
    # Node ID: { Period: Value }
    "cash_and_equivalents": {"2021": 90.0, "2022": 100.0, "2023": 120.0},
    "accounts_receivable": {"2021": 180.0, "2022": 200.0, "2023": 250.0},
    "property_plant_equipment": {"2021": 480.0, "2022": 500.0, "2023": 550.0},
    "accounts_payable": {"2021": 140.0, "2022": 150.0, "2023": 180.0},
    "total_debt": {"2021": 290.0, "2022": 300.0, "2023": 320.0},
    "common_stock": {"2021": 100.0, "2022": 100.0, "2023": 100.0},
    "prior_retained_earnings": {"2021": 80.0, "2022": 100.0, "2023": 125.0},
    "dividends": {"2021": -8.0, "2022": -10.0, "2023": -15.0},
    "revenue": {"2021": 900.0, "2022": 1000.0, "2023": 1200.0},
    "cost_of_goods_sold": {"2021": -350.0, "2022": -400.0, "2023": -500.0},
    "operating_expenses": {"2021": -280.0, "2022": -300.0, "2023": -350.0},
    "interest_expense": {"2021": -20.0, "2022": -25.0, "2023": -30.0},
    "income_tax": {"2021": -70.0, "2022": -80.0, "2023": -95.0},
}

# --- 3. Create Graph with Data ---
# First, create a graph with our data nodes using the dict reader
try:
    logger.info("Creating graph and loading initial data...")
    graph = read_data(format_type="dict", source=historical_data)
    logger.info(f"Graph created with initial periods: {graph.periods}")
except FinancialModelError as e:
    logger.error(f"Error creating graph or loading initial data: {e}", exc_info=True)
    sys.exit(1)

# --- 4. Build Statement Structure and Populate Graph ---
# Use create_statement_dataframe to properly build the statement structure,
# populate the graph with calculation nodes, and generate the initial dataframe
try:
    logger.info(
        "Building statement structure and populating graph with calculation nodes..."
    )

    # Load statement config into memory
    raw_config = yaml.safe_load(CONFIG_PATH.read_text(encoding="utf-8"))
    raw_configs = {raw_config.get("id", "statement"): raw_config}
    initial_df_map = create_statement_dataframe(
        graph=graph,
        raw_configs=raw_configs,
        format_kwargs={
            "should_apply_signs": True,
            "number_format": ",.1f",
        },
    )
    initial_df = initial_df_map.get(list(raw_configs.keys())[0])

    logger.info("Statement structure built and graph populated with calculation nodes.")

except FinancialModelError as e:
    logger.error(f"Error building statement structure: {e}", exc_info=True)
    sys.exit(1)

# --- 5. Forecasting Setup ---
logger.info("Setting up forecasting...")

historical_periods = sorted(list(graph.periods))
forecast_periods = ["2024", "2025", "2026", "2027", "2028"]
all_periods = sorted(historical_periods + forecast_periods)

logger.info(f"Historical periods: {historical_periods}")
logger.info(f"Forecast periods: {forecast_periods}")

# Define forecast configurations using node IDs from historical_data
forecast_configs = {
    "cash_and_equivalents": {"method": "simple", "config": 0.05},
    "accounts_receivable": {"method": "historical_growth", "config": {}},
    "property_plant_equipment": {"method": "simple", "config": 0.02},
    "accounts_payable": {
        "method": "curve",
        "config": [0.04, 0.03, 0.03, 0.02, 0.02],
    },
    "total_debt": {"method": "simple", "config": 0.0},
    "common_stock": {"method": "simple", "config": 0.0},
    "prior_retained_earnings": {"method": "simple", "config": 0.0},
    "dividends": {"method": "historical_growth", "config": {}},
    "revenue": {"method": "curve", "config": [0.10, 0.09, 0.08, 0.07, 0.06]},
    "cost_of_goods_sold": {"method": "historical_growth", "config": {}},
    "operating_expenses": {
        "method": "statistical",
        "config": {"distribution": "normal", "params": {"mean": 0.03, "std": 0.015}},
    },
    "interest_expense": {"method": "historical_growth", "config": {}},
    "income_tax": {"method": "simple", "config": 0.02},
}

# Use the StatementForecaster
try:
    forecaster = StatementForecaster(fsg=graph)
    logger.info(f"Applying forecasts for periods: {forecast_periods}")
    # Apply the forecasts using the defined configs
    forecaster.create_forecast(
        forecast_periods=forecast_periods,
        node_configs=forecast_configs,
        historical_periods=historical_periods,
    )
    logger.info(f"Forecasting complete. Graph now includes periods: {graph.periods}")
except FinancialModelError as e:
    logger.error(f"Error during forecasting: {e}", exc_info=True)
    sys.exit(1)

# --- 6. Add Adjustments ---
logger.info("\n--- Adding Adjustments ---")

try:
    adj_id_1 = graph.add_adjustment(
        node_name="revenue",
        period="2023",  # Adjust historical data
        value=75.0,
        adj_type=AdjustmentType.ADDITIVE,
        reason="Late recognized revenue for 2023.",
        tags={"manual", "revenue_recognition"},
        user="AnalystX",
    )
    logger.info(f"Added additive adjustment {adj_id_1} for revenue 2023.")

    adj_id_2 = graph.add_adjustment(
        node_name="operating_expenses",
        period="2024",  # Adjust forecasted data
        value=-400.0,  # Note: OPEX is typically negative
        adj_type=AdjustmentType.REPLACEMENT,
        reason="Revised forecast for OPEX in 2024 based on restructuring.",
        tags={"forecast_revision", "restructuring"},
        user="AnalystY",
        priority=-10,
    )
    logger.info(f"Added replacement adjustment {adj_id_2} for operating_expenses 2024.")

except FinancialModelError as e:
    logger.error(f"Error adding adjustments: {e}", exc_info=True)
    sys.exit(1)

# --- 7. Regenerate Statement with Adjustments ---
logger.info("\n--- Generating Statement with Adjustments Applied ---")

try:
    # Regenerate with adjustments using same raw_configs
    statement_df_map = create_statement_dataframe(
        graph=graph,
        raw_configs=raw_configs,
        format_kwargs={
            "should_apply_signs": True,
            "number_format": ",.1f",
            "adjustment_filter": None,
            "add_is_adjusted_column": True,
        },
    )
    statement_df = statement_df_map.get(list(raw_configs.keys())[0])

    logger.info("Statement DataFrame generated successfully.")
    # Display the results
    logger.info("\n--- Statement DataFrame with Adjustments ---")
    logger.info(statement_df.to_string(index=False))

    # Export the statement to Excel using the modern helper function
    export_statements_to_excel(
        graph=graph,
        config_path_or_dir=raw_configs,
        output_dir=str(MD_OUTPUT_PATH.parent),
        format_kwargs={
            "should_apply_signs": True,
            "number_format": ",.1f",
            "adjustment_filter": None,
            "add_is_adjusted_column": True,
        },
    )
except FinancialModelError as e:
    logger.error(f"Error generating statement dataframe: {e}", exc_info=True)
    sys.exit(1)
except FileNotFoundError:
    logger.error(
        f"Statement configuration file not found: {CONFIG_PATH}", exc_info=True
    )
    sys.exit(1)

logger.info("\nExample complete.")



================================================================================
File: examples/scripts/med_example.py
================================================================================

"""This example demonstrates the core capabilities of the fin_statement_model library.

It includes:
- Basic setup and configuration
- Historical statement generation
- Forecasting
- Exporting to Excel
"""

import logging
from pathlib import Path
import sys

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import (
    FinancialModelError,
)

from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.io import read_data
from fin_statement_model.statements import (
    export_statements_to_excel,
)
from fin_statement_model.forecasting.forecaster import StatementForecaster
from fin_statement_model.core.metrics.registry import metric_registry
from pathlib import Path as MetricPath
from fin_statement_model.statements.configs.validator import StatementConfig
from fin_statement_model.statements.structure.builder import StatementStructureBuilder
from fin_statement_model.statements.registry import StatementRegistry
from fin_statement_model.statements.orchestration.orchestrator import populate_graph
from fin_statement_model.statements.formatting.formatter import StatementFormatter
from fin_statement_model.statements.orchestration.loader import (
    load_build_register_statements,
)

# --- 1. Setup ---

# Configure logging for visibility
logging.basicConfig(
    level=logging.WARNING, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

SCRIPT_DIR = Path(__file__).parent
CONFIG_DIR = SCRIPT_DIR / "configs"
OUTPUT_DIR = SCRIPT_DIR / "output"

CONFIG_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

logger.info(f"Using temporary config directory: {CONFIG_DIR}")
logger.info(f"Using temporary output directory: {OUTPUT_DIR}")

# --- 2. Configuration Files removed; using code-only StatementStructure in Step 4 ---

# --- 3. Initial Data Loading ---

# Sample historical data (Node IDs match those in YAML `node_id` fields)
historical_data = {
    # Node ID: { Period: Value }
    "Revenue": {"2022": 1000.0, "2023": 1200.0},
    "COGS": {"2022": -400.0, "2023": -500.0},
    "R&D": {"2022": -100.0, "2023": -120.0},
    "SG&A": {"2022": -200.0, "2023": -250.0},
    "Interest Expense": {"2022": -50.0, "2023": -60.0},
    "Taxes": {"2022": -75.0, "2023": -90.0},
    "D&A": {"2022": -30.0, "2023": -35.0},
}

# Create the core graph
graph = Graph()
graph = read_data(format_type="dict", source=historical_data)

# --- 4. Statement Generation (Historical) using code-only StatementStructure ---

# Build the statement structure in code
income_statement_config = {
    "id": "income_statement",
    "name": "Income Statement",
    "description": "Reports financial performance over a specific period.",
    "sections": [
        {
            "id": "revenue_section",
            "name": "Revenue",
            "items": [
                {
                    "id": "revenue",
                    "name": "Total Revenue",
                    "type": "line_item",
                    "node_id": "Revenue",
                    "sign_convention": 1,
                }
            ],
        },
        {
            "id": "cost_of_goods_sold",
            "name": "Cost of Goods Sold",
            "items": [
                {
                    "id": "cogs",
                    "name": "Cost of Goods Sold",
                    "type": "line_item",
                    "node_id": "COGS",
                    "sign_convention": -1,
                }
            ],
        },
        {
            "id": "gross_profit_section",
            "name": "Gross Profit",
            "items": [
                {
                    "id": "gross_profit",
                    "name": "Gross Profit",
                    "type": "metric",
                    "metric_id": "gross_profit",
                    "inputs": {"revenue": "revenue", "cost_of_goods_sold": "cogs"},
                    "sign_convention": 1,
                }
            ],
        },
        # Add additional sections as needed here
    ],
}

stmt_cfg = StatementConfig(income_statement_config)
errors = stmt_cfg.validate_config()
if errors:
    logger.error(f"Statement config validation errors: {errors}")
    sys.exit(1)
structure = StatementStructureBuilder().build(stmt_cfg)
registry = StatementRegistry()
registry.clear()
registry.register(structure)
populate_graph(registry, graph)
formatter = StatementFormatter(structure)
income_statement_df_hist = formatter.generate_dataframe(
    graph,
    should_apply_signs=True,
    number_format=",.0f",
)
logger.info("✓ Statement structure built successfully")
logger.info("\nIncome Statement:")
logger.info(income_statement_df_hist.to_string(index=False))

# --- 5. Forecasting ---

logger.info("Setting up forecasting...")
forecast_periods = ["2024", "2025", "2026"]

# Define how different nodes should be forecasted
# Maps node_id to forecast configuration
forecast_configs = {
    "Revenue": {
        "method": "simple",  # Simple growth method
        "config": 0.10,  # 10% growth rate
    },
    "COGS": {
        "method": "historical_growth",  # Use average historical growth
        "config": None,  # No specific config needed for this method
    },
    "R&D": {
        "method": "curve",  # Different growth rates per period
        "config": [0.08, 0.06, 0.15],  # 8%, 6%, 5% growth for 2024, 2025, 2026
    },
    "SG&A": {
        "method": "statistical",  # Correct method name recognized by StatementForecaster
        "config": {  # Config structure expected by StatementForecaster for statistical
            "distribution": "normal",
            "params": {  # Parameters nested under 'params'
                "mean": 0.02,  # Mean growth rate (2%)
                "std": 0.05,  # Standard deviation key is 'std'
            },
        },
    },
    "Interest Expense": {  # Added forecast
        "method": "simple",
        "config": 0.05,  # 5% growth rate (increase in expense)
    },
    "Taxes": {  # Added forecast
        "method": "historical_growth",  # Assume taxes grow similarly to past trends
        "config": None,
    },
    "D&A": {  # Added forecast
        "method": "simple",  # Simple growth method
        "config": 0.03,  # 3% growth rate (increase in expense)
    },
    # Note: Calculated items (like gross_profit, ebitda, ebit, net_income) don't need forecast configs,
    # they will be recalculated based on their forecasted inputs.
}

# Use the StatementForecaster (adjust if API changes)
try:
    forecaster = StatementForecaster(fsg=graph)  # Pass the core graph
    logger.info(f"Creating forecasts for periods: {forecast_periods}")

    # Apply the forecasts - this modifies the underlying data nodes in the graph
    forecaster.create_forecast(
        forecast_periods=forecast_periods,
        node_configs=forecast_configs,
        historical_periods=["2022", "2023"],  # Explicitly provide historical periods
    )

    logger.info("Forecasting complete. Graph data nodes updated.")
    # logger.debug(f"Graph periods after forecast: {graph.periods}") # Already added periods
    # logger.debug(f"Forecasted Revenue in 2025: {graph.get_node('Revenue').get_value('2025')}")
    # logger.debug(f"Recalculated Gross Profit in 2025: {graph.calculate('gross_profit', '2025')}")

except (ValueError, FinancialModelError):
    logger.exception("Forecasting failed")
    sys.exit()

# --- 6. Statement Generation (Forecasted) using code-only StatementStructure ---
# TODO: Implement forecasted statement generation using code-only StatementStructure.

# --- 7. Exporting ---

# Reload the special metrics directory to pick up the newly added retained_earnings metric
special_dir = (
    MetricPath(__file__).parent.parent.parent
    / "fin_statement_model/core/metrics/metric_defn/special"
)
metric_count = metric_registry.load_metrics_from_directory(special_dir)
logger.info(
    f"Reloaded {metric_count} metrics from special directory to include retained_earnings"
)

excel_output_path = OUTPUT_DIR / "financial_statements.xlsx"
md_output_path = OUTPUT_DIR / "financial_statements.md"  # Added Markdown path

try:
    logger.info(f"Exporting statements to Excel: {excel_output_path}")

    # Create a dummy Balance Sheet config for multi-sheet export demonstration
    # Use core. prefixes for node_ids and add required totals/metrics
    balance_sheet_yaml = """
id: balance_sheet
name: Balance Sheet (Complete)
description: Snapshot of assets, liabilities, equity.
sections:
  - id: current_assets
    name: Current Assets
    items:
      - id: cash
        name: Cash & Equivalents
        type: line_item
        node_id: core.cash # Use core prefix
        sign_convention: 1
      - id: ar
        name: Accounts Receivable
        type: line_item
        node_id: core.accounts_receivable # Use core prefix
        sign_convention: 1
    subtotal:
      id: current_assets_subtotal
      name: Total Current Assets
      type: subtotal
      items_to_sum: [cash, ar]
      sign_convention: 1

  - id: non_current_assets
    name: Non-Current Assets
    items:
      - id: ppe
        name: Property, Plant & Equipment
        type: line_item
        node_id: core.ppe # Use core prefix
        sign_convention: 1
    # Add subtotal if there were more items

  - id: total_assets_section # Section for the final total
    name: Total Assets
    items:
      - id: total_assets
        name: Total Assets
        type: calculated
        calculation:
          type: addition
          inputs: [current_assets_subtotal, ppe]
        sign_convention: 1

  - id: current_liabilities
    name: Current Liabilities
    items:
      - id: ap
        name: Accounts Payable
        type: line_item
        node_id: core.accounts_payable # Use core prefix
        sign_convention: 1 # Liabilities shown positive
    # Add subtotal if there were more items

  - id: non_current_liabilities
    name: Non-Current Liabilities
    items:
      - id: debt
        name: Long-Term Debt
        type: line_item
        node_id: core.debt # Use core prefix
        sign_convention: 1 # Liabilities shown positive

  - id: total_liabilities_section # Section for total liabilities
    name: Total Liabilities
    items:
      - id: total_liabilities
        name: Total Liabilities
        type: calculated
        calculation:
          type: addition
          inputs: [ap, debt]
        sign_convention: 1

  - id: equity
    name: Equity
    items:
      - id: common_stock
        name: Common Stock
        type: line_item
        node_id: core.common_stock # Use core prefix
        sign_convention: 1
      - id: net_income
        name: Net Income
        type: metric
        metric_id: net_income
        inputs:
          operating_income: gross_profit
          interest_expense: "Interest Expense"
          income_tax: "Taxes"
        sign_convention: 1
      - id: retained_earnings # Metric item
        name: Retained Earnings
        type: metric
        metric_id: retained_earnings # Use the built-in metric
        inputs:
          # Map metric inputs to statement item IDs or node IDs
          # Need core.prior_retained_earnings, core.net_income, core.dividends
          # Assume net_income from income statement can be used
          prior_retained_earnings: core.prior_retained_earnings
          net_income: net_income # Reference IS net income (if available in graph)
          dividends: core.dividends
        sign_convention: 1
    subtotal:
      id: total_equity
      name: Total Equity
      type: subtotal
      items_to_sum: [common_stock, retained_earnings]
      sign_convention: 1

  - id: total_liabilities_equity_section # Section for balancing check
    name: Total Liabilities & Equity
    items:
      - id: total_liabs_equity
        name: Total Liabilities & Equity
        type: calculated
        calculation:
          type: addition
          inputs: [total_liabilities, total_equity]
        sign_convention: 1
"""
    bs_stmt_path = CONFIG_DIR / "balance_sheet.yaml"
    with open(bs_stmt_path, "w") as f:
        f.write(balance_sheet_yaml)

    # Add all required BS item data to the graph, using core prefixes
    # and including all historical + forecast periods
    all_periods_list = list(graph.periods)

    # Define some simple forecast logic for the new BS items
    bs_forecasts = {
        "core.cash": 0.10,  # 10% growth
        "core.accounts_receivable": 0.12,  # 12% growth
        "core.ppe": 0.05,  # 5% growth
        "core.accounts_payable": 0.08,  # 8% growth
        "core.debt": 0.02,  # 2% growth (increase)
        "core.common_stock": 0.0,  # No change
        "core.prior_retained_earnings": 0.0,  # This will be calculated by metric usually
        "core.dividends": 0.05,  # Increase dividends by 5%
    }

    # Function to generate data across all periods with simple growth
    def generate_data(
        node_id: str, start_val: float, growth_rate: float
    ) -> dict[str, float]:
        """Generate data for a node across periods with simple growth."""
        data: dict[str, float] = {}
        current_val = start_val
        for i, period in enumerate(all_periods_list):
            if i > 1:  # Apply growth after first two historical periods
                current_val *= 1 + growth_rate
            data[period] = current_val
        return data

    # Add data ensuring all periods exist
    graph.add_financial_statement_item(
        "core.cash", generate_data("core.cash", 50.0, bs_forecasts["core.cash"])
    )
    graph.add_financial_statement_item(
        "core.accounts_receivable",
        generate_data(
            "core.accounts_receivable", 100.0, bs_forecasts["core.accounts_receivable"]
        ),
    )
    graph.add_financial_statement_item(
        "core.ppe", generate_data("core.ppe", 300.0, bs_forecasts["core.ppe"])
    )
    graph.add_financial_statement_item(
        "core.accounts_payable",
        generate_data(
            "core.accounts_payable", 80.0, bs_forecasts["core.accounts_payable"]
        ),
    )
    graph.add_financial_statement_item(
        "core.debt", generate_data("core.debt", 150.0, bs_forecasts["core.debt"])
    )
    graph.add_financial_statement_item(
        "core.common_stock",
        generate_data("core.common_stock", 100.0, bs_forecasts["core.common_stock"]),
    )
    # Need prior RE and dividends for the Retained Earnings metric
    # Let's assume prior RE grows with net income (simple approximation for example)
    # We need net income data in the graph first. Net income IS calculated from IS items.
    # Let's use the previously calculated net income node values
    # TODO: This is tricky - Retained Earnings depends on Net Income, which is calculated.
    # The metric expects 'net_income' as an input node ID. We have 'net_income' as a calculated item ID.
    # We might need to ensure 'net_income' node exists or map differently. For now, assume graph.calculate works.
    # Add dummy prior RE - usually this links period to period
    prior_re = {"2022": 100.0, "2023": 100.0 + 945}  # Start + Previous NI
    prior_re["2024"] = prior_re["2023"] + 1145
    prior_re["2025"] = prior_re["2024"] + 1354  # Use NI values from previous run
    prior_re["2026"] = prior_re["2025"] + 1628  # Use NI values from previous run
    graph.add_financial_statement_item("core.prior_retained_earnings", prior_re)

    graph.add_financial_statement_item(
        "core.dividends",
        generate_data(
            "core.dividends", -10.0, bs_forecasts["core.dividends"]
        ),  # Negative value
    )

    # ------------------------------------------------------------------
    # Additional nodes required by 'test_statement.yaml' configuration
    # ------------------------------------------------------------------

    # Helper to clone data from an existing node if present, otherwise generate
    def _clone_or_generate(
        source_id: str,
        target_id: str,
        fallback_start: float,
        growth_rate: float,
    ) -> None:
        """Clone data from `source_id` node to a new `target_id` or generate if missing.

        Args:
            source_id: Existing node ID to clone from.
            target_id: New node ID to create/overwrite.
            fallback_start: Starting value if source node is not available.
            growth_rate: Growth rate to apply when generating fallback series.
        """

        if graph.has_node(source_id):
            src_node = graph.get_node(source_id)
            data_map: dict[str, float] = {
                period: src_node.get_value(period) for period in all_periods_list
            }
        else:
            data_map = generate_data(target_id, fallback_start, growth_rate)

        graph.add_financial_statement_item(target_id, data_map)

    # Map of (source_id, target_id, fallback_start, growth_rate)
    mapping_specs = [
        ("Revenue", "revenue", 1000.0, 0.10),
        ("COGS", "cost_of_goods_sold", -400.0, 0.08),
        ("R&D", "operating_expenses", -300.0, 0.05),
        ("Interest Expense", "interest_expense", -50.0, 0.05),
        ("Taxes", "income_tax", -75.0, 0.05),
    ]

    for src, tgt, start_val, gr in mapping_specs:
        _clone_or_generate(src, tgt, start_val, gr)

    # Balance-sheet-specific nodes (if not already present without the 'core.' prefix)
    bs_specs = {
        "cash_and_equivalents": (50.0, 0.10),
        "accounts_receivable": (100.0, 0.12),
        "property_plant_equipment": (300.0, 0.05),
        "accounts_payable": (80.0, 0.08),
        "total_debt": (150.0, 0.02),
        "common_stock": (100.0, 0.0),
        "prior_retained_earnings": (100.0, 0.0),
        "dividends": (-10.0, 0.05),
    }

    for node_id, (start_val, gr) in bs_specs.items():
        if not graph.has_node(node_id):
            graph.add_financial_statement_item(
                node_id,
                generate_data(node_id, start_val, gr),
            )

    # Use the high-level export function
    # This will internally call create_statement_dataframe for all configs in CONFIG_DIR
    # and write each resulting DataFrame to a separate sheet in the Excel file.
    export_statements_to_excel(
        graph=graph,
        config_path_or_dir=str(CONFIG_DIR),  # Process all YAML files in the dir
        output_dir=str(excel_output_path),  # Specify the single output Excel file path
        format_kwargs={
            "number_format": ",.0f"
        },  # Formatting for the DataFrames before export
        # writer_kwargs={'freeze_panes': (1, 1)} # Example: Pass args to pandas.to_excel
    )
    logger.info(f"Successfully exported statements to {excel_output_path}")

    # --- Add Markdown Export ---
    logger.info(f"Exporting statements to Markdown: {md_output_path}")
    # Prepare registry and builder for balance sheet
    bs_registry = StatementRegistry()
    builder = StatementStructureBuilder()
    # Load, validate, build, and register the balance_sheet config
    load_build_register_statements(str(bs_stmt_path), bs_registry, builder)
    statement_structure = bs_registry.get("balance_sheet")
    # Populate graph with balance sheet calculation and metric nodes
    populate_graph(bs_registry, graph)
    # Use direct MarkdownWriter with config to avoid facade schema limitations
    from fin_statement_model.io.config.models import MarkdownWriterConfig
    from fin_statement_model.io.formats.markdown.writer import MarkdownWriter

    # Build MarkdownWriterConfig (statement_config_path required by schema)
    md_cfg = MarkdownWriterConfig(
        format_type="markdown",
        target=str(md_output_path),
        statement_config_path=str(bs_stmt_path),
        historical_periods=["2022", "2023"],
        forecast_periods=forecast_periods,
        forecast_configs=forecast_configs,
    )
    # Instantiate MarkdownWriter with Pydantic config
    md_writer = MarkdownWriter(config=md_cfg)
    # Render markdown content using modern StatementStructure approach
    md_content = md_writer.write(
        graph,
        statement_structure=statement_structure,
    )
    # Write to file
    try:
        with open(md_output_path, "w", encoding="utf-8") as f:
            f.write(md_content)
        logger.info(f"Successfully wrote Markdown to {md_output_path}")
    except Exception:
        logger.exception("Failed to write Markdown to file")

except (WriteError, FinancialModelError):
    logger.exception("Failed to export data")

# --- 8. Cleanup ---
finally:
    logger.info("Cleaning up temporary directory...")
    # temp_dir.cleanup()
    logger.info("Example finished.")



================================================================================
File: examples/scripts/metric_interpretation_examples.py
================================================================================

"""Examples of metric interpretation functionality.

This module demonstrates how to use the enhanced metric definitions
with interpretation guidelines.
"""

from fin_statement_model.core.metrics import (
    metric_registry,
    MetricInterpreter,
    interpret_metric,
    MetricRating,
)


def example_basic_interpretation():
    """Example: Basic metric interpretation."""
    print("=== Basic Metric Interpretation Example ===")

    # Get a metric definition (use metric ID, not name)
    current_ratio_def = metric_registry.get("current_ratio")
    if not current_ratio_def:
        print("Current Ratio metric not found!")
        return

    # Test different values
    test_values = [0.5, 0.9, 1.2, 2.0, 2.8, 5.0]

    print(f"\nMetric: {current_ratio_def.name}")
    print(f"Description: {current_ratio_def.description}")
    print(f"Category: {current_ratio_def.category}")
    print("\nValue Interpretations:")
    print("-" * 60)

    interpreter = MetricInterpreter(current_ratio_def)

    for value in test_values:
        rating = interpreter.rate_value(value)
        message = interpreter.get_interpretation_message(value)
        print(f"{value:4.1f} | {rating.value:10} | {message}")


def example_detailed_analysis():
    """Example: Detailed metric analysis."""
    print("\n=== Detailed Analysis Example ===")

    # Get debt-to-equity ratio metric
    debt_equity_def = metric_registry.get("debt_to_equity_ratio")
    if not debt_equity_def:
        print("Debt-to-Equity Ratio metric not found!")
        return

    # Analyze a specific value
    test_value = 0.8

    analysis = interpret_metric(debt_equity_def, test_value)

    print(f"\nDetailed Analysis for {debt_equity_def.name}:")
    print(f"Value: {analysis['value']}")
    print(f"Rating: {analysis['rating']}")
    print(f"Category: {analysis['category']}")
    print(f"Units: {analysis['units']}")
    print(f"\nInterpretation: {analysis['interpretation_message']}")

    if "guidelines" in analysis:
        guidelines = analysis["guidelines"]
        print("\nGuidelines:")
        if guidelines["good_range"]:
            print(f"  Good range: {guidelines['good_range']}")
        if guidelines["warning_above"]:
            print(f"  Warning above: {guidelines['warning_above']}")
        if guidelines["excellent_above"]:
            print(f"  Excellent above: {guidelines['excellent_above']}")

    if "related_metrics" in analysis:
        print(f"\nRelated Metrics: {', '.join(analysis['related_metrics'])}")

    if "notes" in analysis:
        print(f"\nNotes:\n{analysis['notes']}")


def example_multiple_metrics_analysis():
    """Example: Analyzing multiple metrics together."""
    print("\n=== Multiple Metrics Analysis Example ===")

    # Company financial data
    company_data = {
        "current_assets": 500_000,
        "current_liabilities": 300_000,
        "total_debt": 800_000,
        "total_equity": 1_200_000,
        "net_income": 180_000,
        "ebit": 250_000,
        "interest_expense": 40_000,
    }

    # Calculate and interpret multiple metrics (use metric IDs)
    metrics_to_analyze = [
        (
            "current_ratio",
            company_data["current_assets"] / company_data["current_liabilities"],
        ),
        (
            "debt_to_equity_ratio",
            company_data["total_debt"] / company_data["total_equity"],
        ),
        (
            "return_on_equity",
            (company_data["net_income"] / company_data["total_equity"]) * 100,
        ),
        (
            "times_interest_earned",
            company_data["ebit"] / company_data["interest_expense"],
        ),
    ]

    print("\nFinancial Analysis:")
    print("=" * 60)

    for metric_id, calculated_value in metrics_to_analyze:
        try:
            metric_def = metric_registry.get(metric_id)
        except KeyError:
            print(f"Metric '{metric_id}' not found!")
            continue

        interpreter = MetricInterpreter(metric_def)
        rating = interpreter.rate_value(calculated_value)
        message = interpreter.get_interpretation_message(calculated_value)

        print(f"\n{metric_def.name}:")
        print(f"  Value: {calculated_value:.2f} {metric_def.units or ''}")
        print(f"  Rating: {rating.value.upper()}")
        print(f"  {message}")


def example_rating_summary():
    """Example: Summary of ratings across metrics."""
    print("\n=== Rating Summary Example ===")

    # Sample metric values (use metric IDs)
    metric_values = {
        "current_ratio": 2.1,
        "debt_to_equity_ratio": 0.45,
        "return_on_equity": 16.5,
        "times_interest_earned": 7.2,
    }

    ratings_summary = {}

    print("\nCredit Analysis Summary:")
    print("-" * 40)

    for metric_id, value in metric_values.items():
        try:
            metric_def = metric_registry.get(metric_id)
        except KeyError:
            print(f"Metric '{metric_id}' not found!")
            continue

        interpreter = MetricInterpreter(metric_def)
        rating = interpreter.rate_value(value)
        category = metric_def.category or "other"
        if category not in ratings_summary:
            ratings_summary[category] = []
        ratings_summary[category].append((metric_def.name, rating, value))

    # Group by category
    for category, metrics in ratings_summary.items():
        print(f"\n{category.upper()} METRICS:")
        for metric_name, rating, value in metrics:
            status_icon = {
                MetricRating.EXCELLENT: "🟢",
                MetricRating.GOOD: "🟢",
                MetricRating.ADEQUATE: "🟡",
                MetricRating.WARNING: "🟠",
                MetricRating.POOR: "🔴",
                MetricRating.UNKNOWN: "⚪",
            }
            print(f"  {status_icon[rating]} {metric_name}: {rating.value} ({value})")

    # Overall assessment
    all_ratings = [
        rating for _, metrics in ratings_summary.items() for _, rating, _ in metrics
    ]
    excellent_count = sum(1 for r in all_ratings if r == MetricRating.EXCELLENT)
    good_count = sum(1 for r in all_ratings if r == MetricRating.GOOD)
    warning_count = sum(
        1 for r in all_ratings if r in [MetricRating.WARNING, MetricRating.POOR]
    )

    print("\nOVERALL ASSESSMENT:")
    if warning_count == 0 and (excellent_count + good_count) >= len(all_ratings) * 0.8:
        print("🟢 STRONG - Good financial health across key metrics")
    elif warning_count <= 1:
        print("🟡 MODERATE - Generally healthy with some areas for improvement")
    else:
        print("🔴 WEAK - Multiple areas of concern requiring attention")


def example_threshold_analysis():
    """Example: Understanding metric thresholds and ranges."""
    print("\n=== Threshold Analysis Example ===")

    # Get current ratio metric
    current_ratio_def = metric_registry.get("current_ratio")
    if not current_ratio_def:
        print("Current Ratio metric not found!")
        return

    interpreter = MetricInterpreter(current_ratio_def)

    print(f"\nThreshold Analysis for {current_ratio_def.name}:")
    print("-" * 50)

    # Test values around thresholds
    test_values = [0.7, 0.8, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0]

    for value in test_values:
        rating = interpreter.rate_value(value)
        analysis = interpreter.get_detailed_analysis(value)

        print(f"\nValue: {value:.1f}")
        print(f"  Rating: {rating.value}")
        print(f"  Message: {analysis['interpretation_message']}")

        # Show which threshold was triggered
        if rating == MetricRating.POOR:
            print(f"  → Below poor threshold ({analysis['guidelines']['poor_below']})")
        elif rating == MetricRating.WARNING:
            if value < analysis["guidelines"]["warning_below"]:
                print(
                    f"  → Below warning threshold ({analysis['guidelines']['warning_below']})"
                )
            elif value > analysis["guidelines"]["warning_above"]:
                print(
                    f"  → Above warning threshold ({analysis['guidelines']['warning_above']})"
                )
        elif rating == MetricRating.EXCELLENT:
            print(
                f"  → Above excellent threshold ({analysis['guidelines']['excellent_above']})"
            )
        elif rating == MetricRating.GOOD:
            good_range = analysis["guidelines"]["good_range"]
            print(f"  → Within good range ({good_range[0]} - {good_range[1]})")


if __name__ == "__main__":
    # Run all examples
    example_basic_interpretation()
    example_detailed_analysis()
    example_multiple_metrics_analysis()
    example_rating_summary()
    example_threshold_analysis()



================================================================================
File: examples/scripts/node_validation_example.py
================================================================================

"""Example demonstrating UnifiedNodeValidator integration with statement processing.

This example shows how to use the enhanced statement configuration and building
capabilities that include node ID validation using the UnifiedNodeValidator.
"""

from fin_statement_model.core.graph import Graph
from fin_statement_model.statements import (
    StatementConfig,
    create_validated_statement_config,
    create_validated_statement_builder,
    validate_statement_config_with_nodes,
)
from fin_statement_model.io.validation import UnifiedNodeValidator


def example_basic_node_validation():
    """Demonstrate basic node validation during statement configuration."""
    print("=== Basic Node Validation Example ===")

    # Example configuration with mixed node ID quality
    config_data = {
        "id": "sample_income_statement",
        "name": "Sample Income Statement",
        "description": "Demonstrates node validation",
        "sections": [
            {
                "id": "revenue_section",
                "name": "Revenue",
                "type": "section",
                "items": [
                    {
                        "type": "line_item",
                        "id": "revenue",  # Standard node name - should pass
                        "name": "Total Revenue",
                        "node_id": "revenue",
                    },
                    {
                        "type": "line_item",
                        "id": "sales",  # Alternate name for revenue - should be standardized
                        "name": "Sales Revenue",
                        "node_id": "sales",
                    },
                    {
                        "type": "line_item",
                        "id": "custom_revenue_stream",  # Custom name - should generate warning
                        "name": "Custom Revenue Stream",
                        "node_id": "custom_rev_123",
                    },
                ],
            },
            {
                "id": "expenses_section",
                "name": "Expenses",
                "type": "section",
                "items": [
                    {
                        "type": "line_item",
                        "id": "cogs",  # Standard name - should pass
                        "name": "Cost of Goods Sold",
                        "node_id": "cogs",
                    },
                    {
                        "type": "calculated",
                        "id": "gross_profit",  # Standard name - should pass
                        "name": "Gross Profit",
                        "calculation": {
                            "type": "subtraction",
                            "inputs": ["revenue", "cogs"],  # Both standard names
                        },
                    },
                ],
            },
        ],
    }

    print("\n--- Testing Non-Strict Mode (Warnings Only) ---")
    config = StatementConfig(
        config_data,
        enable_node_validation=True,
        node_validation_strict=False,  # Warnings only
    )

    errors = config.validate_config()
    print(f"Validation errors: {len(errors)}")
    for error in errors:
        print(f"  - {error}")

    if config.model:
        print("✅ Configuration successfully validated and parsed")
        print(f"   Statement ID: {config.model.id}")
        print(f"   Sections: {len(config.model.sections)}")

    print("\n--- Testing Strict Mode ---")
    strict_config = StatementConfig(
        config_data,
        enable_node_validation=True,
        node_validation_strict=True,  # Errors for non-standard names
    )

    strict_errors = strict_config.validate_config()
    print(f"Strict validation errors: {len(strict_errors)}")
    for error in strict_errors:
        print(f"  - {error}")

    if strict_config.model:
        print("✅ Strict validation passed")
    else:
        print("❌ Strict validation failed")


def example_custom_validator_configuration():
    """Demonstrate using a custom UnifiedNodeValidator configuration."""
    print("\n=== Custom Validator Configuration Example ===")

    # Create a custom validator with specific settings
    # Demo only: direct use of UnifiedNodeValidator; production code should use StatementConfig/StatementStructureBuilder for validation
    custom_validator = UnifiedNodeValidator(
        strict_mode=False,
        auto_standardize=True,  # Convert alternate names to standard
        warn_on_non_standard=True,
        enable_patterns=True,  # Enable pattern recognition for subnodes
    )

    config_data = {
        "id": "quarterly_statement",
        "name": "Quarterly Statement",
        "sections": [
            {
                "id": "quarterly_revenue",
                "name": "Quarterly Revenue",
                "type": "section",
                "items": [
                    {
                        "type": "line_item",
                        "id": "revenue_q1",  # Pattern: revenue + quarterly suffix
                        "name": "Q1 Revenue",
                        "node_id": "revenue_q1",
                    },
                    {
                        "type": "line_item",
                        "id": "sales_q1",  # Alternate name + quarterly pattern
                        "name": "Q1 Sales",
                        "node_id": "sales_q1",
                    },
                    {
                        "type": "calculated",
                        "id": "revenue_growth",  # Pattern: base + growth suffix
                        "name": "Revenue Growth",
                        "calculation": {
                            "type": "percentage_change",
                            "inputs": ["revenue_q1", "revenue_q0"],
                        },
                    },
                ],
            },
        ],
    }

    config = StatementConfig(
        config_data,
        enable_node_validation=True,
        node_validation_strict=False,
        node_validator=custom_validator,
    )

    errors = config.validate_config()
    print(f"Custom validation errors: {len(errors)}")
    for error in errors:
        print(f"  - {error}")

    if config.model:
        print("✅ Custom validation completed successfully")


def example_convenience_functions():
    """Demonstrate the convenience functions for easier usage."""
    print("\n=== Convenience Functions Example ===")

    config_data = {
        "id": "balance_sheet",
        "name": "Balance Sheet",
        "sections": [
            {
                "id": "assets",
                "name": "Assets",
                "type": "section",
                "items": [
                    {
                        "type": "line_item",
                        "id": "cash",  # Standard name
                        "name": "Cash and Cash Equivalents",
                        "standard_node_ref": "cash",  # Using standard node reference
                    },
                    {
                        "type": "line_item",
                        "id": "receivables",  # Standard name
                        "name": "Accounts Receivable",
                        "node_id": "accounts_receivable",
                    },
                ],
                "subtotal": {
                    "type": "subtotal",
                    "id": "total_current_assets",
                    "name": "Total Current Assets",
                    "items_to_sum": ["cash", "receivables"],
                },
            },
        ],
    }

    print("\n--- Using create_validated_statement_config ---")
    config = create_validated_statement_config(
        config_data,
        enable_node_validation=True,
        strict_mode=False,  # Warnings only
    )

    errors = config.validate_config()
    print(f"Validation errors: {len(errors)}")

    print("\n--- Using create_validated_statement_builder ---")
    builder = create_validated_statement_builder(
        enable_node_validation=True,
        strict_mode=False,
    )

    if config.model:
        statement = builder.build(config)
        print(f"✅ Statement built successfully: {statement.name}")
        print(f"   Sections: {len(statement.sections)}")

        # Show the sections and items
        for section in statement.sections:
            print(f"   Section: {section.name}")
            for item in section.items:
                print(f"     - {item.name} (ID: {item.id})")

    print("\n--- Using validate_statement_config_with_nodes ---")
    validated_config, validation_errors = validate_statement_config_with_nodes(
        config_data,
        strict_mode=False,
        auto_standardize=True,
    )

    print(f"High-level validation errors: {len(validation_errors)}")
    if not validation_errors:
        print("✅ High-level validation passed")


def example_invalid_configuration():
    """Demonstrate validation with invalid node IDs."""
    print("\n=== Invalid Configuration Example ===")

    # Configuration with various invalid node IDs
    invalid_config_data = {
        "id": "bad@statement!",  # Invalid characters
        "name": "Invalid Statement",
        "sections": [
            {
                "id": "section with spaces",  # Spaces not allowed
                "name": "Invalid Section",
                "type": "section",
                "items": [
                    {
                        "type": "line_item",
                        "id": "node#with$symbols",  # Invalid characters
                        "name": "Invalid Node",
                        "node_id": "another_bad_node!!!",  # Invalid characters
                    },
                    {
                        "type": "calculated",
                        "id": "123_starts_with_number",  # Bad practice
                        "name": "Numeric Start",
                        "calculation": {
                            "type": "addition",
                            "inputs": ["input@1", "input@2"],  # Invalid characters
                        },
                    },
                ],
            },
        ],
    }

    print("\n--- Testing Invalid Config in Strict Mode ---")
    try:
        config = StatementConfig(
            invalid_config_data,
            enable_node_validation=True,
            node_validation_strict=True,
        )

        errors = config.validate_config()
        print(f"Validation errors found: {len(errors)}")
        for i, error in enumerate(errors, 1):
            print(f"  {i}. {error}")

        if errors:
            print("❌ Validation failed as expected due to invalid node IDs")

    except Exception as e:
        print(f"Exception during validation: {e}")

    print("\n--- Testing Invalid Config in Non-Strict Mode ---")
    config = StatementConfig(
        invalid_config_data,
        enable_node_validation=True,
        node_validation_strict=False,  # Warnings only
    )

    errors = config.validate_config()
    print(f"Validation completed with {len(errors)} errors (warnings logged)")

    if config.model:
        print("✅ Non-strict validation allowed processing to continue")
    else:
        print("❌ Even non-strict validation failed")


def example_integration_with_graph():
    """Demonstrate integration with actual graph operations."""
    print("\n=== Graph Integration Example ===")

    # Create a sample graph
    graph = Graph()
    graph.add_financial_statement_item("revenue", {"2023": 100000, "2024": 120000})
    graph.add_financial_statement_item("cogs", {"2023": 60000, "2024": 70000})
    graph.add_financial_statement_item(
        "operating_expenses", {"2023": 25000, "2024": 30000}
    )

    config_data = {
        "id": "income_statement",
        "name": "Income Statement",
        "sections": [
            {
                "id": "revenue_section",
                "name": "Revenue",
                "type": "section",
                "items": [
                    {
                        "type": "line_item",
                        "id": "revenue",
                        "name": "Total Revenue",
                        "node_id": "revenue",
                    },
                ],
            },
            {
                "id": "expenses_section",
                "name": "Expenses",
                "type": "section",
                "items": [
                    {
                        "type": "line_item",
                        "id": "cogs",
                        "name": "Cost of Goods Sold",
                        "node_id": "cogs",
                    },
                    {
                        "type": "line_item",
                        "id": "operating_expenses",
                        "name": "Operating Expenses",
                        "node_id": "operating_expenses",
                    },
                    {
                        "type": "calculated",
                        "id": "gross_profit",
                        "name": "Gross Profit",
                        "calculation": {
                            "type": "subtraction",
                            "inputs": ["revenue", "cogs"],
                        },
                    },
                ],
            },
        ],
    }

    print(f"Graph has {len(graph.nodes)} nodes: {list(graph.nodes.keys())}")
    print(f"Periods: {graph.periods}")

    # Build statement with validation
    config = create_validated_statement_config(
        config_data,
        enable_node_validation=True,
        strict_mode=False,
    )

    errors = config.validate_config()
    if errors:
        print(f"Validation issues: {len(errors)}")
        for error in errors:
            print(f"  - {error}")

    builder = create_validated_statement_builder(
        enable_node_validation=True,
        strict_mode=False,
    )

    if config.model:
        statement = builder.build(config)
        print(f"✅ Statement built: {statement.name}")

        # Note: To complete this example, you would typically:
        # 1. Populate the graph with calculation nodes from the statement
        # 2. Use StatementFormatter to create a DataFrame
        # 3. Display the formatted results
        print("   Statement would be ready for graph population and formatting")


def main():
    """Run all node validation examples."""
    print("Node Validation Integration Examples")
    print("=" * 50)

    try:
        example_basic_node_validation()
        example_custom_validator_configuration()
        example_convenience_functions()
        example_invalid_configuration()
        example_integration_with_graph()

        print("\n" + "=" * 50)
        print("✅ All examples completed successfully!")
        print("\nKey Benefits of Node Validation Integration:")
        print("  • Early detection of naming convention violations")
        print("  • Automatic standardization of alternate node names")
        print("  • Improved graph hygiene and consistency")
        print("  • Better error messages with context")
        print("  • Flexible validation modes (strict vs. warnings)")

    except Exception as e:
        print(f"\n❌ Error running examples: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()



================================================================================
File: examples/scripts/provider_examples/fmp_example.py
================================================================================

"""Financial Modeling Prep API Example.

This example demonstrates fetching financial data from the FMP API
and using it with the fin_statement_model library.
"""

import logging
import sys

from fin_statement_model.config import get_config, update_config
from fin_statement_model.io import read_data
from fin_statement_model.statements import create_statement_dataframe

# Get configuration
config = get_config()

# Override logging level for this example if needed
update_config({"logging": {"level": "INFO"}})


logger = logging.getLogger(__name__)

# Configuration
TICKER = "AAPL"  # Apple Inc.
STATEMENT_TYPE = "income_statement"  # income_statement, balance_sheet, or cash_flow
PERIOD_TYPE = "FY"  # FY (annual) or QTR (quarterly)
LIMIT = 5  # Number of periods to fetch

# API configuration from centralized config
# The API key can be set via:
# 1. Environment variable: FSM_API_FMP_API_KEY
# 2. Config file: api.fmp_api_key
# 3. Runtime: update_config({'api': {'fmp_api_key': 'your_key'}})
if not config.api.fmp_api_key:
    logger.error("FMP API key not configured!")
    logger.info("Set it using one of these methods:")
    logger.info("1. Environment variable: export FSM_API_FMP_API_KEY=your_key")
    logger.info("2. Config file: add 'fmp_api_key: your_key' under 'api:' section")
    logger.info("3. Runtime: update_config({'api': {'fmp_api_key': 'your_key'}})")
    sys.exit(1)

# FMP reader configuration using centralized settings
fmp_config = {
    "source": TICKER,
    "format_type": "fmp",
    "statement_type": STATEMENT_TYPE,
    "period_type": PERIOD_TYPE,
    "limit": LIMIT,
    "api_key": config.api.fmp_api_key,  # Use API key from config
}


def fetch_fmp_data() -> object:
    """Fetch financial data from FMP API."""
    logger.info(f"Fetching {STATEMENT_TYPE} data for {TICKER}...")

    # Use configured timeout and retry settings
    if hasattr(read_data, "_reader"):
        # Apply API configuration settings if reader supports them
        read_data._reader.timeout = config.api.api_timeout
        read_data._reader.retry_count = config.api.api_retry_count

    try:
        # Pass validated configuration directly to the IO facade
        graph = read_data(**fmp_config)

        logger.info(f"✓ Successfully fetched data for periods: {graph.periods}")
        logger.info(f"✓ Created {len(graph.nodes)} nodes")

        # Display sample data
        sample_node = next(iter(graph.nodes.values()))
        logger.info(f"\nSample node '{sample_node.name}':")
        for period in sorted(graph.periods)[:3]:  # Show first 3 periods
            value = sample_node.get_value(period)
            if value is not None:
                # Format using display config
                formatted_value = f"{value * config.display.scale_factor:{config.display.default_currency_format}}"
                logger.info(
                    f"  {period}: {formatted_value} {config.display.default_units}"
                )

        return graph

    except Exception as e:
        logger.exception(f"Error fetching data: {e}")
        raise


def main():
    """Run the FMP example."""
    logger.info("=" * 60)
    logger.info(f"FMP API EXAMPLE - {TICKER}")
    logger.info("=" * 60)

    # Fetch data from FMP
    graph = fetch_fmp_data()

    # Try to create a formatted statement if config exists
    try:
        # Use built-in statement configuration
        from fin_statement_model.io.specialized.statements import (
            read_builtin_statement_config,
        )

        raw_config = read_builtin_statement_config(STATEMENT_TYPE)
        raw_configs = {STATEMENT_TYPE: raw_config}
        logger.info(f"\nCreating formatted statement for {STATEMENT_TYPE}...")
        df_map = create_statement_dataframe(
            graph=graph,
            raw_configs=raw_configs,
            format_kwargs={
                "number_format": config.display.default_currency_format,
                "should_apply_signs": True,
                "hide_zero_rows": config.display.hide_zero_rows,
            },
        )
        statement_df = df_map.get(STATEMENT_TYPE)

        logger.info("\nFormatted Statement:")
        logger.info(statement_df.to_string(index=False))

    except Exception as e:
        logger.warning(f"Could not create formatted statement: {e}")
        logger.info("Note: To use statement formatting, create a statement config file")

    logger.info("\n" + "=" * 60)
    logger.info("EXAMPLE COMPLETE")
    logger.info("=" * 60)

    # Show cache info if caching is enabled
    if config.api.cache_api_responses:
        logger.info(
            f"\nNote: API responses are cached for {config.api.cache_ttl_hours} hours"
        )
        logger.info("To refresh data, clear cache or wait for TTL expiration")


if __name__ == "__main__":
    main()



================================================================================
File: examples/scripts/sector_examples/banking_analysis_example.py
================================================================================

"""Banking Analysis Example with Node Validation.

This example demonstrates comprehensive banking sector analysis with node name validation.
It shows how to create a validated financial model, calculate banking-specific metrics,
and perform regulatory compliance checks.
"""

import logging

from fin_statement_model.core.metrics import metric_registry
from fin_statement_model.io.validation import UnifiedNodeValidator
from fin_statement_model.core.metrics import (
    interpret_metric,
    calculate_metric,
)
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.core.nodes.base import Node
from fin_statement_model.core.nodes.calculation_nodes import (
    FormulaCalculationNode,
    CustomCalculationNode,
)
from fin_statement_model.config import get_config, update_config

logger = logging.getLogger(__name__)

config = get_config()
update_config({"logging": {"level": "INFO", "format": "%(message)s"}})


def validate_node_names_example() -> dict[str, str]:
    """Demonstrate node name validation and standardization.

    Returns:
        Dictionary mapping original names to standardized names.
    """
    logger.info("\n=== Node Name Validation Example ===")

    # Example of raw data with various node name formats
    raw_node_names = [
        "total_loans",  # Standard name
        "loan_loss_allowance",  # Alternate name for allowance_for_loan_losses
        "npl",  # Alternate name for non_performing_loans
        "deposits",  # Alternate name for total_deposits
        "shareholders_equity",  # Alternate name for total_shareholders_equity
        "nii",  # Alternate name for net_interest_income
        "tier_1_capital",  # Alternate name for total_tier_1_capital
        "rwa",  # Alternate name for total_risk_weighted_assets
        "custom_metric_xyz",  # Non-standard name
        "revenue_q1",  # Sub-node pattern
        "loan_loss_provision",  # Another alternate for provision_for_credit_losses
    ]

    logger.info(f"Raw node names to validate: {raw_node_names}")

    # Demo only: direct use of UnifiedNodeValidator; production code should use StatementConfig/StatementStructureBuilder for validation
    # Create unified validator for demo
    validator = UnifiedNodeValidator(
        strict_mode=False,  # Allow alternate names
        auto_standardize=True,  # Convert to standard names
        warn_on_non_standard=True,  # Log warnings for non-standard names
        enable_patterns=True,  # Enable pattern recognition for sub-nodes and formulas
    )

    # Validate and standardize names
    validation_results = validator.validate_batch(raw_node_names)

    logger.info("\n--- Basic Validation Results ---")
    standardized_mapping = {}
    for original_name, result in validation_results.items():
        logger.info(
            f"'{original_name}' -> '{result.standardized_name}' (Valid: {result.is_valid})"
        )
        logger.info(f"  Category: {result.category}")
        logger.info(f"  Message: {result.message}")
        if result.suggestions:
            logger.info(f"  Suggestions: {result.suggestions}")
        standardized_mapping[original_name] = result.standardized_name

    # Get validation summary
    logger.info("\n--- Validation Summary ---")
    logger.info(f"Total validated: {len(validation_results)}")
    valid_count = sum(1 for r in validation_results.values() if r.is_valid)
    logger.info(f"Valid: {valid_count}")
    logger.info(f"Invalid: {len(validation_results) - valid_count}")

    # Count by category
    categories = {}
    for result in validation_results.values():
        categories[result.category] = categories.get(result.category, 0) + 1

    for category, count in categories.items():
        logger.info(f"{category}: {count}")

    return standardized_mapping


def context_aware_validation_example():
    """Demonstrate context-aware node validation."""
    logger.info("\n=== Context-Aware Validation Example ===")

    # Create unified validator with pattern recognition
    validator = UnifiedNodeValidator(
        strict_mode=False,
        auto_standardize=True,
        enable_patterns=True,  # Enable pattern recognition
    )

    # Example node names with different patterns
    test_nodes = [
        ("total_loans", "data", None),
        ("revenue_q1", "data", None),  # Quarterly sub-node
        ("loan_loss_provision", "data", None),  # Alternate name
        (
            "debt_yield",
            "calculation",
            ["net_operating_income", "total_debt"],
        ),  # Formula node
        (
            "custom_banking_ratio",
            "formula",
            ["total_assets", "total_equity"],
        ),  # Custom formula
        (
            "npl_ratio",
            "calculation",
            ["non_performing_loans", "total_loans"],
        ),  # Ratio calculation
    ]

    logger.info("--- Context-Aware Validation Results ---")
    for node_name, node_type, parent_nodes in test_nodes:
        result = validator.validate(
            node_name, node_type=node_type, parent_nodes=parent_nodes
        )

        logger.info(f"Node: '{node_name}' (Type: {node_type})")
        logger.info(f"  Standardized: '{result.standardized_name}'")
        logger.info(f"  Valid: {result.is_valid}")
        logger.info(f"  Category: {result.category}")
        logger.info(f"  Message: {result.message}")
        if parent_nodes:
            logger.info(f"  Parent nodes: {parent_nodes}")
        logger.info("")


def create_validated_bank_data() -> dict[str, FinancialStatementItemNode]:
    """Create bank data using validated node names."""
    logger.info("\n=== Creating Validated Bank Data ===")

    # Create validator
    validator = UnifiedNodeValidator(auto_standardize=True)

    # Raw data with potentially non-standard names
    raw_data_mapping = {
        "loan_loss_allowance": {  # Non-standard name
            "2021": 800_000_000,
            "2022": 900_000_000,
            "2023": 1_100_000_000,
        },
        "npl": {  # Abbreviated name
            "2021": 1_200_000_000,
            "2022": 1_400_000_000,
            "2023": 1_800_000_000,
        },
        "tier_1_capital": {  # Standard name
            "2021": 8_500_000_000,
            "2022": 9_200_000_000,
            "2023": 10_000_000_000,
        },
    }

    validated_nodes = {}

    for raw_name, values in raw_data_mapping.items():
        # Validate and standardize the name
        result = validator.validate(raw_name)

        logger.info(
            f"  '{raw_name}' -> '{result.standardized_name}' (Valid: {result.is_valid})"
        )
        if result.message:
            logger.info(f"    Note: {result.message}")

        # Create node with standardized name
        validated_nodes[result.standardized_name] = FinancialStatementItemNode(
            result.standardized_name, values
        )

    return validated_nodes


def validate_data_completeness(
    data_nodes: dict[str, FinancialStatementItemNode],
) -> dict:
    """Validate that all required banking data is present."""
    logger.info("\n=== Data Completeness Validation ===")

    # Define required banking nodes for comprehensive analysis
    required_nodes = [
        "total_loans",
        "non_performing_loans",
        "allowance_for_loan_losses",
        "total_deposits",
        "total_equity",
        "net_interest_income",
        "non_interest_income",
        "non_interest_expense",
        "provision_for_loan_losses",
        "tier_1_capital",
        "risk_weighted_assets",
    ]

    # Optional but recommended nodes
    recommended_nodes = [
        "high_quality_liquid_assets",
        "net_cash_outflows_30_days",
        "available_stable_funding",
        "required_stable_funding",
        "common_equity_tier_1_capital",
        "total_capital",
    ]

    validation_report = {
        "required_missing": [],
        "recommended_missing": [],
        "total_nodes": len(data_nodes),
        "completeness_score": 0.0,
    }

    # Check required nodes
    for node_name in required_nodes:
        if node_name not in data_nodes:
            validation_report["required_missing"].append(node_name)

    # Check recommended nodes
    for node_name in recommended_nodes:
        if node_name not in data_nodes:
            validation_report["recommended_missing"].append(node_name)

    # Calculate completeness score
    total_expected = len(required_nodes) + len(recommended_nodes)
    missing_count = len(validation_report["required_missing"]) + len(
        validation_report["recommended_missing"]
    )
    validation_report["completeness_score"] = (
        (total_expected - missing_count) / total_expected
    ) * 100

    # Print validation results
    logger.info(f"Total nodes available: {validation_report['total_nodes']}")
    logger.info(
        f"Data completeness score: {validation_report['completeness_score']:.1f}%"
    )

    if validation_report["required_missing"]:
        logger.info(
            f"Missing required nodes ({len(validation_report['required_missing'])}):"
        )
        for node in validation_report["required_missing"]:
            logger.info(f"  - {node}")
    else:
        logger.info("✓ All required nodes present")

    if validation_report["recommended_missing"]:
        logger.info(
            f"Missing recommended nodes ({len(validation_report['recommended_missing'])}):"
        )
        for node in validation_report["recommended_missing"]:
            logger.info(f"  - {node}")
    else:
        logger.info("✓ All recommended nodes present")

    return validation_report


def analyze_asset_quality(
    data_nodes: dict[str, FinancialStatementItemNode], period: str
) -> dict:
    """Analyze asset quality metrics for the bank."""
    logger.info(f"\n=== Asset Quality Analysis for {period} ===")

    asset_quality_metrics = [
        "non_performing_loan_ratio",
        "charge_off_rate",
        "provision_coverage_ratio",
        "allowance_to_loans_ratio",
    ]

    results = {}
    for metric_name in asset_quality_metrics:
        try:
            value = calculate_metric(metric_name, data_nodes, period)
            interpretation = interpret_metric(metric_registry.get(metric_name), value)

            results[metric_name] = {
                "value": value,
                "interpretation": interpretation,
            }

            logger.info(f"{metric_name}: {value:.2f}% - {interpretation['rating']}")
            logger.info(f"  {interpretation['interpretation_message']}")

        except Exception as e:
            logger.exception(f"Could not calculate {metric_name}")
            results[metric_name] = {"error": str(e)}

    return results


def analyze_capital_adequacy(
    data_nodes: dict[str, FinancialStatementItemNode], period: str
) -> dict:
    """Analyze capital adequacy metrics for the bank."""
    logger.info(f"\n=== Capital Adequacy Analysis for {period} ===")

    capital_metrics = [
        "common_equity_tier_1_ratio",
        "tier_1_capital_ratio",
        "total_capital_ratio",
        "tier_1_leverage_ratio",
    ]

    results = {}
    for metric_name in capital_metrics:
        try:
            value = calculate_metric(metric_name, data_nodes, period)
            interpretation = interpret_metric(metric_registry.get(metric_name), value)

            results[metric_name] = {
                "value": value,
                "interpretation": interpretation,
            }

            logger.info(f"{metric_name}: {value:.2f}% - {interpretation['rating']}")
            logger.info(f"  {interpretation['interpretation_message']}")

        except Exception as e:
            logger.exception(f"Could not calculate {metric_name}")
            results[metric_name] = {"error": str(e)}

    return results


def analyze_profitability(
    data_nodes: dict[str, FinancialStatementItemNode], period: str
) -> dict:
    """Analyze profitability metrics for the bank."""
    logger.info(f"\n=== Profitability Analysis for {period} ===")

    profitability_metrics = [
        "net_interest_margin",
        "efficiency_ratio",
        "return_on_assets_(banking)",
        "return_on_equity_(banking)",
        "fee_income_ratio",
    ]

    results = {}
    for metric_name in profitability_metrics:
        try:
            value = calculate_metric(metric_name, data_nodes, period)
            interpretation = interpret_metric(metric_registry.get(metric_name), value)

            results[metric_name] = {
                "value": value,
                "interpretation": interpretation,
            }

            logger.info(f"{metric_name}: {value:.2f}% - {interpretation['rating']}")
            logger.info(f"  {interpretation['interpretation_message']}")

        except Exception as e:
            logger.exception(f"Could not calculate {metric_name}")
            results[metric_name] = {"error": str(e)}

    return results


def analyze_liquidity(
    data_nodes: dict[str, FinancialStatementItemNode], period: str
) -> dict:
    """Analyze liquidity metrics for the bank."""
    logger.info(f"\n=== Liquidity Analysis for {period} ===")

    liquidity_metrics = [
        "liquidity_coverage_ratio",
        "net_stable_funding_ratio",
        "deposits_to_loans_ratio",
        "loan_to_deposit_ratio",
        "liquid_assets_ratio",
    ]

    results = {}
    for metric_name in liquidity_metrics:
        try:
            value = calculate_metric(metric_name, data_nodes, period)
            interpretation = interpret_metric(metric_registry.get(metric_name), value)

            results[metric_name] = {
                "value": value,
                "interpretation": interpretation,
            }

            logger.info(f"{metric_name}: {value:.2f}% - {interpretation['rating']}")
            logger.info(f"  {interpretation['interpretation_message']}")

        except Exception as e:
            logger.exception(f"Could not calculate {metric_name}")
            results[metric_name] = {"error": str(e)}

    return results


def generate_banking_dashboard(
    data_nodes: dict[str, FinancialStatementItemNode], period: str
) -> dict:
    """Generate a comprehensive banking analysis dashboard."""
    logger.info(f"\n{'=' * 60}")
    logger.info(f"COMPREHENSIVE BANKING ANALYSIS DASHBOARD - {period}")
    logger.info(f"{'=' * 60}")

    # Asset Quality Analysis
    asset_quality_results = analyze_asset_quality(data_nodes, period)

    # Capital Adequacy Analysis
    capital_results = analyze_capital_adequacy(data_nodes, period)

    # Profitability Analysis
    profitability_results = analyze_profitability(data_nodes, period)

    # Liquidity Analysis
    liquidity_results = analyze_liquidity(data_nodes, period)

    # Summary Assessment
    logger.info(f"\n=== Overall Assessment for {period} ===")

    # Count metrics by rating
    all_results = {
        **asset_quality_results,
        **capital_results,
        **profitability_results,
        **liquidity_results,
    }
    ratings = [
        result["interpretation"]["rating"]
        for result in all_results.values()
        if "interpretation" in result
    ]

    rating_counts = {}
    for rating in ratings:
        rating_counts[rating] = rating_counts.get(rating, 0) + 1

    logger.info("Rating Distribution:")
    for rating, count in rating_counts.items():
        logger.info(f"  {rating}: {count} metrics")

    return {
        "asset_quality": asset_quality_results,
        "capital_adequacy": capital_results,
        "profitability": profitability_results,
        "liquidity": liquidity_results,
        "summary": rating_counts,
    }


def main():
    """Run the banking analysis example showcasing graph capabilities."""
    # Create and demonstrate the graph-based banking example
    create_banking_graph_example()


def create_banking_graph_example() -> dict[str, FinancialStatementItemNode]:
    """Create a comprehensive banking graph with calculation nodes and relationships.

    This demonstrates the difference between a simple collection of nodes
    and a true graph structure with dependencies and calculations.
    """
    logger.info("\n=== Creating Comprehensive Banking Graph ===")

    # 1. Base data nodes (leaf nodes in the graph)
    logger.info("Creating base data nodes...")

    # Asset nodes
    cash_and_equivalents = FinancialStatementItemNode(
        "cash_and_equivalents",
        {"2021": 8_000_000_000, "2022": 8_500_000_000, "2023": 9_000_000_000},
    )

    securities_available_for_sale = FinancialStatementItemNode(
        "securities_available_for_sale",
        {"2021": 10_000_000_000, "2022": 11_000_000_000, "2023": 12_000_000_000},
    )

    securities_held_to_maturity = FinancialStatementItemNode(
        "securities_held_to_maturity",
        {"2021": 2_000_000_000, "2022": 2_000_000_000, "2023": 2_000_000_000},
    )

    gross_loans = FinancialStatementItemNode(
        "gross_loans",
        {"2021": 45_675_000_000, "2022": 48_720_000_000, "2023": 52_780_000_000},
    )

    allowance_for_loan_losses = FinancialStatementItemNode(
        "allowance_for_loan_losses",
        {"2021": 675_000_000, "2022": 720_000_000, "2023": 780_000_000},
    )

    # Liability nodes
    demand_deposits = FinancialStatementItemNode(
        "demand_deposits",
        {"2021": 20_000_000_000, "2022": 22_000_000_000, "2023": 24_000_000_000},
    )

    time_deposits = FinancialStatementItemNode(
        "time_deposits",
        {"2021": 25_000_000_000, "2022": 27_000_000_000, "2023": 29_000_000_000},
    )

    savings_deposits = FinancialStatementItemNode(
        "savings_deposits",
        {"2021": 7_000_000_000, "2022": 7_000_000_000, "2023": 7_000_000_000},
    )

    # Income statement nodes
    interest_income_loans = FinancialStatementItemNode(
        "interest_income_loans",
        {"2021": 2_400_000_000, "2022": 2_650_000_000, "2023": 3_000_000_000},
    )

    interest_income_securities = FinancialStatementItemNode(
        "interest_income_securities",
        {"2021": 300_000_000, "2022": 350_000_000, "2023": 400_000_000},
    )

    interest_expense_deposits = FinancialStatementItemNode(
        "interest_expense_deposits",
        {"2021": 500_000_000, "2022": 600_000_000, "2023": 700_000_000},
    )

    interest_expense_borrowings = FinancialStatementItemNode(
        "interest_expense_borrowings",
        {"2021": 100_000_000, "2022": 100_000_000, "2023": 100_000_000},
    )

    # 2. Create calculation nodes that derive values from base nodes
    logger.info("Creating calculation nodes with dependencies...")

    # Total securities = AFS + HTM
    total_securities = FormulaCalculationNode(
        name="total_securities",
        inputs={
            "afs": securities_available_for_sale,
            "htm": securities_held_to_maturity,
        },
        formula="afs + htm",
    )

    # Net loans = Gross loans - Allowance
    net_loans = FormulaCalculationNode(
        name="net_loans",
        inputs={"gross": gross_loans, "allowance": allowance_for_loan_losses},
        formula="gross - allowance",
    )

    # Total deposits = Demand + Time + Savings
    total_deposits = CustomCalculationNode(
        name="total_deposits",
        inputs=[demand_deposits, time_deposits, savings_deposits],
        formula_func=lambda demand, time, savings: demand + time + savings,
        description="Sum of all deposit types",
    )

    # Total interest income = Loan income + Securities income
    total_interest_income = FormulaCalculationNode(
        name="total_interest_income",
        inputs={
            "loans": interest_income_loans,
            "securities": interest_income_securities,
        },
        formula="loans + securities",
    )

    # Total interest expense = Deposit expense + Borrowing expense
    total_interest_expense = FormulaCalculationNode(
        name="total_interest_expense",
        inputs={
            "deposits": interest_expense_deposits,
            "borrowings": interest_expense_borrowings,
        },
        formula="deposits + borrowings",
    )

    # Net interest income = Total interest income - Total interest expense
    net_interest_income = CustomCalculationNode(
        name="net_interest_income",
        inputs=[total_interest_income, total_interest_expense],
        formula_func=lambda income, expense: income - expense,
        description="Net interest income calculation",
    )

    # Total earning assets = Net loans + Total securities
    total_earning_assets = CustomCalculationNode(
        name="total_earning_assets",
        inputs=[net_loans, total_securities],
        formula_func=lambda loans, securities: loans + securities,
        description="Sum of earning assets",
    )

    # Total assets = Cash + Securities + Net loans + Other (simplified)
    other_assets = FinancialStatementItemNode(
        "other_assets",
        {"2021": 2_000_000_000, "2022": 2_200_000_000, "2023": 2_400_000_000},
    )

    total_assets = CustomCalculationNode(
        name="total_assets",
        inputs=[cash_and_equivalents, total_securities, net_loans, other_assets],
        formula_func=lambda cash, securities, loans, other: cash
        + securities
        + loans
        + other,
        description="Total bank assets",
    )

    # 3. Create ratio calculation nodes
    logger.info("Creating ratio calculation nodes...")

    # Loan to deposit ratio = Net loans / Total deposits
    loan_to_deposit_ratio = CustomCalculationNode(
        name="loan_to_deposit_ratio_calculated",
        inputs=[net_loans, total_deposits],
        formula_func=lambda loans, deposits: loans / deposits if deposits != 0 else 0,
        description="Loan to deposit ratio",
    )

    # Net interest margin = Net interest income / Total earning assets
    net_interest_margin_calculated = CustomCalculationNode(
        name="net_interest_margin_calculated",
        inputs=[net_interest_income, total_earning_assets],
        formula_func=lambda nii, assets: nii / assets if assets != 0 else 0,
        description="Net interest margin calculation",
    )

    # 4. Demonstrate graph traversal and calculation
    logger.info("Demonstrating graph calculations...")

    # Calculate values for 2023 to show graph dependencies
    period = "2023"

    # These will automatically calculate based on their inputs
    calculated_values = {
        "Total Securities": total_securities.calculate(period),
        "Net Loans": net_loans.calculate(period),
        "Total Deposits": total_deposits.calculate(period),
        "Net Interest Income": net_interest_income.calculate(period),
        "Total Assets": total_assets.calculate(period),
        "Loan-to-Deposit Ratio": loan_to_deposit_ratio.calculate(period) * 100,
        "Net Interest Margin": net_interest_margin_calculated.calculate(period) * 100,
    }

    logger.info(f"\nCalculated values for {period}:")
    for name, value in calculated_values.items():
        if "Ratio" in name or "Margin" in name:
            logger.info(f"  {name}: {value:.2f}%")
        else:
            logger.info(f"  {name}: ${value:,.0f}")

    # 5. Show graph dependencies
    logger.info("\nGraph Dependencies:")

    # Handle different input types for different calculation nodes
    def get_input_names(node: Node) -> list[str]:
        if hasattr(node, "inputs"):
            if isinstance(node.inputs, dict):
                # FormulaCalculationNode has dict inputs
                return [input_node.name for input_node in node.inputs.values()]
            elif isinstance(node.inputs, list):
                # CustomCalculationNode has list inputs
                return [input_node.name for input_node in node.inputs]
        return []

    logger.info(f"  total_assets depends on: {get_input_names(total_assets)}")
    logger.info(
        f"  net_interest_income depends on: {get_input_names(net_interest_income)}"
    )
    logger.info(
        f"  total_interest_income depends on: {get_input_names(total_interest_income)}"
    )
    logger.info(f"  total_securities depends on: {get_input_names(total_securities)}")
    logger.info(f"  net_loans depends on: {get_input_names(net_loans)}")

    # Return all nodes (both base and calculated)
    return {
        # Base data nodes
        "cash_and_equivalents": cash_and_equivalents,
        "securities_available_for_sale": securities_available_for_sale,
        "securities_held_to_maturity": securities_held_to_maturity,
        "gross_loans": gross_loans,
        "allowance_for_loan_losses": allowance_for_loan_losses,
        "demand_deposits": demand_deposits,
        "time_deposits": time_deposits,
        "savings_deposits": savings_deposits,
        "interest_income_loans": interest_income_loans,
        "interest_income_securities": interest_income_securities,
        "interest_expense_deposits": interest_expense_deposits,
        "interest_expense_borrowings": interest_expense_borrowings,
        "other_assets": other_assets,
        # Calculated nodes (graph structure)
        "total_securities": total_securities,
        "net_loans": net_loans,
        "total_deposits": total_deposits,
        "total_interest_income": total_interest_income,
        "total_interest_expense": total_interest_expense,
        "net_interest_income": net_interest_income,
        "total_earning_assets": total_earning_assets,
        "total_assets": total_assets,
        "loan_to_deposit_ratio_calculated": loan_to_deposit_ratio,
        "net_interest_margin_calculated": net_interest_margin_calculated,
    }


if __name__ == "__main__":
    main()



================================================================================
File: examples/scripts/sector_examples/corporate_example.py
================================================================================

"""This example demonstrates using the detailed test_statement.yaml configuration.

It includes:
- Loading the complex statement configuration.
- Populating a graph with corresponding sample data.
- Generating the statement DataFrame based on the config.

NEW FEATURES ADDED:
- Node name validation and normalization using UnifiedNodeValidator
- Automatic standardization of alternate node names (e.g., 'cash' -> 'cash_and_equivalents')
- Recognition of sub-node patterns (e.g., 'revenue_q1', 'revenue_2024')
- Formula node pattern detection (e.g., 'gross_margin', 'debt_ratio')
- Comprehensive validation reporting with suggestions for improvement
- Context-aware validation that understands node relationships and dependencies

The validators help ensure:
1. Consistency across financial models
2. Proper metric functionality (metrics expect standard node names)
3. Better code maintainability and readability
4. Helpful suggestions for non-standard names
"""

import logging
import sys
import yaml
from pathlib import Path
import pandas as pd
from fin_statement_model.core.errors import FinancialModelError

from fin_statement_model.io import read_data, write_data
from fin_statement_model.statements import create_statement_dataframe
from fin_statement_model.forecasting.forecaster import StatementForecaster

# Import unified validator
from fin_statement_model.io.validation import UnifiedNodeValidator

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- 1. Setup ---

# Hardcoded paths (as modified by user)
SCRIPT_DIR = Path(__file__).parent
md_output_path = SCRIPT_DIR / "output" / "test_statement.md"
TEST_CONFIG_PATH = SCRIPT_DIR.parent / "configs" / "test_statement.yaml"

# --- 2. Sample Data ---

# Sample historical data matching node_ids in test_statement.yaml
# Note: The keys must match the 'node_id' or 'metric_id'/'inputs' references
historical_data = {
    # Node ID: { Period: Value }
    # Add a 2021 data point for better historical average calculations
    "cash_and_equivalents": {"2021": 90.0, "2022": 100.0, "2023": 120.0},
    "accounts_receivable": {"2021": 180.0, "2022": 200.0, "2023": 250.0},
    "property_plant_equipment": {"2021": 480.0, "2022": 500.0, "2023": 550.0},
    "accounts_payable": {"2021": 140.0, "2022": 150.0, "2023": 180.0},
    "total_debt": {"2021": 290.0, "2022": 300.0, "2023": 320.0},
    "common_stock": {"2021": 100.0, "2022": 100.0, "2023": 100.0},
    "retained_earnings": {"2021": 80.0, "2022": 100.0, "2023": 125.0},
    "revenue": {"2021": 900.0, "2022": 1000.0, "2023": 1200.0},
    "cost_of_goods_sold": {"2021": -350.0, "2022": -400.0, "2023": -500.0},
    "operating_expenses": {"2021": 280.0, "2022": 300.0, "2023": 350.0},
    "operating_income": {
        "2021": 270.0,
        "2022": 300.0,
        "2023": 350.0,
    },  # Calculated: gross_profit - operating_expenses
    "interest_expense": {"2021": 15.0, "2022": 18.0, "2023": 20.0},
    "income_tax": {"2021": 45.0, "2022": 55.0, "2023": 65.0},
    "dividends": {"2021": 20.0, "2022": 25.0, "2023": 30.0},
}

# --- 2b. Node Name Validation and Normalization ---

logger.info("Validating and normalizing node names...")

# Demo only: direct use of UnifiedNodeValidator; production code should use StatementConfig/StatementStructureBuilder for validation
# Create unified validator for demo
validator = UnifiedNodeValidator(
    strict_mode=False,  # Allow alternate names
    auto_standardize=True,  # Automatically convert to standard names
    warn_on_non_standard=True,  # Warn about non-standard names
    enable_patterns=True,  # Enable pattern recognition for sub-nodes and formulas
)

# Demonstrate validation with some non-standard names
logger.info("\n=== DEMONSTRATION: Validating various node name patterns ===")

demo_names = [
    "cash",  # Alternate name
    "ar",  # Alternate name for accounts_receivable
    "revenue_q1",  # Sub-node pattern (quarterly)
    "revenue_2024",  # Sub-node pattern (annual)
    "revenue_north_america",  # Sub-node pattern (geographic)
    "gross_margin",  # Formula pattern
    "debt_ratio",  # Formula pattern
    "custom_metric_xyz",  # Unrecognized name
    "sales",  # Alternate for revenue
    "cogs",  # Alternate for cost_of_goods_sold
]

logger.info(f"Demo names to validate: {demo_names}")

for demo_name in demo_names:
    result = validator.validate(demo_name)
    logger.info(
        f"  '{demo_name}' -> '{result.standardized_name}' [{result.category}] ({result.message})"
    )

    if result.suggestions:
        logger.info(
            f"    Suggestions: {result.suggestions[:2]}"
        )  # Show first 2 suggestions
    logger.info("")

logger.info("=== END DEMONSTRATION ===\n")

# Validate and normalize historical data node names
original_node_names = list(historical_data.keys())
logger.info(f"Original node names: {original_node_names}")

# Use unified validator
validation_results = validator.validate_batch(original_node_names)

# Create normalized historical data
normalized_historical_data = {}
name_changes = []

for original_name, result in validation_results.items():
    if result.standardized_name != original_name:
        name_changes.append((original_name, result.standardized_name))
        logger.info(
            f"  Normalized: '{original_name}' -> '{result.standardized_name}' ({result.message})"
        )
    else:
        logger.info(f"  Validated: '{original_name}' - {result.message}")

    # Copy data with standardized name
    normalized_historical_data[result.standardized_name] = historical_data[
        original_name
    ]

# Show validation summary
logger.info("\nValidation Summary:")
logger.info(f"  Total nodes validated: {len(validation_results)}")
valid_count = sum(1 for r in validation_results.values() if r.is_valid)
logger.info(f"  Valid nodes: {valid_count}")
logger.info(f"  Invalid nodes: {len(validation_results) - valid_count}")

# Count by category
categories = {}
for result in validation_results.values():
    categories[result.category] = categories.get(result.category, 0) + 1

for category, count in categories.items():
    logger.info(f"  {category}: {count}")

# Show unrecognized names with suggestions
unrecognized = [
    name
    for name, result in validation_results.items()
    if result.category in ["custom", "invalid"]
]
if unrecognized:
    logger.info(f"\nUnrecognized node names: {unrecognized}")
    for name in unrecognized:
        if validation_results[name].suggestions:
            logger.info(
                f"  Suggestions for '{name}': {validation_results[name].suggestions}"
            )

# Update historical_data to use normalized names
historical_data = normalized_historical_data

# Also normalize forecast_configs keys to match
logger.info("\nNormalizing forecast configuration keys...")
normalized_forecast_configs = {}
for original_name, config in {
    "cash_and_equivalents": {"method": "simple", "config": 0.05},  # 5% growth
    "accounts_receivable": {
        "method": "curve",
        "config": [0.08, 0.06, 0.05, 0.04, 0.03],  # Slowing growth over 5 years
    },  # Slowing growth
    "property_plant_equipment": {"method": "simple", "config": 0.02},  # 2% growth
    "accounts_payable": {
        "method": "curve",
        "config": [0.04, 0.03, 0.03, 0.02, 0.02],
    },  # Declining curve
    "total_debt": {"method": "simple", "config": 0.0},  # Assume flat debt
    "common_stock": {
        "method": "simple",
        "config": 0.0,
    },  # Assume flat common stock
    "retained_earnings": {
        "method": "simple",
        "config": 0.0,
    },  # Usually calculated, but forecast base if needed
    "dividends": {
        "method": "historical_growth",
        "config": None,
    },  # Grow based on historical trend
    "revenue": {
        "method": "curve",
        "config": [0.10, 0.09, 0.08, 0.07, 0.06],
    },  # Declining revenue growth
    "cost_of_goods_sold": {
        "method": "historical_growth",
        "config": None,
    },  # COGS based on historical growth
    "operating_expenses": {
        "method": "statistical",
        "config": {
            "distribution": "normal",
            "params": {"mean": 0.03, "std": 0.015},  # Normal dist around 3% mean
        },
    },  # Statistical forecast with normal distribution
    "operating_income": {
        "method": "historical_growth",
        "config": None,
    },  # Operating income based on historical growth
    "interest_expense": {
        "method": "historical_growth",
        "config": None,
    },  # Interest expense based on historical growth
    "income_tax": {
        "method": "historical_growth",
        "config": None,
    },  # Tax expense based on historical growth
}.items():
    # Validate and normalize forecast config keys
    result = validator.validate(original_name)
    normalized_forecast_configs[result.standardized_name] = config
    if result.standardized_name != original_name:
        logger.info(
            f"  Forecast config: '{original_name}' -> '{result.standardized_name}'"
        )

forecast_configs = normalized_forecast_configs

logger.info(f"\nFinal normalized node names: {sorted(historical_data.keys())}")

# --- 3. Graph Creation and Initial Data Loading ---

# Restore try-except block
try:
    logger.info("\nCreating graph and loading initial data...")
    graph = read_data(format_type="dict", source=historical_data)
    logger.info(f"Graph created with initial periods: {graph.periods}")
except FinancialModelError:
    logger.exception("Error creating graph or loading initial data", file=sys.stderr)
    sys.exit(1)

# --- 3c. Post-Graph Node Validation ---

logger.info("\nValidating graph nodes with unified validator...")

# Get all nodes from the graph
graph_nodes = list(graph.nodes.values())
logger.info(f"Graph contains {len(graph_nodes)} nodes")

# Validate each node in the graph
node_validation_results = {}
for node in graph_nodes:
    # Determine node type and parent nodes for context-aware validation
    node_type = "data"  # Default to data node
    parent_nodes = None

    if hasattr(node, "inputs") and node.inputs:
        node_type = "calculation"
        # Extract parent node names
        if isinstance(node.inputs, dict):
            parent_nodes = list(node.inputs.keys())
        elif isinstance(node.inputs, list):
            parent_nodes = [n.name for n in node.inputs if hasattr(n, "name")]

    result = validator.validate(
        node.name, node_type=node_type, parent_nodes=parent_nodes
    )
    node_validation_results[node.name] = result

# Display categorized results
categories_in_graph = {}
for node_name, result in node_validation_results.items():
    category = result.category
    if category not in categories_in_graph:
        categories_in_graph[category] = []
    categories_in_graph[category].append(
        {
            "name": node_name,
            "message": result.message,
            "standardized": result.standardized_name,
        }
    )

for category, nodes in categories_in_graph.items():
    if nodes:  # Only show categories that have nodes
        logger.info(f"\n{category.upper()} nodes ({len(nodes)}):")
        for node_info in nodes:
            logger.info(f"  - {node_info['name']}: {node_info['message']}")

# Show any naming improvement suggestions
logger.info("\nNaming improvement suggestions:")
suggestions_found = False
for node_name, result in node_validation_results.items():
    if result.suggestions:
        suggestions_found = True
        logger.info(f"  {node_name}: {result.suggestions}")

if not suggestions_found:
    logger.info(
        "  All node names are using standard conventions - no improvements needed!"
    )

# --- 3b. Forecasting Setup ---
logger.info("Setting up forecasting...")

historical_periods = sorted(list(graph.periods))
forecast_periods = [
    "2024",
    "2025",
    "2026",
    "2027",
    "2028",
]  # Hardcoded as per user changes
all_periods = sorted(historical_periods + forecast_periods)

logger.info(f"Historical periods: {historical_periods}")
logger.info(f"Forecast periods: {forecast_periods}")

# Use the StatementForecaster
# Restore try-except block
forecaster = StatementForecaster(
    fsg=graph
)  # fsg likely stands for financial statement graph
logger.info(f"Applying forecasts for periods: {forecast_periods}")

# Apply the forecasts using the defined configs
forecaster.create_forecast(
    forecast_periods=forecast_periods,
    node_configs=forecast_configs,
    historical_periods=historical_periods,
)

# --- 4. Statement Generation ---

# Load corporate statement config from YAML into memory
raw_config = yaml.safe_load(Path(TEST_CONFIG_PATH).read_text(encoding="utf-8"))
stmt_id = raw_config.get("id", "test_statement")
raw_configs = {stmt_id: raw_config}

# Generate the statement DataFrame
df_map = create_statement_dataframe(
    graph=graph,
    raw_configs=raw_configs,
    format_kwargs={
        "should_apply_signs": True,
        "number_format": ",.1f",
    },
)
statement_df = df_map[stmt_id]

with pd.option_context(
    "display.max_rows", None, "display.max_columns", None, "display.width", 1000
):
    logger.info(statement_df.to_string(index=False))

# Write output data
write_data(
    format_type="markdown",
    graph=graph,
    target=str(md_output_path),
    raw_configs=raw_configs,
    historical_periods=historical_periods,
    forecast_configs=forecast_configs,
)



================================================================================
File: examples/scripts/sector_examples/example_adjustments.py
================================================================================

"""Example demonstrating the use of adjustments in the financial statement model."""

import logging

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.core.adjustments.models import AdjustmentType

# Configure logging for visibility
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- 1. Setup Graph and Nodes ---
logger.info("Setting up the graph and base data...")
graph = Graph()

# Prepare initial data
revenue_data = {"2023": 1000.0, "2024": 1200.0}
cogs_data = {"2023": 400.0, "2024": 500.0}

# Create nodes with initial data
revenue = FinancialStatementItemNode(name="Revenue", values=revenue_data)
cogs = FinancialStatementItemNode(name="COGS", values=cogs_data)

# Add nodes to the graph
graph.add_node(revenue)
graph.add_node(cogs)

# No need for set_node_value here, data is added via constructor

logger.info(f"Initial Revenue 2023: {graph.calculate('Revenue', '2023')}")
logger.info(f"Initial Revenue 2024: {graph.calculate('Revenue', '2024')}")

# --- 2. Additive Adjustment ---
logger.info("\n--- Applying Additive Adjustment ---")

adj_id_1 = graph.add_adjustment(
    node_name="Revenue",
    period="2023",
    value=50.0,
    adj_type=AdjustmentType.ADDITIVE,
    reason="Manual uplift based on late contract signing.",
    tags={"manual", "contract"},
    user="Analyst1",
)
logger.info(f"Added additive adjustment {adj_id_1} for Revenue 2023.")

# Get the value *with* adjustments applied (default behavior)
adjusted_revenue_2023 = graph.get_adjusted_value("Revenue", "2023")
logger.info(f"Adjusted Revenue 2023: {adjusted_revenue_2023}")

# Verify the adjustment was applied
adj_list_2023 = graph.get_adjustments("Revenue", "2023")
logger.info(f"Adjustments applied to Revenue 2023: {adj_list_2023}")

# Value for 2024 should be unaffected
adjusted_revenue_2024 = graph.get_adjusted_value("Revenue", "2024")
logger.info(f"Adjusted Revenue 2024 (should be unchanged): {adjusted_revenue_2024}")

# --- 3. Replacement Adjustment ---
logger.info("\n--- Applying Replacement Adjustment ---")

adj_id_2 = graph.add_adjustment(
    node_name="COGS",
    period="2024",
    value=555.0,
    adj_type=AdjustmentType.REPLACEMENT,
    reason="Revised COGS estimate based on new supplier quote.",
    tags={"estimate", "supplier"},
    user="Analyst2",
    priority=-10,  # Higher priority (lower number)
)
logger.info(f"Added replacement adjustment {adj_id_2} for COGS 2024.")

adjusted_cogs_2024 = graph.get_adjusted_value("COGS", "2024")
logger.info(f"Adjusted COGS 2024: {adjusted_cogs_2024}")

# Check adjustments for COGS 2024
adj_list_cogs_2024 = graph.get_adjustments("COGS", "2024")
logger.info(f"Adjustments applied to COGS 2024: {adj_list_cogs_2024}")


# --- 4. Listing All Adjustments ---
logger.info("\n--- Listing All Adjustments in the Graph ---")
all_adjustments = graph.list_all_adjustments()
logger.info(f"Total adjustments found: {len(all_adjustments)}")
for adj in all_adjustments:
    logger.info(
        f"  - {adj.id}: Node='{adj.node_name}', Period='{adj.period}', Type='{adj.type.name}', Value={adj.value}, Reason='{adj.reason}'"
    )

# --- 5. Removing an Adjustment ---
logger.info("\n--- Removing an Adjustment ---")
removed = graph.remove_adjustment(adj_id_1)
logger.info(f"Attempted to remove adjustment {adj_id_1}. Success: {removed}")

# Check Revenue 2023 value again
adjusted_revenue_2023_after_remove = graph.get_adjusted_value("Revenue", "2023")
logger.info(
    f"Adjusted Revenue 2023 after removal: {adjusted_revenue_2023_after_remove}"
)

# Verify it's gone
adj_list_2023_after_remove = graph.get_adjustments("Revenue", "2023")
logger.info(f"Adjustments for Revenue 2023 after removal: {adj_list_2023_after_remove}")


logger.info("\nExample complete.")



================================================================================
File: examples/scripts/sector_examples/real_estate_analysis_example.py
================================================================================

"""Real Estate Investment Trust (REIT) Analysis Example.

This example demonstrates using specialized real estate metrics and calculations
for analyzing REIT financial performance.
"""

import logging
from pathlib import Path
import yaml

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.statements.orchestration.orchestrator import (
    create_statement_dataframe,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def create_reit_financial_model() -> Graph:
    """Create a financial model for a REIT with specialized metrics."""
    graph = Graph()

    # === Income Statement Items ===
    income_items = {
        # Revenue
        "rental_income": {"2022": 50000000, "2023": 55000000},
        "property_management_fees": {"2022": 2500000, "2023": 2750000},
        "other_income": {"2022": 1000000, "2023": 1200000},
        # Operating Expenses
        "property_operating_expenses": {"2022": 18000000, "2023": 20000000},
        "property_management_expenses": {"2022": 3500000, "2023": 3850000},
        "general_admin_expenses": {"2022": 4000000, "2023": 4400000},
        "depreciation_expense": {"2022": 12000000, "2023": 13200000},
        # Other
        "interest_expense": {"2022": 8000000, "2023": 8500000},
        "gain_on_property_sales": {"2022": 2000000, "2023": 1500000},
    }

    # === Balance Sheet Items ===
    balance_sheet_items = {
        # Assets
        "investment_properties": {"2022": 600000000, "2023": 650000000},
        "accumulated_depreciation": {"2022": -120000000, "2023": -133200000},
        "cash": {"2022": 15000000, "2023": 18000000},
        "accounts_receivable": {"2022": 3000000, "2023": 3500000},
        # Liabilities
        "mortgages_payable": {"2022": 350000000, "2023": 380000000},
        "bonds_payable": {"2022": 50000000, "2023": 50000000},
        "accounts_payable": {"2022": 2000000, "2023": 2200000},
        # Equity
        "common_shares": {"2022": 100000000, "2023": 100000000},
        "preferred_shares": {"2022": 25000000, "2023": 25000000},
        "retained_earnings": {"2022": 70000000, "2023": 77300000},
        # Share count for per-share metrics
        "shares_outstanding": {"2022": 10000000, "2023": 10000000},
    }

    # === Cash Flow Items ===
    cash_flow_items = {
        "capital_expenditures": {"2022": 45000000, "2023": 50000000},
        "property_acquisitions": {"2022": 30000000, "2023": 35000000},
        "dividends_paid": {"2022": 22000000, "2023": 24000000},
    }

    # Add all items to graph
    for name, values in {
        **income_items,
        **balance_sheet_items,
        **cash_flow_items,
    }.items():
        node = FinancialStatementItemNode(name, values)
        graph.add_node(node)

    # Calculations and metrics will be created later via the statement
    # configuration processed by `create_statement_dataframe`.

    return graph


def analyze_reit_performance(graph: Graph, period: str = "2023") -> None:
    """Analyze REIT performance using specialized metrics."""
    logger.info("=== REIT Analysis Example ===\n")

    # Calculate key REIT metrics
    metrics_to_calculate = [
        ("net_operating_income", "Net Operating Income"),
        ("funds_from_operations", "Funds From Operations (FFO)"),
        ("ffo_per_share", "FFO per Share"),
    ]

    logger.info("Key REIT Metrics for 2023:")
    logger.info("-" * 40)

    for metric_id, display_name in metrics_to_calculate:
        try:
            value = graph.calculate(metric_id, period)

            # Format based on metric type
            if metric_id in ["net_operating_income", "funds_from_operations"]:
                logger.info(f"{display_name}: ${value:,.0f}")
            elif metric_id == "ffo_per_share":
                logger.info(f"{display_name}: ${value:.2f}")
            else:
                logger.info(f"{display_name}: {value:.2f}")

        except Exception:
            logger.exception(f"Could not calculate {display_name}")
            logger.info("")

    # Calculate additional real estate metrics using the helper function
    logger.info("Additional Analysis:")
    logger.info("-" * 40)

    # Cap Rate calculation (NOI / Property Value)
    try:
        noi = graph.calculate("net_operating_income", period)
        property_value = graph.calculate("investment_properties", period)
        cap_rate = (noi / property_value) * 100
        logger.info(f"Capitalization Rate: {cap_rate:.1f}%")

        if cap_rate < 4:
            logger.info("  → Low cap rate - properties may be overvalued")
        elif cap_rate > 8:
            logger.info(
                "  → High cap rate - good income yield but check property quality"
            )
        else:
            logger.info("  → Cap rate within typical range for quality properties")
    except Exception:
        logger.exception("Could not calculate cap rate")

    # Debt Service Coverage
    try:
        interest = graph.calculate("interest_expense", period)
        dscr = noi / abs(interest)  # Use absolute value since interest is negative
        logger.info(f"Debt Service Coverage Ratio: {dscr:.2f}x")

        if dscr < 1.2:
            logger.info("  → Low coverage - may struggle to service debt")
        elif dscr > 2.0:
            logger.info("  → Strong debt coverage")
        else:
            logger.info("  → Adequate debt coverage")
    except Exception:
        logger.exception("Could not calculate DSCR")

    # Growth analysis
    logger.info("\nGrowth Analysis:")
    logger.info("-" * 40)

    try:
        # NOI growth
        noi_2022 = graph.calculate("net_operating_income", "2022")
        noi_2023 = graph.calculate("net_operating_income", "2023")
        noi_growth = ((noi_2023 - noi_2022) / noi_2022) * 100
        logger.info(f"NOI Growth (2022-2023): {noi_growth:.1f}%")

        # FFO growth
        ffo_2022 = graph.calculate("funds_from_operations", "2022")
        ffo_2023 = graph.calculate("funds_from_operations", "2023")
        ffo_growth = ((ffo_2023 - ffo_2022) / ffo_2022) * 100
        logger.info(f"FFO Growth (2022-2023): {ffo_growth:.1f}%")
    except Exception:
        logger.exception("Could not calculate growth metrics")

    logger.info("\n=== Analysis Complete ===")


def main():
    """Run the REIT analysis example."""
    # Build the base graph with raw data nodes
    graph = create_reit_financial_model()

    # Load REIT statement configuration from YAML into memory
    config_path = (
        Path(__file__).resolve().parents[2] / "configs" / "reit_statement.yaml"
    )
    raw_config = yaml.safe_load(config_path.read_text(encoding="utf-8"))
    raw_configs = {raw_config.get("id", "reit_statement"): raw_config}
    # Generate calculation nodes via statement orchestration
    create_statement_dataframe(graph, raw_configs)

    # Analyze performance using the populated graph
    analyze_reit_performance(graph)


if __name__ == "__main__":
    main()



================================================================================
File: examples/scripts/sector_examples/real_estate_debt_analysis_example.py
================================================================================

"""Real Estate Company Debt Analysis Example.

This example demonstrates comprehensive debt analysis for a real estate company,
including loan portfolios, maturity schedules, and covenant calculations.
"""

import logging
from pathlib import Path
from typing import Union
import yaml

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.statements.orchestration.orchestrator import (
    create_statement_dataframe,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def create_real_estate_financial_model() -> Graph:
    """Create a financial model for a real estate company with detailed debt analysis."""
    graph = Graph()

    # === Basic Financial Statement Items ===
    # Balance Sheet Items
    balance_sheet_items = {
        # Assets
        "investment_properties": {"2022": 5000000, "2023": 5500000},
        "development_properties": {"2022": 800000, "2023": 1200000},
        "cash": {"2022": 150000, "2023": 180000},
        "other_assets": {"2022": 50000, "2023": 70000},
        # Liabilities - Debt Components
        "mortgage_debt": {"2022": 2800000, "2023": 3100000},
        "construction_loans": {"2022": 500000, "2023": 750000},
        "bridge_loans": {"2022": 200000, "2023": 150000},
        "mezzanine_debt": {"2022": 300000, "2023": 250000},
        "credit_facility": {"2022": 0, "2023": 100000},
        # Other Liabilities
        "accounts_payable": {"2022": 100000, "2023": 120000},
        "accrued_expenses": {"2022": 80000, "2023": 90000},
        # Equity
        "common_equity": {"2022": 1500000, "2023": 1600000},
        "preferred_equity": {"2022": 500000, "2023": 500000},
    }

    # Income Statement Items
    income_items = {
        "rental_income": {"2022": 450000, "2023": 520000},
        "property_management_fees": {"2022": 25000, "2023": 30000},
        "other_income": {"2022": 15000, "2023": 20000},
        "property_operating_expenses": {"2022": 180000, "2023": 210000},
        "interest_expense_mortgage": {"2022": 140000, "2023": 155000},
        "interest_expense_construction": {"2022": 35000, "2023": 52500},
        "interest_expense_other": {"2022": 30000, "2023": 25000},
        "depreciation": {"2022": 125000, "2023": 140000},
        "general_admin": {"2022": 45000, "2023": 50000},
    }

    # Debt Details
    debt_details = {
        # Fixed vs Variable Rate Breakdown
        "fixed_rate_debt": {"2022": 3200000, "2023": 3400000},
        "variable_rate_debt": {"2022": 600000, "2023": 950000},
        # Maturity Schedule
        "debt_maturing_1yr": {"2022": 200000, "2023": 450000},
        "debt_maturing_2_5yr": {"2022": 1500000, "2023": 1600000},
        "debt_maturing_after_5yr": {"2022": 2100000, "2023": 2300000},
        # Credit Facility Details
        "credit_facility_limit": {"2022": 500000, "2023": 500000},
        "unencumbered_assets": {"2022": 800000, "2023": 1000000},
    }

    # Add all items to graph
    for name, values in {**balance_sheet_items, **income_items, **debt_details}.items():
        node = FinancialStatementItemNode(name, values)
        graph.add_node(node)

    # Calculations and ratio metrics will be automatically generated later when
    # the statement configuration is processed via `create_statement_dataframe`.

    return graph


def calculate_debt_metrics(
    graph: Graph, period: str = "2023"
) -> dict[str, Union[float, None]]:
    """Calculate and return key debt metrics for analysis."""
    metrics = {}

    # Basic metrics
    basic_metrics = [
        "ltv_ratio",
        "dscr",
        "debt_to_equity",
        "interest_coverage_ratio",
        "weighted_avg_interest_rate",
    ]

    for metric_name in basic_metrics:
        try:
            metrics[metric_name] = graph.calculate(metric_name, period)
        except Exception as e:
            logger.warning(f"Warning: Could not calculate {metric_name}: {e}")
            metrics[metric_name] = None

    return metrics


def analyze_debt_composition(graph: Graph, period: str = "2023") -> dict[str, float]:
    """Analyze the composition of the debt portfolio."""
    try:
        total_debt = graph.calculate("total_debt", period)

        # Debt type breakdown
        mortgage = graph.get_node("mortgage_debt").get_value(period)
        construction = graph.get_node("construction_loans").get_value(period)
        bridge = graph.get_node("bridge_loans").get_value(period)
        mezzanine = graph.get_node("mezzanine_debt").get_value(period)

        # Fixed vs Variable
        fixed = graph.get_node("fixed_rate_debt").get_value(period)
        variable = graph.get_node("variable_rate_debt").get_value(period)

        # Maturity profile
        short_term = graph.get_node("debt_maturing_1yr").get_value(period)
        medium_term = graph.get_node("debt_maturing_2_5yr").get_value(period)

        # Credit utilization
        credit_used = graph.get_node("credit_facility").get_value(period)
        credit_limit = graph.get_node("credit_facility_limit").get_value(period)

        composition = {
            "mortgage_debt_pct": (mortgage / total_debt) * 100,
            "construction_loans_pct": (construction / total_debt) * 100,
            "bridge_loans_pct": (bridge / total_debt) * 100,
            "mezzanine_debt_pct": (mezzanine / total_debt) * 100,
            "fixed_rate_pct": (fixed / total_debt) * 100,
            "variable_rate_pct": (variable / total_debt) * 100,
            "maturities_1yr_pct": (short_term / total_debt) * 100,
            "maturities_2_5yr_pct": (medium_term / total_debt) * 100,
            "credit_utilization_pct": (
                (credit_used / credit_limit) * 100 if credit_limit > 0 else 0
            ),
        }

        return composition
    except Exception:
        logger.exception("Error analyzing debt composition")
        return {}


def calculate_debt_trends(graph: Graph) -> dict[str, float]:
    """Calculate year-over-year trends in debt metrics."""
    trends = {}

    try:
        # LTV trend
        ltv_2022 = graph.calculate("ltv_ratio", "2022")
        ltv_2023 = graph.calculate("ltv_ratio", "2023")
        trends["ltv_change"] = ltv_2023 - ltv_2022

        # Interest rate trend
        rate_2022 = graph.calculate("weighted_avg_interest_rate", "2022")
        rate_2023 = graph.calculate("weighted_avg_interest_rate", "2023")
        trends["interest_rate_change"] = (rate_2023 - rate_2022) * 100

        # DSCR trend
        dscr_2022 = graph.calculate("dscr", "2022")
        dscr_2023 = graph.calculate("dscr", "2023")
        trends["dscr_change"] = dscr_2023 - dscr_2022

        # Total debt growth
        debt_2022 = graph.calculate("total_debt", "2022")
        debt_2023 = graph.calculate("total_debt", "2023")
        trends["debt_growth"] = ((debt_2023 - debt_2022) / debt_2022) * 100

    except Exception:
        logger.exception("Error calculating trends")

    return trends


def interpret_debt_metrics(
    metrics: dict[str, Union[float, None]],
) -> dict[str, dict[str, str]]:
    """Provide interpretation of debt metrics for real estate context."""
    interpretations = {}

    # LTV Ratio interpretation
    ltv = metrics.get("ltv_ratio", 0)
    if ltv:
        if ltv < 0.5:
            interpretations["ltv"] = {
                "status": "Conservative",
                "interpretation": "Low leverage, strong equity cushion",
            }
        elif ltv < 0.65:
            interpretations["ltv"] = {
                "status": "Moderate",
                "interpretation": "Industry standard leverage",
            }
        elif ltv < 0.75:
            interpretations["ltv"] = {
                "status": "Aggressive",
                "interpretation": "High leverage, limited refinancing flexibility",
            }
        else:
            interpretations["ltv"] = {
                "status": "Very High Risk",
                "interpretation": "Excessive leverage, potential covenant breach",
            }

    # DSCR interpretation
    dscr = metrics.get("dscr", 0)
    if dscr:
        if dscr > 1.5:
            interpretations["dscr"] = {
                "status": "Strong",
                "interpretation": "Comfortable debt service coverage",
            }
        elif dscr > 1.25:
            interpretations["dscr"] = {
                "status": "Adequate",
                "interpretation": "Meeting typical covenant requirements",
            }
        elif dscr > 1.0:
            interpretations["dscr"] = {
                "status": "Thin",
                "interpretation": "Limited margin of safety",
            }
        else:
            interpretations["dscr"] = {
                "status": "Insufficient",
                "interpretation": "Unable to cover debt service from operations",
            }

    return interpretations


def main() -> None:
    """Run the real estate debt analysis example."""
    logger.info("=== Real Estate Debt Analysis Example ===\n")

    # Create the base financial model with raw data nodes
    graph = create_real_estate_financial_model()

    # Load statement configuration from YAML into memory
    config_path = (
        Path(__file__).resolve().parents[2]
        / "configs"
        / "real_estate_debt_statement.yaml"
    )
    raw_config = yaml.safe_load(config_path.read_text(encoding="utf-8"))
    raw_configs = {raw_config.get("id", "real_estate_debt"): raw_config}
    # Build statement structure and calculation nodes
    create_statement_dataframe(graph, raw_configs)

    # Calculate key metrics
    metrics = calculate_debt_metrics(graph)

    # Display results
    logger.info("Key Debt Metrics for 2023:")
    logger.info("-" * 50)

    # Format and display metrics
    metric_display = {
        "ltv_ratio": ("Loan-to-Value Ratio", "%"),
        "dscr": ("Debt Service Coverage Ratio", "x"),
        "debt_to_equity": ("Debt-to-Equity Ratio", "x"),
        "interest_coverage_ratio": ("Interest Coverage Ratio", "x"),
        "weighted_avg_interest_rate": ("Weighted Avg Interest Rate", "%"),
    }

    interpretations = interpret_debt_metrics(metrics)

    for metric_key, (display_name, unit) in metric_display.items():
        value = metrics.get(metric_key)
        if value is not None:
            if unit == "%":
                if metric_key == "weighted_avg_interest_rate":
                    value *= 100  # Convert to percentage
                elif metric_key == "ltv_ratio":
                    value *= 100  # LTV is a ratio, convert to percentage
                logger.info(f"{display_name}: {value:.1f}%")
            else:
                logger.info(f"{display_name}: {value:.2f}x")

            # Add interpretation if available
            if metric_key in interpretations:
                logger.info(f"  → {interpretations[metric_key]['interpretation']}")

            logger.info("")

    # Analyze debt composition
    logger.info("Debt Portfolio Composition (2023):")
    logger.info("-" * 50)

    composition = analyze_debt_composition(graph)

    logger.info("By Debt Type:")
    logger.info(f"  Mortgage Debt: {composition['mortgage_debt_pct']:.1f}%")
    logger.info(f"  Construction Loans: {composition['construction_loans_pct']:.1f}%")
    logger.info(f"  Bridge Loans: {composition['bridge_loans_pct']:.1f}%")
    logger.info(f"  Mezzanine Debt: {composition['mezzanine_debt_pct']:.1f}%")
    logger.info("")

    logger.info("By Interest Rate Type:")
    logger.info(f"  Fixed Rate: {composition['fixed_rate_pct']:.1f}%")
    logger.info(f"  Variable Rate: {composition['variable_rate_pct']:.1f}%")
    logger.info("")

    logger.info("By Maturity:")
    logger.info(f"  Maturing in 1 Year: {composition['maturities_1yr_pct']:.1f}%")
    logger.info(f"  Maturing in 2-5 Years: {composition['maturities_2_5yr_pct']:.1f}%")
    logger.info("")

    logger.info("Credit Facility Utilization:")
    logger.info(f"  Utilization Rate: {composition['credit_utilization_pct']:.1f}%")
    logger.info("")

    # Show trends
    logger.info("Debt Trends (2022-2023):")
    logger.info("-" * 50)

    trends = calculate_debt_trends(graph)

    logger.info(f"LTV Change: {trends['ltv_change']:+.1f} percentage points")
    logger.info(
        f"Interest Rate Change: {trends['interest_rate_change']:+.1f} percentage points"
    )
    logger.info(f"DSCR Change: {trends['dscr_change']:+.2f}x")
    logger.info(f"Total Debt Growth: {trends['debt_growth']:+.1f}%")
    logger.info("")

    # Risk assessment
    logger.info("Risk Assessment:")
    logger.info("-" * 50)

    risk_factors = []

    if metrics.get("ltv_ratio", 0) > 0.65:
        risk_factors.append("High leverage (LTV > 65%)")

    if metrics.get("dscr", 0) < 1.25:
        risk_factors.append("Low debt service coverage")

    if composition.get("variable_rate_pct", 0) > 30:
        risk_factors.append("Significant interest rate risk exposure")

    if composition.get("maturities_1yr_pct", 0) > 20:
        risk_factors.append("Near-term refinancing risk")

    if risk_factors:
        logger.info("Key Risk Factors:")
        for factor in risk_factors:
            logger.info(f"  • {factor}")
    else:
        logger.info("No significant risk factors identified")

    logger.info("\n=== Debt Analysis Complete ===")


if __name__ == "__main__":
    main()



================================================================================
File: examples/scripts/sector_examples/realistic_banking_analysis.py
================================================================================

"""Realistic Banking Analysis Example.

This example demonstrates a realistic banking analysis workflow using the
financial statement model library. It includes:

1. Loading real-world banking data from multiple sources
2. Building a comprehensive banking financial model
3. Calculating key banking metrics and ratios
4. Performing regulatory capital analysis
5. Generating risk-adjusted performance metrics
6. Creating professional banking reports and visualizations

The example uses realistic data structures and calculations that mirror
actual banking analysis practices.
"""

import logging
from datetime import datetime
from pathlib import Path
from typing import Any

import matplotlib.pyplot as plt

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.metrics import (
    calculate_metric,
    interpret_metric,
    metric_registry,
)
from fin_statement_model.io.validation import UnifiedNodeValidator
from fin_statement_model.io import read_data, write_data

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


def create_realistic_banking_data() -> dict[str, dict[str, float]]:
    """Create realistic banking financial data based on a mid-sized regional bank.

    Returns:
        Dictionary of financial data with node names as keys and period values as sub-dict
    """
    return {
        # Balance Sheet - Assets (in millions)
        "cash_and_due_from_banks": {
            "2021": 2_450.5,
            "2022": 2_875.3,
            "2023": 3_125.8,
        },
        "interest_bearing_deposits": {
            "2021": 1_250.0,
            "2022": 1_500.0,
            "2023": 1_750.0,
        },
        "federal_funds_sold": {
            "2021": 500.0,
            "2022": 450.0,
            "2023": 400.0,
        },
        "securities_available_for_sale": {
            "2021": 12_500.0,
            "2022": 13_250.0,
            "2023": 13_750.0,
        },
        "securities_held_to_maturity": {
            "2021": 5_000.0,
            "2022": 5_250.0,
            "2023": 5_500.0,
        },
        "gross_loans": {
            "2021": 48_750.0,
            "2022": 52_500.0,
            "2023": 56_250.0,
        },
        "commercial_loans": {
            "2021": 20_000.0,
            "2022": 21_500.0,
            "2023": 23_000.0,
        },
        "commercial_real_estate_loans": {
            "2021": 15_000.0,
            "2022": 16_250.0,
            "2023": 17_500.0,
        },
        "residential_mortgages": {
            "2021": 10_000.0,
            "2022": 10_750.0,
            "2023": 11_500.0,
        },
        "consumer_loans": {
            "2021": 3_750.0,
            "2022": 4_000.0,
            "2023": 4_250.0,
        },
        "allowance_for_loan_losses": {
            "2021": 731.25,  # 1.5% of gross loans
            "2022": 840.00,  # 1.6% of gross loans
            "2023": 956.25,  # 1.7% of gross loans
        },
        "premises_and_equipment": {
            "2021": 850.0,
            "2022": 900.0,
            "2023": 950.0,
        },
        "other_assets": {
            "2021": 2_150.0,
            "2022": 2_300.0,
            "2023": 2_450.0,
        },
        # Balance Sheet - Liabilities
        "non_interest_bearing_deposits": {
            "2021": 12_500.0,
            "2022": 13_750.0,
            "2023": 15_000.0,
        },
        "demand_deposits": {
            "2021": 15_000.0,
            "2022": 16_500.0,
            "2023": 18_000.0,
        },
        "savings_deposits": {
            "2021": 20_000.0,
            "2022": 21_750.0,
            "2023": 23_500.0,
        },
        "time_deposits": {
            "2021": 10_000.0,
            "2022": 10_500.0,
            "2023": 11_000.0,
        },
        "federal_funds_purchased": {
            "2021": 1_000.0,
            "2022": 750.0,
            "2023": 500.0,
        },
        "short_term_borrowings": {
            "2021": 2_500.0,
            "2022": 2_250.0,
            "2023": 2_000.0,
        },
        "long_term_debt": {
            "2021": 5_000.0,
            "2022": 5_250.0,
            "2023": 5_500.0,
        },
        "other_liabilities": {
            "2021": 1_250.0,
            "2022": 1_375.0,
            "2023": 1_500.0,
        },
        # Balance Sheet - Equity
        "common_stock": {
            "2021": 500.0,
            "2022": 500.0,
            "2023": 500.0,
        },
        "additional_paid_in_capital": {
            "2021": 2_000.0,
            "2022": 2_000.0,
            "2023": 2_000.0,
        },
        "retained_earnings": {
            "2021": 4_268.75,
            "2022": 4_759.55,
            "2023": 5_318.33,
        },
        "accumulated_other_comprehensive_income": {
            "2021": -250.0,
            "2022": -300.0,
            "2023": -275.0,
        },
        # Income Statement (annual)
        "interest_income_loans": {
            "2021": 2_437.50,  # ~5% yield
            "2022": 2_887.50,  # ~5.5% yield
            "2023": 3_375.00,  # ~6% yield
        },
        "interest_income_securities": {
            "2021": 437.50,  # ~2.5% yield
            "2022": 472.50,  # ~2.5% yield
            "2023": 507.50,  # ~2.6% yield
        },
        "interest_income_other": {
            "2021": 50.0,
            "2022": 60.0,
            "2023": 70.0,
        },
        "interest_expense_deposits": {
            "2021": 450.00,  # ~1% cost
            "2022": 731.25,  # ~1.5% cost
            "2023": 1_050.00,  # ~2% cost
        },
        "interest_expense_borrowings": {
            "2021": 175.00,  # ~2% cost
            "2022": 200.00,  # ~2.5% cost
            "2023": 240.00,  # ~3% cost
        },
        "provision_for_credit_losses": {
            "2021": 250.0,
            "2022": 350.0,
            "2023": 450.0,
        },
        "non_interest_income": {
            "2021": 875.0,
            "2022": 950.0,
            "2023": 1_025.0,
        },
        "service_charges": {
            "2021": 350.0,
            "2022": 375.0,
            "2023": 400.0,
        },
        "trading_income": {
            "2021": 125.0,
            "2022": 150.0,
            "2023": 175.0,
        },
        "mortgage_banking_income": {
            "2021": 200.0,
            "2022": 225.0,
            "2023": 250.0,
        },
        "other_non_interest_income": {
            "2021": 200.0,
            "2022": 200.0,
            "2023": 200.0,
        },
        "non_interest_expense": {
            "2021": 1_750.0,
            "2022": 1_875.0,
            "2023": 2_000.0,
        },
        "salaries_and_benefits": {
            "2021": 1_050.0,
            "2022": 1_125.0,
            "2023": 1_200.0,
        },
        "occupancy_expense": {
            "2021": 175.0,
            "2022": 187.5,
            "2023": 200.0,
        },
        "technology_expense": {
            "2021": 262.5,
            "2022": 281.25,
            "2023": 300.0,
        },
        "other_operating_expense": {
            "2021": 262.5,
            "2022": 281.25,
            "2023": 300.0,
        },
        "income_tax_expense": {
            "2021": 225.0,
            "2022": 264.75,
            "2023": 317.5,
        },
        # Regulatory Capital Components
        "common_equity_tier_1": {
            "2021": 6_268.75,
            "2022": 6_709.55,
            "2023": 7_268.33,
        },
        "additional_tier_1_capital": {
            "2021": 500.0,
            "2022": 500.0,
            "2023": 500.0,
        },
        "tier_2_capital": {
            "2021": 731.25,
            "2022": 840.0,
            "2023": 956.25,
        },
        "total_risk_weighted_assets": {
            "2021": 52_500.0,
            "2022": 56_250.0,
            "2023": 60_000.0,
        },
        # Asset Quality Metrics
        "non_performing_loans": {
            "2021": 487.50,  # 1% of gross loans
            "2022": 577.50,  # 1.1% of gross loans
            "2023": 675.00,  # 1.2% of gross loans
        },
        "net_charge_offs": {
            "2021": 195.0,  # 0.4% of average loans
            "2022": 241.5,  # 0.46% of average loans
            "2023": 292.5,  # 0.52% of average loans
        },
        "loans_30_89_days_past_due": {
            "2021": 243.75,
            "2022": 262.50,
            "2023": 281.25,
        },
        "loans_90_plus_days_past_due": {
            "2021": 146.25,
            "2022": 157.50,
            "2023": 168.75,
        },
        # Liquidity Metrics
        "high_quality_liquid_assets": {
            "2021": 16_200.5,
            "2022": 18_075.3,
            "2023": 19_525.8,
        },
        "net_cash_outflows_30_days": {
            "2021": 13_500.0,
            "2022": 14_625.0,
            "2023": 15_750.0,
        },
        "available_stable_funding": {
            "2021": 58_750.0,
            "2022": 63_125.0,
            "2023": 67_500.0,
        },
        "required_stable_funding": {
            "2021": 48_958.33,
            "2022": 52_604.17,
            "2023": 56_250.0,
        },
        # Additional items for calculations
        "average_total_assets": {
            "2021": 71_250.0,
            "2022": 75_625.0,
            "2023": 80_000.0,
        },
        "average_earning_assets": {
            "2021": 65_000.0,
            "2022": 69_062.5,
            "2023": 73_125.0,
        },
        "average_total_loans": {
            "2021": 47_500.0,
            "2022": 50_625.0,
            "2023": 54_375.0,
        },
        "average_total_deposits": {
            "2021": 56_250.0,
            "2022": 60_625.0,
            "2023": 65_000.0,
        },
        "average_total_equity": {
            "2021": 6_393.75,
            "2022": 6_734.55,
            "2023": 7_271.58,
        },
    }


def validate_banking_data(
    raw_data: dict[str, Any],
) -> tuple[dict[str, Any], dict[str, str]]:
    """Validate and standardize banking-specific node names.

    Args:
        raw_data: Dictionary with potentially non-standard node names

    Returns:
        Tuple of (standardized_data, validation_report)
    """
    print("\n" + "=" * 60)
    print("BANKING DATA VALIDATION AND STANDARDIZATION")
    print("=" * 60)

    # Demo only: direct use of UnifiedNodeValidator; production code should use StatementConfig/StatementStructureBuilder for validation
    # Create validator with banking-specific settings (demo)
    validator = UnifiedNodeValidator(
        strict_mode=False,
        auto_standardize=True,
        warn_on_non_standard=True,
        enable_patterns=True,
    )

    # Common banking abbreviations to test
    banking_abbreviations = {
        "npl": "non_performing_loans",
        "rwa": "total_risk_weighted_assets",
        "nii": "net_interest_income",
        "nim": "net_interest_margin",
        "cet1": "common_equity_tier_1",
        "lcr": "liquidity_coverage_ratio",
        "nsfr": "net_stable_funding_ratio",
        "all": "allowance_for_loan_losses",
    }

    print("Testing common banking abbreviations:")
    for abbrev, expected in banking_abbreviations.items():
        result = validator.validate(abbrev)
        print(f"  '{abbrev}' → '{result.standardized_name}' (expected: '{expected}')")

    # Validate all node names
    validation_results = validator.validate_batch(list(raw_data.keys()))

    # Create standardized data dictionary
    standardized_data = {}
    validation_report = {
        "total": len(raw_data),
        "valid": 0,
        "standardized": 0,
        "invalid": 0,
        "banking_specific": 0,
        "mappings": {},
    }

    print("\nValidation Results:")
    for original_name, result in validation_results.items():
        if result.is_valid:
            validation_report["valid"] += 1
            if original_name != result.standardized_name:
                validation_report["standardized"] += 1
                print(
                    f"  ✓ '{original_name}' → '{result.standardized_name}' (standardized)"
                )

            # Check if it's a banking-specific metric
            if any(
                term in original_name
                for term in ["loan", "deposit", "capital", "tier", "risk_weighted"]
            ):
                validation_report["banking_specific"] += 1

            standardized_data[result.standardized_name] = raw_data[original_name]
            validation_report["mappings"][original_name] = result.standardized_name
        else:
            validation_report["invalid"] += 1
            print(f"  ✗ '{original_name}' - {result.message}")

    # Print summary
    print("\nValidation Summary:")
    print(f"  Total nodes: {validation_report['total']}")
    print(f"  Valid: {validation_report['valid']}")
    print(f"  Banking-specific: {validation_report['banking_specific']}")
    print(f"  Standardized: {validation_report['standardized']}")
    print(f"  Invalid: {validation_report['invalid']}")

    return standardized_data, validation_report


def build_banking_graph(data: dict[str, dict[str, float]]) -> Graph:
    """Build a comprehensive banking financial graph.

    Args:
        data: Dictionary of financial data

    Returns:
        Graph object with all nodes and relationships
    """
    print("\n" + "=" * 60)
    print("BUILDING BANKING FINANCIAL GRAPH")
    print("=" * 60)

    # Create graph from data
    graph = read_data(format_type="dict", source=data)
    print(f"Created graph with {len(graph.nodes)} nodes")
    print(f"Time periods: {graph.periods}")

    # Categorize nodes
    node_categories = {
        "assets": [],
        "liabilities": [],
        "equity": [],
        "income": [],
        "expense": [],
        "regulatory": [],
        "quality": [],
        "liquidity": [],
    }

    for node_name in graph.nodes:
        if any(
            term in node_name for term in ["loan", "securities", "cash", "premises"]
        ):
            node_categories["assets"].append(node_name)
        elif any(term in node_name for term in ["deposit", "borrowing", "debt"]):
            node_categories["liabilities"].append(node_name)
        elif any(
            term in node_name for term in ["stock", "capital", "retained", "equity"]
        ):
            if "tier" in node_name or "risk_weighted" in node_name:
                node_categories["regulatory"].append(node_name)
            else:
                node_categories["equity"].append(node_name)
        elif "income" in node_name and "expense" not in node_name:
            node_categories["income"].append(node_name)
        elif "expense" in node_name:
            node_categories["expense"].append(node_name)
        elif any(
            term in node_name for term in ["non_performing", "charge_off", "past_due"]
        ):
            node_categories["quality"].append(node_name)
        elif any(term in node_name for term in ["liquid", "outflow", "funding"]):
            node_categories["liquidity"].append(node_name)

    print("\nNode Categories:")
    for category, nodes in node_categories.items():
        if nodes:
            print(f"  {category.capitalize()}: {len(nodes)} nodes")

    return graph


def calculate_banking_metrics(graph: Graph) -> dict[str, dict[str, Any]]:
    """Calculate comprehensive banking metrics for all periods.

    Args:
        graph: Banking financial graph

    Returns:
        Dictionary of metrics organized by category and period
    """
    print("\n" + "=" * 60)
    print("CALCULATING BANKING METRICS")
    print("=" * 60)

    metrics = {
        "capital_adequacy": {},
        "asset_quality": {},
        "management_efficiency": {},
        "earnings": {},
        "liquidity": {},
        "sensitivity": {},
    }

    # Get data nodes for metric calculation
    data_nodes = {node.name: node for node in graph.nodes.values()}

    # Define CAMELS metrics
    camels_metrics = {
        "capital_adequacy": [
            "common_equity_tier_1_ratio",
            "tier_1_capital_ratio",
            "total_capital_ratio",
            "tier_1_leverage_ratio",
        ],
        "asset_quality": [
            "non_performing_loan_ratio",
            "provision_coverage_ratio",
            "net_charge_off_rate",
            "allowance_to_loans_ratio",
        ],
        "management_efficiency": [
            "efficiency_ratio",
            "operating_expense_ratio",
            "cost_to_income_ratio",
        ],
        "earnings": [
            "return_on_assets_(banking)",
            "return_on_equity_(banking)",
            "net_interest_margin",
            "fee_income_ratio",
        ],
        "liquidity": [
            "liquidity_coverage_ratio",
            "net_stable_funding_ratio",
            "loan_to_deposit_ratio",
            "liquid_assets_ratio",
        ],
        "sensitivity": [
            "securities_to_assets_ratio",
            "interest_rate_sensitivity_ratio",
        ],
    }

    # Calculate metrics for each period
    for period in graph.periods:
        print(f"\nCalculating metrics for {period}:")

        for category, metric_list in camels_metrics.items():
            metrics[category][period] = {}

            for metric_name in metric_list:
                try:
                    value = calculate_metric(metric_name, data_nodes, period)
                    interpretation = interpret_metric(
                        metric_registry.get(metric_name), value
                    )

                    metrics[category][period][metric_name] = {
                        "value": value,
                        "interpretation": interpretation,
                    }

                    # Print key metrics
                    if metric_name in [
                        "common_equity_tier_1_ratio",
                        "non_performing_loan_ratio",
                        "return_on_equity_(banking)",
                        "net_interest_margin",
                        "liquidity_coverage_ratio",
                    ]:
                        print(
                            f"  {metric_name}: {value:.2f}% - {interpretation['rating']}"
                        )

                except Exception as e:
                    logger.warning(
                        f"Could not calculate {metric_name} for {period}: {e}"
                    )

    return metrics


def perform_stress_testing(
    graph: Graph, metrics: dict[str, dict[str, Any]]
) -> dict[str, Any]:
    """Perform basic stress testing on the bank's financial position.

    Args:
        graph: Banking financial graph
        metrics: Calculated banking metrics

    Returns:
        Dictionary containing stress test results
    """
    print("\n" + "=" * 60)
    print("STRESS TESTING ANALYSIS")
    print("=" * 60)

    latest_period = graph.periods[-1]
    data_nodes = {node.name: node for node in graph.nodes.values()}

    stress_results = {
        "baseline": {},
        "adverse": {},
        "severely_adverse": {},
    }

    # Define stress scenarios
    scenarios = {
        "baseline": {
            "loan_loss_rate": 0.01,  # 1% loss rate
            "deposit_runoff": 0.05,  # 5% deposit runoff
            "securities_loss": 0.02,  # 2% securities loss
        },
        "adverse": {
            "loan_loss_rate": 0.03,  # 3% loss rate
            "deposit_runoff": 0.15,  # 15% deposit runoff
            "securities_loss": 0.05,  # 5% securities loss
        },
        "severely_adverse": {
            "loan_loss_rate": 0.05,  # 5% loss rate
            "deposit_runoff": 0.25,  # 25% deposit runoff
            "securities_loss": 0.10,  # 10% securities loss
        },
    }

    print("Stress Test Scenarios:")

    for scenario_name, scenario_params in scenarios.items():
        print(f"\n{scenario_name.upper()} Scenario:")

        # Get baseline values
        if all(
            node in data_nodes
            for node in [
                "gross_loans",
                "total_deposits",
                "securities_available_for_sale",
            ]
        ):
            gross_loans = data_nodes["gross_loans"].get_value(latest_period)
            total_deposits = data_nodes["total_deposits"].get_value(latest_period)
            securities = data_nodes["securities_available_for_sale"].get_value(
                latest_period
            )

            # Calculate stressed values
            loan_losses = gross_loans * scenario_params["loan_loss_rate"]
            deposit_outflows = total_deposits * scenario_params["deposit_runoff"]
            securities_losses = securities * scenario_params["securities_loss"]

            total_losses = loan_losses + securities_losses

            # Calculate impact on capital
            if "common_equity_tier_1" in data_nodes:
                cet1_capital = data_nodes["common_equity_tier_1"].get_value(
                    latest_period
                )
                stressed_cet1 = cet1_capital - total_losses

                if "total_risk_weighted_assets" in data_nodes:
                    rwa = data_nodes["total_risk_weighted_assets"].get_value(
                        latest_period
                    )
                    stressed_cet1_ratio = (stressed_cet1 / rwa) * 100

                    stress_results[scenario_name] = {
                        "loan_losses": loan_losses,
                        "securities_losses": securities_losses,
                        "total_losses": total_losses,
                        "deposit_outflows": deposit_outflows,
                        "stressed_cet1_capital": stressed_cet1,
                        "stressed_cet1_ratio": stressed_cet1_ratio,
                        "capital_buffer": stressed_cet1_ratio
                        - 4.5,  # Minimum CET1 requirement
                    }

                    print(f"  Loan Losses: ${loan_losses:,.1f}M")
                    print(f"  Securities Losses: ${securities_losses:,.1f}M")
                    print(f"  Total Losses: ${total_losses:,.1f}M")
                    print(f"  Deposit Outflows: ${deposit_outflows:,.1f}M")
                    print(f"  Stressed CET1 Ratio: {stressed_cet1_ratio:.2f}%")
                    print(f"  Capital Buffer: {stressed_cet1_ratio - 4.5:.2f}%")

                    if stressed_cet1_ratio < 4.5:
                        print("  ⚠️  WARNING: Below minimum regulatory requirement!")
                    elif stressed_cet1_ratio < 7.0:
                        print("  ⚠️  WARNING: Below well-capitalized threshold!")

    return stress_results


def create_banking_visualizations(
    graph: Graph,
    metrics: dict[str, dict[str, Any]],
    stress_results: dict[str, Any],
    output_dir: Path,
) -> None:
    """Create professional banking analysis visualizations.

    Args:
        graph: Banking financial graph
        metrics: Calculated banking metrics
        stress_results: Stress testing results
        output_dir: Directory to save visualizations
    """
    print("\n" + "=" * 60)
    print("CREATING BANKING VISUALIZATIONS")
    print("=" * 60)

    output_dir.mkdir(exist_ok=True)

    # 1. CAMELS Rating Dashboard
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle("CAMELS Rating Dashboard", fontsize=16, fontweight="bold")

    camels_components = [
        ("Capital Adequacy", "capital_adequacy", axes[0, 0]),
        ("Asset Quality", "asset_quality", axes[0, 1]),
        ("Management", "management_efficiency", axes[0, 2]),
        ("Earnings", "earnings", axes[1, 0]),
        ("Liquidity", "liquidity", axes[1, 1]),
        ("Sensitivity", "sensitivity", axes[1, 2]),
    ]

    periods = graph.periods

    for title, category, ax in camels_components:
        if category in metrics and periods[-1] in metrics[category]:
            # Get a key metric for each category
            key_metrics = {
                "capital_adequacy": "common_equity_tier_1_ratio",
                "asset_quality": "non_performing_loan_ratio",
                "management_efficiency": "efficiency_ratio",
                "earnings": "return_on_equity_(banking)",
                "liquidity": "liquidity_coverage_ratio",
                "sensitivity": "securities_to_assets_ratio",
            }

            metric_name = key_metrics.get(category)
            if metric_name:
                values = []
                for period in periods:
                    if (
                        period in metrics[category]
                        and metric_name in metrics[category][period]
                    ):
                        values.append(metrics[category][period][metric_name]["value"])
                    else:
                        values.append(None)

                # Filter out None values
                valid_periods = [p for p, v in zip(periods, values) if v is not None]
                valid_values = [v for v in values if v is not None]

                if valid_values:
                    ax.plot(
                        valid_periods,
                        valid_values,
                        marker="o",
                        linewidth=2,
                        markersize=8,
                    )
                    ax.set_title(title, fontweight="bold")
                    ax.set_ylabel(f"{metric_name.replace('_', ' ').title()} (%)")
                    ax.grid(True, alpha=0.3)

                    # Add rating zones
                    if "ratio" in metric_name and "capital" in metric_name:
                        ax.axhline(
                            y=10.5,
                            color="green",
                            linestyle="--",
                            alpha=0.5,
                            label="Well Capitalized",
                        )
                        ax.axhline(
                            y=8.0,
                            color="orange",
                            linestyle="--",
                            alpha=0.5,
                            label="Adequately Capitalized",
                        )
                        ax.axhline(
                            y=6.0,
                            color="red",
                            linestyle="--",
                            alpha=0.5,
                            label="Undercapitalized",
                        )

    plt.tight_layout()
    plt.savefig(output_dir / "camels_dashboard.png", dpi=300, bbox_inches="tight")
    plt.close()
    print("  ✓ Created CAMELS rating dashboard")

    # 2. Stress Test Results
    if stress_results:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
        fig.suptitle("Stress Test Results", fontsize=14, fontweight="bold")

        scenarios = list(stress_results.keys())
        cet1_ratios = [
            stress_results[s].get("stressed_cet1_ratio", 0) for s in scenarios
        ]
        total_losses = [stress_results[s].get("total_losses", 0) for s in scenarios]

        # CET1 Ratio under stress
        colors = ["green", "orange", "red"]
        bars1 = ax1.bar(scenarios, cet1_ratios, color=colors, alpha=0.7)
        ax1.axhline(y=4.5, color="red", linestyle="--", label="Minimum Requirement")
        ax1.axhline(y=7.0, color="orange", linestyle="--", label="Well Capitalized")
        ax1.set_ylabel("CET1 Ratio (%)")
        ax1.set_title("Capital Ratios Under Stress")
        ax1.legend()
        ax1.set_ylim(0, max(cet1_ratios) * 1.2)

        # Add value labels on bars
        for bar, value in zip(bars1, cet1_ratios):
            height = bar.get_height()
            ax1.text(
                bar.get_x() + bar.get_width() / 2.0,
                height,
                f"{value:.1f}%",
                ha="center",
                va="bottom",
            )

        # Total losses
        bars2 = ax2.bar(scenarios, total_losses, color=colors, alpha=0.7)
        ax2.set_ylabel("Total Losses ($M)")
        ax2.set_title("Projected Losses by Scenario")
        ax2.set_ylim(0, max(total_losses) * 1.2)

        # Add value labels
        for bar, value in zip(bars2, total_losses):
            height = bar.get_height()
            ax2.text(
                bar.get_x() + bar.get_width() / 2.0,
                height,
                f"${value:,.0f}M",
                ha="center",
                va="bottom",
            )

        plt.tight_layout()
        plt.savefig(
            output_dir / "stress_test_results.png", dpi=300, bbox_inches="tight"
        )
        plt.close()
        print("  ✓ Created stress test visualization")

    # 3. Trend Analysis
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle("Banking Performance Trends", fontsize=14, fontweight="bold")

    # Asset growth
    if "total_assets" in graph.nodes:
        asset_node = graph.get_node("average_total_assets")
        if asset_node:
            assets = [asset_node.get_value(p) / 1000 for p in periods]  # In billions
            ax1.plot(
                periods, assets, marker="o", linewidth=2, markersize=8, color="blue"
            )
            ax1.set_title("Total Assets Growth")
            ax1.set_ylabel("Assets ($B)")
            ax1.grid(True, alpha=0.3)

    # Net Interest Margin trend
    nim_values = [
        metrics["earnings"][period]["net_interest_margin"]["value"]
        for period in periods
        if (
            "earnings" in metrics
            and period in metrics["earnings"]
            and "net_interest_margin" in metrics["earnings"][period]
        )
    ]

    if nim_values:
        ax2.plot(
            periods[: len(nim_values)],
            nim_values,
            marker="o",
            linewidth=2,
            markersize=8,
            color="green",
        )
        ax2.set_title("Net Interest Margin Trend")
        ax2.set_ylabel("NIM (%)")
        ax2.grid(True, alpha=0.3)

    # Asset Quality trend
    npl_values = [
        metrics["asset_quality"][period]["non_performing_loan_ratio"]["value"]
        for period in periods
        if (
            "asset_quality" in metrics
            and period in metrics["asset_quality"]
            and "non_performing_loan_ratio" in metrics["asset_quality"][period]
        )
    ]

    if npl_values:
        ax3.plot(
            periods[: len(npl_values)],
            npl_values,
            marker="o",
            linewidth=2,
            markersize=8,
            color="red",
        )
        ax3.set_title("Non-Performing Loans Trend")
        ax3.set_ylabel("NPL Ratio (%)")
        ax3.grid(True, alpha=0.3)
        ax3.invert_yaxis()  # Lower is better

    # Efficiency Ratio trend
    eff_values = [
        metrics["management_efficiency"][period]["efficiency_ratio"]["value"]
        for period in periods
        if (
            "management_efficiency" in metrics
            and period in metrics["management_efficiency"]
            and "efficiency_ratio" in metrics["management_efficiency"][period]
        )
    ]

    if eff_values:
        ax4.plot(
            periods[: len(eff_values)],
            eff_values,
            marker="o",
            linewidth=2,
            markersize=8,
            color="orange",
        )
        ax4.set_title("Efficiency Ratio Trend")
        ax4.set_ylabel("Efficiency Ratio (%)")
        ax4.grid(True, alpha=0.3)
        ax4.invert_yaxis()  # Lower is better

    plt.tight_layout()
    plt.savefig(output_dir / "performance_trends.png", dpi=300, bbox_inches="tight")
    plt.close()
    print("  ✓ Created performance trends visualization")


def generate_banking_report(
    graph: Graph,
    metrics: dict[str, dict[str, Any]],
    stress_results: dict[str, Any],
    validation_report: dict[str, Any],
    output_dir: Path,
) -> None:
    """Generate a comprehensive banking analysis report.

    Args:
        graph: Banking financial graph
        metrics: Calculated banking metrics
        stress_results: Stress testing results
        validation_report: Data validation report
        output_dir: Directory to save report
    """
    print("\n" + "=" * 60)
    print("GENERATING BANKING ANALYSIS REPORT")
    print("=" * 60)

    output_dir.mkdir(exist_ok=True)

    # Create markdown report
    report_path = output_dir / "banking_analysis_report.md"

    with open(report_path, "w") as f:
        f.write("# Banking Financial Analysis Report\n\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # Executive Summary
        f.write("## Executive Summary\n\n")

        latest_period = graph.periods[-1]

        # Key metrics summary
        if (
            "capital_adequacy" in metrics
            and latest_period in metrics["capital_adequacy"]
            and "common_equity_tier_1_ratio"
            in metrics["capital_adequacy"][latest_period]
        ):
            cet1 = metrics["capital_adequacy"][latest_period][
                "common_equity_tier_1_ratio"
            ]
            f.write(
                f"- **CET1 Ratio**: {cet1['value']:.2f}% ({cet1['interpretation']['rating']})\n"
            )

        if (
            "asset_quality" in metrics
            and latest_period in metrics["asset_quality"]
            and "non_performing_loan_ratio" in metrics["asset_quality"][latest_period]
        ):
            npl = metrics["asset_quality"][latest_period]["non_performing_loan_ratio"]
            f.write(
                f"- **NPL Ratio**: {npl['value']:.2f}% ({npl['interpretation']['rating']})\n"
            )

        if "earnings" in metrics and latest_period in metrics["earnings"]:
            if "return_on_equity_(banking)" in metrics["earnings"][latest_period]:
                roe = metrics["earnings"][latest_period]["return_on_equity_(banking)"]
                f.write(
                    f"- **ROE**: {roe['value']:.2f}% ({roe['interpretation']['rating']})\n"
                )

            if "net_interest_margin" in metrics["earnings"][latest_period]:
                nim = metrics["earnings"][latest_period]["net_interest_margin"]
                f.write(
                    f"- **Net Interest Margin**: {nim['value']:.2f}% ({nim['interpretation']['rating']})\n"
                )

        if (
            "liquidity" in metrics
            and latest_period in metrics["liquidity"]
            and "liquidity_coverage_ratio" in metrics["liquidity"][latest_period]
        ):
            lcr = metrics["liquidity"][latest_period]["liquidity_coverage_ratio"]
            f.write(
                f"- **LCR**: {lcr['value']:.2f}% ({lcr['interpretation']['rating']})\n"
            )

        # CAMELS Assessment
        f.write("\n## CAMELS Assessment\n\n")

        camels_categories = [
            ("Capital Adequacy", "capital_adequacy"),
            ("Asset Quality", "asset_quality"),
            ("Management", "management_efficiency"),
            ("Earnings", "earnings"),
            ("Liquidity", "liquidity"),
            ("Sensitivity", "sensitivity"),
        ]

        for category_name, category_key in camels_categories:
            f.write(f"\n### {category_name}\n\n")

            if category_key in metrics and latest_period in metrics[category_key]:
                f.write("| Metric | Value | Rating | Interpretation |\n")
                f.write("|--------|-------|--------|----------------|\n")

                for metric_name, metric_data in metrics[category_key][
                    latest_period
                ].items():
                    if "value" in metric_data and "interpretation" in metric_data:
                        value = metric_data["value"]
                        rating = metric_data["interpretation"]["rating"]
                        message = metric_data["interpretation"][
                            "interpretation_message"
                        ]

                        # Truncate long messages
                        if len(message) > 50:
                            message = message[:47] + "..."

                        metric_display = metric_name.replace("_", " ").title()
                        f.write(
                            f"| {metric_display} | {value:.2f}% | {rating} | {message} |\n"
                        )

        # Stress Testing Results
        f.write("\n## Stress Testing Results\n\n")

        if stress_results:
            f.write(
                "| Scenario | Total Losses | Stressed CET1 Ratio | Capital Buffer | Status |\n"
            )
            f.write(
                "|----------|--------------|---------------------|----------------|--------|\n"
            )

            for scenario_name, results in stress_results.items():
                if "total_losses" in results:
                    losses = results["total_losses"]
                    cet1_ratio = results.get("stressed_cet1_ratio", 0)
                    buffer = results.get("capital_buffer", 0)

                    status = (
                        "✅ Pass"
                        if cet1_ratio >= 7.0
                        else "⚠️ Watch" if cet1_ratio >= 4.5 else "❌ Fail"
                    )

                    f.write(
                        f"| {scenario_name.title()} | ${losses:,.0f}M | {cet1_ratio:.2f}% | {buffer:.2f}% | {status} |\n"
                    )

        # Risk Assessment
        f.write("\n## Risk Assessment\n\n")

        # Credit Risk
        f.write("### Credit Risk\n")
        if "asset_quality" in metrics and latest_period in metrics["asset_quality"]:
            npl_data = metrics["asset_quality"][latest_period].get(
                "non_performing_loan_ratio", {}
            )
            if npl_data:
                npl_value = npl_data.get("value", 0)
                if npl_value < 1.0:
                    f.write("- **Low Credit Risk**: NPL ratio below 1%\n")
                elif npl_value < 2.0:
                    f.write("- **Moderate Credit Risk**: NPL ratio between 1-2%\n")
                else:
                    f.write("- **Elevated Credit Risk**: NPL ratio above 2%\n")

        # Interest Rate Risk
        f.write("\n### Interest Rate Risk\n")
        data_nodes = {node.name: node for node in graph.nodes.values()}
        if all(
            node in data_nodes
            for node in ["interest_income_loans", "average_earning_assets"]
        ):
            int_income = data_nodes["interest_income_loans"].get_value(latest_period)
            avg_assets = data_nodes["average_earning_assets"].get_value(latest_period)
            asset_yield = (int_income / avg_assets) * 100 if avg_assets > 0 else 0
            f.write(f"- **Asset Yield**: {asset_yield:.2f}%\n")
            f.write("- **Rate Sensitivity**: Monitor for rising rate environment\n")

        # Liquidity Risk
        f.write("\n### Liquidity Risk\n")
        if "liquidity" in metrics and latest_period in metrics["liquidity"]:
            lcr_data = metrics["liquidity"][latest_period].get(
                "liquidity_coverage_ratio", {}
            )
            if lcr_data and "value" in lcr_data:
                lcr_value = lcr_data["value"]
                if lcr_value >= 100:
                    f.write("- **Low Liquidity Risk**: LCR above regulatory minimum\n")
                else:
                    f.write("- **Elevated Liquidity Risk**: LCR below 100%\n")

        # Data Quality
        f.write("\n## Data Quality Assessment\n\n")
        f.write(f"- **Total Nodes Validated**: {validation_report['total']}\n")
        f.write(
            f"- **Banking-Specific Nodes**: {validation_report['banking_specific']}\n"
        )
        f.write(
            f"- **Data Completeness**: {(validation_report['valid'] / validation_report['total'] * 100):.1f}%\n"
        )

        # Recommendations
        f.write("\n## Recommendations\n\n")

        recommendations = []

        # Check capital adequacy
        if (
            "capital_adequacy" in metrics
            and latest_period in metrics["capital_adequacy"]
        ):
            cet1_data = metrics["capital_adequacy"][latest_period].get(
                "common_equity_tier_1_ratio", {}
            )
            if cet1_data and "value" in cet1_data and cet1_data["value"] < 9.0:
                recommendations.append("Consider building additional capital buffers")

        # Check asset quality
        if "asset_quality" in metrics and latest_period in metrics["asset_quality"]:
            npl_data = metrics["asset_quality"][latest_period].get(
                "non_performing_loan_ratio", {}
            )
            if npl_data and "value" in npl_data and npl_data["value"] > 2.0:
                recommendations.append(
                    "Enhance credit underwriting and monitoring processes"
                )

        # Check efficiency
        if (
            "management_efficiency" in metrics
            and latest_period in metrics["management_efficiency"]
        ):
            eff_data = metrics["management_efficiency"][latest_period].get(
                "efficiency_ratio", {}
            )
            if eff_data and "value" in eff_data and eff_data["value"] > 60.0:
                recommendations.append("Focus on operational efficiency improvements")

        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                f.write(f"{i}. {rec}\n")
        else:
            f.write("The bank appears to be well-positioned across all key metrics.\n")

        f.write("\n---\n")
        f.write(
            "*This report was generated using the Financial Statement Model library*\n"
        )

    print(f"  ✓ Created banking analysis report: {report_path}")

    # Export to Excel
    excel_path = output_dir / "banking_financial_data.xlsx"
    write_data(
        format_type="excel",
        graph=graph,
        target=str(excel_path),
    )
    print(f"  ✓ Exported data to Excel: {excel_path}")


def main():
    """Run the realistic banking analysis example."""
    print("=" * 60)
    print("REALISTIC BANKING ANALYSIS EXAMPLE")
    print("=" * 60)
    print("Demonstrating comprehensive banking analysis using the")
    print("Financial Statement Model library with realistic data")

    # Create output directory
    output_dir = Path("examples/output/banking_analysis")
    output_dir.mkdir(parents=True, exist_ok=True)

    # Step 1: Create and validate banking data
    raw_data = create_realistic_banking_data()
    standardized_data, validation_report = validate_banking_data(raw_data)

    # Step 2: Build banking financial graph
    graph = build_banking_graph(standardized_data)

    # Step 3: Calculate comprehensive banking metrics
    metrics = calculate_banking_metrics(graph)

    # Step 4: Perform stress testing
    stress_results = perform_stress_testing(graph, metrics)

    # Step 5: Create visualizations
    create_banking_visualizations(graph, metrics, stress_results, output_dir)

    # Step 6: Generate comprehensive report
    generate_banking_report(
        graph, metrics, stress_results, validation_report, output_dir
    )

    print("\n" + "=" * 60)
    print("ANALYSIS COMPLETE")
    print("=" * 60)
    print(f"\nResults saved to: {output_dir}")
    print("\nKey Features Demonstrated:")
    print("1. Banking-specific data validation and standardization")
    print("2. Comprehensive CAMELS framework analysis")
    print("3. Regulatory capital calculations (Basel III)")
    print("4. Stress testing with multiple scenarios")
    print("5. Professional visualizations and reporting")
    print("6. Risk assessment across multiple dimensions")
    print("\nThis example shows how the Financial Statement Model library")
    print("can be used for sophisticated banking analysis and regulatory reporting.")


if __name__ == "__main__":
    main()



================================================================================
File: examples/scripts/sector_examples/simple_banking_graph_example.py
================================================================================

"""Simple Banking Analysis Example.

This example demonstrates:
1. Node name validation for banking-specific terms
2. Building a basic banking graph with validated nodes
3. Creating a banking-specific statement configuration
4. Analyzing key banking metrics with proper interpretation
5. Using centralized configuration for formatting and forecasting
"""

import logging

from fin_statement_model import get_config, update_config
from fin_statement_model.core.graph import Graph
from fin_statement_model.core.metrics import (
    metric_registry,
    interpret_metric,
    calculate_metric,
)
from fin_statement_model.io.validation import UnifiedNodeValidator
from fin_statement_model.statements import create_statement_dataframe

# Get configuration
config = get_config()

# Configure logging
logger = logging.getLogger(__name__)

# Banking-specific configuration overrides
banking_config = {
    "display": {
        "default_units": "USD Millions",
        "scale_factor": 0.000001,  # Convert to millions
        "default_currency_format": ",.1f",
    },
    "validation": {
        "strict_mode": True,  # Banks need strict validation
        "warn_on_non_standard": True,
    },
}
update_config(banking_config)


def step_1_validate_node_names() -> dict[str, str]:
    """Step 1: Validate banking-specific node names.

    Returns:
        Dictionary mapping original names to standardized names
    """
    logger.info("=" * 60)
    logger.info("STEP 1: NODE NAME VALIDATION")
    logger.info("=" * 60)

    # Demo only: direct use of UnifiedNodeValidator; production code should use StatementConfig/StatementStructureBuilder for validation
    # Create validator using config settings (demo)
    validator = UnifiedNodeValidator(
        strict_mode=config.validation.strict_mode,
        auto_standardize=config.validation.auto_standardize_names,
    )

    # Common banking terms that need validation
    banking_terms = [
        # Standard banking items
        "interest_income",
        "interest_expense",
        "net_interest_income",
        "non_interest_income",
        "loan_loss_provision",
        # Assets
        "loans_and_advances",
        "securities_portfolio",
        "cash_and_due_from_banks",
        # Liabilities
        "customer_deposits",
        "wholesale_funding",
        "subordinated_debt",
        # Regulatory metrics
        "tier_1_capital",
        "risk_weighted_assets",
        "common_equity_tier_1",
    ]

    logger.info("Validating banking terminology...")
    mappings = {}

    for term in banking_terms:
        result = validator.validate(term)
        mappings[term] = result.standardized_name
        if result.is_valid:
            logger.info(f"✓ {term} -> {result.standardized_name}")
        else:
            logger.warning(f"✗ {term}: {result.message}")

    return mappings


def create_banking_statement_config() -> str:
    """Create a YAML configuration for a banking income statement."""
    config_yaml = """
id: banking_income_statement
name: Banking Income Statement
description: Standard income statement structure for banks

sections:
  - id: interest_income_section
    name: Interest Income
    items:
      - type: line_item
        id: loans_interest
        name: Interest on Loans
        node_id: interest_income_loans

      - type: line_item
        id: securities_interest
        name: Interest on Securities
        node_id: interest_income_securities

      - type: calculated
        id: total_interest_income
        name: Total Interest Income
        calculation:
          type: addition
          inputs: ["loans_interest", "securities_interest"]

  - id: interest_expense_section
    name: Interest Expense
    items:
      - type: line_item
        id: deposits_interest
        name: Interest on Deposits
        node_id: interest_expense_deposits
        sign_convention: -1

      - type: line_item
        id: borrowings_interest
        name: Interest on Borrowings
        node_id: interest_expense_borrowings
        sign_convention: -1

      - type: calculated
        id: total_interest_expense
        name: Total Interest Expense
        calculation:
          type: addition
          inputs: ["deposits_interest", "borrowings_interest"]

  - id: net_interest_section
    name: Net Interest Income
    items:
      - type: calculated
        id: net_interest_income
        name: Net Interest Income
        calculation:
          type: addition
          inputs: ["total_interest_income", "total_interest_expense"]

  - id: non_interest_section
    name: Non-Interest Income
    items:
      - type: line_item
        id: fee_income
        name: Fee and Commission Income
        node_id: fee_income

      - type: line_item
        id: trading_income
        name: Trading Income
        node_id: trading_income

      - type: calculated
        id: total_non_interest_income
        name: Total Non-Interest Income
        calculation:
          type: addition
          inputs: ["fee_income", "trading_income"]

  - id: operating_income_section
    name: Operating Income
    items:
      - type: calculated
        id: total_operating_income
        name: Total Operating Income
        calculation:
          type: addition
          inputs: ["net_interest_income", "total_non_interest_income"]

      - type: line_item
        id: loan_loss_provision
        name: Loan Loss Provision
        node_id: loan_loss_provision
        sign_convention: -1

      - type: calculated
        id: operating_income_after_provisions
        name: Operating Income After Provisions
        calculation:
          type: addition
          inputs: ["total_operating_income", "loan_loss_provision"]

  - id: expenses_section
    name: Operating Expenses
    items:
      - type: line_item
        id: staff_costs
        name: Staff Costs
        node_id: staff_costs
        sign_convention: -1

      - type: line_item
        id: other_operating_expenses
        name: Other Operating Expenses
        node_id: other_operating_expenses
        sign_convention: -1

      - type: calculated
        id: total_operating_expenses
        name: Total Operating Expenses
        calculation:
          type: addition
          inputs: ["staff_costs", "other_operating_expenses"]

  - id: net_income_section
    name: Net Income
    items:
      - type: calculated
        id: pre_tax_income
        name: Income Before Tax
        calculation:
          type: addition
          inputs: ["operating_income_after_provisions", "total_operating_expenses"]

      - type: line_item
        id: income_tax
        name: Income Tax
        node_id: income_tax
        sign_convention: -1

      - type: calculated
        id: net_income
        name: Net Income
        calculation:
          type: addition
          inputs: ["pre_tax_income", "income_tax"]
"""
    return config_yaml


def step_2_build_graph() -> tuple[Graph, str]:
    """Step 2: Build a banking graph with sample data.

    Returns:
        Tuple of (Graph, period) for analysis
    """
    logger.info("\n" + "=" * 60)
    logger.info("STEP 2: BUILDING BANKING GRAPH")
    logger.info("=" * 60)

    # Sample banking data (in actual dollars, will be scaled by config)
    banking_data = {
        # Interest Income
        "interest_income_loans": {
            "2021": 8500000000,  # $8.5 billion
            "2022": 9200000000,  # $9.2 billion
            "2023": 10500000000,  # $10.5 billion
        },
        "interest_income_securities": {
            "2021": 1200000000,
            "2022": 1400000000,
            "2023": 1800000000,
        },
        # Interest Expense
        "interest_expense_deposits": {
            "2021": 2100000000,
            "2022": 2800000000,
            "2023": 4200000000,
        },
        "interest_expense_borrowings": {
            "2021": 800000000,
            "2022": 950000000,
            "2023": 1200000000,
        },
        # Non-Interest Income
        "fee_income": {"2021": 3200000000, "2022": 3400000000, "2023": 3600000000},
        "trading_income": {"2021": 1100000000, "2022": 800000000, "2023": 1200000000},
        # Provisions and Expenses
        "loan_loss_provision": {
            "2021": 800000000,
            "2022": 1200000000,
            "2023": 900000000,
        },
        "staff_costs": {"2021": 4500000000, "2022": 4700000000, "2023": 4900000000},
        "other_operating_expenses": {
            "2021": 2800000000,
            "2022": 2900000000,
            "2023": 3100000000,
        },
        "income_tax": {"2021": 1200000000, "2022": 1050000000, "2023": 1400000000},
        # Balance Sheet items for ratios
        "total_assets": {
            "2021": 280000000000,  # $280 billion
            "2022": 295000000000,
            "2023": 310000000000,
        },
        "total_loans": {
            "2021": 180000000000,
            "2022": 190000000000,
            "2023": 205000000000,
        },
        "total_deposits": {
            "2021": 220000000000,
            "2022": 235000000000,
            "2023": 248000000000,
        },
        "shareholders_equity": {
            "2021": 28000000000,
            "2022": 29500000000,
            "2023": 31000000000,
        },
        "tier_1_capital": {
            "2021": 25000000000,
            "2022": 26500000000,
            "2023": 28000000000,
        },
        "risk_weighted_assets": {
            "2021": 195000000000,
            "2022": 205000000000,
            "2023": 215000000000,
        },
    }

    # Collect all periods from the data
    all_periods = set()
    for periods_data in banking_data.values():
        all_periods.update(periods_data.keys())
    # Create graph with periods
    graph = Graph(periods=sorted(all_periods))
    logger.info(f"Created graph with periods: {graph.periods}")

    # Add nodes to graph using validated names
    from fin_statement_model.core.nodes import FinancialStatementItemNode

    logger.info(
        f"Adding banking data nodes (values in {config.display.default_units})..."
    )
    for node_name, periods_data in banking_data.items():
        node = FinancialStatementItemNode(node_name, {})  # Initialize with empty values
        for period, value in periods_data.items():
            node.set_value(period, value)
        graph.add_node(node)

        # Show sample value with configured formatting
        sample_value = periods_data.get("2023", 0)
        scaled_value = sample_value * config.display.scale_factor
        logger.info(
            f"  Added {node_name}: 2023 = {scaled_value:{config.display.default_currency_format}}"
        )

    # Add calculated nodes based on statement config
    from fin_statement_model.core.nodes import CalculationNode
    from fin_statement_model.core.calculations import AdditionCalculation

    # Net Interest Income
    nii_node = CalculationNode(
        "net_interest_income",
        inputs=[
            graph.get_node("interest_income_loans"),
            graph.get_node("interest_income_securities"),
            graph.get_node("interest_expense_deposits"),
            graph.get_node("interest_expense_borrowings"),
        ],
        calculation=AdditionCalculation(),
    )
    graph.add_node(nii_node)

    # Total Operating Income
    operating_income = CalculationNode(
        "total_operating_income",
        inputs=[
            nii_node,  # Use the node object directly
            graph.get_node("fee_income"),
            graph.get_node("trading_income"),
        ],
        calculation=AdditionCalculation(),
    )
    graph.add_node(operating_income)

    # Add more nodes for metric calculations
    # Total Non-Interest Income (alias for fee_income + trading_income)
    total_non_interest_income = CalculationNode(
        "total_non_interest_income",
        inputs=[
            graph.get_node("fee_income"),
            graph.get_node("trading_income"),
        ],
        calculation=AdditionCalculation(),
    )
    graph.add_node(total_non_interest_income)

    # Net Income calculation (simplified)
    # This is very simplified - in reality would need pre-tax income, etc.
    from fin_statement_model.core.calculations import SubtractionCalculation

    # Operating expenses
    total_operating_expenses = CalculationNode(
        "total_operating_expenses",
        inputs=[
            graph.get_node("staff_costs"),
            graph.get_node("other_operating_expenses"),
            graph.get_node("loan_loss_provision"),
        ],
        calculation=AdditionCalculation(),
    )
    graph.add_node(total_operating_expenses)
    # Pre-tax income
    pre_tax_income = CalculationNode(
        "pre_tax_income",
        inputs=[
            operating_income,
            total_operating_expenses,
        ],
        calculation=SubtractionCalculation(),
    )
    graph.add_node(pre_tax_income)
    # Net income
    net_income = CalculationNode(
        "net_income",
        inputs=[
            pre_tax_income,
            graph.get_node("income_tax"),
        ],
        calculation=SubtractionCalculation(),
    )
    graph.add_node(net_income)

    # Add total_equity as a copy of shareholders_equity for metric compatibility
    total_equity = FinancialStatementItemNode("total_equity", {})
    for period, value in banking_data["shareholders_equity"].items():
        total_equity.set_value(period, value)
    graph.add_node(total_equity)

    return graph, "2023"


def step_3_analyze_structure(graph: Graph) -> None:
    """Step 3: Analyze the graph structure."""
    logger.info("\n" + "=" * 60)
    logger.info("STEP 3: ANALYZING GRAPH STRUCTURE")
    logger.info("=" * 60)

    logger.info(f"Total nodes: {len(graph.nodes)}")
    logger.info(f"Periods: {sorted(graph.periods)}")

    # Count node types
    from fin_statement_model.core.nodes import (
        FinancialStatementItemNode,
        CalculationNode,
    )

    item_nodes = sum(
        1 for n in graph.nodes.values() if isinstance(n, FinancialStatementItemNode)
    )
    calc_nodes = sum(1 for n in graph.nodes.values() if isinstance(n, CalculationNode))

    logger.info("\nNode types:")
    logger.info(f"  Item nodes: {item_nodes}")
    logger.info(f"  Calculation nodes: {calc_nodes}")

    # Show dependencies
    logger.info("\nKey calculations:")
    for node in graph.nodes.values():
        if isinstance(node, CalculationNode):
            input_names = [input_node.name for input_node in node.inputs]
            logger.info(f"  {node.name} = f({', '.join(input_names)})")


def analyze_key_banking_metrics(graph: Graph, period: str = "2023") -> None:
    """Analyze key banking metrics using the metrics registry."""
    logger.info("\n" + "=" * 60)
    logger.info("STEP 4: ANALYZING KEY BANKING METRICS")
    logger.info("=" * 60)

    # Key banking metrics to calculate
    banking_metrics = [
        "net_interest_margin",
        "efficiency_ratio",
        "cost_to_income_ratio",
        "return_on_assets",
        "return_on_equity",
        "tier_1_capital_ratio",
        "loan_to_deposit_ratio",
    ]

    logger.info(f"Calculating metrics for {period}...")
    logger.info(f"Values displayed in {config.display.default_units}\n")

    # Get all nodes from graph as a dictionary
    data_nodes = graph.nodes

    for metric_name in banking_metrics:
        try:
            # Check if metric exists
            if metric_name not in metric_registry:
                logger.warning(f"⚠ {metric_name}: Not in registry")
                continue

            # Get metric definition
            metric_def = metric_registry.get(metric_name)

            # Try to calculate value using calculate_metric
            try:
                value = calculate_metric(metric_name, data_nodes, period)
            except ValueError as ve:
                # Log missing inputs and skip this metric
                logger.info(f"Cannot calculate {metric_def.name}: {ve}")
                continue

            # Format based on metric type
            formatted_value = (
                f"{value * 100:.1f}%"
                if getattr(metric_def, "units", None) == "%"
                else f"{value:.2f}"
            )

            # Interpret the metric value
            interpretation = interpret_metric(metric_def, value)

            # Display results
            logger.info(f"{metric_def.name}:")
            logger.info(f"  Value: {formatted_value}")
            # Handle interpretation as dict or object
            if isinstance(interpretation, dict):
                logger.info(f"  Rating: {interpretation.get('rating', 'N/A')}")
                logger.info(
                    f"  Interpretation: {interpretation.get('message', 'No interpretation available')}"
                )
                if interpretation.get("peer_comparison"):
                    logger.info(
                        f"  Peer Comparison: {interpretation['peer_comparison']}"
                    )
            else:
                logger.info(f"  Rating: {interpretation.rating}")
                logger.info(f"  Interpretation: {interpretation.message}")
                if interpretation.peer_comparison:
                    logger.info(f"  Peer Comparison: {interpretation.peer_comparison}")

            logger.info("")

        except Exception:
            logger.exception(f"❌ {metric_name}")


def explore_metrics_registry() -> None:
    """Explore available banking metrics in the registry."""
    logger.info("\n" + "=" * 60)
    logger.info("STEP 5: EXPLORING METRICS REGISTRY")
    logger.info("=" * 60)

    # Get all metrics
    all_metrics = metric_registry.list_metrics()

    # Filter banking-specific metrics
    banking_keywords = ["bank", "loan", "deposit", "tier", "capital", "interest"]
    banking_metrics = [
        m
        for m in all_metrics
        if any(keyword in m.lower() for keyword in banking_keywords)
    ]

    logger.info(f"Found {len(banking_metrics)} banking-related metrics:")
    for metric_name in sorted(banking_metrics):
        try:
            metric_def = metric_registry.get(metric_name)
            logger.info(f"  • {metric_def.name} ({metric_name})")
            if metric_def.description:
                logger.info(f"    {metric_def.description}")
        except Exception:
            logger.info(f"  • {metric_name} (definition error)")


def main() -> None:
    """Run the complete banking analysis example."""
    # Configure logging to ensure we see INFO messages
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    logger.info("=" * 80)
    logger.info("SIMPLE BANKING ANALYSIS EXAMPLE")
    logger.info("=" * 80)
    logger.info("\nUsing configuration:")
    logger.info(f"  Units: {config.display.default_units}")
    logger.info(f"  Format: {config.display.default_currency_format}")
    logger.info(
        f"  Validation: {'Strict' if config.validation.strict_mode else 'Flexible'}"
    )

    # Step 1: Validate node names
    step_1_validate_node_names()

    # Step 2: Build graph
    graph, analysis_period = step_2_build_graph()

    # Step 3: Analyze structure
    step_3_analyze_structure(graph)

    # Step 4: Calculate metrics
    analyze_key_banking_metrics(graph, analysis_period)

    # Step 5: Explore registry
    explore_metrics_registry()

    # Show statement creation
    logger.info("\n" + "=" * 60)
    logger.info("CREATING FORMATTED STATEMENT")
    logger.info("=" * 60)

    try:
        # Build in-memory banking statement config
        import yaml

        config_yaml = create_banking_statement_config()
        raw_config = yaml.safe_load(config_yaml)
        stmt_id = raw_config.get("id", "banking_statement")
        raw_configs = {stmt_id: raw_config}

        # Create statement using in-memory config
        statement_df_map = create_statement_dataframe(
            graph=graph,
            raw_configs=raw_configs,
            format_kwargs={
                "number_format": config.display.default_currency_format,
                "should_apply_signs": True,
                "include_empty_items": not config.display.hide_zero_rows,
            },
        )
        statement_df = statement_df_map[stmt_id]

        logger.info("Banking Income Statement:")
        logger.info(statement_df.to_string(index=False))

    except Exception:
        logger.exception("Could not create statement")

    logger.info("\n" + "=" * 80)
    logger.info("EXAMPLE COMPLETE")
    logger.info("=" * 80)
    logger.info("\nKey Takeaways:")
    logger.info("• Node validation ensures consistent naming across the model")
    logger.info("• Banking metrics have specific interpretations and thresholds")
    logger.info("• Statement configurations define presentation structure")
    logger.info("• The metrics registry provides comprehensive analysis tools")
    logger.info("• Centralized config controls formatting, validation, and display")


if __name__ == "__main__":
    main()



================================================================================
File: examples/scripts/validation_examples.py
================================================================================

"""Examples demonstrating the UnifiedNodeValidator's capabilities.

This example shows how validation settings are controlled through
the centralized configuration system.

AUTOMATIC CASE HANDLING:
The validator now automatically handles case variations for standard node names.
All standard names are stored in lowercase, but the validator will recognize
any case variation:
  - "Revenue", "REVENUE", "revenue" → all recognized as standard "revenue"
  - "COGS", "cogs", "Cogs" → all map to "cost_of_goods_sold"
  - "EBITDA", "ebitda", "Ebitda" → all recognized as standard "ebitda"

Note: Names with spaces still need preprocessing (replace spaces with underscores).

"""

import logging
from fin_statement_model.config import get_config, update_config
from fin_statement_model.io.validation import UnifiedNodeValidator

# Get configuration
config = get_config()

# Configure logging using centralized config
logger = logging.getLogger(__name__)


def example_basic_validation():
    """Show basic node name validation using config settings."""
    logger.info("=== Basic Validation Example ===")

    # Demo only: production code should use StatementConfig/StatementStructureBuilder for validation
    validator = (
        UnifiedNodeValidator()
    )  # Demo only: demonstrates direct validator instantiation

    # The validator now uses these config defaults internally:
    # - config.validation.strict_mode
    # - config.validation.auto_standardize_names
    # - config.validation.warn_on_non_standard

    logger.info("Validation Config (from centralized config):")
    logger.info(f"  Strict Mode: {validator.strict_mode}")
    logger.info(f"  Auto Standardize: {validator.auto_standardize}")
    logger.info(f"  Warn on Non-Standard: {validator.warn_on_non_standard}")

    # Test various node names
    # The validator now handles case automatically!
    test_names = [
        "revenue",  # Standard financial item
        "Revenue",  # Now recognized as standard!
        "accounts_receivable",  # Standard name (with underscore)
        "Accounts Receivable",  # Still needs space replacement
        "ebitda",  # Standard calculated metric
        "core.ebitda",  # Namespaced metric (custom)
        "custom_metric",  # Custom node
        "revenue_growth_yoy",  # Pattern-based metric
        "revenue_q1",  # Sub-node pattern
        "invalid name!",  # Invalid characters
    ]

    for name in test_names:
        result = validator.validate(name)
        logger.info(f"\nNode: '{name}'")
        logger.info(f"  Standardized: '{result.standardized_name}'")
        logger.info(f"  Valid: {result.is_valid}")
        logger.info(f"  Category: {result.category}")
        logger.info(f"  Message: {result.message}")
        if result.suggestions:
            logger.info(f"  Suggestions: {result.suggestions}")

    # Batch validation
    logger.info("\n--- Batch Validation ---")
    results = validator.validate_batch(test_names)
    for name, result in results.items():
        logger.info(f"{name}: {result.category} - {result.standardized_name}")


def example_case_sensitivity_handling():
    """Show automatic case handling in action."""
    logger.info("\n=== Automatic Case Handling ===")

    # Demo only: production code should use StatementConfig/StatementStructureBuilder for validation
    validator = (
        UnifiedNodeValidator()
    )  # Demo only: demonstrates direct validator instantiation
    # Test cases showing automatic case handling
    test_cases = [
        ("revenue", "Standard lowercase"),
        ("Revenue", "Title case"),
        ("REVENUE", "Uppercase"),
        ("cost_of_goods_sold", "Standard with underscores"),
        ("Cost of Goods Sold", "Title case with spaces"),
        ("COGS", "Common abbreviation uppercase"),
        ("cogs", "Common abbreviation lowercase"),
    ]
    logger.info("Automatic case normalization:")
    for name, description in test_cases:
        result = validator.validate(name)
        logger.info(
            f"{name} ({description}): {result.category} -> '{result.standardized_name}'"
        )
    logger.info("\n--- Names with spaces still need preprocessing ---")
    logger.info("Example: 'Cost of Goods Sold' needs to become 'cost_of_goods_sold'")
    # Show preprocessing for names with spaces
    name_with_spaces = "Cost of Goods Sold"
    preprocessed = name_with_spaces.replace(" ", "_")
    result = validator.validate(preprocessed)
    logger.info(
        f"'{name_with_spaces}' -> '{preprocessed}' -> '{result.standardized_name}' ({result.category})"
    )


def example_context_aware_validation():
    """Show context-aware validation with node types and relationships."""
    logger.info("\n=== Context-Aware Validation Example ===")

    # Demo only: production code should use StatementConfig/StatementStructureBuilder for validation
    validator = UnifiedNodeValidator(
        enable_patterns=True,
        strict_mode=config.validation.strict_mode,
    )  # Demo only: demonstrates direct validator instantiation

    # Test nodes with context
    test_cases = [
        ("revenue", "data", []),
        ("gross_profit", "calculation", ["revenue", "cost_of_goods_sold"]),
        ("revenue_growth_yoy", "metric", ["revenue"]),
        ("custom_analysis", "custom", []),
    ]

    for name, node_type, parent_nodes in test_cases:
        result = validator.validate(
            name, node_type=node_type, parent_nodes=parent_nodes
        )
        logger.info(f"\nNode: '{name}' (Type: {node_type})")
        logger.info(f"  Parents: {parent_nodes}")
        logger.info(f"  Category: {result.category}")
        logger.info(f"  Valid: {result.is_valid}")
        logger.info(f"  Message: {result.message}")


def example_graph_building():
    """Show validation during graph building with config settings."""
    logger.info("\n=== Graph Building with Validation ===")

    # Demo only: production code should use StatementConfig/StatementStructureBuilder for validation
    validator = UnifiedNodeValidator(
        auto_standardize=config.validation.auto_standardize_names,
        warn_on_non_standard=config.validation.warn_on_non_standard,
        strict_mode=config.validation.strict_mode,
    )  # Demo only: demonstrates direct validator instantiation

    from fin_statement_model.core.graph import Graph
    from fin_statement_model.core.nodes import FinancialStatementItemNode

    graph = Graph()

    # Simulate adding nodes with validation
    # Mixed case works automatically now!
    node_data = {
        "Revenue": {"2023": 1000},
        "COGS": {"2023": 600},
        "gross profit": {"2023": 400},
        "Operating Expenses": {"2023": 200},
    }

    for name, values in node_data.items():
        # For names with spaces, we still need to replace them
        preprocessed_name = name.replace(" ", "_")
        result = validator.validate(preprocessed_name)

        if result.is_valid:
            if result.standardized_name != preprocessed_name:
                logger.info(f"Standardized '{name}' -> '{result.standardized_name}'")

            # Add node with standardized name
            node_name = (
                result.standardized_name
                if config.validation.auto_standardize_names
                else name
            )
            node = FinancialStatementItemNode(node_name, values)
            graph.add_node(node)
        elif config.validation.strict_mode:
            logger.error(f"Invalid node name '{name}' - {result.message}")
            # In strict mode, skip invalid nodes
            continue
        else:
            logger.warning(f"Warning: '{name}' is invalid - {result.message}")
            # In non-strict mode, add anyway
            node = FinancialStatementItemNode(name, values)
            graph.add_node(node)

    # Generate validation report
    all_node_names = [node.name for node in graph.nodes.values()]
    results = validator.validate_batch(all_node_names)
    # Create report from results
    report = {
        "total": len(results),
        "by_validity": {"valid": 0, "invalid": 0},
        "by_category": {},
        "suggestions": {},
    }
    for name, result in results.items():
        # Count by validity
        if result.is_valid:
            report["by_validity"]["valid"] += 1
        else:
            report["by_validity"]["invalid"] += 1
        # Count by category
        if result.category not in report["by_category"]:
            report["by_category"][result.category] = []
        report["by_category"][result.category].append(name)
        # Collect suggestions
        if result.suggestions:
            report["suggestions"][name] = result.suggestions

    logger.info("\n--- Graph Validation Report ---")
    logger.info(f"Total nodes: {report['total']}")
    logger.info(f"Valid: {report['by_validity']['valid']}")
    logger.info(f"Invalid: {report['by_validity']['invalid']}")

    logger.info("\nBy category:")
    for category, nodes in report["by_category"].items():
        logger.info(f"  {category}: {len(nodes)} nodes")

    if report["suggestions"]:
        logger.info("\nSuggestions for improvement:")
        for node_name, suggestions in report["suggestions"].items():
            logger.info(f"  {node_name}:")
            for suggestion in suggestions:
                logger.info(f"    - {suggestion}")


def example_flexible_vs_strict():
    """Show difference between flexible and strict validation modes using config."""
    logger.info("\n=== Strict vs Flexible Validation ===")

    # Demo only: production code should use StatementConfig/StatementStructureBuilder for validation
    strict_validator = UnifiedNodeValidator(
        strict_mode=config.validation.strict_mode
    )  # Demo only
    # Demo only: production code should use StatementConfig/StatementStructureBuilder for validation
    flexible_validator = UnifiedNodeValidator(
        strict_mode=config.validation.strict_mode
    )  # Demo only

    # Save current config
    current_strict = config.validation.strict_mode

    # Test with strict mode
    update_config({"validation": {"strict_mode": True}})
    strict_validator = UnifiedNodeValidator(strict_mode=config.validation.strict_mode)

    # Test with flexible mode
    update_config({"validation": {"strict_mode": False}})
    flexible_validator = UnifiedNodeValidator(strict_mode=config.validation.strict_mode)

    test_names = ["revenue_2023_q1", "custom_metric", "My Special Node"]

    logger.info("Strict Mode Results:")
    for name in test_names:
        result = strict_validator.validate(name)
        logger.info(f"  {name}: Valid={result.is_valid}, Category={result.category}")

    logger.info("\nFlexible Mode Results:")
    for name in test_names:
        result = flexible_validator.validate(name)
        logger.info(f"  {name}: Valid={result.is_valid}, Category={result.category}")

    # Restore original config
    update_config({"validation": {"strict_mode": current_strict}})


def example_excel_reader_integration():
    """Show how validation integrates with Excel reading using config."""
    logger.info("\n=== Reader Integration Example ===")

    # Demo only: production code should use StatementConfig/StatementStructureBuilder for validation
    validator = UnifiedNodeValidator(
        auto_standardize=config.io.auto_standardize_columns,
        enable_patterns=True,
    )  # Demo only: demonstrates direct validator instantiation

    # Simulate Excel column headers (often in mixed case)
    excel_headers = [
        "Revenue",
        "Cost of Goods Sold",
        "Operating Income",
        "EBITDA",
        "Accounts Receivable",
        "PP&E",
    ]

    # Map Excel headers to standardized node names
    mapping = {}
    for excel_name in excel_headers:
        # Only need to handle spaces and special characters now
        # Case is handled automatically!
        normalized = excel_name.replace(" ", "_").replace("&", "_and_")
        result = validator.validate(normalized)
        if result.is_valid:
            mapping[excel_name] = result.standardized_name
            if normalized != result.standardized_name:
                logger.info(f"Mapped '{excel_name}' -> '{result.standardized_name}'")
        elif config.io.strict_validation:
            logger.error(f"Invalid column name '{excel_name}' - {result.message}")
        else:
            logger.warning(
                f"No mapping found for '{excel_name}', using normalized: '{normalized}'"
            )
            mapping[excel_name] = normalized

    logger.info(f"\nFinal mapping: {mapping}")

    # Show how config affects behavior
    logger.info("\nIO Validation Config:")
    logger.info(f"  Strict Validation: {config.io.strict_validation}")
    logger.info(f"  Auto Standardize: {config.io.auto_standardize_columns}")
    logger.info(f"  Skip Invalid: {config.io.skip_invalid_rows}")


def demonstrate_config_override():
    """Show how to temporarily override validation config."""
    logger.info("\n=== Config Override Example ===")

    # Show current config
    logger.info("Current Validation Config:")
    logger.info(f"  Strict Mode: {config.validation.strict_mode}")
    logger.info(f"  Balance Check: {config.validation.check_balance_sheet_balance}")

    # Temporarily override for a specific operation
    with_balance_check = {
        "validation": {"check_balance_sheet_balance": True, "balance_tolerance": 0.01}
    }

    logger.info("\nTemporarily enabling balance sheet validation...")
    update_config(with_balance_check)

    logger.info("Updated Config:")
    logger.info(f"  Balance Check: {config.validation.check_balance_sheet_balance}")
    logger.info(f"  Tolerance: {config.validation.balance_tolerance}")

    # Perform validation with updated config
    # ... validation logic here ...

    # Note: In practice, you'd want to restore the original config
    # or use a context manager for temporary changes


if __name__ == "__main__":
    # Show initial config source
    logger.info(
        f"Configuration loaded from: {getattr(config, '_source', 'defaults')}\n"
    )

    # Check if standard nodes are loaded
    from fin_statement_model.core.nodes import standard_node_registry

    logger.info(f"Standard nodes loaded: {len(standard_node_registry)}")
    if len(standard_node_registry) == 0:
        logger.warning("No standard nodes loaded! The registry is empty.")
    else:
        # Show a few examples
        sample_names = list(standard_node_registry.list_standard_names())[:5]
        logger.info(f"Sample standard node names: {sample_names}")

    logger.info("\n" + "=" * 60 + "\n")

    example_basic_validation()
    example_case_sensitivity_handling()
    example_context_aware_validation()
    example_graph_building()
    example_flexible_vs_strict()
    example_excel_reader_integration()
    demonstrate_config_override()



================================================================================
File: fin_statement_model/__init__.py
================================================================================

"""Financial Statement Model library.

A comprehensive library for building and analyzing financial statement models
using a node-based graph structure.
"""

# Import key components at package level for easier access
from fin_statement_model.core.errors import FinancialModelError
from fin_statement_model.core.graph import Graph
from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.core.nodes import (
    CalculationNode,
    CustomGrowthForecastNode,
    CurveGrowthForecastNode,
    FinancialStatementItemNode,
    FixedGrowthForecastNode,
    ForecastNode,
    MultiPeriodStatNode,
    Node,
    StatisticalGrowthForecastNode,
    YoYGrowthNode,
)

# Import configuration management
from fin_statement_model.config import get_config, update_config, reset_config

# ensure our library-wide logging policy is applied immediately
from . import logging_config  # noqa: F401

__version__ = "0.1.0"

__all__ = [
    "CalculationNode",
    "CurveGrowthForecastNode",
    "CustomGrowthForecastNode",
    "FinancialModelError",
    "FinancialStatementItemNode",
    "FixedGrowthForecastNode",
    "ForecastNode",
    "Graph",
    "MultiPeriodStatNode",
    "Node",
    "NodeFactory",
    "StatisticalGrowthForecastNode",
    "YoYGrowthNode",
    "__version__",
    "get_config",
    "reset_config",
    "update_config",
]

# Core API Exports (ensure essential classes/functions are accessible)
# Example:
# from .core.graph import Graph
# from .core.nodes import Node, FinancialStatementItemNode
# from .core.calculation_engine import CalculationEngine
# from .statements.manager import StatementManager

# Placeholder: Explicitly list key public API components later.
# For now, just rely on sub-package __init__ files if they exist.



================================================================================
File: fin_statement_model/config/__init__.py
================================================================================

"""Centralized configuration management for fin_statement_model.

This module provides a unified interface for managing all library configurations,
including defaults, user overrides, and environment variables.

Example:
    >>> from fin_statement_model.config import get_config, update_config
    >>>
    >>> # Get current configuration
    >>> config = get_config()
    >>> print(config.logging.level)
    >>>
    >>> # Update configuration
    >>> update_config({
    ...     'forecasting': {
    ...         'default_method': 'historical_growth',
    ...         'default_periods': 5
    ...     }
    ... })
"""

from .helpers import cfg, cfg_or_param, get_typed_config

# Importing helpers first ensures that "cfg" is available early, avoiding
# circular import issues when other modules import fin_statement_model.config
# during the initialization of sub-packages (e.g., io.formats.api.fmp).

from .models import Config
from .manager import get_config, update_config, reset_config

__all__ = [
    "Config",
    "cfg",
    "cfg_or_param",
    "get_config",
    "get_typed_config",
    "reset_config",
    "update_config",
]



================================================================================
File: fin_statement_model/config/helpers.py
================================================================================

"""Utility functions for accessing configuration values."""

from __future__ import annotations
from typing import Any, Optional, TypeVar, overload
from collections.abc import Sequence
from fin_statement_model.core.errors import FinancialModelError


class ConfigurationAccessError(FinancialModelError):
    """Raised when there's an error accessing configuration values."""


T = TypeVar("T")


@overload
def cfg(path: str) -> Any: ...


@overload
def cfg(path: str, default: T) -> T: ...


@overload
def cfg(path: Sequence[str]) -> Any: ...


@overload
def cfg(path: Sequence[str], default: T) -> T: ...


def cfg(path: str | Sequence[str], default: Any = None) -> Any:
    """Get a configuration value by dotted path."""
    from .manager import get_config

    # Convert string path to sequence
    if isinstance(path, str):
        if not path:
            raise ConfigurationAccessError("Configuration path cannot be empty")
        parts = path.split(".")
    else:
        parts = list(path)

    if not parts:
        raise ConfigurationAccessError("Configuration path cannot be empty")

    obj = get_config()
    for i, part in enumerate(parts):
        full_path = ".".join(parts[: i + 1])
        if not hasattr(obj, part):
            if default is not None:
                return default
            raise ConfigurationAccessError(
                f"Configuration key '{full_path}' does not exist"
            )
        obj = getattr(obj, part)
    return obj if obj is not None else default


def get_typed_config(
    path: str | Sequence[str], expected_type: type[T], default: Optional[T] = None
) -> T:
    """Get a configuration value with type checking."""
    value = cfg(path, default)
    if value is None and default is None:
        raise ConfigurationAccessError(
            f"Configuration key '{path}' is None and no default provided"
        )
    if not isinstance(value, expected_type):
        raise TypeError(
            f"Configuration key '{path}' has type {type(value).__name__}, "
            f"expected {expected_type.__name__}"
        )
    return value


def cfg_or_param(config_path: str, param_value: Any) -> Any:
    """Return the parameter value if provided, otherwise get from config."""
    return param_value if param_value is not None else cfg(config_path)


def parse_env_value(value: str) -> bool | int | float | str:
    """Parse an environment variable string into bool, int, float, or str."""
    val = value.strip()
    low = val.lower()
    # Boolean
    if low in ("true", "false"):
        return low == "true"
    # Integer (including negatives)
    if (val.startswith("-") and val[1:].isdigit()) or val.isdigit():
        try:
            return int(val)
        except ValueError:
            pass
    # Float
    try:
        float_val = float(val)
        if "." in val or "e" in low or "E" in value:
            return float_val
    except ValueError:
        pass
    # Fallback to string
    return val



================================================================================
File: fin_statement_model/config/manager.py
================================================================================

"""Configuration manager for fin_statement_model.

This module provides the ConfigManager class that handles loading configurations
from multiple sources and merging them according to precedence rules.
"""

import os
from pathlib import Path
from typing import Any, Optional, Union, get_origin, get_args, cast
import logging
from threading import Lock
from pydantic import BaseModel

from .models import Config
from fin_statement_model.core.errors import FinancialModelError

logger = logging.getLogger(__name__)


# ----------------------------------------------------------------------
# Environment variable mapping utility
def generate_env_mappings(
    model: type[BaseModel], prefix: Optional[str] = None, path: list[str] | None = None
) -> dict[str, list[str]]:
    """Generate environment variable mappings for a Pydantic model."""
    from .manager import ConfigManager

    if prefix is None:
        prefix = ConfigManager.ENV_PREFIX.rstrip("_")
    if path is None:
        path = []

    mappings: dict[str, list[str]] = {}
    for field_name, field in model.model_fields.items():
        # Field annotation gives the declared type
        annotation = field.annotation
        origin = get_origin(annotation)
        # Handle Optional or Union types
        if origin is Union:
            args = get_args(annotation)
            nested = next(
                (
                    arg
                    for arg in args
                    if isinstance(arg, type) and issubclass(arg, BaseModel)
                ),
                None,
            )
            if nested:
                mappings.update(
                    generate_env_mappings(nested, prefix, [*path, field_name])
                )
                continue
        # Nested BaseModel
        if isinstance(annotation, type) and issubclass(annotation, BaseModel):
            mappings.update(
                generate_env_mappings(annotation, prefix, [*path, field_name])
            )
        else:
            leaf_path = [*path, field_name]
            env_name = prefix + "_" + "_".join(p.upper() for p in leaf_path)
            mappings[env_name] = leaf_path

    logger.debug(f"Generated {len(mappings)} environment variable mappings")
    return mappings


# ----------------------------------------------------------------------


class ConfigurationError(FinancialModelError):
    """Exception raised for configuration-related errors."""


class ConfigManager:
    """Manages configuration loading and merging from multiple sources.

    Configuration precedence (highest to lowest):
    1. Runtime updates via update_config()
    2. Environment variables (FSM_* prefix)
    3. User config file (fsm_config.yaml in current directory or specified path)
    4. Project config file (.fsm_config.yaml in project root)
    5. Default configuration

    Example:
        >>> config = ConfigManager()
        >>> config.get().logging.level
        'WARNING'
        >>> config.update({'logging': {'level': 'DEBUG'}})
        >>> config.get().logging.level
        'DEBUG'
    """

    # Environment variable prefix
    ENV_PREFIX = "FSM_"

    # Default config file names
    USER_CONFIG_FILE = "fsm_config.yaml"
    PROJECT_CONFIG_FILE = ".fsm_config.yaml"

    def __init__(self, config_file: Optional[Path] = None):
        """Initialize the configuration manager.

        Args:
            config_file: Optional path to a configuration file.
                        If not provided, searches for default config files.
        """
        self._lock = Lock()
        self._config: Optional[Config] = None
        self._runtime_overrides: dict[str, Any] = {}
        self._config_file = config_file

    def get(self) -> Config:
        """Get the current configuration.

        Returns:
            The merged configuration object.
        """
        with self._lock:
            if self._config is None:
                self._load_config()
            assert self._config is not None
            return self._config

    def update(self, updates: dict[str, Any]) -> None:
        """Update configuration with runtime values.

        Args:
            updates: Dictionary of configuration updates.
                    Can be nested, e.g., {'logging': {'level': 'DEBUG'}}
        """
        with self._lock:
            self._runtime_overrides = self._deep_merge(self._runtime_overrides, updates)
            self._config = None  # Force reload on next get()

    def reset(self) -> None:
        """Reset configuration to defaults."""
        with self._lock:
            self._runtime_overrides = {}
            self._config = None

    def _load_config(self) -> None:
        """Load and merge configuration from all sources."""
        # Load environment variables from a .env file (if present) before any
        # configuration layers are processed. This allows users to keep secrets
        # like API keys in a `.env` file without explicitly exporting them in
        # the shell. Values already present in the process environment are NOT
        # overwritten.
        self._load_dotenv()

        # Start with defaults
        config_dict = Config(
            project_name="fin_statement_model",
            config_file_path=None,
            auto_save_config=False,
        ).to_dict()

        # Layer 1: Project config file
        project_config = self._find_project_config()
        if project_config:
            logger.debug(f"Loading project config from {project_config}")
            config_dict = self._deep_merge(config_dict, self._load_file(project_config))

        # Layer 2: User config file
        user_config = self._find_user_config()
        if user_config:
            logger.debug(f"Loading user config from {user_config}")
            config_dict = self._deep_merge(config_dict, self._load_file(user_config))

        # Layer 3: Environment variables
        env_config = self._load_from_env()
        if env_config:
            logger.debug("Loading config from environment variables")
            config_dict = self._deep_merge(config_dict, env_config)

        # Layer 4: Runtime overrides
        if self._runtime_overrides:
            logger.debug("Applying runtime overrides")
            config_dict = self._deep_merge(config_dict, self._runtime_overrides)

        # Create and validate final config
        self._config = Config.from_dict(config_dict)

        # Apply logging configuration immediately
        self._apply_logging_config()

    def _find_project_config(self) -> Optional[Path]:
        """Find project-level configuration file."""
        # Look for .fsm_config.yaml in parent directories
        current = Path.cwd()
        while current != current.parent:
            config_path = current / self.PROJECT_CONFIG_FILE
            if config_path.exists():
                return config_path
            current = current.parent
        return None

    def _find_user_config(self) -> Optional[Path]:
        """Find user configuration file."""
        if self._config_file and self._config_file.exists():
            return self._config_file

        # Check current directory
        user_config = Path.cwd() / self.USER_CONFIG_FILE
        if user_config.exists():
            return user_config

        # Check home directory
        home_config = Path.home() / f".{self.USER_CONFIG_FILE}"
        if home_config.exists():
            return home_config

        return None

    def _load_file(self, path: Path) -> dict[str, Any]:
        """Load configuration from file."""
        try:
            if path.suffix in [".yaml", ".yml"]:
                import yaml

                result = yaml.safe_load(path.read_text()) or {}
                return cast(dict[str, Any], result)
            elif path.suffix == ".json":
                import json

                result = json.loads(path.read_text())
                return cast(dict[str, Any], result)
            else:
                raise ConfigurationError(
                    f"Unsupported config file format: {path.suffix}"
                )
        except Exception as e:
            raise ConfigurationError(f"Failed to load config from {path}: {e}") from e

    def _load_from_env(self) -> dict[str, Any]:
        """Load configuration from environment variables.

        Environment variables are mapped to config paths:
        FSM_LOGGING_LEVEL -> logging.level
        FSM_IO_DEFAULT_EXCEL_SHEET -> io.default_excel_sheet
        FSM_API_FMP_API_KEY -> api.fmp_api_key

        Mappings are now generated dynamically from the Config model.
        """
        config: dict[str, Any] = {}

        # Generate mappings dynamically from the Config model
        env_mappings = generate_env_mappings(Config)
        # Import helper to parse raw environment variable values
        from .helpers import parse_env_value

        for env_key, config_path in env_mappings.items():
            if env_key in os.environ:
                # Parse the raw environment variable string into proper type
                raw_value = os.environ[env_key]
                value = parse_env_value(raw_value)

                # Build nested dictionary with the parsed value
                current = config
                for key in config_path[:-1]:
                    if key not in current:
                        current[key] = {}
                    current = current[key]
                current[config_path[-1]] = value

        return config

    def _deep_merge(
        self, base: dict[str, Any], update: dict[str, Any]
    ) -> dict[str, Any]:
        """Deep merge two dictionaries."""
        result = base.copy()

        for key, value in update.items():
            if (
                key in result
                and isinstance(result[key], dict)
                and isinstance(value, dict)
            ):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value

        return result

    def _apply_logging_config(self) -> None:
        """Apply logging configuration to the library."""
        if self._config:
            from fin_statement_model import logging_config

            logging_config.setup_logging(
                level=self._config.logging.level,
                format_string=self._config.logging.format,
                detailed=self._config.logging.detailed,
                log_file_path=(
                    str(self._config.logging.log_file_path)
                    if self._config.logging.log_file_path
                    else None
                ),
            )

    # ------------------------------------------------------------------
    # .env loading utilities

    def _load_dotenv(self) -> None:
        """Populate os.environ with variables from a ``.env`` file, if found.

        The search starts in the current working directory and ascends parent
        directories until it reaches the filesystem root (similar to how Git
        discovers its repository root). The first ``.env`` file encountered is
        used. Existing environment variables **are not** overwritten—this
        preserves any values the user has explicitly exported.
        """
        try:
            import os
            from pathlib import Path

            current = Path.cwd()
            while True:
                candidate = current / ".env"
                if candidate.exists() and candidate.is_file():
                    try:
                        for raw_line in candidate.read_text().splitlines():
                            line = raw_line.strip()
                            # Skip blanks and comments
                            if not line or line.startswith("#"):
                                continue
                            if "=" not in line:
                                continue
                            key, value = line.split("=", 1)
                            key = key.strip()
                            # Remove any surrounding quotes from the value
                            value = value.strip().strip("'\"")
                            if key and key not in os.environ:
                                os.environ[key] = value
                        logger.debug("Loaded environment variables from %s", candidate)

                        # Special fallback: if a generic FMP_API_KEY is defined, expose it
                        # via the namespaced FSM_API_FMP_API_KEY expected by the Config
                        # model. This provides compatibility with existing environment
                        # setups without forcing users to duplicate variables.
                        if (
                            "FMP_API_KEY" in os.environ
                            and "FSM_API_FMP_API_KEY" not in os.environ
                        ):
                            os.environ["FSM_API_FMP_API_KEY"] = os.environ[
                                "FMP_API_KEY"
                            ]
                            logger.debug(
                                "Mapped FMP_API_KEY → FSM_API_FMP_API_KEY for config integration"
                            )
                        break  # Stop searching after the first .env file
                    except Exception as err:  # noqa: BLE001  (broad but safe here)
                        logger.warning(
                            "Failed to load .env file %s: %s", candidate, err
                        )
                else:
                    # Ascend to parent directory, stop at filesystem root
                    if current.parent == current:
                        break
                    current = current.parent
        except Exception as err:  # noqa: BLE001
            # Never fail config loading due to .env issues
            logger.debug("_load_dotenv encountered an error: %s", err, exc_info=False)


# Global configuration instance
_config_manager = ConfigManager()


def get_config() -> Config:
    """Get the current global configuration.

    Returns:
        The current configuration object.
    """
    return _config_manager.get()


def update_config(updates: dict[str, Any]) -> None:
    """Update the global configuration.

    Args:
        updates: Dictionary of configuration updates.
    """
    _config_manager.update(updates)


def reset_config() -> None:
    """Reset configuration to defaults."""
    _config_manager.reset()



================================================================================
File: fin_statement_model/config/models.py
================================================================================

"""Configuration models for fin_statement_model.

This module defines the configuration schema using Pydantic models,
providing validation and type safety for all configuration options.
"""

from typing import Optional, Literal, Any, Union
from pathlib import Path
from pydantic import BaseModel, Field, field_validator, ConfigDict
from fin_statement_model.statements.configs.models import AdjustmentFilterSpec
from fin_statement_model.preprocessing.config import (
    StatementFormattingConfig,
    TransformationType,
)


class LoggingConfig(BaseModel):
    """Logging configuration settings."""

    level: Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"] = Field(
        "WARNING", description="Default logging level for the library"
    )
    format: str = Field(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        description="Log message format string",
    )
    detailed: bool = Field(
        False, description="Enable detailed logging with file and line numbers"
    )
    log_file_path: Optional[Path] = Field(
        None,
        description=(
            "If provided, logs are written to this path (rotating handler). "
            "If None, file logging is disabled."
        ),
    )

    model_config = ConfigDict(extra="forbid")


class IOConfig(BaseModel):
    """Input/Output configuration settings."""

    default_excel_sheet: str = Field(
        "Sheet1", description="Default sheet name for Excel operations"
    )
    default_csv_delimiter: str = Field(
        ",", description="Default delimiter for CSV files"
    )
    auto_create_output_dirs: bool = Field(
        True, description="Automatically create output directories if they don't exist"
    )
    validate_on_read: bool = Field(True, description="Validate data on read operations")
    default_mapping_configs_dir: Optional[Path] = Field(
        None, description="Directory containing custom mapping configuration files"
    )
    auto_standardize_columns: bool = Field(
        True, description="Automatically standardize column names when reading data"
    )
    skip_invalid_rows: bool = Field(
        False, description="Skip rows with invalid data instead of raising errors"
    )
    strict_validation: bool = Field(
        False, description="Use strict validation when reading data"
    )

    model_config = ConfigDict(extra="forbid")


class ForecastingConfig(BaseModel):
    """Forecasting configuration settings."""

    default_method: Literal[
        "simple", "historical_growth", "curve", "statistical", "ml"
    ] = Field("simple", description="Default forecasting method")
    default_periods: int = Field(5, description="Default number of periods to forecast")
    default_growth_rate: float = Field(
        0.0, description="Default growth rate for simple forecasting"
    )
    min_historical_periods: int = Field(
        3, description="Minimum historical periods required for forecasting"
    )
    allow_negative_forecasts: bool = Field(
        True, description="Allow negative values in forecasts"
    )
    add_missing_periods: bool = Field(
        True, description="Whether to add missing forecast periods to the graph"
    )
    default_bad_forecast_value: float = Field(
        0.0, description="Default value to use for NaN, Inf, or error forecasts"
    )
    continue_on_error: bool = Field(
        True,
        description="Whether to continue forecasting other nodes if one node fails",
    )
    historical_growth_aggregation: Literal["mean", "median"] = Field(
        "mean",
        description="Aggregation method for historical growth rate: 'mean' or 'median'",
    )
    random_seed: Optional[int] = Field(
        None,
        description="Random seed for statistical forecasting to ensure reproducible results",
    )
    base_period_strategy: Literal[
        "preferred_then_most_recent", "most_recent", "last_historical"
    ] = Field(
        "preferred_then_most_recent",
        description=(
            "Strategy for selecting base period: 'preferred_then_most_recent' (default), "
            "'most_recent' (ignore preferred, pick most recent with data), or "
            "'last_historical' (always use last historical period)."
        ),
    )

    @field_validator("default_periods")
    def validate_periods(cls, v: int) -> int:
        """Ensure periods is positive."""
        if v <= 0:
            raise ValueError("default_periods must be positive")
        return v

    model_config = ConfigDict(extra="forbid")


class PreprocessingConfig(BaseModel):
    """Data preprocessing configuration settings."""

    auto_clean_data: bool = Field(
        True, description="Automatically clean data on import"
    )
    fill_missing_with_zero: bool = Field(
        False, description="Fill missing values with zero instead of None"
    )
    remove_empty_periods: bool = Field(
        True, description="Remove periods with all empty values"
    )
    standardize_period_format: bool = Field(
        True, description="Standardize period names to consistent format"
    )
    default_normalization_type: Optional[
        Literal["percent_of", "minmax", "standard", "scale_by"]
    ] = Field(None, description="Default normalization method")
    default_transformation_type: TransformationType = Field(
        TransformationType.GROWTH_RATE,
        description="Default time series transformation type",
    )
    default_time_series_periods: int = Field(
        1, description="Default number of periods for time series transformations"
    )
    default_time_series_window_size: int = Field(
        3, description="Default window size for time series transformations"
    )
    default_conversion_aggregation: str = Field(
        "sum", description="Default aggregation method for period conversion"
    )
    statement_formatting: StatementFormattingConfig = Field(
        default=StatementFormattingConfig.model_validate({}),
        description="Default statement formatting configuration for preprocessing",
    )

    model_config = ConfigDict(extra="forbid")


class DisplayConfig(BaseModel):
    """Display and formatting configuration settings."""

    default_number_format: str = Field(
        ",.2f", description="Default number format string"
    )
    default_currency_format: str = Field(
        ",.2f", description="Default currency format string"
    )
    default_percentage_format: str = Field(
        ".1%", description="Default percentage format string"
    )
    hide_zero_rows: bool = Field(
        False, description="Hide rows where all values are zero"
    )
    contra_display_style: Literal["parentheses", "brackets", "negative"] = Field(
        "parentheses", description="How to display contra items"
    )
    thousands_separator: str = Field(",", description="Thousands separator character")
    decimal_separator: str = Field(".", description="Decimal separator character")
    default_units: str = Field("USD", description="Default currency/units for display")
    scale_factor: float = Field(
        1.0, description="Default scale factor for display (e.g., 0.001 for thousands)"
    )

    # --- New advanced formatting options ---
    indent_character: str = Field(
        "  ", description="Indentation characters used for nested line items"
    )
    subtotal_style: str = Field(
        "bold", description="CSS/markup style keyword for subtotal rows"
    )
    total_style: str = Field(
        "bold", description="CSS/markup style keyword for total rows"
    )
    header_style: str = Field(
        "bold", description="CSS/markup style keyword for header cells"
    )
    contra_css_class: str = Field(
        "contra-item", description="Default CSS class name for contra items"
    )
    show_negative_sign: bool = Field(
        True,
        description="Whether to prefix negative numbers with a minus sign when not using parentheses",
    )
    # Statement formatting defaults
    apply_sign_conventions: bool = Field(
        True, description="Whether to apply sign conventions by default"
    )
    include_empty_items: bool = Field(
        False, description="Whether to include items with no data by default"
    )
    include_metadata_cols: bool = Field(
        False, description="Whether to include metadata columns by default"
    )
    add_is_adjusted_column: bool = Field(
        False, description="Whether to add an 'is_adjusted' column by default"
    )
    include_units_column: bool = Field(
        False, description="Whether to include units column by default"
    )
    include_css_classes: bool = Field(
        False, description="Whether to include CSS class column by default"
    )
    include_notes_column: bool = Field(
        False, description="Whether to include notes column by default"
    )
    apply_item_scaling: bool = Field(
        True, description="Whether to apply item-specific scaling by default"
    )
    apply_item_formatting: bool = Field(
        True, description="Whether to apply item-specific formatting by default"
    )
    apply_contra_formatting: bool = Field(
        True, description="Whether to apply contra-specific formatting by default"
    )
    add_contra_indicator_column: bool = Field(
        False, description="Whether to add a contra indicator column by default"
    )

    @field_validator("scale_factor")
    def validate_scale_factor(cls, v: float) -> float:
        """Ensure scale factor is positive."""
        if v <= 0:
            raise ValueError("scale_factor must be positive")
        return v

    model_config = ConfigDict(extra="forbid")


class APIConfig(BaseModel):
    """API and external service configuration."""

    fmp_api_key: Optional[str] = Field(
        None, description="Financial Modeling Prep API key"
    )
    fmp_base_url: str = Field(
        "https://financialmodelingprep.com/api/v3", description="FMP API base URL"
    )
    api_timeout: int = Field(30, description="API request timeout in seconds")
    api_retry_count: int = Field(
        3, description="Number of retries for failed API requests"
    )
    cache_api_responses: bool = Field(
        True, description="Cache API responses to reduce API calls"
    )
    cache_ttl_hours: int = Field(24, description="Cache time-to-live in hours")

    @field_validator("api_timeout", "api_retry_count", "cache_ttl_hours")
    def validate_positive(cls, v: int) -> int:
        """Ensure values are positive."""
        if v <= 0:
            raise ValueError("Value must be positive")
        return v

    model_config = ConfigDict(extra="forbid")


class MetricsConfig(BaseModel):
    """Metrics configuration settings."""

    custom_metrics_dir: Optional[Path] = Field(
        None, description="Directory containing custom metric definitions"
    )
    validate_metric_inputs: bool = Field(
        True, description="Validate metric inputs exist in graph"
    )
    auto_register_metrics: bool = Field(
        True, description="Automatically register metrics from definition files"
    )

    model_config = ConfigDict(extra="forbid")


class ValidationConfig(BaseModel):
    """Data validation configuration settings."""

    strict_mode: bool = Field(False, description="Enable strict validation mode")
    auto_standardize_names: bool = Field(
        True, description="Automatically standardize node names to canonical form"
    )
    warn_on_non_standard: bool = Field(
        True, description="Warn when using non-standard node names"
    )
    check_balance_sheet_balance: bool = Field(
        True, description="Validate that Assets = Liabilities + Equity"
    )
    balance_tolerance: float = Field(
        1.0, description="Maximum acceptable difference for balance sheet validation"
    )
    warn_on_negative_assets: bool = Field(
        True, description="Warn when asset values are negative"
    )
    validate_sign_conventions: bool = Field(
        True, description="Validate that items follow expected sign conventions"
    )

    @field_validator("balance_tolerance")
    def validate_tolerance(cls, v: float) -> float:
        """Ensure tolerance is non-negative."""
        if v < 0:
            raise ValueError("balance_tolerance must be non-negative")
        return v

    model_config = ConfigDict(extra="forbid")


class StatementsConfig(BaseModel):
    """Statement formatting and building configuration settings."""

    default_adjustment_filter: Optional[Union[AdjustmentFilterSpec, list[str]]] = Field(
        None,
        description="Default adjustment filter spec or list of tags to apply when building statements",
    )
    enable_node_validation: bool = Field(
        False,
        description="Whether to enable node ID validation during statement building by default",
    )
    node_validation_strict: bool = Field(
        False,
        description="Whether to treat node validation failures as errors (strict) by default",
    )

    model_config = ConfigDict(extra="forbid")


class Config(BaseModel):
    """Main configuration container for fin_statement_model."""

    # Sub-configurations
    logging: LoggingConfig = Field(
        default=LoggingConfig.model_validate({}), description="Logging configuration"
    )
    io: IOConfig = Field(
        default=IOConfig.model_validate({}), description="Input/Output configuration"
    )
    forecasting: ForecastingConfig = Field(
        default=ForecastingConfig.model_validate({}),
        description="Forecasting configuration",
    )
    preprocessing: PreprocessingConfig = Field(
        default=PreprocessingConfig.model_validate({}),
        description="Data preprocessing configuration",
    )
    display: DisplayConfig = Field(
        default=DisplayConfig.model_validate({}),
        description="Display and formatting configuration",
    )
    api: APIConfig = Field(
        default=APIConfig.model_validate({}),
        description="API and external service configuration",
    )
    metrics: MetricsConfig = Field(
        default=MetricsConfig.model_validate({}), description="Metrics configuration"
    )
    validation: ValidationConfig = Field(
        default=ValidationConfig.model_validate({}),
        description="Data validation configuration",
    )
    statements: StatementsConfig = Field(
        default=StatementsConfig.model_validate({}),
        description="Statement structure and formatting configuration",
    )

    # Global settings
    project_name: str = Field(
        "fin_statement_model", description="Project name for identification"
    )
    config_file_path: Optional[Path] = Field(
        None, description="Path to user configuration file"
    )
    auto_save_config: bool = Field(
        False, description="Automatically save configuration changes to file"
    )

    model_config = ConfigDict(extra="forbid", validate_assignment=True)

    def to_dict(self) -> dict[str, Any]:
        """Convert configuration to a JSON-serializable dictionary.

        The default ``model_dump`` output may include complex Python objects
        such as ``Enum`` instances.  Passing ``mode='json'`` ensures that
        those objects are converted to their JSON representation (e.g. raw
        strings for ``Enum`` values), allowing the resulting dictionary to be
        safely serialized via ``yaml.dump`` or ``json.dumps`` without emitting
        Python-specific YAML tags that require an *unsafe* loader.
        """
        return self.model_dump(exclude_none=True, mode="json")

    def to_yaml(self) -> str:
        """Convert configuration to YAML string."""
        import yaml

        return yaml.dump(self.to_dict(), default_flow_style=False, sort_keys=True)

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "Config":
        """Create configuration from dictionary."""
        return cls(**data)

    @classmethod
    def from_yaml(cls, yaml_str: str) -> "Config":
        """Create configuration from YAML string."""
        import yaml

        data = yaml.safe_load(yaml_str)
        return cls.from_dict(data)

    @classmethod
    def from_file(cls, path: Path) -> "Config":
        """Load configuration from file."""
        if path.suffix in [".yaml", ".yml"]:
            return cls.from_yaml(path.read_text())
        elif path.suffix == ".json":
            import json

            return cls.from_dict(json.loads(path.read_text()))
        else:
            raise ValueError(f"Unsupported config file format: {path.suffix}")



================================================================================
File: fin_statement_model/core/__init__.py
================================================================================

"""Core components for the Financial Statement Model.

This package forms the foundation of the library, providing the core infrastructure
for building, calculating, and managing financial models. It includes:

- Graph engine (`core.graph`): For representing financial relationships.
- Base node hierarchy (`core.nodes`): Abstract and concrete node types.
- Calculation engine (`calculation_engine.py`): For evaluating the graph.
- Metric registry and definitions (`core.metrics`): For managing financial metrics.
- Data management (`data_manager.py`): For handling financial data.
- Calculation strategies (`core.strategies`): Reusable calculation logic.
- Core utilities and exceptions (`errors.py`, `node_factory.py`).

This `core` package is designed to be self-contained and does not depend on
other higher-level packages like `statements`, `io`, or `forecasting`.
"""

from .node_factory import NodeFactory
from .graph import Graph
from .nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    YoYGrowthNode,
    MultiPeriodStatNode,
    FormulaCalculationNode,
    CustomCalculationNode,
    TwoPeriodAverageNode,
)
from .calculations import (
    AdditionCalculation,
    SubtractionCalculation,
    MultiplicationCalculation,
    DivisionCalculation,
)
from .errors import (
    FinancialModelError,
    ConfigurationError,
    CalculationError,
    NodeError,
    GraphError,
    DataValidationError,
    CircularDependencyError,
    PeriodError,
    StatementError,
    StrategyError,
    TransformationError,
)

__all__ = [
    "AdditionCalculation",
    "CalculationError",
    "CalculationNode",
    "CircularDependencyError",
    "ConfigurationError",
    "CustomCalculationNode",
    "DataValidationError",
    "DivisionCalculation",
    "FinancialModelError",
    "FinancialStatementItemNode",
    "FormulaCalculationNode",
    "Graph",
    "GraphError",
    "MultiPeriodStatNode",
    "MultiplicationCalculation",
    "Node",
    "NodeError",
    "NodeFactory",
    "PeriodError",
    "StatementError",
    "StrategyError",
    "SubtractionCalculation",
    "TransformationError",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]



================================================================================
File: fin_statement_model/core/adjustments/__init__.py
================================================================================

"""Core adjustment models and filters."""

from .models import (
    Adjustment,
    AdjustmentFilter,
    AdjustmentFilterInput,
    AdjustmentType,
    DEFAULT_SCENARIO,
)

__all__ = [
    "DEFAULT_SCENARIO",
    "Adjustment",
    "AdjustmentFilter",
    "AdjustmentFilterInput",
    "AdjustmentType",
]



================================================================================
File: fin_statement_model/core/adjustments/analytics.py
================================================================================

"""Analytics functions for summarizing and analyzing adjustments."""

import logging
from typing import Optional, Union
from collections.abc import Callable

import pandas as pd

from fin_statement_model.core.adjustments.manager import AdjustmentManager
from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentFilter,
    AdjustmentTag,
)
from fin_statement_model.core.adjustments.helpers import tag_matches

logger = logging.getLogger(__name__)


def _filter_adjustments_static(
    all_adjustments: list[Adjustment],
    filter_input: Optional[
        Union[AdjustmentFilter, set[AdjustmentTag], Callable[[Adjustment], bool]]
    ],
) -> list[Adjustment]:
    """Apply filtering to adjustments based on filter_input, excluding period context.

    This helper function centralizes the common filtering logic used by both
    summary() and list_by_tag() methods. It applies filters based on the static
    properties of adjustments (tags, scenario, type, etc.) but ignores any
    period-based filtering since that requires runtime context.

    Args:
        all_adjustments: List of all adjustments to filter.
        filter_input: Filter criteria (AdjustmentFilter, set of tags, callable, or None).

    Returns:
        Filtered list of adjustments matching the filter criteria.
    """
    if filter_input is None:
        # No filter means include all adjustments
        logger.debug("No filter applied.")
        return all_adjustments

    elif isinstance(filter_input, AdjustmentFilter):
        # Apply filter, ignoring its period attribute
        temp_filter = filter_input.model_copy(update={"period": None})
        filtered = [adj for adj in all_adjustments if temp_filter.matches(adj)]
        logger.debug(
            f"Applied AdjustmentFilter (ignoring period). Filter: {temp_filter}"
        )
        return filtered

    elif isinstance(filter_input, set):
        # Shorthand for include_tags
        filtered = [
            adj for adj in all_adjustments if tag_matches(adj.tags, filter_input)
        ]
        logger.debug(f"Applied tag filter. Tags: {filter_input}")
        return filtered

    elif callable(filter_input):
        filtered = [adj for adj in all_adjustments if filter_input(adj)]
        logger.debug("Applied callable filter.")
        return filtered

    else:
        # Should not happen due to type hint, but defensive
        logger.warning(
            f"Invalid filter_input type: {type(filter_input)}. No filtering applied."
        )
        return all_adjustments


def summary(
    manager: AdjustmentManager,
    filter_input: Optional[
        Union[AdjustmentFilter, set[AdjustmentTag], Callable[[Adjustment], bool]]
    ] = None,
    group_by: list[str] = ["period", "node_name"],
) -> pd.DataFrame:
    """Generate a summary DataFrame of adjustments, optionally filtered and grouped.

    Calculates count, sum of values, and mean of absolute values for adjustments.

    Args:
        manager: The AdjustmentManager instance containing the adjustments.
        filter_input: Optional filter criteria (AdjustmentFilter, set of tags, callable, or None)
                      to apply before summarizing.
        group_by: List of Adjustment attributes to group the summary by.
                  Defaults to ["period", "node_name"]. Valid fields include
                  'period', 'node_name', 'scenario', 'type', 'user'.

    Returns:
        A pandas DataFrame with the summary statistics (count, sum, mean_abs_value)
        indexed by the specified group_by columns.
    """
    logger.debug(f"Generating adjustment summary, grouping by: {group_by}")

    # Get all adjustments first
    # TODO: Optimization - If filtering is very restrictive, could filter first.
    # However, filtering requires period context which isn't directly available here.
    # Get all adjustments and filter based on the filter_input's static properties.
    # The period-based filtering (effective window) cannot be applied generically here.
    all_adjustments = manager.get_all_adjustments()

    # Apply filtering using the helper function
    filtered_adjustments = _filter_adjustments_static(all_adjustments, filter_input)

    if not filtered_adjustments:
        logger.info("No adjustments found matching the filter criteria for summary.")
        # Return empty DataFrame with expected columns
        cols = [*group_by, "count", "sum_value", "mean_abs_value"]
        return pd.DataFrame(columns=cols).set_index(group_by)

    # Convert to DataFrame for easier aggregation
    adj_data = [
        adj.model_dump(
            include=set([*group_by, "value"])
        )  # Include value for aggregation
        for adj in filtered_adjustments
    ]
    df = pd.DataFrame(adj_data)

    # Ensure correct types for grouping columns if needed (e.g., type as string)
    if "type" in group_by:
        df["type"] = df["type"].astype(str)

    # Add absolute value for mean calculation
    df["abs_value"] = df["value"].abs()

    # Perform aggregation
    summary_df = df.groupby(group_by).agg(
        count=("value", "size"),
        sum_value=("value", "sum"),
        mean_abs_value=("abs_value", "mean"),
    )

    logger.info(f"Generated adjustment summary with {len(summary_df)} groups.")
    return summary_df


def list_by_tag(
    manager: AdjustmentManager,
    tag_prefix: str,
    filter_input: Optional[
        Union[AdjustmentFilter, set[AdjustmentTag], Callable[[Adjustment], bool]]
    ] = None,
) -> list[Adjustment]:
    """List all adjustments matching a tag prefix, optionally applying further filters.

    Args:
        manager: The AdjustmentManager instance containing the adjustments.
        tag_prefix: The tag prefix string to match (e.g., "NonRecurring").
        filter_input: Optional additional filter criteria (AdjustmentFilter, set of tags,
                      callable, or None).

    Returns:
        A list of Adjustment objects that have at least one tag starting with
        the tag_prefix and also match the optional filter_input.
    """
    logger.debug(f"Listing adjustments by tag prefix: '{tag_prefix}'")

    # Get all adjustments and apply filters using the helper function
    all_adjustments = manager.get_all_adjustments()
    filtered_adjustments = _filter_adjustments_static(all_adjustments, filter_input)

    # Apply the primary tag prefix filter
    prefix_set = {tag_prefix}
    final_list = [
        adj for adj in filtered_adjustments if tag_matches(adj.tags, prefix_set)
    ]

    logger.info(
        f"Found {len(final_list)} adjustments matching prefix '{tag_prefix}' and other filters."
    )
    # Sort by priority/timestamp for consistent output
    return sorted(final_list, key=lambda x: (x.priority, x.timestamp))



================================================================================
File: fin_statement_model/core/adjustments/helpers.py
================================================================================

"""Helper functions for the adjustments module."""


def tag_matches(target_tags: set[str], prefixes: set[str]) -> bool:
    """Check if any target tag starts with any of the given prefixes.

    Allows for hierarchical matching: a prefix "A/B" matches tag "A/B/C".
    A simple prefix "A" matches tag "A/B".

    Args:
        target_tags: The set of tags on an adjustment.
        prefixes: The set of prefixes to check against (e.g., from a filter).

    Returns:
        True if at least one tag in target_tags starts with at least one
        prefix in prefixes, False otherwise.
    """
    if not prefixes:  # Optimization: if no prefixes specified, it can't match
        return False
    if not target_tags:  # Optimization: if no tags exist, it can't match
        return False

    # Check if any combination of tag and prefix matches
    return any(t.startswith(p) for t in target_tags for p in prefixes)



================================================================================
File: fin_statement_model/core/adjustments/manager.py
================================================================================

"""Manages the storage, retrieval, and application of adjustments."""

from __future__ import annotations

import logging
from collections import defaultdict
from typing import Optional
from uuid import UUID

from .models import (
    Adjustment,
    AdjustmentFilter,
    AdjustmentFilterInput,
    AdjustmentType,
    DEFAULT_SCENARIO,
)


logger = logging.getLogger(__name__)


class AdjustmentManager:
    """Handles the lifecycle and application of Adjustment objects.

    Provides methods for adding, removing, filtering, and applying adjustments
    to base values.
    """

    def __init__(self) -> None:
        """Initializes the AdjustmentManager with empty storage."""
        # Primary index: (scenario, node_name, period) -> list[Adjustment]
        self._by_location: dict[tuple[str, str, str], list[Adjustment]] = defaultdict(
            list
        )
        # Secondary index for quick lookup and removal by ID
        self._by_id: dict[UUID, Adjustment] = {}

    def add_adjustment(self, adj: Adjustment) -> None:
        """Adds an adjustment to the manager, replacing if ID exists."""
        # If an adjustment with the same ID already exists, remove it first
        if adj.id in self._by_id:
            self.remove_adjustment(adj.id)

        self._by_id[adj.id] = adj
        key = (adj.scenario, adj.node_name, adj.period)
        self._by_location[key].append(adj)
        # Keep the list sorted by priority, then timestamp for consistent application order
        self._by_location[key].sort(key=lambda x: (x.priority, x.timestamp))

    def remove_adjustment(self, adj_id: UUID) -> bool:
        """Removes an adjustment by its ID. Returns True if found, False otherwise."""
        if adj_id not in self._by_id:
            return False

        adj_to_remove = self._by_id.pop(adj_id)
        key = (adj_to_remove.scenario, adj_to_remove.node_name, adj_to_remove.period)

        if key in self._by_location:
            # Filter out the specific adjustment object instance
            self._by_location[key] = [
                a for a in self._by_location[key] if a.id != adj_id
            ]
            # If the list becomes empty, remove the key
            if not self._by_location[key]:
                del self._by_location[key]
        return True

    def _apply_one(self, base_value: float, adj: Adjustment) -> float:
        """Applies a single adjustment to a value based on its type and scale."""
        if adj.type == AdjustmentType.ADDITIVE:
            # Ensuring result is float
            return float(base_value + adj.value * adj.scale)
        elif adj.type == AdjustmentType.MULTIPLICATIVE:
            # Ensure base_value is not zero to avoid issues with 0**(negative scale)
            # If base is 0, multiplicative adjustment usually results in 0 unless value is 0.
            # We also need to handle potential complex numbers if base is negative and scale is fractional.
            # For simplicity, let's assume standard financial contexts where this is less common
            # or handle it by convention (e.g., multiplicative doesn't apply to zero/negative base).
            # Let's default to returning 0 if base is 0 for multiplicative.
            if base_value == 0:
                return 0.0
            # Consider adding checks or specific handling for negative base + fractional scale if needed.
            # Cast to float after exponentiation and multiplication
            return float(base_value * (adj.value**adj.scale))
        elif adj.type == AdjustmentType.REPLACEMENT:
            # Scale is ignored for replacement type as per spec
            # Cast to float to satisfy return type
            return float(adj.value)
        else:
            # Should not happen with Enum, but defensively return base value
            return base_value  # pragma: no cover

    def apply_adjustments(
        self, base_value: float, adjustments: list[Adjustment]
    ) -> tuple[float, bool]:
        """Applies a list of adjustments sequentially to a base value.

        Adjustments are applied in order of priority (lower first), then timestamp.

        Args:
            base_value: The starting value before adjustments.
            adjustments: A list of Adjustment objects to apply.

        Returns:
            A tuple containing: (final adjusted value, boolean indicating if any adjustment was applied).
        """
        if not adjustments:
            return base_value, False

        current_value = base_value
        applied_flag = False

        # Sort by priority (ascending), then timestamp (ascending) as per spec
        # Note: add_adjustment already sorts the list in _by_location, but
        # this ensures correctness if an unsorted list is passed directly.
        sorted_adjustments = sorted(
            adjustments, key=lambda x: (x.priority, x.timestamp)
        )

        for adj in sorted_adjustments:
            current_value = self._apply_one(current_value, adj)
            applied_flag = True

        return current_value, applied_flag

    def get_adjustments(
        self, node_name: str, period: str, *, scenario: str = DEFAULT_SCENARIO
    ) -> list[Adjustment]:
        """Retrieves all adjustments for a specific node, period, and scenario."""
        key = (scenario, node_name, period)
        # Return a copy to prevent external modification of the internal list
        return list(self._by_location.get(key, []))

    def _normalize_filter(
        self, filter_input: AdjustmentFilterInput, period: Optional[str] = None
    ) -> AdjustmentFilter:
        """Converts flexible filter input into a standard AdjustmentFilter instance."""
        if filter_input is None:
            # Default filter includes only the default scenario and sets the period context
            return AdjustmentFilter(include_scenarios={DEFAULT_SCENARIO}, period=period)
        elif isinstance(filter_input, AdjustmentFilter):
            # If period context wasn't set on the filter, set it now
            if filter_input.period is None:
                return filter_input.model_copy(update={"period": period})
            return filter_input
        elif isinstance(filter_input, set):
            # Shorthand for include_tags filter
            # Assume shorthand applies only to DEFAULT_SCENARIO unless specified otherwise?
            # Let's keep it simple: Shorthand applies to DEFAULT_SCENARIO only.
            return AdjustmentFilter(
                include_tags=filter_input,
                include_scenarios={DEFAULT_SCENARIO},
                period=period,
            )
        elif callable(filter_input):
            # This case is complex as the callable doesn't inherently know the period.
            # We wrap the callable in a filter, but the effective window check might not work
            # as expected unless the callable itself uses the period.
            # For simplicity, we create a default filter and rely on the callable for matching.
            # The manager will still filter by callable *after* potentially getting adjustments.
            # A more robust solution might involve passing period to the callable.
            # Let's just return a base filter for now and handle callable later.
            # TODO: Revisit handling of callable filters if period context is critical.
            logger.warning(
                "Callable filter used; period context for effective window check might be ignored."
            )
            # Apply callable, but filter to default scenario like other shorthand.
            return AdjustmentFilter(
                include_scenarios={DEFAULT_SCENARIO}, period=period
            )  # Base filter, callable applied later
        else:
            raise TypeError(f"Invalid filter_input type: {type(filter_input)}")

    def get_filtered_adjustments(
        self, node_name: str, period: str, filter_input: AdjustmentFilterInput = None
    ) -> list[Adjustment]:
        """Retrieves adjustments for a node/period that match the given filter criteria.

        Args:
            node_name: The target node name.
            period: The target period.
            filter_input: The filter criteria (AdjustmentFilter, set of tags, callable, or None).

        Returns:
            A list of matching Adjustment objects, sorted by priority and timestamp.
        """
        normalized_filter = self._normalize_filter(filter_input, period)

        candidate_adjustments: list[Adjustment] = []

        # Determine which scenarios to check based on the filter
        scenarios_to_check: set[str]
        if normalized_filter.include_scenarios is not None:
            scenarios_to_check = (
                normalized_filter.include_scenarios.copy()
            )  # Work on a copy
            if normalized_filter.exclude_scenarios is not None:
                scenarios_to_check -= normalized_filter.exclude_scenarios
        elif normalized_filter.exclude_scenarios is not None:
            # Get all scenarios currently known to the manager
            all_known_scenarios = {adj.scenario for adj in self._by_id.values()}
            scenarios_to_check = (
                all_known_scenarios - normalized_filter.exclude_scenarios
            )
        else:
            # No include/exclude specified: check all scenarios relevant for this node/period
            # This requires checking keys in _by_location
            scenarios_to_check = {
                key[0]
                for key in self._by_location
                if key[1] == node_name and key[2] == period
            }
            # If no specific adjustments exist for this node/period, we might check default?
            # Let's assume we only check scenarios that *have* adjustments for this location.
            if not scenarios_to_check:
                # Maybe return empty list early if no scenarios found for location?
                # Or should it behave differently? For now, proceed with empty set.
                pass

        # Gather candidates from relevant locations
        for scenario in scenarios_to_check:
            key = (scenario, node_name, period)
            candidate_adjustments.extend(self._by_location.get(key, []))

        # Apply the filter logic
        matching_adjustments: list[Adjustment] = []
        if callable(filter_input):
            # Apply the callable filter directly
            matching_adjustments = [
                adj for adj in candidate_adjustments if filter_input(adj)
            ]
        else:
            # Apply the normalized AdjustmentFilter's matches method
            matching_adjustments = [
                adj for adj in candidate_adjustments if normalized_filter.matches(adj)
            ]

        # Return sorted list (sorting might be redundant if fetched lists are pre-sorted
        # and filtering maintains order, but ensures correctness)
        return sorted(matching_adjustments, key=lambda x: (x.priority, x.timestamp))

    def get_all_adjustments(self) -> list[Adjustment]:
        """Returns a list of all adjustments currently stored in the manager."""
        # Return a copy to prevent external modification
        return list(self._by_id.values())

    def clear_all(self) -> None:
        """Removes all adjustments from the manager."""
        self._by_location.clear()
        self._by_id.clear()

    def load_adjustments(self, adjustments: list[Adjustment]) -> None:
        """Clears existing adjustments and loads a new list."""
        self.clear_all()
        for adj in adjustments:
            self.add_adjustment(adj)



================================================================================
File: fin_statement_model/core/adjustments/models.py
================================================================================

"""Adjustment data models and related types."""

from __future__ import annotations

import uuid
from datetime import datetime
from enum import Enum
from typing import Final, Optional
from collections.abc import Callable

from pydantic import BaseModel, ConfigDict, Field, field_validator
import logging

from .helpers import tag_matches

logger = logging.getLogger(__name__)

# --------------------------------------------------------------------
# Core Types and Constants
# --------------------------------------------------------------------


class AdjustmentType(Enum):
    """Defines how an adjustment modifies a base value."""

    ADDITIVE = "additive"  # base + (value * scale)
    MULTIPLICATIVE = "multiplicative"  # base * (value ** scale)
    REPLACEMENT = "replacement"  # use value (scale ignored)


AdjustmentTag = str  # Slash (/) separates hierarchy levels in tags
DEFAULT_SCENARIO: Final[str] = "default"

# --------------------------------------------------------------------
# Adjustment Model
# --------------------------------------------------------------------


class Adjustment(BaseModel):
    """Immutable record describing a discretionary adjustment to a node's value.

    Attributes:
        id: Unique identifier for the adjustment.
        node_name: The name of the target node.
        period: The primary period the adjustment applies to.
        start_period: The first period the adjustment is effective (inclusive, Phase 2).
        end_period: The last period the adjustment is effective (inclusive, Phase 2).
        value: The numeric value of the adjustment.
        type: How the adjustment combines with the base value.
        scale: Attenuation factor for the adjustment (0.0 to 1.0, Phase 2).
        priority: Tie-breaker for applying multiple adjustments (lower number applied first).
        tags: Set of descriptive tags for filtering and analysis.
        scenario: The named scenario this adjustment belongs to (Phase 2).
        reason: Text description of why the adjustment was made.
        user: Identifier for the user who created the adjustment.
        timestamp: UTC timestamp when the adjustment was created.
    """

    # Target
    id: uuid.UUID = Field(default_factory=uuid.uuid4)
    node_name: str
    period: str  # Primary target period
    start_period: Optional[str] = None  # Phase 2 - effective range start (inclusive)
    end_period: Optional[str] = None  # Phase 2 - effective range end (inclusive)

    # Behaviour
    value: float
    type: AdjustmentType = AdjustmentType.ADDITIVE
    scale: float = 1.0  # Phase 2 - 0.0 <= scale <= 1.0
    priority: int = 0  # Lower value means higher priority (applied first)

    # Classification
    tags: set[AdjustmentTag] = Field(default_factory=set)
    scenario: str = DEFAULT_SCENARIO  # Phase 2 - Scenario grouping

    # Metadata
    reason: str
    user: Optional[str] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)

    model_config = ConfigDict(frozen=True)

    @field_validator("scale")
    @classmethod
    def _scale_bounds(cls, v: float) -> float:
        """Validate that the scale factor is between 0.0 and 1.0."""
        if not 0.0 <= v <= 1.0:
            raise ValueError("Scale must be between 0.0 and 1.0 (inclusive)")
        return v


# --------------------------------------------------------------------
# Adjustment Filtering
# --------------------------------------------------------------------


class AdjustmentFilter(BaseModel):
    """Defines criteria for selecting adjustments.

    Attributes:
        include_scenarios: Only include adjustments from these scenarios.
        exclude_scenarios: Exclude adjustments from these scenarios.
        include_tags: Include adjustments matching any of these tag prefixes.
        exclude_tags: Exclude adjustments matching any of these tag prefixes.
        require_all_tags: Include only adjustments having *all* these exact tags.
        include_types: Only include adjustments of these types.
        exclude_types: Exclude adjustments of these types.
        period: The specific period context for effective window checks.
    """

    # Scenario Filtering
    include_scenarios: Optional[set[str]] = None
    exclude_scenarios: Optional[set[str]] = None

    # Tag Filtering (supports hierarchical matching via helpers.tag_matches)
    # Need to import the helper function first.
    # Let's assume it will be imported at the top level of the module later.

    include_tags: Optional[set[AdjustmentTag]] = None
    exclude_tags: Optional[set[AdjustmentTag]] = None
    require_all_tags: Optional[set[AdjustmentTag]] = None  # Exact match required

    # Type Filtering
    include_types: Optional[set[AdjustmentType]] = None
    exclude_types: Optional[set[AdjustmentType]] = None

    # Context for Effective Window Checks (Phase 2)
    period: Optional[str] = None  # The current period being calculated/viewed

    def matches(self, adj: Adjustment) -> bool:
        """Check if a given adjustment meets the filter criteria."""
        # Need to import the helper function here to avoid circular dependency issues at module level

        # Start assuming it matches, then progressively set to False if any check fails.
        is_match = True

        # --- Scenario Checks ---
        if (
            self.include_scenarios is not None
            and adj.scenario not in self.include_scenarios
        ) or (
            self.exclude_scenarios is not None
            and adj.scenario in self.exclude_scenarios
        ):
            is_match = False

        # --- Tag Checks ---
        # Only check if still potentially a match
        if is_match and (
            (
                self.include_tags is not None
                and not tag_matches(adj.tags, self.include_tags)
            )
            or (
                self.exclude_tags is not None
                and tag_matches(adj.tags, self.exclude_tags)
            )
            or (
                self.require_all_tags is not None
                and not self.require_all_tags.issubset(adj.tags)
            )
        ):
            is_match = False

        # --- Type Checks ---
        # Only check if still potentially a match
        if is_match and (
            (self.include_types is not None and adj.type not in self.include_types)
            or (self.exclude_types is not None and adj.type in self.exclude_types)
        ):
            is_match = False

        # --- Effective Window Check (Phase 2) ---
        # Assumes periods are sortable strings (e.g., 'YYYY-MM' or 'Q1-2023')
        if (
            is_match and self.period is not None
        ):  # Only check if still potentially a match
            logger.debug(
                f"Period check: FilterPeriod={self.period}, AdjStart={adj.start_period}, AdjEnd={adj.end_period}"
            )
            period_match = True  # Assume period is ok unless proven otherwise
            if adj.start_period is not None and self.period < adj.start_period:
                logger.debug("Period check failed: FilterPeriod < AdjStart")
                period_match = False
            # Use 'if period_match' to avoid unnecessary log if start check already failed
            if (
                period_match
                and adj.end_period is not None
                and self.period > adj.end_period
            ):
                logger.debug("Period check failed: FilterPeriod > AdjEnd")
                period_match = False

            if period_match:
                logger.debug("Period check passed.")
            else:
                is_match = False  # Period check failed
        # else: # Optional log if period check was skipped
        #     if self.period is not None:
        #         logger.debug("Period check skipped because is_match was already False")
        #     else:
        #         logger.debug("Period check skipped: Filter has no period context.")

        return is_match


# Type alias for flexible filter input
AdjustmentFilterInput = Optional[
    AdjustmentFilter | set[AdjustmentTag] | Callable[[Adjustment], bool]
]



================================================================================
File: fin_statement_model/core/calculations/__init__.py
================================================================================

"""Calculations module for the Financial Statement Model.

This module provides classes for implementing the Calculation Pattern for calculations
in the Financial Statement Model. It allows different calculation algorithms to be
defined, registered, and applied to financial data.
"""

from .calculation import (
    Calculation,
    AdditionCalculation,
    SubtractionCalculation,
    MultiplicationCalculation,
    DivisionCalculation,
    WeightedAverageCalculation,
    CustomFormulaCalculation,
    FormulaCalculation,
)
from .registry import Registry

# Register calculations
Registry.register(AdditionCalculation)
Registry.register(SubtractionCalculation)
Registry.register(MultiplicationCalculation)
Registry.register(DivisionCalculation)
Registry.register(WeightedAverageCalculation)
Registry.register(CustomFormulaCalculation)
Registry.register(FormulaCalculation)

__all__ = [
    "AdditionCalculation",
    "Calculation",
    "CustomFormulaCalculation",
    "DivisionCalculation",
    "FormulaCalculation",
    "MultiplicationCalculation",
    "Registry",
    "SubtractionCalculation",
    "WeightedAverageCalculation",
]



================================================================================
File: fin_statement_model/core/calculations/calculation.py
================================================================================

"""Calculation for the Financial Statement Model.

This module provides the Calculation Pattern implementation for calculations,
allowing different calculation types to be encapsulated in calculation classes.
"""

from abc import ABC, abstractmethod
import ast
import logging
import operator
from typing import Optional, ClassVar, Any, Type
from collections.abc import Callable

from fin_statement_model.core.nodes.base import Node  # Absolute
from fin_statement_model.core.errors import CalculationError, StrategyError

# Configure logging
logger = logging.getLogger(__name__)


class Calculation(ABC):
    """Abstract base class for all calculations.

    This class defines the interface that all concrete calculation classes must
    implement. It employs a calculation pattern, allowing the algorithm
    used by a CalculationNode to be selected at runtime.

    Each concrete calculation encapsulates a specific method for computing a
    financial value based on a list of input nodes and a given time period.
    """

    @abstractmethod
    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculate a value based on input nodes for a specific period.

        This abstract method must be implemented by all concrete calculation classes.
        It defines the core logic for the calculation.

        Args:
            inputs: A list of input Node objects whose values will be used in
                the calculation.
            period: The time period string (e.g., "2023Q1") for which the
                calculation should be performed.

        Returns:
            The calculated numerical value as a float.

        Raises:
            NotImplementedError: If the method is not implemented by a subclass.
            ValueError: If the inputs are invalid for the specific calculation
                (e.g., wrong number of inputs, incompatible types).
            ZeroDivisionError: If the calculation involves division and a divisor
                is zero.
            Exception: Other exceptions depending on the calculation logic.
        """
        # pragma: no cover

    @property
    def description(self) -> str:
        """Provides a human-readable description of the calculation.

        This is useful for documentation, debugging, and for user interfaces
        that need to explain how a value is derived.

        Returns:
            A string describing the calculation.
        """
        # Default implementation returns the class name. Subclasses should override
        # for more specific descriptions.
        class_name = self.__class__.__name__  # pragma: no cover
        return class_name


class AdditionCalculation(Calculation):
    """Implements an addition calculation, summing values from multiple input nodes.

    This calculation sums the values obtained from calling
    the `calculate` method on each of the provided input nodes for a given period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Sums the calculated values from all input nodes for the specified period.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023Q4") for the calculation.

        Returns:
            The total sum of the values calculated from the input nodes. Returns
            0.0 if the input list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = AdditionCalculation()
            >>> nodes = [MockNode(10), MockNode(20), MockNode(5)]
            >>> strategy.calculate(nodes, "2023")
            35.0
            >>> strategy.calculate([], "2023")
            0.0
        """
        logger.debug(f"Applying addition calculation for period {period}")
        # Using a generator expression for potentially better memory efficiency
        return sum(input_node.calculate(period) for input_node in inputs)

    @property
    def description(self) -> str:
        """Returns a description of the addition calculation."""
        return "Addition (sum of all inputs)"


class SubtractionCalculation(Calculation):
    """Implements a subtraction calculation: first input minus the sum of the rest.

    This calculation takes the calculated value of the first node in the input list
    and subtracts the sum of the calculated values of all subsequent nodes for
    a specific period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the difference: value of the first input minus the sum of others.

        Args:
            inputs: A list of Node objects. Must contain at least one node.
            period: The time period string (e.g., "2024Q1") for the calculation.

        Returns:
            The result of the subtraction. If only one input node is provided,
            its value is returned.

        Raises:
            ValueError: If the `inputs` list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = SubtractionCalculation()
            >>> nodes = [MockNode(100), MockNode(20), MockNode(30)]
            >>> strategy.calculate(nodes, "2023")
            50.0
            >>> nodes_single = [MockNode(100)]
            >>> strategy.calculate(nodes_single, "2023")
            100.0
        """
        if not inputs:
            raise CalculationError(
                "Subtraction calculation requires at least one input node",
                details={"strategy": "SubtractionCalculation"},
            )

        logger.debug(f"Applying subtraction calculation for period {period}")
        # Calculate values first to avoid multiple calls if nodes are complex
        values = [node.calculate(period) for node in inputs]
        return values[0] - sum(values[1:])

    @property
    def description(self) -> str:
        """Returns a description of the subtraction calculation."""
        return "Subtraction (first input minus sum of subsequent inputs)"


class MultiplicationCalculation(Calculation):
    """Implements a multiplication calculation, calculating the product of input values.

    This calculation multiplies the calculated values of all provided input nodes
    for a given period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the product of the values from all input nodes.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023FY") for the calculation.

        Returns:
            The product of all input values. Returns 1.0 (multiplicative identity)
            if the input list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = MultiplicationCalculation()
            >>> nodes = [MockNode(2), MockNode(3), MockNode(4)]
            >>> strategy.calculate(nodes, "2023")
            24.0
            >>> strategy.calculate([], "2023")
            1.0
        """
        # Multiplication calculation should ideally return 1.0 for empty inputs.
        # Raising error if empty seems less conventional for multiplication.
        if not inputs:
            logger.warning(
                "Multiplication calculation called with empty inputs, returning 1.0"
            )
            return 1.0

        logger.debug(f"Applying multiplication calculation for period {period}")
        result = 1.0
        for input_node in inputs:
            result *= input_node.calculate(period)
        return result

    @property
    def description(self) -> str:
        """Returns a description of the multiplication calculation."""
        return "Multiplication (product of all inputs)"


class DivisionCalculation(Calculation):
    """Implements a division calculation: first input divided by the product of the rest.

    This calculation takes the calculated value of the first node (numerator) and
    divides it by the product of the calculated values of all subsequent nodes
    (denominator) for a specific period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the division: first input / (product of subsequent inputs).

        Args:
            inputs: A list of Node objects. Must contain at least two nodes.
            period: The time period string (e.g., "2024Q2") for the calculation.

        Returns:
            The result of the division.

        Raises:
            ValueError: If `inputs` list contains fewer than two nodes.
            ZeroDivisionError: If the calculated product of the subsequent nodes
                (denominator) is zero.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = DivisionCalculation()
            >>> nodes = [MockNode(100), MockNode(5), MockNode(2)]
            >>> strategy.calculate(nodes, "2023")
            10.0
            >>> nodes_zero_denom = [MockNode(100), MockNode(5), MockNode(0)]
            >>> try:
            ...     strategy.calculate(nodes_zero_denom, "2023")
            ... except ZeroDivisionError as e:
            ...     # Example: logging the error instead of printing
            ...     logger.error(e)
            Division by zero: Denominator product is zero
        """
        if len(inputs) < 2:
            raise CalculationError(
                "Division calculation requires at least two input nodes",
                details={"strategy": "DivisionCalculation", "input_count": len(inputs)},
            )

        logger.debug(f"Applying division calculation for period {period}")

        values = [node.calculate(period) for node in inputs]
        numerator = values[0]

        denominator = 1.0
        for val in values[1:]:
            denominator *= val

        if denominator == 0.0:
            raise CalculationError(
                "Division by zero: Denominator product is zero",
                period=period,
                details={"numerator": numerator, "denominator": denominator},
            )

        return numerator / denominator

    @property
    def description(self) -> str:
        """Returns a description of the division calculation."""
        return "Division (first input / product of subsequent inputs)"


class WeightedAverageCalculation(Calculation):
    """Calculates the weighted average of input node values.

    This calculation computes the average of the values from input nodes, where each
    node's contribution is weighted. If no weights are provided during
    initialization, it defaults to an equal weighting (simple average).
    """

    def __init__(self, weights: Optional[list[float]] = None):
        """Initializes the WeightedAverageCalculation.

        Args:
            weights: An optional list of floats representing the weight for each
                corresponding input node. The length of this list must match the
                number of input nodes provided to the `calculate` method. If None,
                equal weights are assumed.
        """
        # Validate weights if provided immediately? No, validation happens in calculate
        # as the number of inputs isn't known here.
        self.weights = weights
        logger.info(f"Initialized WeightedAverageCalculation with weights: {weights}")

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Computes the weighted average of the input node values for the period.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023H1") for the calculation.

        Returns:
            The calculated weighted average as a float.

        Raises:
            ValueError: If the `inputs` list is empty.
            ValueError: If `weights` were provided during initialization and their
                count does not match the number of `inputs`.
            ValueError: If the sum of weights is zero (to prevent division by zero
                if normalization were implemented differently).

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> # Equal weights (simple average)
            >>> strategy_equal = WeightedAverageCalculation()
            >>> nodes = [MockNode(10), MockNode(20), MockNode(30)]
            >>> strategy_equal.calculate(nodes, "2023")
            20.0
            >>> # Custom weights
            >>> strategy_custom = WeightedAverageCalculation(weights=[0.5, 0.3, 0.2])
            >>> strategy_custom.calculate(nodes, "2023")
            17.0
            >>> # Mismatched weights
            >>> strategy_mismatch = WeightedAverageCalculation(weights=[0.5, 0.5])
            >>> try:
            ...     strategy_mismatch.calculate(nodes, "2023")
            ... except ValueError as e:
            ...     # Example: logging the error instead of printing
            ...     logger.error(e)
            Number of weights (2) must match number of inputs (3)
        """
        if not inputs:
            raise CalculationError(
                "Weighted average calculation requires at least one input node",
                details={"strategy": "WeightedAverageCalculation"},
            )

        num_inputs = len(inputs)
        effective_weights: list[float]

        if self.weights is None:
            # Use equal weights if none provided
            if num_inputs == 0:  # Should be caught by the check above, but defensive
                return 0.0
            equal_weight = 1.0 / num_inputs
            effective_weights = [equal_weight] * num_inputs
            logger.debug("Using equal weights for weighted average.")
        elif len(self.weights) == num_inputs:
            effective_weights = self.weights
            logger.debug(f"Using provided weights: {effective_weights}")
        else:
            raise StrategyError(
                f"Number of weights ({len(self.weights)}) must match "
                f"number of inputs ({num_inputs})",
                strategy_type="WeightedAverageCalculation",
            )

        logger.debug(f"Applying weighted average calculation for period {period}")
        weighted_sum = 0.0
        total_weight = sum(effective_weights)
        input_values = [node.calculate(period) for node in inputs]

        if total_weight == 0.0:
            # Avoid division by zero. If weights are all zero, the concept is ill-defined.
            # Returning 0 might be a reasonable default, or raising an error.
            # Let's raise ValueError for clarity.
            raise CalculationError(
                "Total weight for weighted average cannot be zero.",
                period=period,
                details={"weights": effective_weights},
            )

        for value, weight in zip(input_values, effective_weights):
            weighted_sum += value * weight

        # If weights don't sum to 1, this isn't a standard weighted average.
        # Decide whether to normalize or return the weighted sum directly.
        # Normalize by total weight for a true weighted average.
        return weighted_sum / total_weight

    @property
    def description(self) -> str:
        """Returns a description of the weighted average calculation."""
        if self.weights:
            return f"Weighted Average (using provided weights: {self.weights})"
        else:
            return "Weighted Average (using equal weights)"


# Type alias for the custom formula function
FormulaFunc = Callable[[dict[str, float]], float]


class CustomFormulaCalculation(Calculation):
    """Executes a user-defined Python function to calculate a value.

    This calculation provides maximum flexibility by allowing any custom Python
    function to be used for calculation. The function receives a dictionary
    mapping input node names (or fallback names) to their calculated values
    for the period and should return a single float result.
    """

    def __init__(self, formula_function: FormulaFunc):
        """Initializes the CustomFormulaCalculation with a calculation function.

        Args:
            formula_function: A callable (function, lambda, etc.) that accepts
                a single argument: a dictionary mapping string keys (input node
                names or `input_<i>`) to their float values for the period.
                It must return a float.

        Raises:
            TypeError: If `formula_function` is not callable.
        """
        if not callable(formula_function):
            raise StrategyError(
                "formula_function must be callable",
                strategy_type="CustomFormulaCalculation",
            )
        self.formula_function = formula_function
        logger.info(
            f"Initialized CustomFormulaCalculation with function: {formula_function.__name__}"
        )

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Applies the custom formula function to the calculated input values.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2025M1") for the calculation.

        Returns:
            The float result returned by the `formula_function`.

        Raises:
            ValueError: If the `formula_function` encounters an error during execution
                (e.g., incorrect input keys, calculation errors). Wraps the original
                exception.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, name, value): self.name = name; self._value = value
            ...     def calculate(self, period): return self._value
            >>> def my_formula(data):
            ...     # Example: Gross Profit Margin
            ...     return (data['revenue'] - data['cogs']) / data['revenue'] * 100
            >>> strategy = CustomFormulaCalculation(my_formula)
            >>> nodes = [MockNode('revenue', 1000), MockNode('cogs', 600)]
            >>> strategy.calculate(nodes, "2023")
            40.0
            >>> # Example with unnamed nodes
            >>> def simple_sum(data):
            ...     return data['input_0'] + data['input_1']
            >>> strategy_unnamed = CustomFormulaCalculation(simple_sum)
            >>> nodes_unnamed = [MockNode(None, 10), MockNode(None, 20)] # No names
            >>> strategy_unnamed.calculate(nodes_unnamed, "2023")
            30.0
        """
        # Prepare input values dictionary, using names if available
        input_values: dict[str, float] = {}
        for i, node in enumerate(inputs):
            # Prefer node.name if it exists and is a non-empty string
            key = getattr(node, "name", None)
            if not isinstance(key, str) or not key:
                key = f"input_{i}"
            input_values[key] = node.calculate(period)

        logger.debug(
            f"Applying custom formula calculation for period {period} with inputs: {input_values}"
        )
        try:
            # Execute the user-provided function
            result = self.formula_function(input_values)
            if not isinstance(result, int | float):
                logger.warning(
                    f"Custom formula function {self.formula_function.__name__} "
                    f"returned non-numeric type: {type(result)}. Attempting cast."
                )
                # Attempt conversion, but be aware this might fail or be lossy
                try:
                    return float(result)
                except (ValueError, TypeError) as cast_err:
                    raise CalculationError(
                        f"Custom formula {self.formula_function.__name__} result "
                        f"({result!r}) could not be cast to float.",
                        period=period,
                        details={
                            "result": result,
                            "result_type": type(result).__name__,
                        },
                    ) from cast_err
            return float(result)  # Ensure result is float
        except Exception as e:
            # Catch any exception from the custom function and wrap it
            logger.error(
                f"Error executing custom formula '{self.formula_function.__name__}': {e}",
                exc_info=True,
            )
            raise CalculationError(
                f"Error in custom formula '{self.formula_function.__name__}': {e}",
                period=period,
                details={"original_error": str(e)},
            ) from e

    @property
    def description(self) -> str:
        """Returns a description of the custom formula calculation."""
        func_name = getattr(self.formula_function, "__name__", "[anonymous function]")
        return f"Custom Formula (using function: {func_name})"


class FormulaCalculation(Calculation):
    """Evaluates a mathematical formula string as a calculation strategy.

    This calculation parses and evaluates simple mathematical expressions
    involving input nodes. Supports basic arithmetic operators (+, -, *, /)
    and unary negation.

    Attributes:
        formula: The mathematical expression string to evaluate.
        input_variable_names: List of variable names used in the formula,
            corresponding to the order of input nodes.
        _ast: The parsed Abstract Syntax Tree of the formula.
    """

    # Supported AST operators mapping to Python operator functions
    OPERATORS: ClassVar[dict[Type[Any], Callable[..., float]]] = {
        ast.Add: operator.add,
        ast.Sub: operator.sub,
        ast.Mult: operator.mul,
        ast.Div: operator.truediv,
        ast.USub: operator.neg,
    }

    def __init__(self, formula: str, input_variable_names: list[str]):
        """Initialize the FormulaCalculation.

        Args:
            formula: The mathematical formula string (e.g., "a + b / 2").
            input_variable_names: List of variable names used in the formula,
                in the same order as the input nodes that will be provided
                to the calculate method.

        Raises:
            ValueError: If the formula string has invalid syntax.
        """
        self.formula = formula
        self.input_variable_names = input_variable_names
        try:
            # Parse the formula string into an AST expression
            self._ast = ast.parse(formula, mode="eval").body
        except SyntaxError as e:
            raise StrategyError(
                f"Invalid formula syntax: {formula}",
                strategy_type="FormulaCalculation",
            ) from e
        logger.info(
            f"Initialized FormulaCalculation with formula: {formula} and variables: {input_variable_names}"
        )

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculate the value by evaluating the formula with input node values.

        Args:
            inputs: A list of Node objects, in the same order as input_variable_names.
            period: The time period string for the calculation.

        Returns:
            The result of the formula evaluation.

        Raises:
            ValueError: If the number of inputs doesn't match input_variable_names,
                or if an error occurs during evaluation.
        """
        if len(inputs) != len(self.input_variable_names):
            raise StrategyError(
                f"Number of inputs ({len(inputs)}) must match number of variable names "
                f"({len(self.input_variable_names)})",
                strategy_type="FormulaCalculation",
            )

        # Create mapping of variable names to nodes
        variable_map = dict(zip(self.input_variable_names, inputs))

        logger.debug(f"Applying formula calculation for period {period}")
        try:
            return self._evaluate(self._ast, period, variable_map)
        except (ValueError, TypeError, KeyError, ZeroDivisionError) as e:
            raise CalculationError(
                f"Error evaluating formula: {self.formula}. Error: {e!s}",
                period=period,
                details={"formula": self.formula, "original_error": str(e)},
            ) from e

    def _evaluate(
        self, node: ast.AST, period: str, variable_map: dict[str, Node]
    ) -> float:
        """Recursively evaluate the parsed AST node for the formula.

        Args:
            node: The current AST node to evaluate.
            period: The time period context for the evaluation.
            variable_map: Mapping of variable names to Node objects.

        Returns:
            The result of evaluating the AST node.

        Raises:
            TypeError: If a non-numeric constant or input node value is encountered.
            ValueError: If an unknown variable or unsupported operator/syntax is found.
            ZeroDivisionError: If division by zero occurs.
        """
        # Numeric literal (Constant in Python 3.8+)
        if isinstance(node, ast.Constant):
            if isinstance(node.value, int | float):
                return float(node.value)
            else:
                raise CalculationError(
                    f"Unsupported constant type '{type(node.value).__name__}' in formula",
                    period=period,
                    details={"constant_type": type(node.value).__name__},
                )

        # Variable reference
        elif isinstance(node, ast.Name):
            var_name = node.id
            if var_name not in variable_map:
                raise CalculationError(
                    f"Unknown variable '{var_name}' in formula. Available: {list(variable_map.keys())}",
                    period=period,
                    details={
                        "unknown_var": var_name,
                        "available_vars": list(variable_map.keys()),
                    },
                )
            input_node = variable_map[var_name]
            # Recursively calculate the value of the input node
            value = input_node.calculate(period)
            if not isinstance(value, int | float):
                raise CalculationError(
                    f"Input node '{input_node.name}' (variable '{var_name}') did not return a numeric value for period '{period}'",
                    node_id=input_node.name,
                    period=period,
                    details={"value_type": type(value).__name__},
                )
            return float(value)

        # Binary operation (e.g., a + b)
        elif isinstance(node, ast.BinOp):
            left_val = self._evaluate(node.left, period, variable_map)
            right_val = self._evaluate(node.right, period, variable_map)
            op_type = type(node.op)  # type: Type[Any]
            if op_type not in self.OPERATORS:
                raise StrategyError(
                    f"Unsupported binary operator '{op_type.__name__}' in formula",
                    strategy_type="FormulaCalculation",
                )
            # Perform the operation
            return float(self.OPERATORS[op_type](left_val, right_val))

        # Unary operation (e.g., -a)
        elif isinstance(node, ast.UnaryOp):
            operand_val = self._evaluate(node.operand, period, variable_map)
            unary_op_type = type(node.op)
            if unary_op_type not in self.OPERATORS:
                raise StrategyError(
                    f"Unsupported unary operator '{unary_op_type.__name__}' in formula",
                    strategy_type="FormulaCalculation",
                )
            # Perform the operation
            return float(self.OPERATORS[unary_op_type](operand_val))

        # If the node type is unsupported
        else:
            raise StrategyError(
                f"Unsupported syntax node type '{type(node).__name__}' in formula: {ast.dump(node)}",
                strategy_type="FormulaCalculation",
            )

    @property
    def description(self) -> str:
        """Returns a description of the formula calculation."""
        return f"Formula: {self.formula}"



================================================================================
File: fin_statement_model/core/calculations/registry.py
================================================================================

"""Registry for calculation classes in the Financial Statement Model.

This module provides a central registry for discovering and accessing different
calculation classes. Calculations can be registered using their class
object and later retrieved by their class name.
"""

# Use lowercase built-in types
from typing import ClassVar  # Keep Type for now
import logging

from .calculation import Calculation

# Configure logging
logger = logging.getLogger(__name__)


class Registry:
    """A central registry for managing and accessing calculation classes.

    This class uses class methods to provide a global registry. Calculations
    are stored in a dictionary mapping their class name (string) to the
    calculation class itself.

    Attributes:
        _strategies: A dictionary holding the registered calculation classes.
                     Keys are calculation class names (str), values are calculation
                     types (Type[Calculation]).
    """

    _strategies: ClassVar[dict[str, type[Calculation]]] = {}  # Use dict, type

    @classmethod
    def register(cls, calculation: type[Calculation]) -> None:
        """Register a calculation class with the registry.

        If a calculation with the same name is already registered, it will be
        overwritten.

        Args:
            calculation: The calculation class (Type[Calculation]) to register.
                         The class's __name__ attribute will be used as the key.
        """
        if not issubclass(calculation, Calculation):
            raise TypeError(
                f"Can only register subclasses of Calculation, not {calculation}"
            )
        cls._strategies[calculation.__name__] = calculation
        logger.debug(f"Registered calculation: {calculation.__name__}")

    @classmethod
    def get(cls, name: str) -> type[Calculation]:
        """Retrieve a calculation class from the registry by its name.

        Args:
            name: The string name of the calculation class to retrieve.

        Returns:
            The calculation class (Type[Calculation]) associated with the given name.

        Raises:
            KeyError: If no calculation with the specified name is found in the
                      registry.
        """
        # Debug print including id of the dictionary
        if name not in cls._strategies:
            logger.error(f"Attempted to access unregistered calculation: {name}")
            raise KeyError(f"Calculation '{name}' not found in registry.")
        return cls._strategies[name]

    @classmethod
    def list(cls) -> dict[str, type[Calculation]]:  # Use dict, type
        """List all registered calculation classes.

        Returns:
            A dictionary containing all registered calculation names (str) and their
            corresponding calculation classes (Type[Calculation]). Returns a copy
            to prevent modification of the internal registry.
        """
        return cls._strategies.copy()



================================================================================
File: fin_statement_model/core/errors.py
================================================================================

"""Define custom exceptions for the Financial Statement Model.

This module defines exception classes for specific error cases in the
Financial Statement Model, allowing for more precise error handling
and better error messages.
"""

from typing import Optional, Any


class FinancialModelError(Exception):
    """Define the base exception class for all Financial Statement Model errors.

    All custom exceptions raised within the library should inherit from this class.

    Args:
        message: A human-readable description of the error.
    """

    def __init__(self, message: str):
        """Initializes the FinancialModelError."""
        self.message = message
        super().__init__(self.message)


class ConfigurationError(FinancialModelError):
    """Raise an error for invalid configuration files or objects.

    This typically occurs when parsing or validating configuration data,
    such as YAML files defining metrics or statement structures.

    Args:
        message: The base error message.
        config_path: Optional path to the configuration file where the error occurred.
        errors: Optional list of specific validation errors found.

    Examples:
        >>> raise ConfigurationError("Invalid syntax", config_path="config.yaml")
        >>> raise ConfigurationError(
        ...     "Missing required fields",
        ...     config_path="metrics.yaml",
        ...     errors=["Missing 'formula' for 'revenue'"]
        ... )
    """

    def __init__(
        self,
        message: str,
        config_path: Optional[str] = None,
        errors: Optional[list[str]] = None,
    ):
        """Initializes the ConfigurationError."""
        self.config_path = config_path
        self.errors = errors or []

        if config_path and errors:
            full_message = f"{message} in {config_path}: {'; '.join(errors)}"
        elif config_path:
            full_message = f"{message} in {config_path}"
        elif errors:
            full_message = f"{message}: {'; '.join(errors)}"
        else:
            full_message = message

        super().__init__(full_message)


class CalculationError(FinancialModelError):
    """Raise an error during calculation operations.

    This indicates a problem while computing the value of a node, often due
    to issues with the calculation logic, input data, or strategy used.

    Args:
        message: The base error message.
        node_id: Optional ID of the node where the calculation failed.
        period: Optional period for which the calculation failed.
        details: Optional dictionary containing additional context about the error.

    Examples:
        >>> raise CalculationError("Division by zero", node_id="profit_margin", period="2023-Q1")
        >>> raise CalculationError(
        ...     "Incompatible input types",
        ...     node_id="total_assets",
        ...     details={"input_a_type": "str", "input_b_type": "int"}
        ... )
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str] = None,
        period: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
    ):
        """Initializes the CalculationError."""
        self.node_id = node_id
        self.period = period
        self.details = details or {}

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if period:
            context.append(f"period '{period}'")

        full_message = f"{message} for {' and '.join(context)}" if context else message

        # Append details to the message for better context
        if self.details:
            details_str = ", ".join(f'{k}="{v}"' for k, v in self.details.items())
            # Prioritize showing the original underlying error if captured
            original_error_str = self.details.get("original_error")
            if original_error_str:
                full_message = f"{full_message}: {original_error_str}"
            else:
                full_message = f"{full_message} (Details: {details_str})"

        super().__init__(full_message)


class NodeError(FinancialModelError):
    """Raise an error for issues related to graph nodes.

    This covers issues like trying to access a non-existent node,
    invalid node configurations, or type mismatches related to nodes.

    Args:
        message: The base error message.
        node_id: Optional ID of the node related to the error.

    Examples:
        >>> raise NodeError("Node not found", node_id="non_existent_node")
        >>> raise NodeError("Invalid node type for operation", node_id="revenue")
    """

    def __init__(self, message: str, node_id: Optional[str] = None):
        """Initializes the NodeError."""
        self.node_id = node_id

        full_message = f"{message} for node '{node_id}'" if node_id else message

        super().__init__(full_message)


class MissingInputError(FinancialModelError):
    """Raise an error when a required calculation input is missing.

    This occurs when a calculation node needs data from another node for a
    specific period, but that data is unavailable.

    Args:
        message: The base error message.
        node_id: Optional ID of the node requiring the input.
        input_name: Optional name or ID of the missing input node.
        period: Optional period for which the input was missing.

    Examples:
        >>> raise MissingInputError(
        ...     "Required input data unavailable",
        ...     node_id="cogs",
        ...     input_name="inventory",
        ...     period="2023-12-31"
        ... )
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str] = None,
        input_name: Optional[str] = None,
        period: Optional[str] = None,
    ):
        """Initializes the MissingInputError."""
        self.node_id = node_id
        self.input_name = input_name
        self.period = period

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if input_name:
            context.append(f"input '{input_name}'")
        if period:
            context.append(f"period '{period}'")

        full_message = f"{message} for {' in '.join(context)}" if context else message

        super().__init__(full_message)


class GraphError(FinancialModelError):
    """Raise an error for invalid graph structure or operations.

    This covers issues like inconsistencies in the graph (e.g., orphaned nodes),
    problems during graph traversal, or invalid modifications to the graph.

    Args:
        message: The base error message.
        nodes: Optional list of node IDs involved in the graph error.

    Examples:
        >>> raise GraphError("Orphaned node detected", nodes=["unconnected_node"])
        >>> raise GraphError("Failed to add edge due to type mismatch")
    """

    def __init__(self, message: str, nodes: Optional[list[str]] = None):
        """Initializes the GraphError."""
        self.nodes = nodes or []

        full_message = (
            f"{message} involving nodes: {', '.join(nodes)}" if nodes else message
        )

        super().__init__(full_message)


class DataValidationError(FinancialModelError):
    """Raise an error for data validation failures.

    This typically occurs during data import or preprocessing when data
    does not conform to expected formats, types, or constraints.

    Args:
        message: The base error message.
        validation_errors: Optional list of specific validation failures.

    Examples:
        >>> raise DataValidationError(
        ...     "Input data failed validation",
        ...     validation_errors=["Column 'Date' has invalid format", "Value '-100' is not allowed for 'Revenue'"]
        ... )
    """

    def __init__(self, message: str, validation_errors: Optional[list[str]] = None):
        """Initializes the DataValidationError."""
        self.validation_errors = validation_errors or []

        if validation_errors:
            full_message = f"{message}: {'; '.join(validation_errors)}"
        else:
            full_message = message

        super().__init__(full_message)


class CircularDependencyError(FinancialModelError):
    """Raise an error when a circular dependency is detected in calculations.

    This occurs if the calculation graph contains cycles, meaning a node
    directly or indirectly depends on itself.

    Args:
        message: The base error message. Defaults to "Circular dependency detected".
        cycle: Optional list of node IDs forming the detected cycle.

    Examples:
        >>> raise CircularDependencyError(cycle=["node_a", "node_b", "node_c", "node_a"])
    """

    def __init__(
        self,
        message: str = "Circular dependency detected",
        cycle: Optional[list[str]] = None,
    ):
        """Initializes the CircularDependencyError."""
        self.cycle = cycle or []

        if cycle:
            cycle_str = " -> ".join(cycle)
            full_message = f"{message}: {cycle_str}"
        else:
            full_message = message

        super().__init__(full_message)


class PeriodError(FinancialModelError):
    """Raise an error for invalid or missing periods.

    This covers issues like requesting data for a non-existent period or
    using invalid period formats.

    Args:
        message: The base error message.
        period: Optional specific period involved in the error.
        available_periods: Optional list of valid periods.

    Examples:
        >>> raise PeriodError("Invalid period format", period="2023Q5")
        >>> raise PeriodError("Period not found", period="2024-01-01", available_periods=["2023-12-31"])
    """

    def __init__(
        self,
        message: str,
        period: Optional[str] = None,
        available_periods: Optional[list[str]] = None,
    ):
        """Initializes the PeriodError."""
        self.period = period
        self.available_periods = available_periods or []

        if period and available_periods:
            full_message = f"{message} for period '{period}'. Available periods: {', '.join(available_periods)}"
        elif period:
            full_message = f"{message} for period '{period}'"
        else:
            full_message = message

        super().__init__(full_message)


class StatementError(FinancialModelError):
    """Raise an error for issues related to financial statements.

    This is used for errors specific to the structure, definition, or
    processing of financial statements (e.g., Balance Sheet, P&L).

    Args:
        message: The base error message.
        statement_id: Optional ID or name of the statement involved.

    Examples:
        >>> raise StatementError("Balance sheet does not balance", statement_id="BS_2023")
        >>> raise StatementError("Required account missing from P&L", statement_id="PnL_Q1")
    """

    def __init__(self, message: str, statement_id: Optional[str] = None):
        """Initializes the StatementError."""
        self.statement_id = statement_id

        full_message = (
            f"{message} for statement '{statement_id}'" if statement_id else message
        )

        super().__init__(full_message)


class StrategyError(FinancialModelError):
    """Raise an error for issues related to calculation strategies.

    This indicates a problem with the configuration or execution of a
    specific calculation strategy (e.g., Summation, GrowthRate).

    Args:
        message: The base error message.
        strategy_type: Optional name or type of the strategy involved.
        node_id: Optional ID of the node using the strategy.

    Examples:
        >>> raise StrategyError("Invalid parameter for GrowthRate strategy", strategy_type="GrowthRate", node_id="revenue_forecast")
        >>> raise StrategyError("Strategy not applicable to node type", strategy_type="Summation", node_id="text_description")
    """

    def __init__(
        self,
        message: str,
        strategy_type: Optional[str] = None,
        node_id: Optional[str] = None,
    ):
        """Initializes the StrategyError."""
        self.strategy_type = strategy_type
        self.node_id = node_id

        context = []
        if strategy_type:
            context.append(f"strategy type '{strategy_type}'")
        if node_id:
            context.append(f"node '{node_id}'")

        full_message = f"{message} for {' in '.join(context)}" if context else message

        super().__init__(full_message)


class TransformationError(FinancialModelError):
    """Raise an error during data transformation.

    This occurs during preprocessing steps when a specific transformation
    (e.g., normalization, scaling) fails.

    Args:
        message: The base error message.
        transformer_type: Optional name or type of the transformer involved.
        parameters: Optional dictionary of parameters used by the transformer.

    Examples:
        >>> raise TransformationError("Log transform requires positive values", transformer_type="LogTransformer")
        >>> raise TransformationError(
        ...     "Incompatible data type for scaling",
        ...     transformer_type="MinMaxScaler",
        ...     parameters={"feature_range": (0, 1)}
        ... )
    """

    def __init__(
        self,
        message: str,
        transformer_type: Optional[str] = None,
        parameters: Optional[dict[str, Any]] = None,
    ):
        """Initializes the TransformationError."""
        self.transformer_type = transformer_type
        self.parameters = parameters or {}

        if transformer_type:
            full_message = f"{message} in transformer '{transformer_type}'"
            if parameters:
                params_str = ", ".join(f"{k}={v}" for k, v in parameters.items())
                full_message = f"{full_message} with parameters: {params_str}"
        else:
            full_message = message

        super().__init__(full_message)


class MetricError(FinancialModelError):
    """Raise an error for issues related to metric definitions or registry.

    This covers issues with loading, validating, or accessing financial metrics,
    whether defined in YAML or Python code.

    Args:
        message: The base error message.
        metric_name: Optional name of the metric involved in the error.
        details: Optional dictionary containing additional context about the error.

    Examples:
        >>> raise MetricError("Metric definition not found", metric_name="unknown_ratio")
        >>> raise MetricError(
        ...     "Invalid formula syntax in metric definition",
        ...     metric_name="profitability_index",
        ...     details={"formula": "NPV / Initial Investment)"} # Missing parenthesis
        ... )
    """

    def __init__(
        self,
        message: str,
        metric_name: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
    ):
        """Initializes the MetricError."""
        self.metric_name = metric_name
        self.details = details or {}

        full_message = (
            f"{message} related to metric '{metric_name}'" if metric_name else message
        )

        super().__init__(full_message)



================================================================================
File: fin_statement_model/core/graph/__init__.py
================================================================================

"""Graph module for the financial statement model.

This module provides the core graph functionality for building and evaluating
financial statement models.
"""

from fin_statement_model.core.graph.graph import Graph
from fin_statement_model.core.graph.manipulator import GraphManipulator
from fin_statement_model.core.graph.traverser import GraphTraverser

__all__ = ["Graph", "GraphManipulator", "GraphTraverser"]



================================================================================
File: fin_statement_model/core/graph/graph.py
================================================================================

"""Provide graph operations for the financial statement model.

This module provides the `Graph` class that combines manipulation, traversal,
forecasting, and calculation capabilities for building and evaluating
financial statement models.
"""

import logging
from typing import Any, Optional, overload, Literal
from collections.abc import Callable
from uuid import UUID

from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.core.nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
)
from fin_statement_model.core.errors import (
    NodeError,
    ConfigurationError,
    CalculationError,
    CircularDependencyError,
)
from fin_statement_model.core.metrics import metric_registry
from fin_statement_model.core.calculations import Registry
from fin_statement_model.core.graph.manipulator import GraphManipulator
from fin_statement_model.core.graph.traverser import GraphTraverser
from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentType,
    AdjustmentTag,
    DEFAULT_SCENARIO,
    AdjustmentFilterInput,
)
from fin_statement_model.core.adjustments.manager import AdjustmentManager


# Configure logging
logger = logging.getLogger(__name__)

__all__ = ["Graph"]


class Graph:
    """Represent the financial statement model as a directed graph.

    This class integrates graph manipulation, traversal, forecasting, and
    calculation capabilities. It serves as the central orchestrator for nodes,
    periods, caching, and calculation workflows.

    Attributes:
        _nodes: A dict mapping node names (str) to Node objects.
        _periods: A list of time period identifiers (str) managed by the graph.
        _cache: A nested dict caching calculated values per node per period.
        _node_factory: An instance of NodeFactory for creating new nodes.
    """

    def __init__(self, periods: Optional[list[str]] = None):
        """Initialize the Graph instance.

        Set up core components: node registry, `DataManager`, and `CalculationEngine`.
        Optionally initialize the graph with a list of time periods.

        Args:
            periods: An optional list of strings representing the initial time
                     periods for the financial model (e.g., ["2023", "2024"]).
                     The `DataManager` will handle sorting and ensuring uniqueness.

        Raises:
            TypeError: If `periods` is provided but is not a list.

        Examples:
            >>> graph_no_periods = Graph()
            >>> logger.info(graph_no_periods.periods) # Output: []
            >>> graph_with_periods = Graph(periods=["2023", "2022"])
            >>> logger.info(graph_with_periods.periods) # Periods are sorted
            >>> try:
            ...     Graph(periods="2023") # Invalid type
            ... except TypeError as e:
            ...     logger.error(e)
        """
        # No super().__init__() needed as mixins don't have __init__
        # and GraphCore is removed.

        self._nodes: dict[str, Node] = {}

        # Initialize core attributes for periods, cache, and node factory
        self._periods: list[str] = []
        self._cache: dict[str, dict[str, float]] = {}
        self._node_factory: NodeFactory = NodeFactory()

        # Handle initial periods directly
        if periods:
            if not isinstance(periods, list):
                raise TypeError("Initial periods must be a list")
            self.add_periods(periods)

        self.manipulator = GraphManipulator(self)
        self.traverser = GraphTraverser(self)

        # --- Adjustment Manager Integration ---
        self.adjustment_manager = AdjustmentManager()
        # --- End Adjustment Manager Integration ---

    @property
    def nodes(self) -> dict[str, Node]:
        """Provide access to the dictionary of all nodes in the graph.

        Returns:
            A dictionary where keys are node names (str) and values are
            `Node` objects. This dictionary represents the shared node registry.

        Examples:
            >>> graph = Graph()
            >>> item_node = graph.add_financial_statement_item("Revenue", {"2023": 100})
            >>> logger.info(list(graph.nodes.keys()))
            >>> logger.info(graph.nodes["Revenue"] == item_node)
        """
        return self._nodes

    @property
    def periods(self) -> list[str]:
        """Retrieve the list of time periods currently managed by the graph.

        Returns:
            A sorted list of unique time period strings managed by the graph.

        Examples:
            >>> graph = Graph(periods=["2024", "2023"])
            >>> logger.info(graph.periods)
            >>> graph.add_periods(["2025"])
            >>> logger.info(graph.periods)
        """
        return self._periods

    def add_periods(self, periods: list[str]) -> None:
        """Add new time periods to the graph.

        Update the internal period list, ensuring uniqueness and sorting.

        Args:
            periods: A list of strings representing the time periods to add.

        Raises:
            TypeError: If `periods` is not a list.
        """
        if not isinstance(periods, list):
            raise TypeError("Periods must be provided as a list.")
        # Ensure unique and sorted periods
        combined = set(self._periods).union(periods)
        self._periods = sorted(combined)
        logger.debug(f"Added periods {periods}; current periods: {self._periods}")

    def add_calculation(
        self,
        name: str,
        input_names: list[str],
        operation_type: str,
        formula_variable_names: Optional[list[str]] = None,
        **calculation_kwargs: Any,
    ) -> Node:
        """Add a new calculation node to the graph using the node factory.

        Resolve input node names to Node objects, create a CalculationNode,
        register it in the graph, and return it.

        Args:
            name: Unique name for the calculation node.
            input_names: List of node names to use as inputs.
            operation_type: Calculation type key (e.g., 'addition').
            formula_variable_names: Optional list of variable names used in the formula
                string, required if creating a FormulaCalculationNode via this method.
            **calculation_kwargs: Additional parameters for the calculation constructor.

        Returns:
            The created calculation node.

        Raises:
            NodeError: If any input node name does not exist.
            ValueError: If the name is invalid or creation fails.
            TypeError: If inputs are invalid.
            CircularDependencyError: If adding the node would create a cycle.
        """
        # Validate inputs
        if not isinstance(input_names, list):
            raise TypeError("input_names must be a list of node names.")

        # Resolve input node names to Node objects
        resolved_inputs = self._resolve_input_nodes(input_names)

        # Create the node via factory
        try:
            node = self._node_factory.create_calculation_node(
                name=name,
                inputs=resolved_inputs,
                calculation_type=operation_type,
                formula_variable_names=formula_variable_names,
                **calculation_kwargs,
            )
        except (ValueError, TypeError):
            logger.exception(
                f"Failed to create calculation node '{name}' with type '{operation_type}'"
            )
            raise

        # Add with validation (includes cycle detection)
        added_node = self._add_node_with_validation(node)

        logger.info(
            f"Added calculation node '{name}' of type '{operation_type}' with inputs {input_names}"
        )
        return added_node

    def add_metric(
        self,
        metric_name: str,
        node_name: Optional[str] = None,
        *,
        input_node_map: Optional[dict[str, str]] = None,
    ) -> Node:
        """Add a metric calculation node based on a metric definition.

        If `node_name` is None, uses `metric_name` as the node name.

        Uses the metric registry to load inputs and formula, creates a
        calculation node using the formula strategy, registers it, and stores metric
        metadata on the node itself.

        Args:
            metric_name: Key of the metric definition to add.
            node_name: Optional name for the metric node; defaults to metric_name.
            input_node_map: Optional dictionary mapping metric input variable names
                (from metric definition) to the actual node names present in the graph.
                If None, assumes graph node names match metric input variable names.

        Returns:
            The created calculation node.

        Raises:
            TypeError: If node_name is invalid.
            ValueError: If node_name already exists.
            ConfigurationError: If metric definition is missing or invalid.
            NodeError: If required input nodes (after mapping) are missing.
        """
        # Default node_name to metric_name if not provided
        if node_name is None:
            node_name = metric_name
        if not node_name or not isinstance(node_name, str):
            raise TypeError("Metric node name must be a non-empty string.")
        # Check for name conflict
        if node_name in self._nodes:
            raise ValueError(
                f"A node with name '{node_name}' already exists in the graph."
            )

        # Load metric definition (Pydantic model)
        try:
            metric_def = metric_registry.get(metric_name)
        except KeyError as e:
            raise ConfigurationError(
                f"Unknown metric definition: '{metric_name}'"
            ) from e

        # Extract required fields from definition
        required_inputs = metric_def.inputs
        formula = metric_def.formula
        description = metric_def.description

        # Build list of input node names and formula variable names
        input_node_names: list[str] = []
        formula_variable_names: list[str] = []
        missing = []

        for req_input_name in required_inputs:
            # Determine the actual graph node name to look for
            target_node_name = req_input_name  # Default case
            if input_node_map and req_input_name in input_node_map:
                target_node_name = input_node_map[req_input_name]
            elif input_node_map:
                # If map provided but doesn't contain the required input, it's an error in the map
                missing.append(f"{req_input_name} (mapping missing in input_node_map)")
                continue  # Skip trying to find the node

            # Check if the node exists in the graph
            if target_node_name not in self._nodes:
                missing.append(target_node_name)  # Report the name we looked for
            else:
                input_node_names.append(target_node_name)
                formula_variable_names.append(
                    req_input_name
                )  # Use the metric's variable name

        if missing:
            raise NodeError(
                f"Cannot create metric '{metric_name}': missing required nodes {missing}",
                node_id=node_name,
            )

        # Create calculation node using add_calculation
        try:
            new_node = self.add_calculation(
                name=node_name,
                input_names=input_node_names,
                operation_type="formula",
                formula_variable_names=formula_variable_names,
                formula=formula,
                metric_name=metric_name,  # Pass metric metadata
                metric_description=description,  # Pass metric description
            )
        except Exception as e:
            logger.exception(
                f"Failed to create calculation node for metric '{metric_name}' as node '{node_name}'"
            )
            # Re-raise as ConfigurationError or keep original, depending on desired error reporting
            raise ConfigurationError(
                f"Error creating node for metric '{metric_name}': {e}"
            ) from e

        logger.info(
            f"Added metric '{metric_name}' as calculation node '{node_name}' with inputs {input_node_names}"
        )
        return new_node

    def add_custom_calculation(
        self,
        name: str,
        calculation_func: Callable[..., float],
        inputs: Optional[list[str]] = None,
        description: str = "",
    ) -> Node:
        """Add a custom calculation node using a Python callable.

        Args:
            name: Unique name for the custom calculation node.
            calculation_func: A callable that accepts (period, **inputs) and returns float.
            inputs: Optional list of node names to use as inputs.
            description: Optional description of the calculation.

        Returns:
            The created custom calculation node.

        Raises:
            NodeError: If any specified input nodes are missing.
            TypeError: If calculation_func is not callable.
            CircularDependencyError: If adding the node would create a cycle.
        """
        # Validate callable
        if not callable(calculation_func):
            raise TypeError("calculation_func must be callable.")

        # Resolve inputs if provided
        resolved_inputs: list[Node] = []
        if inputs is not None:
            if not isinstance(inputs, list):
                raise TypeError("inputs must be a list of node names.")
            resolved_inputs = self._resolve_input_nodes(inputs)

        # Create custom node via factory
        try:
            custom_node = self._node_factory._create_custom_node_from_callable(
                name=name,
                inputs=resolved_inputs,
                formula=calculation_func,
                description=description,
            )
        except (ValueError, TypeError):
            logger.exception(f"Failed to create custom calculation node '{name}'")
            raise

        # Add with validation (includes cycle detection)
        added_node = self._add_node_with_validation(custom_node)

        logger.info(f"Added custom calculation node '{name}' with inputs {inputs}")
        return added_node

    def change_calculation_method(
        self,
        node_name: str,
        new_method_key: str,
        **kwargs: dict[str, Any],
    ) -> None:
        """Change the calculation method for an existing calculation-based node.

        Args:
            node_name: Name of the existing calculation node.
            new_method_key: Key of the new calculation method to apply.
            **kwargs: Additional parameters required by the new calculation.

        Returns:
            None

        Raises:
            NodeError: If the target node does not exist or is not a CalculationNode.
            ValueError: If `new_method_key` is not a recognized calculation key.
            TypeError: If the new calculation cannot be instantiated with the provided arguments.

        Examples:
            >>> graph.change_calculation_method("GrossProfit", "addition")
        """
        node = self.manipulator.get_node(node_name)
        if node is None:
            raise NodeError("Node not found for calculation change", node_id=node_name)
        if not isinstance(node, CalculationNode):
            raise NodeError(
                f"Node '{node_name}' is not a CalculationNode", node_id=node_name
            )
        # Map method key to registry name
        if new_method_key not in self._node_factory._calculation_methods:
            raise ValueError(f"Calculation '{new_method_key}' is not recognized.")
        calculation_class_name = self._node_factory._calculation_methods[new_method_key]
        try:
            calculation_cls = Registry.get(calculation_class_name)
        except KeyError as e:
            raise ValueError(
                f"Calculation class '{calculation_class_name}' not found in registry."
            ) from e
        try:
            calculation_instance = calculation_cls(**kwargs)
        except TypeError as e:
            raise TypeError(
                f"Failed to instantiate calculation '{new_method_key}': {e}"
            )
        # Apply new calculation
        node.set_calculation(calculation_instance)
        # Clear cached calculations for this node
        if node_name in self._cache:
            del self._cache[node_name]
        logger.info(f"Changed calculation for node '{node_name}' to '{new_method_key}'")

    def get_metric(self, metric_id: str) -> Optional[Node]:
        """Return the metric node for a given metric ID, if present.

        Searches for a node with the given ID that was created as a metric
        (identified by having a `metric_name` attribute).

        Args:
            metric_id: Identifier of the metric node to retrieve.

        Returns:
            The Node corresponding to `metric_id` if it's a metric node, or None.

        Examples:
            >>> m = graph.get_metric("current_ratio")
            >>> if m:
            ...     logger.info(m.name)
        """
        node = self._nodes.get(metric_id)
        # Check if the node exists and has the metric_name attribute populated
        if node and getattr(node, "metric_name", None) == metric_id:
            return node
        return None

    def get_available_metrics(self) -> list[str]:
        """Return a sorted list of all metric node IDs currently in the graph.

        Identifies metric nodes by checking for the presence and non-None value
        of the `metric_name` attribute.

        Returns:
            A sorted list of metric node names.

        Examples:
            >>> graph.get_available_metrics()
            ['current_ratio', 'debt_equity_ratio']
        """
        # Iterate through all nodes and collect names of those that are metrics
        metric_node_names = [
            node.name
            for node in self._nodes.values()
            if getattr(node, "metric_name", None) is not None
        ]
        return sorted(metric_node_names)

    def get_metric_info(self, metric_id: str) -> dict[str, Any]:
        """Return detailed information for a specific metric node.

        Args:
            metric_id: Identifier of the metric node to inspect.

        Returns:
            A dict containing 'id', 'name', 'description', and 'inputs' for the metric.

        Raises:
            ValueError: If `metric_id` does not correspond to a metric node.

        Examples:
            >>> info = graph.get_metric_info("current_ratio")
            >>> logger.info(info['inputs'])
        """
        metric_node = self.get_metric(metric_id)
        if metric_node is None:
            if metric_id in self._nodes:
                raise ValueError(
                    f"Node '{metric_id}' exists but is not a metric (missing metric_name attribute)."
                )
            raise ValueError(f"Metric node '{metric_id}' not found in graph.")

        # Extract info directly from the FormulaCalculationNode
        try:
            # Use getattr for safety, retrieving stored metric metadata
            description = getattr(metric_node, "metric_description", "N/A")
            # metric_name stored on the node is the key from the registry
            registry_key = getattr(metric_node, "metric_name", metric_id)

            # We might want the display name from the original definition.
            # Fetch the definition again if needed for the display name.
            try:
                metric_def = metric_registry.get(registry_key)
                display_name = metric_def.name
            except Exception:
                logger.warning(
                    f"Could not reload metric definition for '{registry_key}' to get display name. Using node name '{metric_id}' instead."
                )
                display_name = metric_id  # Fallback to node name

            inputs = metric_node.get_dependencies()
        except Exception as e:
            # Catch potential attribute errors or other issues
            logger.error(
                f"Error retrieving info for metric node '{metric_id}': {e}",
                exc_info=True,
            )
            raise ValueError(
                f"Failed to retrieve metric info for '{metric_id}': {e}"
            ) from e

        return {
            "id": metric_id,
            "name": display_name,
            "description": description,
            "inputs": inputs,
        }

    @overload
    def get_adjusted_value(
        self,
        node_name: str,
        period: str,
        filter_input: "AdjustmentFilterInput" = None,
        *,
        return_flag: Literal[True],
    ) -> tuple[float, bool]: ...

    @overload
    def get_adjusted_value(
        self,
        node_name: str,
        period: str,
        filter_input: "AdjustmentFilterInput" = None,
        *,
        return_flag: Literal[False] = False,
    ) -> float: ...

    def get_adjusted_value(
        self,
        node_name: str,
        period: str,
        filter_input: "AdjustmentFilterInput" = None,
        *,
        return_flag: bool = False,
    ) -> float | tuple[float, bool]:
        """Calculates the value of a node for a period, applying selected adjustments.

        Fetches the base calculated value, retrieves adjustments matching the filter,
        applies them in order, and returns the result.

        Args:
            node_name: The name of the node to calculate.
            period: The time period identifier.
            filter_input: Criteria for selecting which adjustments to apply.
                          Can be an AdjustmentFilter instance, a set of tags (for include_tags),
                          a callable predicate `fn(adj: Adjustment) -> bool`, or None
                          (applies all adjustments in the default scenario).
            return_flag: If True, return a tuple (adjusted_value, was_adjusted_flag).
                         If False (default), return only the adjusted_value.

        Returns:
            The adjusted float value, or a tuple (value, flag) if return_flag is True.

        Raises:
            NodeError: If the specified node does not exist.
            CalculationError: If an error occurs during the base calculation or adjustment application.
            TypeError: If filter_input is an invalid type.
        """
        # 1. Get the base value (result of underlying node calculation)
        try:
            base_value = self.calculate(node_name, period)
        except (NodeError, CalculationError, TypeError):
            # Propagate errors from base calculation
            logger.exception(
                f"Error getting base value for '{node_name}' in period '{period}'"
            )
            raise

        # 2. Get filtered adjustments from the manager
        try:
            adjustments_to_apply = self.adjustment_manager.get_filtered_adjustments(
                node_name=node_name, period=period, filter_input=filter_input
            )
        except TypeError:
            logger.exception("Invalid filter type provided for get_adjusted_value")
            raise

        # 3. Apply the adjustments
        adjusted_value, was_adjusted = self.adjustment_manager.apply_adjustments(
            base_value, adjustments_to_apply
        )

        # 4. Return result based on flag
        if return_flag:
            return adjusted_value, was_adjusted
        else:
            return adjusted_value

    def calculate(self, node_name: str, period: str) -> float:
        """Calculate and return the value of a specific node for a given period.

        This method uses internal caching to speed repeated calls, and wraps
        underlying errors in CalculationError for clarity.

        Args:
            node_name: Name of the node to calculate.
            period: Time period identifier for the calculation.

        Returns:
            The calculated float value for the node and period.

        Raises:
            NodeError: If the specified node does not exist.
            TypeError: If the node has no callable `calculate` method.
            CalculationError: If an error occurs during the node's calculation.

        Examples:
            >>> value = graph.calculate("Revenue", "2023")
        """
        # Return cached value if present
        if node_name in self._cache and period in self._cache[node_name]:
            logger.debug(f"Cache hit for node '{node_name}', period '{period}'")
            return self._cache[node_name][period]
        # Resolve node
        node = self.manipulator.get_node(node_name)
        if node is None:
            raise NodeError(f"Node '{node_name}' not found", node_id=node_name)
        # Validate calculate method
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise TypeError(f"Node '{node_name}' has no callable calculate method.")
        # Perform calculation with error handling
        try:
            value = node.calculate(period)
        except (
            NodeError,
            ConfigurationError,
            CalculationError,
            ValueError,
            KeyError,
            ZeroDivisionError,
        ) as e:
            logger.error(
                f"Error calculating node '{node_name}' for period '{period}': {e}",
                exc_info=True,
            )
            raise CalculationError(
                message=f"Failed to calculate node '{node_name}'",
                node_id=node_name,
                period=period,
                details={"original_error": str(e)},
            ) from e
        # Cache and return
        self._cache.setdefault(node_name, {})[period] = value
        logger.debug(f"Cached value for node '{node_name}', period '{period}': {value}")
        return value

    def recalculate_all(self, periods: Optional[list[str]] = None) -> None:
        """Recalculate all nodes for given periods, clearing all caches first.

        Args:
            periods: List of period strings, a single string, or None to use all periods.

        Returns:
            None

        Raises:
            TypeError: If `periods` is not a list, string, or None.

        Examples:
            >>> graph.recalculate_all(["2023", "2024"])
        """
        # Normalize periods input
        if periods is None:
            periods_to_use = self.periods
        elif isinstance(periods, str):
            periods_to_use = [periods]
        elif isinstance(periods, list):
            periods_to_use = periods
        else:
            raise TypeError(
                "Periods must be a list of strings, a single string, or None."
            )
        # Clear all caches (node-level and central) to force full recalculation
        self.clear_all_caches()
        if not periods_to_use:
            return
        # Recalculate each node for each period
        for node_name in list(self._nodes.keys()):
            for period in periods_to_use:
                try:
                    self.calculate(node_name, period)
                except Exception as e:
                    logger.warning(
                        f"Error recalculating node '{node_name}' for period '{period}': {e}"
                    )

    def clear_all_caches(self) -> None:
        """Clear all node-level and central calculation caches.

        Returns:
            None

        Examples:
            >>> graph.clear_all_caches()
        """
        logger.debug(f"Clearing node-level caches for {len(self.nodes)} nodes.")
        for node in self.nodes.values():
            if hasattr(node, "clear_cache"):
                try:
                    node.clear_cache()
                except Exception as e:
                    logger.warning(f"Failed to clear cache for node '{node.name}': {e}")
        # Clear central calculation cache
        self.clear_calculation_cache()
        logger.debug("Cleared central calculation cache.")

    def clear_calculation_cache(self) -> None:
        """Clear the graph's internal calculation cache.

        Returns:
            None

        Examples:
            >>> graph.clear_calculation_cache()
        """
        self._cache.clear()
        logger.debug("Cleared graph calculation cache.")

    def clear(self) -> None:
        """Reset the graph by clearing nodes, periods, adjustments, and caches."""
        self._nodes = {}
        self._periods = []
        self._cache = {}

        # --- Adjustment Manager Integration ---
        self.adjustment_manager.clear_all()
        # --- End Adjustment Manager Integration ---

        logger.info("Graph cleared: nodes, periods, adjustments, and caches reset.")

    def add_financial_statement_item(
        self, name: str, values: dict[str, float]
    ) -> FinancialStatementItemNode:
        """Add a basic financial statement item (data node) to the graph.

        Args:
            name: Unique name for the financial statement item node.
            values: Mapping of period strings to float values for this item.

        Returns:
            The newly created `FinancialStatementItemNode`.

        Raises:
            ValueError: If node name is invalid.
            TypeError: If `values` is not a dict or contains invalid types.

        Examples:
            >>> item_node = graph.add_financial_statement_item("SG&A", {"2023": 50.0})
            >>> item_node.get_value("2023")
            50.0
        """
        # Validate inputs
        if not isinstance(values, dict):
            raise TypeError("Values must be provided as a dict[str, float]")

        # Create a new financial statement item node
        new_node = self._node_factory.create_financial_statement_item(
            name=name, values=values.copy()
        )

        # Add with validation (no cycle detection needed for data nodes)
        # Cast to FinancialStatementItemNode for correct return type
        from typing import cast

        added_node = cast(
            FinancialStatementItemNode,
            self._add_node_with_validation(
                new_node,
                check_cycles=False,  # Data nodes don't have inputs, so no cycles possible
                validate_inputs=False,  # Data nodes don't have inputs to validate
            ),
        )

        logger.info(
            f"Added FinancialStatementItemNode '{name}' with periods {list(values.keys())}"
        )
        return added_node

    def update_financial_statement_item(
        self, name: str, values: dict[str, float], replace_existing: bool = False
    ) -> FinancialStatementItemNode:
        """Update values for an existing financial statement item node.

        Args:
            name: Name of the existing financial statement item node.
            values: Mapping of new period strings to float values.
            replace_existing: If True, replace existing values entirely; otherwise merge.

        Returns:
            The updated `FinancialStatementItemNode`.

        Raises:
            NodeError: If the node does not exist.
            TypeError: If the node is not a `FinancialStatementItemNode` or `values` is not a dict.

        Examples:
            >>> graph.update_financial_statement_item("SG&A", {"2024": 60.0})
        """
        node = self.manipulator.get_node(name)
        if node is None:
            raise NodeError("Node not found", node_id=name)
        if not isinstance(node, FinancialStatementItemNode):
            raise TypeError(f"Node '{name}' is not a FinancialStatementItemNode")
        if not isinstance(values, dict):
            raise TypeError("Values must be provided as a dict[str, float]")
        if replace_existing:
            node.values = values.copy()
        else:
            node.values.update(values)
        self.add_periods(list(values.keys()))
        logger.info(
            f"Updated FinancialStatementItemNode '{name}' with periods {list(values.keys())}; replace_existing={replace_existing}"
        )
        return node

    def get_financial_statement_items(self) -> list[Node]:
        """Retrieve all financial statement item nodes from the graph.

        Returns:
            A list of `FinancialStatementItemNode` objects currently in the graph.

        Examples:
            >>> items = graph.get_financial_statement_items()
        """
        from fin_statement_model.core.nodes import (
            FinancialStatementItemNode,
        )  # Keep import local as it's specific

        return [
            node
            for node in self.nodes.values()
            if isinstance(node, FinancialStatementItemNode)
        ]

    def __repr__(self) -> str:
        """Provide a concise, developer-friendly string representation of the graph.

        Summarize total nodes, FS items, calculations, dependencies, and periods.

        Returns:
            A string summarizing the graph's structure and contents.

        Examples:
            >>> logger.info(repr(graph))
        """
        from fin_statement_model.core.nodes import (
            FinancialStatementItemNode,
        )  # Keep import local

        num_nodes = len(self.nodes)
        periods_str = ", ".join(map(repr, self.periods)) if self.periods else "None"

        fs_item_count = 0
        calc_node_count = 0
        other_node_count = 0
        dependencies_count = 0

        for node in self.nodes.values():
            if isinstance(node, FinancialStatementItemNode):
                fs_item_count += 1
            elif node.has_calculation():
                calc_node_count += 1
                # Prioritize get_dependencies if available, otherwise check inputs
                if hasattr(node, "get_dependencies"):
                    try:
                        dependencies_count += len(node.get_dependencies())
                    except Exception as e:
                        logger.warning(
                            f"Error calling get_dependencies for node '{node.name}': {e}"
                        )
                elif hasattr(node, "inputs"):
                    try:
                        if isinstance(node.inputs, list):
                            # Ensure inputs are nodes with names
                            dep_names = [
                                inp.name for inp in node.inputs if hasattr(inp, "name")
                            ]
                            dependencies_count += len(dep_names)
                        elif isinstance(node.inputs, dict):
                            # Assume keys are dependency names for dict inputs
                            dependencies_count += len(node.inputs)
                    except Exception as e:
                        logger.warning(
                            f"Error processing inputs for node '{node.name}': {e}"
                        )
            else:
                other_node_count += 1

        repr_parts = [
            f"Total Nodes: {num_nodes}",
            f"FS Items: {fs_item_count}",
            f"Calculations: {calc_node_count}",
        ]
        if other_node_count > 0:
            repr_parts.append(f"Other: {other_node_count}")
        repr_parts.append(f"Dependencies: {dependencies_count}")
        repr_parts.append(f"Periods: [{periods_str}]")

        return f"<{type(self).__name__}({', '.join(repr_parts)})>"

    def has_cycle(self, source_node: Node, target_node: Node) -> bool:
        """Check if a cycle exists from a source node to a target node.

        This method delegates to GraphTraverser to determine if `target_node` is
        reachable from `source_node` via successors, indicating that adding an edge
        from `target_node` to `source_node` would create a cycle.

        Args:
            source_node: The starting node for cycle detection.
            target_node: The node to detect return path to.

        Returns:
            True if a cycle exists, False otherwise.
        """
        if source_node.name not in self._nodes or target_node.name not in self._nodes:
            return False

        # Use GraphTraverser's reachability check
        return self.traverser._is_reachable(source_node.name, target_node.name)

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its name.

        Args:
            name: The unique name of the node to retrieve.

        Returns:
            The `Node` instance if found, else None.

        Examples:
            >>> node = graph.get_node("Revenue")
        """
        return self.manipulator.get_node(name)

    def _add_node_with_validation(
        self, node: Node, check_cycles: bool = True, validate_inputs: bool = True
    ) -> Node:
        """Internal method for adding nodes with common validation logic.

        Args:
            node: The Node instance to add
            check_cycles: Whether to perform cycle detection
            validate_inputs: Whether to validate input node references

        Returns:
            The added node

        Raises:
            ValueError: If node name is invalid
            NodeError: If input validation fails
            CircularDependencyError: If adding the node would create a cycle
        """
        # 1. Name validation
        if not node.name or not isinstance(node.name, str):
            raise ValueError("Node name must be a non-empty string")

        # 2. Check for existing node
        if node.name in self._nodes:
            logger.warning(f"Overwriting existing node '{node.name}'")

        # 3. Input validation (if applicable)
        if validate_inputs and hasattr(node, "inputs") and node.inputs:
            self._validate_node_inputs(node)

        # 4. Cycle detection (if applicable)
        if (
            check_cycles
            and hasattr(node, "inputs")
            and node.inputs
            and self.traverser.would_create_cycle(node)
        ):
            # Try to find the actual cycle path for better error message
            cycle_path = None
            for input_node in node.inputs:
                if hasattr(input_node, "name"):
                    path = self.traverser.find_cycle_path(input_node.name, node.name)
                    if path:
                        cycle_path = path
                        break

            raise CircularDependencyError(
                f"Adding node '{node.name}' would create a cycle",
                cycle=cycle_path or [node.name, "...", node.name],
            )

        # 5. Register node
        self._nodes[node.name] = node

        # 6. Update periods if applicable
        if hasattr(node, "values") and isinstance(node.values, dict):
            self.add_periods(list(node.values.keys()))

        logger.debug(f"Added node '{node.name}' to graph")
        return node

    def _validate_node_inputs(self, node: Node) -> None:
        """Validate that all input nodes exist in the graph.

        Args:
            node: The node whose inputs to validate

        Raises:
            NodeError: If any input node is missing
        """
        missing_inputs = []

        if hasattr(node, "inputs") and node.inputs:
            for input_node in node.inputs:
                if hasattr(input_node, "name"):
                    if input_node.name not in self._nodes:
                        missing_inputs.append(input_node.name)
                # Handle case where inputs might be strings instead of Node objects
                elif isinstance(input_node, str) and input_node not in self._nodes:
                    missing_inputs.append(input_node)

        if missing_inputs:
            raise NodeError(
                f"Cannot add node '{node.name}': missing required input nodes {missing_inputs}",
                node_id=node.name,
            )

    def _resolve_input_nodes(self, input_names: list[str]) -> list[Node]:
        """Resolve input node names to Node objects.

        Args:
            input_names: List of node names to resolve

        Returns:
            List of resolved Node objects

        Raises:
            NodeError: If any input node name does not exist
        """
        resolved_inputs: list[Node] = []
        missing = []

        for name in input_names:
            node = self._nodes.get(name)
            if node is None:
                missing.append(name)
            else:
                resolved_inputs.append(node)

        if missing:
            raise NodeError(f"Cannot resolve input nodes: missing nodes {missing}")

        return resolved_inputs

    def add_node(self, node: Node) -> None:
        """Add a node to the graph.

        Args:
            node: A ``Node`` instance to add to the graph.

        Raises:
            TypeError: If the provided object is not a Node instance.

        Examples:
            >>> from fin_statement_model.core.nodes import FinancialStatementItemNode
            >>> node = FinancialStatementItemNode("Revenue", {"2023": 1000})
            >>> graph.add_node(node)
        """
        from fin_statement_model.core.nodes.base import Node as _NodeBase

        if not isinstance(node, _NodeBase):
            raise TypeError(f"Expected Node instance, got {type(node).__name__}")

        return self.manipulator.add_node(node)

    def remove_node(self, node_name: str) -> None:
        """Remove a node from the graph by name, updating dependencies.

        Args:
            node_name: The name of the node to remove.

        Returns:
            None

        Examples:
            >>> graph.remove_node("OldItem")
        """
        return self.manipulator.remove_node(node_name)

    def replace_node(self, node_name: str, new_node: Node) -> None:
        """Replace an existing node with a new node instance.

        Args:
            node_name: Name of the node to replace.
            new_node: The new `Node` instance to substitute.

        Returns:
            None

        Examples:
            >>> graph.replace_node("Item", updated_node)
        """
        return self.manipulator.replace_node(node_name, new_node)

    def has_node(self, node_id: str) -> bool:
        """Check if a node with the given ID exists in the graph.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> graph.has_node("Revenue")
        """
        return self.manipulator.has_node(node_id)

    def set_value(self, node_id: str, period: str, value: float) -> None:
        """Set or update the value for a node in a specific period.

        Args:
            node_id: The name of the node.
            period: The period identifier to set the value for.
            value: The float value to assign.

        Returns:
            None

        Raises:
            ValueError: If the period is not recognized by the graph.
            NodeError: If the node does not exist.
            TypeError: If the node does not support setting a value.

        Examples:
            >>> graph.set_value("SG&A", "2024", 55.0)
        """
        return self.manipulator.set_value(node_id, period, value)

    def topological_sort(self) -> list[str]:
        """Perform a topological sort of all graph nodes.

        Returns:
            A list of node IDs in topological order.

        Raises:
            ValueError: If a cycle is detected in the graph.

        Examples:
            >>> order = graph.topological_sort()
        """
        return self.traverser.topological_sort()

    def get_calculation_nodes(self) -> list[str]:
        """Get all calculation node IDs in the graph.

        Returns:
            A list of node names that have associated calculations.

        Examples:
            >>> graph.get_calculation_nodes()
        """
        return self.traverser.get_calculation_nodes()

    def get_dependencies(self, node_id: str) -> list[str]:
        """Get the direct predecessor node IDs (dependencies) for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node depends on.

        Examples:
            >>> graph.get_dependencies("GrossProfit")
        """
        return self.traverser.get_dependencies(node_id)

    def get_dependency_graph(self) -> dict[str, list[str]]:
        """Get the full dependency graph mapping of node IDs to their inputs.

        Returns:
            A dict mapping each node ID to a list of its dependency node IDs.

        Examples:
            >>> graph.get_dependency_graph()
        """
        return self.traverser.get_dependency_graph()

    def detect_cycles(self) -> list[list[str]]:
        """Detect all cycles in the graph's dependency structure.

        Returns:
            A list of cycles, each represented as a list of node IDs.

        Examples:
            >>> graph.detect_cycles()
        """
        return self.traverser.detect_cycles()

    def validate(self) -> list[str]:
        """Validate the graph structure for errors such as cycles or missing nodes.

        Returns:
            A list of validation error messages, empty if valid.

        Examples:
            >>> graph.validate()
        """
        return self.traverser.validate()

    def breadth_first_search(
        self, start_node: str, direction: str = "successors"
    ) -> list[list[str]]:
        """Perform a breadth-first search (BFS) traversal of the graph.

        Args:
            start_node: The starting node ID for BFS.
            direction: Either 'successors' or 'predecessors' to traverse.

        Returns:
            A nested list of node IDs per BFS level.

        Raises:
            ValueError: If `direction` is not 'successors' or 'predecessors'.

        Examples:
            >>> graph.breadth_first_search("Revenue", "successors")
        """
        return self.traverser.breadth_first_search(start_node, direction)

    def get_direct_successors(self, node_id: str) -> list[str]:
        """Get immediate successor node IDs for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that directly follow the given node.

        Examples:
            >>> graph.get_direct_successors("Revenue")
        """
        return self.traverser.get_direct_successors(node_id)

    def get_direct_predecessors(self, node_id: str) -> list[str]:
        """Get immediate predecessor node IDs (inputs) for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node directly depends on.

        Examples:
            >>> graph.get_direct_predecessors("GrossProfit")
        """
        return self.traverser.get_direct_predecessors(node_id)

    def merge_from(self, other_graph: "Graph") -> None:
        """Merge nodes and periods from another Graph into this one.

        Adds periods from the other graph if they don't exist in this graph.
        Adds nodes from the other graph if they don't exist.
        If a node exists in both graphs, attempts to merge the 'values' dictionary
        from the other graph's node into this graph's node.

        Args:
            other_graph: The Graph instance to merge data from.

        Raises:
            TypeError: If other_graph is not a Graph instance.
        """
        if not isinstance(other_graph, Graph):
            raise TypeError("Can only merge from another Graph instance.")

        logger.info(f"Starting merge from graph {other_graph!r} into {self!r}")

        # 1. Update periods
        new_periods = [p for p in other_graph.periods if p not in self.periods]
        if new_periods:
            self.add_periods(new_periods)
            logger.debug(f"Merged periods: {new_periods}")

        # 2. Merge nodes
        nodes_added = 0
        nodes_updated = 0
        for node_name, other_node in other_graph.nodes.items():
            existing_node = self.get_node(node_name)
            if existing_node is not None:
                # Node exists, merge values if applicable
                if (
                    hasattr(existing_node, "values")
                    and hasattr(other_node, "values")
                    and isinstance(getattr(existing_node, "values", None), dict)
                    and isinstance(getattr(other_node, "values", None), dict)
                ):
                    try:
                        # Perform the update
                        existing_node.values.update(other_node.values)
                        nodes_updated += 1
                        logger.debug(f"Merged values into existing node '{node_name}'")
                        # No need to call self.add_node(existing_node) as it's already there
                    except AttributeError:
                        # Should not happen due to hasattr checks, but defensive
                        logger.warning(
                            f"Could not merge values for node '{node_name}' due to missing 'values' attribute despite hasattr check."
                        )
                    except Exception as e:
                        logger.warning(
                            f"Could not merge values for node '{node_name}': {e}"
                        )
                else:
                    # Nodes exist but cannot merge values (e.g., calculation nodes without stored values)
                    logger.debug(
                        f"Node '{node_name}' exists in both graphs, but values not merged (missing/incompatible 'values' attribute). Keeping target graph's node."
                    )
            else:
                # Node doesn't exist in target graph, add it
                try:
                    # Ensure we add a copy if nodes might be shared or mutable in complex ways,
                    # but for now, assume adding the instance is okay.
                    self.add_node(other_node)
                    nodes_added += 1
                except Exception:
                    logger.exception(
                        f"Failed to add new node '{node_name}' during merge:"
                    )

        logger.info(
            f"Merge complete. Nodes added: {nodes_added}, Nodes updated (values merged): {nodes_updated}"
        )

    # --- Adjustment Management API ---

    def add_adjustment(
        self,
        node_name: str,
        period: str,
        value: float,
        reason: str,
        adj_type: AdjustmentType = AdjustmentType.ADDITIVE,
        scale: float = 1.0,
        priority: int = 0,
        tags: Optional[set[AdjustmentTag]] = None,
        scenario: Optional[str] = None,
        user: Optional[str] = None,
        start_period: Optional[str] = None,  # Phase 2
        end_period: Optional[str] = None,  # Phase 2
        adj_id: Optional[UUID] = None,  # Allow specifying ID, e.g., for re-creation
    ) -> UUID:
        """Adds a discretionary adjustment to a specific node and period.

        Creates an Adjustment object and delegates storage to the AdjustmentManager.

        Args:
            node_name: The name of the target node.
            period: The primary period the adjustment applies to.
            value: The numeric value of the adjustment.
            reason: Text description of why the adjustment was made.
            adj_type: How the adjustment combines with the base value.
            scale: Attenuation factor for the adjustment (0.0 to 1.0, Phase 2).
            priority: Tie-breaker for applying multiple adjustments (lower number applied first).
            tags: Set of descriptive tags for filtering and analysis.
            scenario: The named scenario this adjustment belongs to. Defaults to DEFAULT_SCENARIO if None.
            user: Identifier for the user who created the adjustment.
            start_period: The first period the adjustment is effective (inclusive, Phase 2).
            end_period: The last period the adjustment is effective (inclusive, Phase 2).
            adj_id: Optional specific UUID to use for the adjustment.

        Returns:
            The UUID of the created or updated adjustment.

        Raises:
            NodeError: If the target node_name does not exist in the graph.
            ValidationError: If adjustment parameters are invalid (e.g., scale out of bounds).
        """
        if not self.has_node(node_name):
            raise NodeError(
                f"Cannot add adjustment: Node '{node_name}' not found.",
                node_id=node_name,
            )

        # Need Pydantic's ValidationError and uuid4
        from pydantic import ValidationError
        from uuid import uuid4

        # Need Adjustment model details
        from fin_statement_model.core.adjustments.models import Adjustment

        # Assign default scenario if None was passed
        actual_scenario = scenario if scenario is not None else DEFAULT_SCENARIO

        # Create the adjustment object - Pydantic handles validation (e.g., scale)
        try:
            adj = Adjustment(
                id=adj_id or uuid4(),  # Generate new ID if not provided
                node_name=node_name,
                period=period,
                start_period=start_period,
                end_period=end_period,
                value=value,
                type=adj_type,
                scale=scale,
                priority=priority,
                tags=tags or set(),
                scenario=actual_scenario,  # Use the actual scenario
                reason=reason,
                user=user,
                # timestamp is added automatically by the model
            )
        except ValidationError:
            logger.exception(f"Failed to create adjustment for node '{node_name}'")
            raise  # Re-raise Pydantic's validation error

        self.adjustment_manager.add_adjustment(adj)
        logger.info(
            f"Added adjustment {adj.id} for node '{node_name}', period '{period}', scenario '{scenario}'."
        )
        return adj.id

    def remove_adjustment(self, adj_id: UUID) -> bool:
        """Removes an adjustment by its unique ID.

        Args:
            adj_id: The UUID of the adjustment to remove.

        Returns:
            True if an adjustment was found and removed, False otherwise.
        """
        removed = self.adjustment_manager.remove_adjustment(adj_id)
        if removed:
            logger.info(f"Removed adjustment {adj_id}.")
        else:
            logger.warning(f"Attempted to remove non-existent adjustment {adj_id}.")
        return removed

    def get_adjustments(
        self, node_name: str, period: str, *, scenario: Optional[str] = None
    ) -> list[Adjustment]:
        """Retrieves all adjustments for a specific node, period, and scenario.

        Args:
            node_name: The name of the target node.
            period: The target period.
            scenario: The scenario to retrieve adjustments for. Defaults to DEFAULT_SCENARIO if None.

        Returns:
            A list of Adjustment objects matching the criteria, sorted by application order.
        """
        if not self.has_node(node_name):
            # Or return empty list? Returning empty seems safer.
            logger.warning(f"Node '{node_name}' not found when getting adjustments.")
            return []
        # Assign default scenario if None was passed
        actual_scenario = scenario if scenario is not None else DEFAULT_SCENARIO
        return self.adjustment_manager.get_adjustments(
            node_name, period, scenario=actual_scenario
        )

    def list_all_adjustments(self) -> list[Adjustment]:
        """Returns a list of all adjustments currently managed by the graph.

        Returns:
            A list containing all Adjustment objects across all nodes, periods, and scenarios.
        """
        return self.adjustment_manager.get_all_adjustments()

    def was_adjusted(
        self, node_name: str, period: str, filter_input: "AdjustmentFilterInput" = None
    ) -> bool:
        """Checks if a node's value for a given period was affected by any selected adjustments.

        Args:
            node_name: The name of the node to check.
            period: The time period identifier.
            filter_input: Criteria for selecting which adjustments to consider (same as get_adjusted_value).

        Returns:
            True if any adjustment matching the filter was applied to the base value, False otherwise.

        Raises:
            NodeError: If the specified node does not exist.
            CalculationError: If an error occurs during the underlying calculation.
            TypeError: If filter_input is an invalid type.
        """
        try:
            _, was_adjusted_flag = self.get_adjusted_value(
                node_name, period, filter_input, return_flag=True
            )
            return was_adjusted_flag
        except (NodeError, CalculationError, TypeError):
            # Propagate errors consistently
            logger.exception(f"Error checking if node '{node_name}' was adjusted")
            raise

    # --- End Adjustment Management API ---



================================================================================
File: fin_statement_model/core/graph/manipulator.py
================================================================================

"""Provide graph manipulation utilities.

This module defines the GraphManipulator class, encapsulating node-level mutation helpers for Graph.
"""

import logging
from typing import Optional, Any, cast
from fin_statement_model.core.errors import NodeError
from fin_statement_model.core.nodes import Node

logger = logging.getLogger(__name__)


class GraphManipulator:
    """Encapsulate node-level mutation helpers for Graph.

    Attributes:
        graph: The Graph instance this manipulator operates on.
    """

    def __init__(self, graph: Any) -> None:
        """Initialize the GraphManipulator with a Graph reference.

        Args:
            graph: The Graph instance to manipulate.
        """
        self.graph = graph

    def add_node(self, node: Node) -> None:
        """Add a node to the graph, replacing any existing node with the same name.

        Args:
            node: The Node instance to add.

        Returns:
            None

        Raises:
            TypeError: If the provided object is not a Node instance.

        Examples:
            >>> manipulator.add_node(node)
        """
        if not isinstance(node, Node):
            raise TypeError(f"Object {node} is not a valid Node instance.")
        if self.has_node(node.name):
            self.remove_node(node.name)
        self.graph._nodes[node.name] = node

    def _update_calculation_nodes(self) -> None:
        """Refresh input references for all calculation nodes after structure changes.

        This method re-resolves `input_names` to current Node objects and clears
        individual node caches.

        Returns:
            None
        """
        for nd in self.graph._nodes.values():
            if nd.has_calculation() and hasattr(nd, "input_names") and nd.input_names:
                try:
                    resolved_inputs: list[Node] = []
                    for name in nd.input_names:
                        input_node = self.get_node(name)
                        if input_node is None:
                            raise NodeError(
                                f"Input node '{name}' not found for calculation node '{nd.name}'"
                            )
                        resolved_inputs.append(input_node)
                    nd.inputs = resolved_inputs
                    if hasattr(nd, "clear_cache"):
                        nd.clear_cache()
                except NodeError:
                    logger.exception(f"Error updating inputs for node '{nd.name}'")
                except AttributeError:
                    logger.warning(
                        f"Node '{nd.name}' has input_names but no 'inputs' attribute to update."
                    )

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its unique name.

        Args:
            name: The unique node name to retrieve.

        Returns:
            The Node instance if found, else None.

        Examples:
            >>> manipulator.get_node("Revenue")
        """
        return cast(Optional[Node], self.graph._nodes.get(name))

    def replace_node(self, node_name: str, new_node: Node) -> None:
        """Replace an existing node with a new one, ensuring consistency.

        Args:
            node_name: Name of the node to replace.
            new_node: The new Node instance; its name must match `node_name`.

        Returns:
            None

        Raises:
            NodeError: If `node_name` does not exist.
            ValueError: If `new_node.name` does not match `node_name`.

        Examples:
            >>> manipulator.replace_node("Revenue", updated_node)
        """
        if not self.has_node(node_name):
            raise NodeError(f"Node '{node_name}' not found, cannot replace.")
        if node_name != new_node.name:
            raise ValueError(
                "New node name must match the name of the node being replaced."
            )
        self.remove_node(node_name)
        self.add_node(new_node)

    def has_node(self, node_id: str) -> bool:
        """Check if a node with the given ID exists.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> manipulator.has_node("Revenue")
        """
        return node_id in self.graph._nodes

    def remove_node(self, node_name: str) -> None:
        """Remove a node from the graph and update calculation nodes.

        Args:
            node_name: The name of the node to remove.

        Returns:
            None

        Examples:
            >>> manipulator.remove_node("OldItem")
        """
        if not self.has_node(node_name):
            return
        self.graph._nodes.pop(node_name, None)
        self._update_calculation_nodes()

    def set_value(self, node_id: str, period: str, value: float) -> None:
        """Set the value for a specific node and period, clearing all caches.

        Args:
            node_id: The name of the node.
            period: The time period identifier.
            value: The numeric value to assign.

        Returns:
            None

        Raises:
            ValueError: If `period` is not recognized by the graph.
            NodeError: If the node does not exist.
            TypeError: If the node does not support setting a value.

        Examples:
            >>> manipulator.set_value("Revenue", "2023", 1100.0)
        """
        if period not in self.graph._periods:
            raise ValueError(f"Period '{period}' not in graph periods")
        nd = self.get_node(node_id)
        if not nd:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if not hasattr(nd, "set_value"):
            raise TypeError(
                f"Node '{node_id}' of type {type(nd).__name__} does not support set_value."
            )
        nd.set_value(period, value)
        self.graph.clear_all_caches()

    def clear_all_caches(self) -> None:
        """Clear caches associated with individual nodes in the graph.

        Returns:
            None

        Examples:
            >>> manipulator.clear_all_caches()
        """
        for nd in self.graph._nodes.values():
            if hasattr(nd, "clear_cache"):
                nd.clear_cache()



================================================================================
File: fin_statement_model/core/graph/traverser.py
================================================================================

"""Provide graph traversal and validation utilities.

This module defines the GraphTraverser class, encapsulating read-only graph traversal helpers.
"""

import logging
from typing import Optional, Any, TYPE_CHECKING, cast
from collections import deque

from fin_statement_model.core.errors import NodeError
from fin_statement_model.core.nodes import Node

if TYPE_CHECKING:
    from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class GraphTraverser:
    """Encapsulate traversal and validation helpers for Graph.

    Attributes:
        graph: The Graph instance this traverser operates on.
    """

    def __init__(self, graph: Any) -> None:
        """Initialize the GraphTraverser with a Graph reference.

        Args:
            graph: The Graph instance to traverse.
        """
        self.graph = graph

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its name.

        Args:
            name: The unique name of the node to retrieve.

        Returns:
            The Node instance if found, else None.

        Examples:
            >>> traverser.get_node("Revenue")
        """
        return cast(Optional[Node], self.graph.manipulator.get_node(name))

    def has_node(self, node_id: str) -> bool:
        """Check if a node exists in the graph.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> traverser.has_node("Revenue")
        """
        return cast(bool, self.graph.manipulator.has_node(node_id))

    @property
    def nodes(self) -> dict[str, Node]:
        """Access the full node registry dictionary.

        Returns:
            A dict mapping node names to Node instances.

        Examples:
            >>> list(traverser.nodes.keys())
        """
        return cast(dict[str, Node], self.graph.nodes)

    def get_direct_successors(self, node_id: str) -> list[str]:
        """Get immediate successor node IDs for a given node.

        Args:
            node_id: The name of the node whose successors to retrieve.

        Returns:
            A list of node IDs that directly follow the given node.

        Examples:
            >>> traverser.get_direct_successors("Revenue")
        """
        successors: list[str] = []
        for other_id, node in self.nodes.items():
            if hasattr(node, "inputs"):
                input_nodes: list[Node] = []
                if isinstance(node.inputs, list):
                    input_nodes = node.inputs
                elif isinstance(node.inputs, dict):
                    input_nodes = list(node.inputs.values())

                if any(
                    inp.name == node_id for inp in input_nodes if hasattr(inp, "name")
                ):
                    successors.append(other_id)
        return successors

    def get_direct_predecessors(self, node_id: str) -> list[str]:
        """Get immediate predecessor node IDs (dependencies) for a given node.

        Args:
            node_id: The name of the node whose dependencies to retrieve.

        Returns:
            A list of node IDs that the given node depends on.

        Raises:
            NodeError: If the node does not exist.

        Examples:
            >>> traverser.get_direct_predecessors("GrossProfit")
        """
        node = self.get_node(node_id)
        if not node:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if hasattr(node, "inputs"):
            return [inp.name for inp in node.inputs]
        return []

    def topological_sort(self) -> list[str]:
        """Perform a topological sort of nodes based on dependencies.

        Returns:
            A list of node IDs in topological order.

        Raises:
            ValueError: If a cycle is detected in the graph.

        Examples:
            >>> traverser.topological_sort()
        """
        in_degree: dict[str, int] = {n: 0 for n in self.nodes}
        adjacency: dict[str, list[str]] = {n: [] for n in self.nodes}
        for name, node in self.nodes.items():
            if hasattr(node, "inputs"):
                for inp in node.inputs:
                    adjacency[inp.name].append(name)
                    in_degree[name] += 1
        queue: list[str] = [n for n, d in in_degree.items() if d == 0]
        topo_order: list[str] = []
        while queue:
            current = queue.pop()
            topo_order.append(current)
            for nbr in adjacency[current]:
                in_degree[nbr] -= 1
                if in_degree[nbr] == 0:
                    queue.append(nbr)
        if len(topo_order) != len(self.nodes):
            raise ValueError(
                "Cycle detected in graph, can't do a valid topological sort."
            )
        return topo_order

    def get_calculation_nodes(self) -> list[str]:
        """Identify all nodes in the graph that represent calculations.

        Returns:
            A list of node IDs for nodes with calculations.

        Examples:
            >>> traverser.get_calculation_nodes()
        """
        return [
            node_id for node_id, node in self.nodes.items() if node.has_calculation()
        ]

    def get_dependencies(self, node_id: str) -> list[str]:
        """Retrieve the direct dependencies (inputs) of a specific node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node depends on.

        Raises:
            NodeError: If the node does not exist.

        Examples:
            >>> traverser.get_dependencies("GrossProfit")
        """
        node = self.get_node(node_id)
        if not node:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if hasattr(node, "inputs"):
            return [inp.name for inp in node.inputs]
        return []

    def get_dependency_graph(self) -> dict[str, list[str]]:
        """Construct a representation of the full dependency graph.

        Returns:
            A dict mapping each node ID to its list of dependency node IDs.

        Examples:
            >>> traverser.get_dependency_graph()
        """
        dependencies: dict[str, list[str]] = {}
        for node_id, node in self.nodes.items():
            try:
                if hasattr(node, "inputs"):
                    dependencies[node_id] = [inp.name for inp in node.inputs]
                else:
                    dependencies[node_id] = []
            except NodeError:
                dependencies[node_id] = []
        return dependencies

    def detect_cycles(self) -> list[list[str]]:
        """Detect all cycles present in the graph's dependency structure.

        Returns:
            A list of cycles, each cycle is a list of node IDs forming the cycle.

        Examples:
            >>> traverser.detect_cycles()
        """
        dependency_graph = self.get_dependency_graph()
        visited: set[str] = set()
        rec_stack: set[str] = set()
        cycles: list[list[str]] = []

        def dfs_detect_cycles(n_id: str, path: Optional[list[str]] = None) -> None:
            if path is None:
                path = []
            if n_id in rec_stack:
                cycle_start = path.index(n_id)
                cycle = path[cycle_start:] + [n_id]
                if cycle not in cycles:
                    cycles.append(cycle)
                return
            if n_id in visited:
                return
            visited.add(n_id)
            rec_stack.add(n_id)
            path.append(n_id)
            for dep in dependency_graph.get(n_id, []):
                dfs_detect_cycles(dep, path[:])
            rec_stack.remove(n_id)

        for node_id in self.nodes:
            if node_id not in visited:
                dfs_detect_cycles(node_id)
        return cycles

    def validate(self) -> list[str]:
        """Perform validation checks on the graph structure.

        Returns:
            A list of validation error messages; empty list if graph is valid.

        Examples:
            >>> traverser.validate()
        """
        errors: list[str] = [
            f"Circular dependency detected: {' -> '.join(cycle)}"
            for cycle in self.detect_cycles()
        ]
        errors.extend(
            f"Node '{node_id}' depends on non-existent node '{inp.name}'"
            for node_id, node in self.nodes.items()
            if hasattr(node, "inputs")
            for inp in node.inputs
            if not self.has_node(inp.name)
        )
        return errors

    def breadth_first_search(
        self, start_node: str, direction: str = "successors"
    ) -> list[list[str]]:
        """Perform a breadth-first search (BFS) traversal of the graph.

        Args:
            start_node: The starting node ID for the traversal.
            direction: The traversal direction, either 'successors' or 'predecessors'.

        Returns:
            A list of levels, each level is a list of node IDs visited at that depth.

        Raises:
            ValueError: If `direction` is not 'successors' or 'predecessors'.

        Examples:
            >>> traverser.breadth_first_search("Revenue", "successors")
        """
        if direction not in ["successors", "predecessors"]:
            raise ValueError("Invalid direction. Use 'successors' or 'predecessors'.")

        visited = set()
        queue = deque([start_node])
        visited.add(start_node)
        traversal_order = []

        while queue:
            level_size = len(queue)
            current_level = []

            for _ in range(level_size):
                n_id = queue.popleft()
                current_level.append(n_id)

                if direction == "successors":
                    for successor in self.get_direct_successors(n_id):
                        if successor not in visited:
                            visited.add(successor)
                            queue.append(successor)
                elif direction == "predecessors":
                    for predecessor in self.get_direct_predecessors(n_id):
                        if predecessor not in visited:
                            visited.add(predecessor)
                            queue.append(predecessor)

            traversal_order.append(current_level)

        return traversal_order

    def would_create_cycle(self, new_node: "Node") -> bool:
        """Check if adding a node would create a cycle.

        Args:
            new_node: The node to be added (must have 'inputs' attribute)

        Returns:
            True if adding the node would create a cycle
        """
        if not hasattr(new_node, "inputs") or not new_node.inputs:
            return False

        # For each input, check if new_node is reachable from it
        for input_node in new_node.inputs:
            if hasattr(input_node, "name") and self._is_reachable(
                input_node.name, new_node.name
            ):
                return True
        return False

    def _is_reachable(self, from_node: str, to_node: str) -> bool:
        """Check if to_node is reachable from from_node.

        Args:
            from_node: Starting node name
            to_node: Target node name

        Returns:
            True if to_node is reachable from from_node via successors
        """
        # If from_node doesn't exist, no reachability
        if from_node not in self.graph._nodes:
            return False

        # If to_node doesn't exist yet, check temporary reachability
        if to_node not in self.graph._nodes:
            return False

        try:
            bfs_levels = self.breadth_first_search(
                start_node=from_node, direction="successors"
            )
            reachable_nodes = {n for level in bfs_levels for n in level}
            return to_node in reachable_nodes
        except (ValueError, KeyError):
            # Handle cases where BFS fails (e.g., invalid node)
            return False

    def find_cycle_path(self, from_node: str, to_node: str) -> Optional[list[str]]:
        """Find the actual cycle path if one exists.

        Args:
            from_node: Starting node name
            to_node: Target node name that would complete the cycle

        Returns:
            List of node names forming the cycle path, or None if no cycle
        """
        if not self._is_reachable(from_node, to_node):
            return None

        # Use DFS to find the actual path
        visited = set()
        path: list[str] = []

        def dfs_find_path(current: str, target: str) -> bool:
            if current == target and len(path) > 0:
                return True
            if current in visited:
                return False

            visited.add(current)
            path.append(current)

            # Get successors of current node
            for successor in self.get_direct_successors(current):
                if dfs_find_path(successor, target):
                    return True

            path.pop()
            return False

        if dfs_find_path(from_node, to_node):
            return [*path, to_node]  # Complete the cycle
        return None



================================================================================
File: fin_statement_model/core/metrics/__init__.py
================================================================================

"""Financial Metrics Module.

This module provides a comprehensive system for defining, calculating, and interpreting
financial metrics. It includes:

- MetricDefinition: Pydantic model for metric definitions
- MetricRegistry: Registry for loading and managing metrics
- MetricInterpreter: System for interpreting metric values with ratings
- Built-in metrics: 75+ professional financial metrics organized by category

The metrics are organized into logical categories:
- Liquidity: Current ratio, quick ratio, working capital analysis
- Leverage: Debt ratios, coverage ratios, capital structure
- Profitability: Margins, returns on assets/equity/capital
- Efficiency: Asset turnover, working capital efficiency
- Valuation: Price multiples, enterprise value ratios
- Cash Flow: Cash generation, cash returns, quality metrics
- Growth: Revenue, earnings, asset growth rates
- Credit Risk: Altman Z-scores, warning flags
- Advanced: DuPont analysis, specialized ratios
"""

import logging
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from fin_statement_model.core.nodes.base import Node

from .models import MetricDefinition, MetricInterpretation
from .registry import MetricRegistry, metric_registry
from .interpretation import MetricInterpreter, MetricRating, interpret_metric

logger = logging.getLogger(__name__)

# Load metrics from the organized structure by default
try:
    # Load from organized structure
    organized_path = Path(__file__).parent / "metric_defn"
    if organized_path.exists():
        logger.info("Loading metrics from organized structure")
        from .metric_defn import load_organized_metrics

        organized_count = load_organized_metrics()
        logger.info(
            f"Successfully loaded {organized_count} metrics from organized structure"
        )
    else:
        logger.warning("Organized metric structure not found - no metrics loaded")

except Exception:
    logger.exception("Failed to load built-in metrics from organized structure")


def calculate_metric(
    metric_name: str,
    data_nodes: dict[str, "Node"],
    period: str,
    node_name: str | None = None,
) -> float:
    """Calculate a metric value using the metric registry and data nodes.

    This helper function simplifies the common pattern of:
    1. Getting a metric definition from the registry
    2. Creating a FormulaCalculationNode with the appropriate inputs
    3. Calculating the result for a specific period

    Args:
        metric_name: Name of the metric in the registry (e.g., "debt_yield")
        data_nodes: Dictionary mapping node names to Node instances
        period: Time period for calculation (e.g., "2023")
        node_name: Optional name for the calculation node (defaults to metric_name)

    Returns:
        The calculated metric value as a float

    Raises:
        KeyError: If the metric is not found in the registry
        ValueError: If required input nodes are missing from data_nodes
        CalculationError: If the calculation fails

    Examples:
        >>> data_nodes = {
        ...     "net_operating_income": FinancialStatementItemNode("noi", {"2023": 1000000}),
        ...     "total_debt": FinancialStatementItemNode("debt", {"2023": 10000000})
        ... }
        >>> debt_yield = calculate_metric("debt_yield", data_nodes, "2023")
        >>> print(f"Debt Yield: {debt_yield:.1f}%")
        Debt Yield: 10.0%
    """
    # Import here to avoid circular imports
    from fin_statement_model.core.nodes.calculation_nodes import FormulaCalculationNode

    # Get metric definition from registry
    try:
        metric_def = metric_registry.get(metric_name)
    except KeyError:
        available_metrics = metric_registry.list_metrics()
        raise KeyError(
            f"Metric '{metric_name}' not found in registry. "
            f"Available metrics: {available_metrics[:10]}..."  # Show first 10
        )

    # Build input mapping for the formula
    inputs = {}
    missing_inputs = []

    for input_name in metric_def.inputs:
        if input_name in data_nodes:
            inputs[input_name] = data_nodes[input_name]
        else:
            missing_inputs.append(input_name)

    if missing_inputs:
        available_nodes = list(data_nodes.keys())
        raise ValueError(
            f"Missing required input nodes for metric '{metric_name}': {missing_inputs}. "
            f"Available nodes: {available_nodes}"
        )

    # Create calculation node
    calc_node_name = node_name or f"{metric_name}_calc"
    calc_node = FormulaCalculationNode(
        calc_node_name,
        inputs=inputs,
        formula=metric_def.formula,
        metric_name=metric_name,
        metric_description=metric_def.description,
    )

    # Calculate and return result
    return calc_node.calculate(period)


__all__ = [
    "MetricDefinition",
    "MetricInterpretation",
    "MetricInterpreter",
    "MetricRating",
    "MetricRegistry",
    "calculate_metric",
    "interpret_metric",
    "metric_registry",
]



================================================================================
File: fin_statement_model/core/metrics/interpretation.py
================================================================================

"""Utilities for interpreting metric values based on defined guidelines."""

from enum import Enum
from typing import Any
from fin_statement_model.core.metrics.models import (
    MetricDefinition,
)


class MetricRating(Enum):
    """Rating levels for metric values."""

    EXCELLENT = "excellent"
    GOOD = "good"
    ADEQUATE = "adequate"
    WARNING = "warning"
    POOR = "poor"
    UNKNOWN = "unknown"


class MetricInterpreter:
    """Interprets metric values based on defined guidelines."""

    def __init__(self, metric_definition: MetricDefinition):
        """Initialize with a metric definition.

        Args:
            metric_definition: The metric definition containing interpretation guidelines.
        """
        self.metric_definition = metric_definition
        self.interpretation = metric_definition.interpretation

    def rate_value(self, value: float) -> MetricRating:
        """Rate a metric value based on interpretation guidelines.

        Args:
            value: The metric value to rate.

        Returns:
            MetricRating indicating the quality of the value.
        """
        if not self.interpretation:
            return MetricRating.UNKNOWN

        # Check for excellent rating
        if (
            self.interpretation.excellent_above is not None
            and value >= self.interpretation.excellent_above
        ):
            return MetricRating.EXCELLENT

        # Check for poor rating
        if (
            self.interpretation.poor_below is not None
            and value < self.interpretation.poor_below
        ):
            return MetricRating.POOR

        # Check for warning conditions
        warning_conditions = []
        if (
            self.interpretation.warning_below is not None
            and value < self.interpretation.warning_below
        ):
            warning_conditions.append("below_threshold")

        if (
            self.interpretation.warning_above is not None
            and value > self.interpretation.warning_above
        ):
            warning_conditions.append("above_threshold")

        if warning_conditions:
            return MetricRating.WARNING

        # Check if in good range
        if (
            self.interpretation.good_range is not None
            and len(self.interpretation.good_range) == 2
        ):
            min_good, max_good = self.interpretation.good_range
            if min_good <= value <= max_good:
                return MetricRating.GOOD

        # Default to adequate if no specific conditions met
        return MetricRating.ADEQUATE

    def get_interpretation_message(self, value: float) -> str:
        """Get a human-readable interpretation message for a metric value.

        Args:
            value: The metric value to interpret.

        Returns:
            A descriptive message about the metric value.
        """
        rating = self.rate_value(value)

        # Base message based on rating
        rating_messages = {
            MetricRating.EXCELLENT: f"Excellent performance: {value:.2f}",
            MetricRating.GOOD: f"Good performance: {value:.2f}",
            MetricRating.ADEQUATE: f"Adequate performance: {value:.2f}",
            MetricRating.WARNING: f"Warning level: {value:.2f}",
            MetricRating.POOR: f"Poor performance: {value:.2f}",
            MetricRating.UNKNOWN: f"Value: {value:.2f} (no interpretation guidelines available)",
        }

        return rating_messages[rating]

    def get_detailed_analysis(self, value: float) -> dict[str, Any]:
        """Get a detailed analysis of a metric value.

        Args:
            value: The metric value to analyze.

        Returns:
            Dictionary containing detailed analysis information.
        """
        rating = self.rate_value(value)

        analysis: dict[str, Any] = {
            "value": value,
            "rating": rating.value,
            "metric_name": self.metric_definition.name,
            "units": self.metric_definition.units,
            "category": self.metric_definition.category,
            "interpretation_message": self.get_interpretation_message(value),
        }

        # Add interpretation details if available
        if self.interpretation:
            analysis["guidelines"] = {
                "good_range": self.interpretation.good_range,
                "warning_below": self.interpretation.warning_below,
                "warning_above": self.interpretation.warning_above,
                "excellent_above": self.interpretation.excellent_above,
                "poor_below": self.interpretation.poor_below,
            }

            if self.interpretation.notes:
                analysis["notes"] = self.interpretation.notes

        # Add related metrics for context
        if self.metric_definition.related_metrics:
            analysis["related_metrics"] = self.metric_definition.related_metrics

        return analysis


def interpret_metric(
    metric_definition: MetricDefinition, value: float
) -> dict[str, Any]:
    """Convenience function to interpret a metric value.

    Args:
        metric_definition: The metric definition.
        value: The value to interpret.

    Returns:
        Detailed interpretation analysis.
    """
    interpreter = MetricInterpreter(metric_definition)
    return interpreter.get_detailed_analysis(value)



================================================================================
File: fin_statement_model/core/metrics/metric_defn/__init__.py
================================================================================

"""Organized Built-in Metrics Package.

This package contains financial metrics organized by analytical category for easier
maintenance and understanding. All metrics are automatically loaded into the
metric_registry when this package is imported.
"""

import logging
from pathlib import Path
from typing import Optional

from fin_statement_model.core.metrics.registry import metric_registry

logger = logging.getLogger(__name__)


def load_organized_metrics(base_path: Optional[Path] = None) -> int:
    """Load all metrics from the organized structure.

    Args:
        base_path: Base path to the metric_defn directory. If None, uses default.

    Returns:
        Total number of metrics loaded.
    """
    if base_path is None:
        base_path = Path(__file__).parent

    total_loaded = 0

    # Define the organized metric files to load
    metric_files = [
        # Liquidity metrics
        "liquidity/ratios.yaml",
        "liquidity/working_capital.yaml",
        # Leverage metrics
        "leverage/debt_ratios.yaml",
        "leverage/net_leverage.yaml",
        # Coverage metrics
        "coverage/interest_coverage.yaml",
        "coverage/debt_service.yaml",
        # Profitability metrics
        "profitability/margins.yaml",
        "profitability/returns.yaml",
        # Efficiency metrics
        "efficiency/asset_turnover.yaml",
        "efficiency/component_turnover.yaml",
        # Valuation metrics
        "valuation/price_multiples.yaml",
        "valuation/enterprise_multiples.yaml",
        "valuation/yields.yaml",
        # Cash flow metrics
        "cash_flow/generation.yaml",
        "cash_flow/returns.yaml",
        # Growth metrics
        "growth/growth_rates.yaml",
        # Per share metrics
        "per_share/per_share_metrics.yaml",
        # Credit risk metrics
        "credit_risk/altman_scores.yaml",
        "credit_risk/warning_flags.yaml",
        # Advanced metrics
        "advanced/dupont_analysis.yaml",
        # Special calculated items
        "special/gross_profit.yaml",
        "special/net_income.yaml",
        "special/retained_earnings.yaml",
        # Real estate metrics
        "real_estate/operational_metrics.yaml",
        "real_estate/valuation_metrics.yaml",
        "real_estate/per_share_metrics.yaml",
        "real_estate/debt_metrics.yaml",
        # Banking metrics
        "banking/asset_quality.yaml",
        "banking/capital_adequacy.yaml",
        "banking/profitability.yaml",
        "banking/liquidity.yaml",
    ]

    # Collect unique parent directories from the metric files
    unique_directories = set()
    for file_path in metric_files:
        full_path = base_path / file_path
        if full_path.exists():
            unique_directories.add(full_path.parent)
        else:
            logger.warning(f"Organized metric file not found: {full_path}")

    # Load metrics from each unique directory
    for directory in unique_directories:
        try:
            # load_metrics_from_directory returns the count of metrics loaded
            metrics_count = metric_registry.load_metrics_from_directory(directory)
            total_loaded += metrics_count
            logger.debug(f"Loaded {metrics_count} metrics from {directory}")
        except Exception:
            logger.exception(f"Failed to load metrics from directory {directory}")

    logger.info(f"Loaded {total_loaded} total metrics from organized structure")
    return total_loaded


# Auto-load on import
try:
    load_organized_metrics()
except Exception as e:
    logger.warning(f"Failed to auto-load organized metrics: {e}")



================================================================================
File: fin_statement_model/core/metrics/metric_defn/advanced/__init__.py
================================================================================

"""Advanced Metrics."""

# Advanced metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/banking/__init__.py
================================================================================

"""Banking Metrics."""

# Banking metrics organized by subcategory:
# - asset_quality.yaml: NPL ratios, charge-offs, provision coverage
# - capital_adequacy.yaml: Tier 1/2 capital ratios, leverage ratios
# - profitability.yaml: Net interest margin, efficiency ratio, ROA/ROE
# - liquidity.yaml: LCR, NSFR, deposit composition
# - credit_risk.yaml: Credit loss rates, risk-weighted asset ratios
# - regulatory.yaml: Regulatory compliance and stress test metrics



================================================================================
File: fin_statement_model/core/metrics/metric_defn/cash_flow/__init__.py
================================================================================

"""Cash_Flow Metrics."""

# Cash_Flow metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/coverage/__init__.py
================================================================================

"""Coverage Metrics."""

# Coverage metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/credit_risk/__init__.py
================================================================================

"""Credit_Risk Metrics."""

# Credit_Risk metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/efficiency/__init__.py
================================================================================

"""Efficiency Metrics."""

# Efficiency metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/growth/__init__.py
================================================================================

"""Growth Metrics."""

# Growth metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/leverage/__init__.py
================================================================================

"""Leverage Metrics."""

# Leverage metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/liquidity/__init__.py
================================================================================

"""Liquidity Metrics."""

# Liquidity metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/per_share/__init__.py
================================================================================

"""Per_Share Metrics."""

# Per_Share metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/profitability/__init__.py
================================================================================

"""Profitability Metrics."""

# Profitability metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/real_estate/__init__.py
================================================================================

"""Real Estate Metrics."""

# Real estate metrics organized by subcategory:
# - operational_metrics.yaml: NOI, FFO, AFFO, occupancy
# - valuation_metrics.yaml: Cap rates, price per SF, REIT multiples
# - per_share_metrics.yaml: FFO/share, AFFO/share, NAV/share
# - debt_metrics.yaml: LTV, DSCR, debt yield, interest coverage



================================================================================
File: fin_statement_model/core/metrics/metric_defn/valuation/__init__.py
================================================================================

"""Valuation Metrics."""

# Valuation metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/models.py
================================================================================

"""Models for metric definitions."""

from typing import Optional, Any
from pydantic import BaseModel, Field, model_validator
from pydantic import ConfigDict


class MetricInterpretation(BaseModel):
    """Interpretation guidelines for a metric."""

    good_range: Optional[list[float]] = Field(
        None, description="Range of values considered good [min, max]"
    )
    warning_below: Optional[float] = Field(
        None, description="Value below which a warning should be issued"
    )
    warning_above: Optional[float] = Field(
        None, description="Value above which a warning should be issued"
    )
    excellent_above: Optional[float] = Field(
        None, description="Value above which the metric is considered excellent"
    )
    poor_below: Optional[float] = Field(
        None, description="Value below which the metric is considered poor"
    )
    notes: Optional[str] = Field(
        None, description="Additional interpretation notes and context"
    )


class MetricDefinition(BaseModel):
    """Schema for one metric definition loaded from YAML."""

    name: str = Field(..., min_length=1, description="The name of the metric")
    description: str = Field(
        ..., min_length=1, max_length=500, description="The description of the metric"
    )
    inputs: list[str] = Field(..., min_length=1, description="The inputs of the metric")
    formula: str = Field(..., min_length=1, description="The formula of the metric")
    tags: list[str] = Field(default_factory=list, description="The tags of the metric")
    units: Optional[str] = Field(None, description="The units of the metric")
    category: Optional[str] = Field(
        None, description="Category of the metric (e.g., liquidity, profitability)"
    )
    interpretation: Optional[MetricInterpretation] = Field(
        None, description="Guidelines for interpreting the metric values"
    )
    related_metrics: Optional[list[str]] = Field(
        None, description="Names of related metrics that should be considered together"
    )

    model_config = ConfigDict(extra="forbid", frozen=False)

    @model_validator(mode="before")
    def _strip_whitespace(cls, values: dict[str, Any]) -> dict[str, Any]:
        # tiny quality-of-life clean-up
        for k, v in values.items():
            if isinstance(v, str):
                values[k] = v.strip()
        return values



================================================================================
File: fin_statement_model/core/metrics/registry.py
================================================================================

"""Manage loading and accessing metric definitions from YAML files.

This module provides a registry to discover, validate, and retrieve
metric definitions from YAML files and associate them with calculation classes.
"""

import logging
from pathlib import Path
from typing import ClassVar, Union

# Use a try-except block for the YAML import
try:
    import yaml

    HAS_YAML = True
except ImportError:
    HAS_YAML = False

from pydantic import ValidationError

from fin_statement_model.core.metrics.models import MetricDefinition

logger = logging.getLogger(__name__)


class MetricRegistry:
    """Manage loading and accessing metric definitions from YAML files.

    This includes discovering YAML definitions, validating their structure,
    and providing retrieval methods by metric ID.
    """

    _REQUIRED_FIELDS: ClassVar[list[str]] = ["inputs", "formula", "description", "name"]

    def __init__(self) -> None:
        """Initialize the MetricRegistry with an empty metrics store.

        Examples:
            >>> registry = MetricRegistry()
            >>> len(registry)
            0
        """
        self._metrics: dict[str, MetricDefinition] = {}
        logger.info("MetricRegistry initialized.")

    def load_metrics_from_directory(self, directory_path: Union[str, Path]) -> int:
        """Load all metric definitions from a directory.

        This method searches for '*.yaml' files, validates their content,
        and stores them in the registry. Each YAML file can contain either:
        - A single metric definition (a YAML dictionary at the root)
        - A list of metric definitions (a YAML list of dictionaries at the root)

        Args:
            directory_path: Path to the directory containing metric YAML files.

        Returns:
            The number of metrics successfully loaded.

        Raises:
            ImportError: If PyYAML is not installed.
            FileNotFoundError: If the directory_path does not exist.
            ConfigurationError: If a YAML file is invalid or missing required fields.

        Examples:
            >>> registry = MetricRegistry()
            >>> count = registry.load_metrics_from_directory("./metrics")
            >>> print(f"Loaded {count} metrics.")
        """
        if not HAS_YAML:
            logger.error(
                "PyYAML is required to load metrics from YAML files. Please install it."
            )
            raise ImportError("PyYAML is required to load metrics from YAML files.")

        dir_path = Path(directory_path)
        if not dir_path.is_dir():
            logger.error(f"Metric directory not found: {dir_path}")
            raise FileNotFoundError(f"Metric directory not found: {dir_path}")

        logger.info(f"Loading metrics from directory: {dir_path}")
        loaded_count = 0

        for filepath in dir_path.glob("*.yaml"):
            logger.debug(f"Processing file: {filepath}")
            try:
                with open(filepath, encoding="utf-8") as f:
                    content = f.read()

                # Use standard YAML parsing
                try:
                    data = yaml.safe_load(content)
                except yaml.YAMLError as e:
                    logger.warning(f"Failed to parse YAML file {filepath}: {e}")
                    continue

                if not data:
                    logger.debug(f"Empty or null content in {filepath}, skipping")
                    continue

                # Handle both single metric and list of metrics
                metrics_to_process = []

                if isinstance(data, dict):
                    # Single metric definition
                    metrics_to_process = [data]
                elif isinstance(data, list):
                    # List of metric definitions
                    metrics_to_process = data
                else:
                    logger.warning(
                        f"Invalid YAML structure in {filepath}: "
                        f"expected dict or list, got {type(data).__name__}"
                    )
                    continue

                # Process each metric definition
                for i, metric_data in enumerate(metrics_to_process):
                    if not isinstance(metric_data, dict):
                        logger.warning(
                            f"Invalid metric definition at index {i} in {filepath}: "
                            f"expected dict, got {type(metric_data).__name__}"
                        )
                        continue

                    try:
                        # Validate and register the metric
                        model = MetricDefinition.model_validate(metric_data)
                        self.register_definition(model)
                        loaded_count += 1
                        logger.debug(
                            f"Successfully loaded metric '{model.name}' from {filepath}"
                        )

                    except ValidationError as ve:
                        logger.warning(
                            f"Invalid metric definition at index {i} in {filepath}: {ve}"
                        )
                        continue

            except Exception:
                logger.exception(f"Failed to process file {filepath}")
                continue

        logger.info(f"Successfully loaded {loaded_count} metrics from {dir_path}.")
        return loaded_count

    def get(self, metric_id: str) -> MetricDefinition:
        """Retrieve a loaded metric definition by its ID.

        Args:
            metric_id: Identifier of the metric (filename stem).

        Returns:
            A MetricDefinition object containing the metric definition.

        Raises:
            KeyError: If the metric_id is not found in the registry.

        Examples:
            >>> definition = registry.get("gross_profit")
            >>> print(definition["formula"])
        """
        try:
            return self._metrics[metric_id]
        except KeyError:
            logger.warning(f"Metric ID '{metric_id}' not found in registry.")
            raise KeyError(
                f"Metric ID '{metric_id}' not found. Available: {self.list_metrics()}"
            )

    def list_metrics(self) -> list[str]:
        """Get a sorted list of all loaded metric IDs.

        Returns:
            A sorted list of available metric IDs.

        Examples:
            >>> registry.list_metrics()
            ['current_ratio', 'debt_equity_ratio']
        """
        return sorted(self._metrics.keys())

    def __len__(self) -> int:
        """Return the number of loaded metrics.

        Returns:
            The count of metrics loaded into the registry.

        Examples:
            >>> len(registry)
            5
        """
        return len(self._metrics)

    def __contains__(self, metric_id: str) -> bool:
        """Check if a metric ID exists in the registry.

        Args:
            metric_id: The metric identifier to check.

        Returns:
            True if the metric is present, False otherwise.

        Examples:
            >>> 'current_ratio' in registry
        """
        return metric_id in self._metrics

    def register_definition(self, definition: MetricDefinition) -> None:
        """Register a single metric definition.

        Args:
            definition: The metric definition to register.
        """
        metric_id = definition.name.lower().replace(" ", "_").replace("-", "_")
        if metric_id in self._metrics:
            logger.debug(f"Overwriting existing metric definition for '{metric_id}'")
        self._metrics[metric_id] = definition
        logger.debug(f"Registered metric definition: {metric_id}")


# Create the singleton instance (without auto-loading to prevent duplicates)
metric_registry = MetricRegistry()



================================================================================
File: fin_statement_model/core/node_factory.py
================================================================================

"""Provide a factory for creating nodes in the financial statement model.

This module centralizes node creation logic and ensures consistent initialization
for different types of nodes used in the financial statement model.
"""

import logging
from typing import Any, Union, Optional, ClassVar, Callable, cast

# Force import of strategies package to ensure registration happens

from .nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    CustomCalculationNode,
)
from .nodes.calculation_nodes import FormulaCalculationNode
from .nodes.stats_nodes import (
    YoYGrowthNode,
    MultiPeriodStatNode,
    TwoPeriodAverageNode,
)
from .nodes.forecast_nodes import (
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
    CustomGrowthForecastNode,
)

# Force import of calculations package to ensure registration happens
from fin_statement_model.core.calculations import Registry, Calculation
from fin_statement_model.core.errors import ConfigurationError

# Configure logging
logger = logging.getLogger(__name__)


class NodeFactory:
    """Provide a factory for creating nodes in the financial statement model.

    This class centralizes node creation for financial statement items,
    calculations, metrics, forecasts, and custom logic.

    Attributes:
        _calculation_methods: Maps simple string keys (e.g., 'addition') to
            the class names of Calculation implementations registered in the
            `Registry`. This allows creating CalculationNodes without
            directly importing Calculation classes.
        _node_type_registry: Maps node type strings to their corresponding
            Node class implementations for deserialization.
    """

    # Mapping of calculation type strings to Calculation class names (keys in the Registry)
    _calculation_methods: ClassVar[dict[str, str]] = {
        "addition": "AdditionCalculation",
        "subtraction": "SubtractionCalculation",
        "formula": "FormulaCalculation",
        "division": "DivisionCalculation",
        "weighted_average": "WeightedAverageCalculation",
        "custom_formula": "CustomFormulaCalculation",
    }

    # Mapping from node type names to Node classes for deserialization
    _node_type_registry: ClassVar[dict[str, type[Node]]] = {
        "financial_statement_item": FinancialStatementItemNode,
        "calculation": CalculationNode,
        "formula_calculation": FormulaCalculationNode,
        "custom_calculation": CustomCalculationNode,
        "forecast": ForecastNode,
        # Specific forecast types
        "fixed_growth_forecast": FixedGrowthForecastNode,
        "curve_growth_forecast": CurveGrowthForecastNode,
        "statistical_growth_forecast": StatisticalGrowthForecastNode,
        "average_value_forecast": AverageValueForecastNode,
        "average_historical_growth_forecast": AverageHistoricalGrowthForecastNode,
        "custom_growth_forecast": CustomGrowthForecastNode,
        # Stats node types
        "yoy_growth": YoYGrowthNode,
        "multi_period_stat": MultiPeriodStatNode,
        "two_period_average": TwoPeriodAverageNode,
    }

    # Mapping from forecast type strings to specific forecast node classes
    _forecast_type_registry: ClassVar[dict[str, type[ForecastNode]]] = {
        "simple": FixedGrowthForecastNode,
        "curve": CurveGrowthForecastNode,
        "statistical": StatisticalGrowthForecastNode,
        "average": AverageValueForecastNode,
        "historical_growth": AverageHistoricalGrowthForecastNode,
        "custom": CustomGrowthForecastNode,
    }

    @classmethod
    def create_financial_statement_item(
        cls, name: str, values: dict[str, float]
    ) -> FinancialStatementItemNode:
        """Create a FinancialStatementItemNode representing a base financial item.

        This node holds historical or projected values for a specific
        line item (e.g., Revenue, COGS) over different periods.

        Args:
            name: Identifier for the node (e.g., "Revenue").
            values: Mapping of period identifiers to numerical values.

        Returns:
            A FinancialStatementItemNode initialized with the provided values.

        Raises:
            ValueError: If the provided name is empty or not a string.

        Examples:
            >>> revenue_node = NodeFactory.create_financial_statement_item(
            ...     name="Revenue",
            ...     values={"2023": 1000.0, "2024": 1100.0}
            ... )
            >>> revenue_node.get_value("2023")
            1000.0
        """
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        logger.debug(f"Creating financial statement item node: {name}")
        return FinancialStatementItemNode(name, values)

    @classmethod
    def create_calculation_node(
        cls,
        name: str,
        inputs: list[Node],
        calculation_type: str,
        formula_variable_names: Optional[list[str]] = None,
        **calculation_kwargs: Any,
    ) -> CalculationNode:
        """Create a CalculationNode using a pre-defined calculation.

        This method resolves a calculation class from a calculation_type key,
        instantiates it with optional parameters, and wraps it in
        a CalculationNode.

        Args:
            name: Identifier for the calculation node instance.
            inputs: List of Node instances serving as inputs to the calculation.
            calculation_type: Key for the desired calculation in the registry.
            formula_variable_names: Optional list of variable names used in the formula
                string. Required & used only if creating a FormulaCalculationNode
                via the 'custom_formula' type with a 'formula' kwarg.
            **calculation_kwargs: Additional parameters for the calculation constructor.

        Returns:
            A CalculationNode configured with the selected calculation.

        Raises:
            ValueError: If name is invalid, inputs list is empty, or the
                calculation_type is unrecognized.
            TypeError: If the calculation cannot be instantiated with given kwargs.

        Examples:
            >>> gross_profit = NodeFactory.create_calculation_node(
            ...     name="GrossProfit",
            ...     inputs=[revenue, cogs],
            ...     calculation_type="subtraction"
            ... )
        """
        # Validate inputs
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        if not inputs:
            raise ValueError("Calculation node must have at least one input")

        # Check if the calculation type maps to a known calculation name
        if calculation_type not in cls._calculation_methods:
            valid_types = list(cls._calculation_methods.keys())
            raise ValueError(
                f"Invalid calculation type: '{calculation_type}'. Valid types are: {valid_types}"
            )

        # Get the calculation name
        calculation_name = cls._calculation_methods[calculation_type]

        # For other types, resolve the Calculation class from the registry
        try:
            calculation_cls: type[Calculation] = Registry.get(calculation_name)
        except KeyError:
            # This should ideally not happen if _calculation_methods is synced with registry
            raise ValueError(
                f"Calculation class '{calculation_name}' (for type '{calculation_type}') not found in Registry."
            ) from None  # Prevent chaining the KeyError

        # Instantiate the calculation, passing any extra kwargs
        try:
            # Extract any metadata that should be stored on the node, not passed to calculation
            node_kwargs = {}
            if "metric_name" in calculation_kwargs:
                node_kwargs["metric_name"] = calculation_kwargs.pop("metric_name")
            if "metric_description" in calculation_kwargs:
                node_kwargs["metric_description"] = calculation_kwargs.pop(
                    "metric_description"
                )

            # Special handling for FormulaCalculation which needs input_variable_names
            if calculation_type == "formula":
                if formula_variable_names and len(formula_variable_names) == len(
                    inputs
                ):
                    calculation_kwargs["input_variable_names"] = formula_variable_names
                elif not formula_variable_names:
                    # Generate default names like var_0, var_1, ...
                    calculation_kwargs["input_variable_names"] = [
                        f"var_{i}" for i in range(len(inputs))
                    ]
                    logger.warning(
                        f"No formula_variable_names provided for formula node '{name}'. Using defaults: {calculation_kwargs['input_variable_names']}"
                    )
                else:
                    # Mismatch between provided names and number of inputs
                    raise ConfigurationError(
                        f"Mismatch between formula_variable_names ({len(formula_variable_names)}) and number of inputs ({len(inputs)}) for node '{name}'"
                    )

            calculation_instance = calculation_cls(**calculation_kwargs)
        except TypeError as e:
            logger.exception(
                f"Failed to instantiate calculation '{calculation_name}' with kwargs {calculation_kwargs}"
            )
            raise TypeError(
                f"Could not instantiate calculation '{calculation_name}' for node '{name}'. "
                f"Check required arguments for {calculation_cls.__name__}. Provided kwargs: {calculation_kwargs}"
            ) from e

        # Create and return a CalculationNode with the instantiated calculation
        logger.debug(
            f"Creating calculation node '{name}' with '{calculation_name}' calculation."
        )

        return CalculationNode(name, inputs, calculation_instance, **node_kwargs)

    @classmethod
    def create_forecast_node(
        cls,
        name: str,
        base_node: Node,
        base_period: str,
        forecast_periods: list[str],
        forecast_type: str,
        growth_params: Union[float, list[float], Callable[[], float]],
    ) -> Node:
        """Create a forecast node of the specified type using core forecast classes.

        Args:
            name: Custom name for the forecast node.
            base_node: The Node instance to base projections on.
            base_period: Period identifier providing the base value.
            forecast_periods: List of periods for which to forecast.
            forecast_type: Forecast method ('simple', 'curve', 'statistical',
                'average', 'historical_growth').
            growth_params: Parameters controlling forecast behavior (float,
                list of floats, or callable). Ignored for 'average' and 'historical_growth'.

        Returns:
            A Node instance implementing the chosen forecast.

        Raises:
            ValueError: If an unsupported forecast_type is provided.

        Examples:
            >>> forecast = NodeFactory.create_forecast_node(
            ...     name="RevForecast",
            ...     base_node=revenue,
            ...     base_period="2023",
            ...     forecast_periods=["2024", "2025"],
            ...     forecast_type="simple",
            ...     growth_params=0.05
            ... )
        """
        # Prepare placeholder to unify forecast node type
        node: ForecastNode
        # Instantiate the appropriate forecast node with proper type checking
        if forecast_type == "simple":
            if not isinstance(growth_params, (int, float)):
                raise TypeError("growth_params must be a float for 'simple' forecast")
            node = FixedGrowthForecastNode(
                base_node, base_period, forecast_periods, float(growth_params)
            )
        elif forecast_type == "curve":
            if not isinstance(growth_params, list):
                raise TypeError(
                    "growth_params must be a list of floats for 'curve' forecast"
                )
            rates: list[float] = [float(r) for r in growth_params]
            node = CurveGrowthForecastNode(
                base_node, base_period, forecast_periods, rates
            )
        elif forecast_type == "statistical":
            if not callable(growth_params):
                raise TypeError(
                    "growth_params must be a callable returning float for 'statistical' forecast"
                )
            node = StatisticalGrowthForecastNode(
                base_node, base_period, forecast_periods, growth_params
            )
        elif forecast_type == "average":
            node = AverageValueForecastNode(base_node, base_period, forecast_periods)
        elif forecast_type == "historical_growth":
            node = AverageHistoricalGrowthForecastNode(
                base_node, base_period, forecast_periods
            )
        else:
            raise ValueError(f"Invalid forecast type: {forecast_type}")

        # Override forecast node's name to match factory 'name' argument
        node.name = name
        logger.debug(
            f"Forecast node created with custom name: {name} (original: {base_node.name})"
        )
        return node

    @classmethod
    def create_from_dict(  # noqa: PLR0911
        cls, data: dict[str, Any], context: Optional[dict[str, Node]] = None
    ) -> Node:
        """Create a node from its dictionary representation.

        This method provides a unified interface for deserializing nodes from
        their dictionary representations, handling dependency resolution and
        type-specific deserialization logic.

        Args:
            data: Serialized node data containing at minimum a 'type' field.
            context: Optional dictionary of existing nodes for resolving dependencies.
                Required for nodes that have dependencies (calculation, forecast nodes).

        Returns:
            Reconstructed node instance.

        Raises:
            ValueError: If the data is invalid, missing required fields, or contains
                an unknown node type.
            ConfigurationError: If dependencies cannot be resolved or node creation fails.

        Examples:
            >>> # Simple node without dependencies
            >>> data = {
            ...     'type': 'financial_statement_item',
            ...     'name': 'Revenue',
            ...     'values': {'2023': 1000.0}
            ... }
            >>> node = NodeFactory.create_from_dict(data)

            >>> # Node with dependencies
            >>> calc_data = {
            ...     'type': 'calculation',
            ...     'name': 'GrossProfit',
            ...     'inputs': ['Revenue', 'COGS'],
            ...     'calculation_type': 'subtraction'
            ... }
            >>> context = {'Revenue': revenue_node, 'COGS': cogs_node}
            >>> calc_node = NodeFactory.create_from_dict(calc_data, context)
        """
        if not isinstance(data, dict):
            raise TypeError("Node data must be a dictionary")

        node_type = data.get("type")
        if not node_type:
            raise ValueError("Missing 'type' field in node data")

        logger.debug(f"Creating node of type '{node_type}' from dictionary")

        # Handle nodes without dependencies first
        if node_type == "financial_statement_item":
            return FinancialStatementItemNode.from_dict(data)

        # Handle nodes that require context for dependency resolution
        if context is None:
            context = {}

        # For calculation nodes, use the appropriate from_dict_with_context method
        if node_type == "calculation":
            return cast(Node, CalculationNode.from_dict_with_context(data, context))
        elif node_type == "formula_calculation":
            return cast(
                Node, FormulaCalculationNode.from_dict_with_context(data, context)
            )
        elif node_type == "custom_calculation":
            raise ConfigurationError(
                "CustomCalculationNode cannot be deserialized because it contains "
                "non-serializable Python functions. Manual reconstruction required."
            )

        # Handle stats nodes
        elif node_type == "yoy_growth":
            return YoYGrowthNode.from_dict_with_context(data, context)
        elif node_type == "multi_period_stat":
            return MultiPeriodStatNode.from_dict_with_context(data, context)
        elif node_type == "two_period_average":
            return TwoPeriodAverageNode.from_dict_with_context(data, context)

        # Handle forecast nodes
        elif node_type == "forecast":
            # Determine the specific forecast type from the data
            forecast_type = data.get("forecast_type")
            if not forecast_type:
                raise ValueError("Missing 'forecast_type' field in forecast node data")

            # Get the appropriate forecast node class
            forecast_class = cls._forecast_type_registry.get(forecast_type)
            if not forecast_class:
                valid_types = list(cls._forecast_type_registry.keys())
                raise ValueError(
                    f"Unknown forecast type '{forecast_type}'. Valid types: {valid_types}"
                )

            # Handle non-serializable forecast types
            if forecast_type in ["statistical", "custom"]:
                raise ConfigurationError(
                    f"Forecast type '{forecast_type}' cannot be deserialized because it contains "
                    "non-serializable functions. Manual reconstruction required."
                )

            # Use the specific forecast class's from_dict_with_context method
            return forecast_class.from_dict_with_context(data, context)

        # Handle specific forecast node types (for backward compatibility)
        elif node_type in cls._node_type_registry:
            node_class = cls._node_type_registry[node_type]
            if hasattr(node_class, "from_dict_with_context"):
                return cast(Node, node_class.from_dict_with_context(data, context))
            else:
                return node_class.from_dict(data)

        else:
            valid_types = list(cls._node_type_registry.keys())
            raise ValueError(
                f"Unknown node type: '{node_type}'. Valid types: {valid_types}"
            )

    @classmethod
    def _create_custom_node_from_callable(
        cls,
        name: str,
        inputs: list[Node],
        formula: Callable[..., Any],
        description: Optional[str] = None,
    ) -> CustomCalculationNode:
        """Create a CustomCalculationNode using a Python callable for the calculation logic.

        This supports ad-hoc or complex calculations not covered by standard
        strategies or metrics. The `formula` callable will be invoked with
        input node values at calculation time.

        Note:
            Renamed from `create_metric_node` to avoid confusion with metric-based nodes.

        Args:
            name: Identifier for the custom calculation node.
            inputs: List of Node instances providing values to the formula.
            formula: Callable that computes a value from input node values.
            description: Optional description of the calculation logic.

        Returns:
            A CustomCalculationNode configured with the provided formula.

        Raises:
            ValueError: If name is empty or not a string.
            TypeError: If formula is not callable or inputs contain non-Node items.

        Examples:
            >>> def complex_tax_logic(revenue, expenses, tax_rate_node):
            ...     profit = revenue - expenses
            ...     if profit <= 0:
            ...         return 0.0
            ...     tax_rate = tax_rate_node
            ...     return profit * tax_rate
            ...
            >>> tax_node = NodeFactory._create_custom_node_from_callable(
            ...     name="CalculatedTaxes",
            ...     inputs=[revenue_node, expenses_node, tax_rate_schedule_node],
            ...     formula=complex_tax_logic,
            ...     description="Calculates income tax based on profit and a variable rate."
            ... )

            Using a lambda for a simple ratio:
            >>> quick_ratio_node = NodeFactory._create_custom_node_from_callable(
            ...    name="QuickRatioCustom",
            ...    inputs=[cash_node, receivables_node, current_liabilities_node],
            ...    formula=lambda cash, rec, liab: (cash + rec) / liab if liab else 0
            ... )
        """
        # Validate inputs
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        if not inputs:
            # Allowing no inputs might be valid for some custom functions (e.g., constants)
            # Reconsider if this check is always needed here.
            logger.warning(f"Creating CustomCalculationNode '{name}' with no inputs.")
            # raise ValueError("Custom node must have at least one input")

        if not callable(formula):
            raise TypeError("Formula must be a callable function")
        if not all(isinstance(i, Node) for i in inputs):
            raise TypeError("All items in inputs must be Node instances.")

        # Use the imported CustomCalculationNode
        logger.debug(f"Creating CustomCalculationNode: {name} using provided callable.")
        return CustomCalculationNode(
            name, inputs, formula_func=formula, description=description
        )

    # Consider adding a method for creating FormulaCalculationNode if needed directly
    # @classmethod
    # def create_formula_node(cls, name: str, inputs: Dict[str, Node], formula: str) -> FormulaCalculationNode:
    #     ...



================================================================================
File: fin_statement_model/core/nodes/__init__.py
================================================================================

"""Core Node Implementations for the Financial Statement Model.

This package exports the base `Node` class and various concrete node types
used to build the financial model graph. These include:

- Data Nodes:
    - `FinancialStatementItemNode`: Stores raw numerical data for specific periods.

- Calculation Nodes:
    - `FormulaCalculationNode`: Calculates based on mathematical string formulas.
    - `CalculationNode`: Uses a calculation object for calculation logic.
    - `CustomCalculationNode`: Uses arbitrary Python functions for calculation.

- Statistical Nodes:
    - `YoYGrowthNode`: Calculates year-over-year percentage growth.
    - `MultiPeriodStatNode`: Computes statistics (mean, stddev) over multiple periods.
    - `TwoPeriodAverageNode`: Calculates the average over two specific periods.

- Forecast Nodes:
    - `ForecastNode`: Base class for forecasting nodes.
    - Various forecast implementations for different forecasting strategies.

Standard Node Registry:
The package also provides a registry of standardized node names for financial
statement items, ensuring consistency across models and enabling metrics to work properly.
Standard nodes are organized by category:
- Balance Sheet: Assets, liabilities, equity
- Income Statement: Revenue, expenses, profit measures
- Cash Flow: Operating, investing, financing activities
- Calculated Items: EBITDA, working capital, leverage measures
- Market Data: Stock price, market cap, per-share metrics
"""

import logging

# Import all node classes using actual file names
from .base import Node
from .item_node import FinancialStatementItemNode
from .calculation_nodes import (
    CalculationNode,
    FormulaCalculationNode,
    CustomCalculationNode,
)
from .stats_nodes import (
    YoYGrowthNode,
    MultiPeriodStatNode,
    TwoPeriodAverageNode,
)
from .forecast_nodes import (
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    CustomGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
)

# Import standard registry
from .standard_registry import standard_node_registry

logger = logging.getLogger(__name__)

# Initialize standard nodes from the organized definition directory
try:
    count = standard_node_registry.initialize_default_nodes()
    if count == 0:
        logger.warning(
            "No standard nodes were loaded. The registry is empty. "
            "This may cause issues with metrics and node validation."
        )
except Exception:
    logger.exception("Failed to initialize standard nodes")

__all__ = [
    "AverageHistoricalGrowthForecastNode",
    "AverageValueForecastNode",
    "CalculationNode",
    "CurveGrowthForecastNode",
    "CustomCalculationNode",
    "CustomGrowthForecastNode",
    "FinancialStatementItemNode",
    "FixedGrowthForecastNode",
    "ForecastNode",
    "FormulaCalculationNode",
    "MultiPeriodStatNode",
    "Node",
    "StatisticalGrowthForecastNode",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
    "standard_node_registry",
]



================================================================================
File: fin_statement_model/core/nodes/base.py
================================================================================

"""Define the abstract base class for all nodes in the graph.

This module provides the Node base class with interfaces for calculation,
attribute access, and optional caching behavior.
"""

from abc import ABC, abstractmethod
from typing import Any, TYPE_CHECKING

if TYPE_CHECKING:
    pass


class Node(ABC):
    """Define the abstract base class for graph nodes.

    Provide the essential interface for all nodes in the financial statement
    model graph, including calculation, caching, and attribute access.

    Attributes:
    name (str): Unique identifier for the node instance.
    values: dict[str, Any]
    """

    name: str
    values: dict[str, Any]

    def __init__(self, name: str):
        """Initialize the Node instance with a unique name.

        Args:
            name: Unique identifier for the node. Must be a non-empty string.

        Raises:
            ValueError: If `name` is empty or not a string.

        Examples:
            >>> class Dummy(Node):
            ...     def calculate(self, period): return 0.0
            >>> dn = Dummy("Revenue")
            >>> dn.name
            'Revenue'
        """
        # Check if name is a non-empty string
        if not isinstance(name, str) or not name:
            raise ValueError("Node name must be a non-empty string.")
        # Check for invalid characters (including newline, tab)
        if "\n" in name or "\t" in name:
            raise ValueError(
                f"Invalid node name: '{name}'. Contains invalid characters."
            )
        # Check for leading/trailing whitespace
        if name != name.strip():
            raise ValueError(
                f"Invalid node name: '{name}'. Cannot have leading/trailing whitespace."
            )
        self.name = name

    @abstractmethod
    def calculate(self, period: str) -> float:
        """Calculate the node's value for a specific period.

        This abstract method must be implemented by subclasses to define how to
        determine the node's value for a given time period.

        Args:
            period: The time period identifier for the calculation.

        Returns:
            The calculated float value for the specified period.

        Raises:
            NotImplementedError: If the subclass does not implement this method.

        Examples:
            >>> class Dummy(Node):
            ...     def calculate(self, period): return 100.0
            >>> d = Dummy("Test")
            >>> d.calculate("2023")
            100.0
        """

    def clear_cache(self) -> None:
        """Clear cached calculation results for this node.

        Subclasses with caching should override this method to clear their internal cache.

        Returns:
            None

        Examples:
            >>> node.clear_cache()
        """
        # Default: no cache to clear

    def has_attribute(self, attr_name: str) -> bool:
        """Check if the node has a specific attribute.

        Args:
            attr_name: The name of the attribute to check.

        Returns:
            True if the attribute exists, otherwise False.

        Examples:
            >>> node.has_attribute("name")
            True
        """
        return hasattr(self, attr_name)

    def get_attribute(self, attribute_name: str) -> object:
        """Get a named attribute from the node.

        Args:
            attribute_name: The name of the attribute to retrieve.

        Returns:
            The value of the specified attribute.

        Raises:
            AttributeError: If the attribute does not exist.

        Examples:
            >>> node.get_attribute("name")
            'Revenue'
        """
        try:
            return getattr(self, attribute_name)
        except AttributeError:
            raise AttributeError(
                f"Node '{self.name}' has no attribute '{attribute_name}'"
            )

    def has_value(self, period: str) -> bool:
        """Indicate whether the node stores a direct value for a period.

        Primarily for data-bearing nodes; calculation nodes override has_calculation.

        Args:
            period: The time period to check for a stored value.

        Returns:
            True if a direct value is stored, otherwise False.

        Examples:
            >>> node.has_value("2023")
            False
        """
        return False

    def get_value(self, period: str) -> float:
        """Retrieve the node's directly stored value for a period.

        This method must be overridden by data-bearing nodes to return stored values.

        Args:
            period: The time period string for which to retrieve the value.

        Returns:
            The float value stored for the given period.

        Raises:
            NotImplementedError: If the node does not store direct values.

        Examples:
            >>> node.get_value("2023")
        """
        raise NotImplementedError(f"Node {self.name} does not implement get_value")

    def set_value(self, period: str, value: float) -> None:
        """Optional: Set a value for a specific period on data-bearing nodes.

        Default implementation raises NotImplementedError if not overridden.
        """
        raise NotImplementedError(f"Node '{self.name}' does not support set_value")

    def has_calculation(self) -> bool:
        """Indicate whether this node performs calculation.

        Distinguish calculation nodes from data-holding nodes.

        Returns:
            True if the node performs calculations, otherwise False.

        Examples:
            >>> node.has_calculation()
            False
        """
        return False

    @abstractmethod
    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        This method should return a dictionary containing all information
        necessary to reconstruct the node, including:
        - node type
        - name
        - any configuration parameters
        - values (for data nodes)
        - input references (for calculation nodes)

        Returns:
            Dictionary representation of the node.

        Examples:
            >>> node_dict = node.to_dict()
            >>> node_dict['type']
            'financial_statement_item'
        """

    @staticmethod
    @abstractmethod
    def from_dict(data: dict[str, Any]) -> "Node":
        """Create a node instance from a dictionary representation.

        This static method should be implemented by each concrete node class
        to reconstruct an instance from its serialized form.

        Args:
            data: Dictionary containing the node's serialized data.

        Returns:
            A new instance of the node.

        Raises:
            ValueError: If the data is invalid or missing required fields.

        Examples:
            >>> data = {'type': 'financial_statement_item', 'name': 'Revenue', 'values': {'2023': 1000}}
            >>> node = FinancialStatementItemNode.from_dict(data)
        """

    def get_dependencies(self) -> list[str]:
        """Get the names of nodes this node depends on.

        Default implementation returns empty list. Override in nodes that have dependencies.

        Returns:
            List of node names this node depends on.
        """
        return []



================================================================================
File: fin_statement_model/core/nodes/calculation_nodes.py
================================================================================

"""Provide node implementations for performing calculations in the financial statement model.

This module defines the different types of calculation nodes available in the system:
- FormulaCalculationNode: Evaluates a formula expression string (e.g., "a + b / 2")
- CalculationNode: Uses a calculation object for calculation logic
- CustomCalculationNode: Calculates using a Python callable/function
"""

from typing import Optional, Any, Callable

from fin_statement_model.core.calculations.calculation import (
    Calculation,
    FormulaCalculation,
)
from fin_statement_model.core.errors import (
    CalculationError,
)
from fin_statement_model.core.nodes.base import Node


# === CalculationNode ===


class CalculationNode(Node):
    """Delegate calculation logic to a separate calculation object.

    Uses a calculation object. The actual calculation algorithm is
    encapsulated in a `calculation` object provided during initialization.
    This allows for flexible and interchangeable calculation logic.

    Attributes:
        name (str): Identifier for this node.
        inputs (List[Node]): A list of input nodes required by the calculation.
        calculation (Any): An object possessing a `calculate(inputs: List[Node], period: str) -> float` method.
        _values (Dict[str, float]): Internal cache for calculated results.

    Examples:
        >>> class SumCalculation:
        ...     def calculate(self, inputs: List[Node], period: str) -> float:
        ...         return sum(node.calculate(period) for node in inputs)
        >>> node_a = FinancialStatementItemNode("a", {"2023": 10})
        >>> node_b = FinancialStatementItemNode("b", {"2023": 20})
        >>> sum_node = CalculationNode(
        ...     "sum_ab",
        ...     inputs=[node_a, node_b],
        ...     calculation=SumCalculation()
        ... )
        >>> print(sum_node.calculate("2023"))
        30.0
    """

    def __init__(
        self, name: str, inputs: list[Node], calculation: Calculation, **kwargs: Any
    ):
        """Initialize the CalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (List[Node]): List of input nodes needed by the calculation.
            calculation (Any): The calculation object implementing the calculation.
                Must have a `calculate` method.
            **kwargs: Additional attributes to store on the node (e.g., metric_name, metric_description).

        Raises:
            TypeError: If `inputs` is not a list of Nodes, or if `calculation`
                does not have a callable `calculate` method.
        """
        super().__init__(name)
        if not isinstance(inputs, list) or not all(isinstance(n, Node) for n in inputs):
            raise TypeError("CalculationNode inputs must be a list of Node instances.")
        if not hasattr(calculation, "calculate") or not callable(
            getattr(calculation, "calculate")
        ):
            raise TypeError(
                "Calculation object must have a callable 'calculate' method."
            )

        self.inputs = inputs
        self.calculation = calculation
        self._values: dict[str, float] = {}  # Cache for calculated values

        # Store any additional attributes passed via kwargs
        for key, value in kwargs.items():
            setattr(self, key, value)

    def calculate(self, period: str) -> float:
        """Calculate the value for a period using the assigned calculation.

        Checks the cache first. If not found, delegates to the calculation's
        `calculate` method and stores the result.

        Args:
            period (str): The time period for the calculation.

        Returns:
            float: The calculated value from the calculation.

        Raises:
            CalculationError: If the calculation fails or returns
                a non-numeric value.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Delegate to the calculation object's calculate method
            result = self.calculation.calculate(self.inputs, period)
            if not isinstance(result, int | float):
                raise TypeError(
                    f"Calculation for node '{self.name}' did not return a numeric value (got {type(self.calculation).__name__})."
                )
            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the calculation
            raise CalculationError(
                message=f"Error during calculation for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={
                    "calculation": type(self.calculation).__name__,
                    "error": str(e),
                },
            ) from e

    def set_calculation(self, calculation: Calculation) -> None:
        """Change the calculation object for the node.

        Args:
            calculation (Any): The new calculation object. Must have a callable
                `calculate` method.

        Raises:
            TypeError: If the new calculation is invalid.
        """
        if not hasattr(calculation, "calculate") or not callable(
            getattr(calculation, "calculate")
        ):
            raise TypeError(
                "New calculation object must have a callable 'calculate' method."
            )
        self.calculation = calculation
        self.clear_cache()  # Clear cache as logic has changed

    def clear_cache(self) -> None:
        """Clear the internal cache of calculated values.

        Returns:
            None
        """
        self._values.clear()

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used by the calculation.

        Returns:
            A list of input node names.
        """
        return [node.name for node in self.inputs]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as CalculationNode performs calculations.
        """
        return True

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's type, name, inputs, and calculation info.

        Note:
            This method requires access to NodeFactory's calculation registry
            to properly serialize the calculation type. Some calculation types
            with non-serializable parameters may include warnings.
        """
        # Import here to avoid circular imports
        from fin_statement_model.core.node_factory import NodeFactory

        node_dict: dict[str, Any] = {
            "type": "calculation",
            "name": self.name,
            "inputs": self.get_dependencies(),
        }

        # Add calculation type information
        calc_class_name = type(self.calculation).__name__
        node_dict["calculation_type_class"] = calc_class_name

        # Find the calculation type key from NodeFactory registry
        inv_map = {v: k for k, v in NodeFactory._calculation_methods.items()}
        type_key = inv_map.get(calc_class_name)
        if type_key:
            node_dict["calculation_type"] = type_key

            # Extract calculation-specific arguments
            calculation_args = {}

            # Handle specific calculation types
            if type_key == "weighted_average" and hasattr(self.calculation, "weights"):
                calculation_args["weights"] = self.calculation.weights
            elif type_key == "formula" and hasattr(self.calculation, "formula"):
                calculation_args["formula"] = self.calculation.formula
                if hasattr(self.calculation, "input_variable_names"):
                    node_dict["formula_variable_names"] = (
                        self.calculation.input_variable_names
                    )
            elif type_key == "custom_formula":
                node_dict["serialization_warning"] = (
                    "CustomFormulaCalculation uses a Python function which cannot be serialized. "
                    "Manual reconstruction required."
                )

            if calculation_args:
                node_dict["calculation_args"] = calculation_args

        # Add any additional attributes (like metric info)
        if hasattr(self, "metric_name") and self.metric_name:
            node_dict["metric_name"] = self.metric_name
        if hasattr(self, "metric_description") and self.metric_description:
            node_dict["metric_description"] = self.metric_description

        return node_dict

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "CalculationNode":
        """Create a CalculationNode from a dictionary representation.

        Args:
            data: Dictionary containing the node's serialized data.

        Returns:
            A new CalculationNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: This method requires context (existing nodes) to resolve
                input dependencies. Use from_dict_with_context instead.

        Note:
            This method cannot resolve input node dependencies without context.
            Use NodeFactory.create_from_dict() or from_dict_with_context() instead.
        """
        raise NotImplementedError(
            "CalculationNode.from_dict() requires context to resolve input dependencies. "
            "Use NodeFactory.create_from_dict() or from_dict_with_context() instead."
        )

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "CalculationNode":
        """Create a CalculationNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new CalculationNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        # Import here to avoid circular imports
        from fin_statement_model.core.node_factory import NodeFactory

        if data.get("type") != "calculation":
            raise ValueError(f"Invalid type for CalculationNode: {data.get('type')}")

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in CalculationNode data")

        input_names = data.get("inputs", [])
        if not isinstance(input_names, list):
            raise TypeError("'inputs' field must be a list")

        # Resolve input nodes from context
        input_nodes = []
        for input_name in input_names:
            if input_name not in context:
                raise ValueError(f"Input node '{input_name}' not found in context")
            input_nodes.append(context[input_name])

        calculation_type = data.get("calculation_type")
        if not calculation_type:
            raise ValueError("Missing 'calculation_type' field in CalculationNode data")

        # Get calculation arguments
        calculation_args = data.get("calculation_args", {})

        # Handle formula variable names for formula calculations
        formula_variable_names = data.get("formula_variable_names")

        # Extract metric information
        metric_name = data.get("metric_name")
        metric_description = data.get("metric_description")

        # Create the node using NodeFactory
        return NodeFactory.create_calculation_node(
            name=name,
            inputs=input_nodes,
            calculation_type=calculation_type,
            formula_variable_names=formula_variable_names,
            metric_name=metric_name,
            metric_description=metric_description,
            **calculation_args,
        )


# === FormulaCalculationNode ===


class FormulaCalculationNode(CalculationNode):
    """Calculate a value based on a mathematical formula string.

    This node extends CalculationNode and uses a FormulaCalculation strategy
    internally to parse and evaluate mathematical expressions.

    Attributes:
        name (str): Identifier for this node.
        inputs (Dict[str, Node]): Mapping of variable names used in the formula
            to their corresponding input Node instances.
        formula (str): The mathematical expression string to evaluate (e.g., "a + b").
        metric_name (Optional[str]): The original metric identifier from the registry, if applicable.
        metric_description (Optional[str]): The description from the metric definition, if applicable.

    Examples:
        >>> # Assume revenue and cogs are Node instances
        >>> revenue = FinancialStatementItemNode("revenue", {"2023": 100})
        >>> cogs = FinancialStatementItemNode("cogs", {"2023": 60})
        >>> gross_profit = FormulaCalculationNode(
        ...     "gross_profit",
        ...     inputs={"rev": revenue, "cost": cogs},
        ...     formula="rev - cost"
        ... )
        >>> print(gross_profit.calculate("2023"))
        40.0
    """

    def __init__(
        self,
        name: str,
        inputs: dict[str, Node],
        formula: str,
        metric_name: Optional[str] = None,
        metric_description: Optional[str] = None,
    ):
        """Initialize the FormulaCalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (Dict[str, Node]): Dictionary mapping variable names in the
                formula to the corresponding input nodes.
            formula (str): The mathematical formula string.
            metric_name (Optional[str]): The original metric identifier from the
                registry, if this node represents a defined metric. Defaults to None.
            metric_description (Optional[str]): The description from the metric
                definition, if applicable. Defaults to None.

        Raises:
            ValueError: If the formula string has invalid syntax.
            TypeError: If any value in `inputs` is not a Node instance.
        """
        if not isinstance(inputs, dict) or not all(
            isinstance(n, Node) for n in inputs.values()
        ):
            raise TypeError(
                "FormulaCalculationNode inputs must be a dict of Node instances."
            )

        # Store the formula and metric attributes
        self.formula = formula
        self.metric_name = metric_name
        self.metric_description = metric_description

        # Extract variable names and input nodes in consistent order
        input_variable_names = list(inputs.keys())
        input_nodes = list(inputs.values())

        # Create FormulaCalculation strategy
        formula_calculation = FormulaCalculation(formula, input_variable_names)

        # Initialize parent CalculationNode with the strategy
        super().__init__(name, input_nodes, formula_calculation)

        # Store the inputs dict for compatibility (separate from parent's inputs list)
        self.inputs_dict = inputs

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used in the formula.

        Returns:
            A list of variable names corresponding to the formula inputs.
        """
        return [node.name for node in self.inputs_dict.values()]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as FormulaCalculationNode performs calculations.
        """
        return True

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's type, name, inputs, and formula info.
        """
        return {
            "type": "formula_calculation",
            "name": self.name,
            "inputs": self.get_dependencies(),
            "formula_variable_names": list(self.inputs_dict.keys()),
            "formula": self.formula,
            "calculation_type": "formula",
            "metric_name": self.metric_name,
            "metric_description": self.metric_description,
        }

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "FormulaCalculationNode":
        """Create a FormulaCalculationNode from a dictionary representation.

        Args:
            data: Dictionary containing the node's serialized data.

        Returns:
            A new FormulaCalculationNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: This method requires context (existing nodes) to resolve
                input dependencies. Use from_dict_with_context instead.
        """
        raise NotImplementedError(
            "FormulaCalculationNode.from_dict() requires context to resolve input dependencies. "
            "Use NodeFactory.create_from_dict() or from_dict_with_context() instead."
        )

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "FormulaCalculationNode":
        """Create a FormulaCalculationNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new FormulaCalculationNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        if data.get("type") != "formula_calculation":
            raise ValueError(
                f"Invalid type for FormulaCalculationNode: {data.get('type')}"
            )

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in FormulaCalculationNode data")

        formula = data.get("formula")
        if not formula:
            raise ValueError("Missing 'formula' field in FormulaCalculationNode data")

        input_names = data.get("inputs", [])
        formula_variable_names = data.get("formula_variable_names", [])

        if len(input_names) != len(formula_variable_names):
            raise ValueError(
                "Mismatch between inputs and formula_variable_names in FormulaCalculationNode data"
            )

        # Resolve input nodes from context and create inputs dict
        inputs_dict = {}
        for var_name, input_name in zip(formula_variable_names, input_names):
            if input_name not in context:
                raise ValueError(f"Input node '{input_name}' not found in context")
            inputs_dict[var_name] = context[input_name]

        # Extract metric information
        metric_name = data.get("metric_name")
        metric_description = data.get("metric_description")

        return FormulaCalculationNode(
            name=name,
            inputs=inputs_dict,
            formula=formula,
            metric_name=metric_name,
            metric_description=metric_description,
        )


# === CustomCalculationNode ===


class CustomCalculationNode(Node):
    """Calculate a value using a Python callable/function.

    Uses a Python callable/function to calculate the value for a node.
    The function is provided during initialization.

    Attributes:
        name (str): Identifier for this node.
        inputs (List[Node]): List of input nodes needed for calculation.
        formula_func (Callable): The Python callable function to use for calculation.
        description (str, optional): Description of what this calculation does.
        _values (Dict[str, float]): Internal cache for calculated results.

    Examples:
        >>> def custom_calculation(a, b):
        ...     return a + b
        >>> node_a = FinancialStatementItemNode("NodeA", values={"2023": 10.0})
        >>> node_b = FinancialStatementItemNode("NodeB", values={"2023": 5.0})
        >>> node = CustomCalculationNode(
        ...     "custom_calc",
        ...     inputs=[node_a, node_b],
        ...     formula_func=custom_calculation
        ... )
        >>> print(node.calculate("2023"))
        15.0
    """

    def __init__(
        self,
        name: str,
        inputs: list[Node],
        formula_func: Callable[..., float],
        description: Optional[str] = None,
    ) -> None:
        """Initialize the CustomCalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (List[Node]): The input nodes whose values will be passed to formula_func.
            formula_func (Callable): The Python callable function to use for calculation.
            description (str, optional): Description of what this calculation does.

        Raises:
            TypeError: If `inputs` is not a list of Nodes or `formula_func` is not a callable.
        """
        super().__init__(name)
        if not isinstance(inputs, list) or not all(isinstance(n, Node) for n in inputs):
            raise TypeError(
                "CustomCalculationNode inputs must be a list of Node instances"
            )
        if not callable(formula_func):
            raise TypeError(
                "CustomCalculationNode formula_func must be a callable function"
            )

        self.inputs = inputs
        self.formula_func = formula_func
        self.description = description
        self._values: dict[str, float] = {}  # Cache for calculated results

    def calculate(self, period: str) -> float:
        """Calculate the node's value for a period using the provided function.

        Args:
            period (str): The time period for which to perform the calculation.

        Returns:
            float: The calculated value from the function.

        Raises:
            CalculationError: If an error occurs during calculation, such as
                if an input node fails to provide a numeric value for the period.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Get input values
            input_values = []
            for node in self.inputs:
                value = node.calculate(period)
                if not isinstance(value, int | float):
                    raise TypeError(
                        f"Input node '{node.name}' did not return a numeric value for period '{period}'. Got {type(value).__name__}."
                    )
                input_values.append(value)

            # Calculate the value using the provided function
            result = self.formula_func(*input_values)
            if not isinstance(result, int | float):
                raise TypeError(
                    f"Formula did not return a numeric value. Got {type(result).__name__}."
                )

            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the function
            raise CalculationError(
                message=f"Error during custom calculation for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={"function": self.formula_func.__name__, "error": str(e)},
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used in the function.

        Returns:
            A list of input node names.
        """
        return [node.name for node in self.inputs]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as CustomCalculationNode performs calculations.
        """
        return True

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's type, name, inputs, and description.

        Note:
            The formula_func cannot be serialized, so a warning is included.
        """
        return {
            "type": "custom_calculation",
            "name": self.name,
            "inputs": self.get_dependencies(),
            "description": self.description,
            "serialization_warning": (
                "CustomCalculationNode uses a Python function which cannot be serialized. "
                "Manual reconstruction required."
            ),
        }

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "CustomCalculationNode":
        """Create a CustomCalculationNode from a dictionary representation.

        Args:
            data: Dictionary containing the node's serialized data.

        Returns:
            A new CustomCalculationNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: CustomCalculationNode cannot be fully deserialized
                because the formula_func cannot be serialized.
        """
        raise NotImplementedError(
            "CustomCalculationNode cannot be fully deserialized because the formula_func "
            "cannot be serialized. Manual reconstruction required."
        )



================================================================================
File: fin_statement_model/core/nodes/forecast_nodes.py
================================================================================

"""Provide forecast nodes to project future values from historical data.

This module defines the base `ForecastNode` class and its subclasses,
implementing various forecasting strategies (fixed, curve, statistical,
custom, average, and historical growth).
"""

import logging
from collections.abc import Callable
from typing import Optional, Any

# Use absolute imports
from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class ForecastNode(Node):
    """Define base class for forecast nodes to project future values from historical data.

    A forecast node takes an input node (typically a financial statement item) and projects its
    future values using various growth methods. The node caches calculated values to avoid
    redundant computations.

    Attributes:
        name (str): Identifier for the forecast node (derived from input_node.name)
        input_node (Node): Source node containing historical values to forecast from
        base_period (str): Last historical period to use as basis for forecasting
        forecast_periods (List[str]): List of future periods to generate forecasts for
        _cache (dict): Internal cache of calculated values
        values (dict): Dictionary mapping periods to values (including historical)

    Methods:
        calculate(period): Get value for a specific period (historical or forecast)
        _calculate_value(period): Core calculation logic for a period
        _get_previous_period(period): Helper to get chronologically previous period
        _get_growth_factor_for_period(): Abstract method for growth rate calculation

    Examples:
        # Create 5% fixed growth forecast for revenue
        base = "FY2022"
        forecasts = ["FY2023", "FY2024", "FY2025"]
        node = FixedGrowthForecastNode(revenue_node, base, forecasts, 0.05)

        # Get forecasted value
        fy2024_revenue = node.calculate("FY2024")
    """

    _cache: dict[str, float]

    def __init__(self, input_node: Node, base_period: str, forecast_periods: list[str]):
        """Initialize ForecastNode with input node and forecast periods.

        Args:
            input_node: Source node containing historical values.
            base_period: The last historical period serving as the forecast base.
            forecast_periods: List of future periods for which forecasts will be generated.
        """
        # Initialize with a default name based on input node, but allow it to be overridden
        super().__init__(input_node.name)
        self.input_node = input_node
        self.base_period = base_period
        self.forecast_periods = forecast_periods
        self._cache = {}

        # Copy historical values from input node
        if hasattr(input_node, "values"):
            self.values = input_node.values.copy()
        else:
            self.values = {}

    def calculate(self, period: str) -> float:
        """Calculate the value for a specific period, using cached results if available.

        This method returns historical values for periods up to the base period, and
        calculates forecasted values for future periods. Results are cached to avoid
        redundant calculations.

        Args:
            period (str): The period to calculate the value for (e.g. "FY2023")

        Returns:
            float: The calculated value for the specified period

        Raises:
            ValueError: If the requested period is not in base_period or forecast_periods

        Examples:
            # Get historical value
            base_value = node.calculate("FY2022")  # Returns actual historical value

            # Get forecasted value
            forecast_value = node.calculate("FY2024")  # Returns projected value
        """
        if period not in self._cache:
            self._cache[period] = self._calculate_value(period)
        return self._cache[period]

    def clear_cache(self) -> None:
        """Clear the calculation cache.

        This method clears any cached calculation results, forcing future calls to
        calculate() to recompute values rather than using cached results.

        Examples:
            # Clear cached calculations
            node.clear_cache()  # Future calculate() calls will recompute values
        """
        self._cache.clear()

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this forecast node depends on.

        Returns:
            List containing the base node name.
        """
        return [self.input_node.name]

    def _calculate_value(self, period: str) -> float:
        """Calculate the value for a specific period.

        For historical periods (up to base_period), returns the actual value.
        For forecast periods, calculates the value using the growth rate.

        Args:
            period: The period to calculate the value for

        Returns:
            float: The calculated value for the period

        Raises:
            ValueError: If the period is not in base_period or forecast_periods
        """
        # For historical periods, return the actual value
        if period <= self.base_period:
            # Return historical value, ensuring float type
            return float(self.values.get(period, 0.0))

        # For forecast periods, calculate using growth rate
        if period not in self.forecast_periods:
            raise ValueError(
                f"Period '{period}' not in forecast periods for {self.name}"
            )

        # Get the previous period's value
        prev_period = self._get_previous_period(period)
        prev_value = self.calculate(prev_period)

        # Get the growth rate for this period
        growth_factor = self._get_growth_factor_for_period(
            period, prev_period, prev_value
        )

        # Calculate the new value
        return prev_value * (1 + growth_factor)

    def _get_previous_period(self, current_period: str) -> str:
        all_periods = sorted([self.base_period, *self.forecast_periods])
        idx = all_periods.index(current_period)
        return all_periods[idx - 1]

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        raise NotImplementedError("Implement in subclass.")

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's type, name, and forecast configuration.

        Note:
            This base implementation should be overridden by subclasses to include
            specific forecast parameters.
        """
        return {
            "type": "forecast",
            "name": self.name,
            "base_node_name": self.input_node.name,
            "base_period": self.base_period,
            "forecast_periods": self.forecast_periods.copy(),
            "forecast_type": "base",  # Override in subclasses
        }

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "ForecastNode":
        """Create a ForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new ForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: This base method should be overridden by subclasses.
        """
        raise NotImplementedError("Subclasses must implement from_dict_with_context")

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "ForecastNode":
        """Create a ForecastNode from a dictionary representation.

        Note:
            Forecast nodes require access to the existing node context to correctly
            resolve dependencies (e.g., the base node that the forecast is built
            upon). Therefore, plain deserialization via this method is **not**
            supported.  Instead, use :py:meth:`from_dict_with_context` on the
            appropriate concrete subclass where you can also supply the required
            *context* mapping.

        Args:
            data: Serialized node representation.

        Raises:
            NotImplementedError: Always – plain deserialization without context is
                not supported for forecast nodes.
        """
        raise NotImplementedError(
            "ForecastNode deserialization requires node context. "
            "Use the `from_dict_with_context` method of the concrete subclass "
            "and provide a mapping of existing nodes."
        )


class FixedGrowthForecastNode(ForecastNode):
    """A forecast node that applies a constant growth rate to all forecast periods.

    This node projects future values by applying the same growth rate to each period.
    It's the simplest forecasting method and is useful when you expect consistent
    growth patterns.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_rate (float): The constant growth rate to apply (e.g. 0.05 for 5% growth)

    Examples:
        # Create 5% growth forecast for revenue
        forecast = FixedGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            0.05
        )

        # Get forecasted value
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1.05)^2
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_rate: Optional[float] = None,
    ):
        """Initialize FixedGrowthForecastNode with a constant growth rate.

        Args:
            input_node: Node containing historical values to base the forecast on.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_rate: Fixed growth rate (e.g., 0.05 for 5% growth).
                        If None, uses config.forecasting.default_growth_rate.
        """
        super().__init__(input_node, base_period, forecast_periods)

        # Use config default if not provided (import inside to avoid circular import)
        if growth_rate is None:
            from fin_statement_model.config.helpers import cfg

            growth_rate = cfg("forecasting.default_growth_rate")

        self.growth_rate = float(growth_rate)  # Ensure it's a float
        logger.debug(
            f"Created FixedGrowthForecastNode with growth rate: {self.growth_rate}"
        )

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        logger.debug(
            f"FixedGrowthForecastNode: Using growth rate {self.growth_rate} for period {period}"
        )
        return self.growth_rate

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "simple",
                "growth_params": self.growth_rate,
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "FixedGrowthForecastNode":
        """Create a FixedGrowthForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new FixedGrowthForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in FixedGrowthForecastNode data")

        base_node_name = data.get("base_node_name")
        if not base_node_name:
            raise ValueError(
                "Missing 'base_node_name' field in FixedGrowthForecastNode data"
            )

        if base_node_name not in context:
            raise ValueError(f"Base node '{base_node_name}' not found in context")

        base_node = context[base_node_name]
        base_period = data.get("base_period")
        forecast_periods = data.get("forecast_periods", [])
        growth_params = data.get("growth_params")

        if not base_period:
            raise ValueError(
                "Missing 'base_period' field in FixedGrowthForecastNode data"
            )

        node = FixedGrowthForecastNode(
            input_node=base_node,
            base_period=base_period,
            forecast_periods=forecast_periods,
            growth_rate=growth_params,
        )

        # Set the correct name from the serialized data
        node.name = name
        return node


class CurveGrowthForecastNode(ForecastNode):
    """A forecast node that applies different growth rates to different forecast periods.

    This node allows for more sophisticated forecasting by specifying different growth
    rates for each forecast period. This is useful when you expect growth to change
    over time (e.g., declining growth rates as a company matures).

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_rates (List[float]): List of growth rates, one for each forecast period

    Examples:
        # Create declining growth forecast for revenue
        forecast = CurveGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            [0.10, 0.08, 0.05]  # 10%, 8%, 5% growth
        )

        # Get forecasted value
        fy2025_revenue = forecast.calculate("FY2025")
        # Returns: base_value * 1.10 * 1.08 * 1.05
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_rates: list[float],
    ):
        """Initialize CurveGrowthForecastNode with variable growth rates per period.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_rates: List of growth rates matching each forecast period.
        """
        super().__init__(input_node, base_period, forecast_periods)
        if len(growth_rates) != len(forecast_periods):
            raise ValueError("Number of growth rates must match forecast periods.")
        self.growth_rates = [
            float(rate) for rate in growth_rates
        ]  # Ensure all are floats
        logger.debug(
            f"Created CurveGrowthForecastNode with growth rates: {self.growth_rates}"
        )
        logger.debug(f"  Base period: {base_period}")
        logger.debug(f"  Forecast periods: {forecast_periods}")
        logger.debug(f"  Base value: {input_node.calculate(base_period)}")

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Get the growth factor for a specific period."""
        idx = self.forecast_periods.index(period)
        growth_rate = self.growth_rates[idx]
        logger.debug(
            f"CurveGrowthForecastNode: Using growth rate {growth_rate} for period {period}"
        )
        logger.debug(f"  Previous period: {prev_period}")
        logger.debug(f"  Previous value: {prev_value}")
        return growth_rate

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "curve",
                "growth_params": self.growth_rates.copy(),
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "CurveGrowthForecastNode":
        """Create a CurveGrowthForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new CurveGrowthForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in CurveGrowthForecastNode data")

        base_node_name = data.get("base_node_name")
        if not base_node_name:
            raise ValueError(
                "Missing 'base_node_name' field in CurveGrowthForecastNode data"
            )

        if base_node_name not in context:
            raise ValueError(f"Base node '{base_node_name}' not found in context")

        base_node = context[base_node_name]
        base_period = data.get("base_period")
        forecast_periods = data.get("forecast_periods", [])
        growth_params = data.get("growth_params", [])

        if not base_period:
            raise ValueError(
                "Missing 'base_period' field in CurveGrowthForecastNode data"
            )

        if not isinstance(growth_params, list):
            raise TypeError(
                "'growth_params' must be a list for CurveGrowthForecastNode"
            )

        node = CurveGrowthForecastNode(
            input_node=base_node,
            base_period=base_period,
            forecast_periods=forecast_periods,
            growth_rates=growth_params,
        )

        # Set the correct name from the serialized data
        node.name = name
        return node


class StatisticalGrowthForecastNode(ForecastNode):
    """A forecast node that generates growth rates from a statistical distribution.

    This node uses a provided statistical distribution function to randomly generate
    growth rates for each forecast period. This is useful for modeling uncertainty
    and running Monte Carlo simulations of different growth scenarios.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        distribution_callable (Callable[[], float]): Function that returns random growth rates
                                                   from a statistical distribution

    Examples:
        # Create node with normally distributed growth rates
        from numpy.random import normal
        forecast = StatisticalGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            lambda: normal(0.05, 0.02)  # Mean 5% growth, 2% std dev
        )

        # Get forecasted value (will vary due to randomness)
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1 + r1) * (1 + r2) where r1,r2 are random
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        distribution_callable: Callable[[], float],
    ):
        """Initialize StatisticalGrowthForecastNode with a distribution function.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            distribution_callable: Function that returns a random growth rate.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.distribution_callable = distribution_callable

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.distribution_callable()

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.

        Note:
            The distribution_callable cannot be serialized, so a warning is included.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "statistical",
                "serialization_warning": (
                    "StatisticalGrowthForecastNode uses a distribution callable which cannot be serialized. "
                    "Manual reconstruction required."
                ),
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "StatisticalGrowthForecastNode":
        """Create a StatisticalGrowthForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new StatisticalGrowthForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: StatisticalGrowthForecastNode cannot be fully deserialized
                because the distribution_callable cannot be serialized.
        """
        raise NotImplementedError(
            "StatisticalGrowthForecastNode cannot be fully deserialized because the "
            "distribution_callable cannot be serialized. Manual reconstruction required."
        )


class CustomGrowthForecastNode(ForecastNode):
    """A forecast node that uses a custom function to calculate growth rates.

    This node allows for completely custom growth logic by providing a function that
    calculates the growth rate based on the current period, previous period, and
    previous value. This is the most flexible forecasting option.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_function (Callable): Function that calculates growth rate given
                                  (period, prev_period, prev_value) -> growth_rate

    Examples:
        # Create custom growth logic
        def custom_growth(period, prev_period, prev_value):
            # Declining growth based on company size
            if prev_value < 1000:
                return 0.15  # 15% for small companies
            elif prev_value < 5000:
                return 0.10  # 10% for medium companies
            else:
                return 0.05  # 5% for large companies

        forecast = CustomGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            custom_growth
        )
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_function: Callable[[str, str, float], float],
    ):
        """Initialize CustomGrowthForecastNode with a custom growth function.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_function: Callable(period, prev_period, prev_value) -> growth rate.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.growth_function = growth_function

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.growth_function(period, prev_period, prev_value)

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.

        Note:
            The growth_function cannot be serialized, so a warning is included.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "custom",
                "serialization_warning": (
                    "CustomGrowthForecastNode uses a growth function which cannot be serialized. "
                    "Manual reconstruction required."
                ),
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "CustomGrowthForecastNode":
        """Create a CustomGrowthForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new CustomGrowthForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: CustomGrowthForecastNode cannot be fully deserialized
                because the growth_function cannot be serialized.
        """
        raise NotImplementedError(
            "CustomGrowthForecastNode cannot be fully deserialized because the "
            "growth_function cannot be serialized. Manual reconstruction required."
        )


class AverageValueForecastNode(ForecastNode):
    """A forecast node that uses the average of historical values for all forecast periods.

    This node calculates the average of historical values and returns that constant value
    for all forecast periods. It's useful when you want to project future values based
    on the historical average, without any growth.
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
    ):
        """Initialize AverageValueForecastNode by computing historical average.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.

        """
        super().__init__(input_node, base_period, forecast_periods)
        self.average_value = self._calculate_average_value()
        logger.debug(
            f"Created AverageValueForecastNode with average value: {self.average_value}"
        )

    def _calculate_average_value(self) -> float:
        """Calculate the average historical value up to the base period.

        Returns:
            float: The average of historical values or 0.0 if none.
        """
        values = [
            value for period, value in self.values.items() if period <= self.base_period
        ]
        if not values:
            logger.warning(
                f"No historical values found for {self.name}, using 0.0 as average"
            )
            return 0.0
        # Compute average and ensure float type
        return float(sum(values)) / len(values)

    def _calculate_value(self, period: str) -> float:
        """Calculate the value for a specific period using the computed average value."""
        # For historical periods, return the actual value
        if period <= self.base_period:
            # Return historical value, ensuring float type
            return float(self.values.get(period, 0.0))

        # For forecast periods, return the constant average value
        if period not in self.forecast_periods:
            raise ValueError(
                f"Period '{period}' not in forecast periods for {self.name}"
            )

        return self.average_value

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Not used for average value forecasts."""
        return 0.0

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "average",
                "average_value": self.average_value,
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "AverageValueForecastNode":
        """Create an AverageValueForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new AverageValueForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in AverageValueForecastNode data")

        base_node_name = data.get("base_node_name")
        if not base_node_name:
            raise ValueError(
                "Missing 'base_node_name' field in AverageValueForecastNode data"
            )

        if base_node_name not in context:
            raise ValueError(f"Base node '{base_node_name}' not found in context")

        base_node = context[base_node_name]
        base_period = data.get("base_period")
        forecast_periods = data.get("forecast_periods", [])

        if not base_period:
            raise ValueError(
                "Missing 'base_period' field in AverageValueForecastNode data"
            )

        node = AverageValueForecastNode(
            input_node=base_node,
            base_period=base_period,
            forecast_periods=forecast_periods,
        )

        # Set the correct name from the serialized data
        node.name = name
        return node


class AverageHistoricalGrowthForecastNode(ForecastNode):
    """A forecast node that uses the average historical growth rate for forecasting.

    This node calculates the average growth rate from historical values and applies
    that same growth rate consistently to all forecast periods. It's useful when you
    want to project future values based on the historical growth pattern.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
    ):
        """Initialize AverageHistoricalGrowthForecastNode by computing average growth.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.avg_growth_rate = self._calculate_average_growth_rate()
        logger.debug(
            f"Created AverageHistoricalGrowthForecastNode with growth rate: {self.avg_growth_rate}"
        )

    def _calculate_average_growth_rate(self) -> float:
        """Calculate the average growth rate from historical values.

        Returns:
            float: The average growth rate or 0.0 if insufficient data.
        """
        # Get historical periods up to base_period, sorted
        historical_periods = sorted([p for p in self.values if p <= self.base_period])

        if len(historical_periods) < 2:
            logger.warning(
                f"Insufficient historical data for {self.name}, using 0.0 as growth rate"
            )
            return 0.0

        # Calculate growth rates between consecutive periods
        growth_rates = []
        for i in range(1, len(historical_periods)):
            prev_period = historical_periods[i - 1]
            curr_period = historical_periods[i]
            prev_value = self.values.get(prev_period, 0.0)
            curr_value = self.values.get(curr_period, 0.0)

            if prev_value != 0:
                growth_rate = (curr_value - prev_value) / prev_value
                growth_rates.append(growth_rate)

        if not growth_rates:
            logger.warning(
                f"No valid growth rates calculated for {self.name}, using 0.0"
            )
            return 0.0

        # Compute average growth rate and ensure float type
        return float(sum(growth_rates)) / len(growth_rates)

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.avg_growth_rate

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "historical_growth",
                "avg_growth_rate": self.avg_growth_rate,
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "AverageHistoricalGrowthForecastNode":
        """Create an AverageHistoricalGrowthForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new AverageHistoricalGrowthForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        name = data.get("name")
        if not name:
            raise ValueError(
                "Missing 'name' field in AverageHistoricalGrowthForecastNode data"
            )

        base_node_name = data.get("base_node_name")
        if not base_node_name:
            raise ValueError(
                "Missing 'base_node_name' field in AverageHistoricalGrowthForecastNode data"
            )

        if base_node_name not in context:
            raise ValueError(f"Base node '{base_node_name}' not found in context")

        base_node = context[base_node_name]
        base_period = data.get("base_period")
        forecast_periods = data.get("forecast_periods", [])

        if not base_period:
            raise ValueError(
                "Missing 'base_period' field in AverageHistoricalGrowthForecastNode data"
            )

        node = AverageHistoricalGrowthForecastNode(
            input_node=base_node,
            base_period=base_period,
            forecast_periods=forecast_periods,
        )

        # Set the correct name from the serialized data
        node.name = name
        return node



================================================================================
File: fin_statement_model/core/nodes/item_node.py
================================================================================

"""Define a node representing a basic financial statement item."""

import logging
from typing import Any

# Use absolute imports
from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class FinancialStatementItemNode(Node):
    """Define a leaf node containing raw financial statement data.

    This node type typically stores actual reported values (e.g., Revenue,
    COGS) for different time periods.

    Attributes:
        name (str): The unique identifier for the financial item (e.g., "Revenue").
        values (Dict[str, float]): A dictionary mapping time periods (str)
            to their corresponding numerical values (float).

    Examples:
        >>> revenue_data = {"2022": 1000.0, "2023": 1200.0}
        >>> revenue_node = FinancialStatementItemNode("Revenue", revenue_data)
        >>> print(revenue_node.name)
        Revenue
        >>> print(revenue_node.get_value("2023"))
        1200.0
        >>> print(revenue_node.calculate("2022")) # Calculate retrieves the value
        1000.0
        >>> revenue_node.set_value("2024", 1500.0)
        >>> print(revenue_node.get_value("2024"))
        1500.0
        >>> print(revenue_node.has_value("2021"))
        False
    """

    values: dict[str, float]

    def __init__(self, name: str, values: dict[str, float]):
        """Initialize the financial statement item node.

        Args:
            name (str): The name of the financial statement item.
            values (Dict[str, float]): Dictionary of period-value pairs.
        """
        super().__init__(name)
        self.values = values

    def calculate(self, period: str) -> float:
        """Retrieve the value for the specified period.

        For this node type, calculation simply means retrieving the stored value.

        Args:
            period (str): The time period for which to retrieve the value.

        Returns:
            float: The value for the given period, or 0.0 if the period is not found.
        """
        return self.get_value(period)

    def set_value(self, period: str, value: float) -> None:
        """Update or add a value for a specific period.

        Modifies the stored data for the given period.

        Args:
            period (str): The time period to set the value for.
            value (float): The numerical value to store for the period.
        """
        self.values[period] = value

    def has_value(self, period: str) -> bool:
        """Check if a value exists for the specified period.

        Args:
            period (str): The time period to check.

        Returns:
            bool: True if a value is stored for the period, False otherwise.
        """
        return period in self.values

    def get_value(self, period: str) -> float:
        """Retrieve the stored value for a specific period.

        Args:
            period (str): The time period for which to get the value.

        Returns:
            float: The stored value, defaulting to 0.0 if the period is not found.
        """
        return self.values.get(period, 0.0)

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's type, name, and values.

        Examples:
            >>> node = FinancialStatementItemNode("Revenue", {"2023": 1000.0})
            >>> data = node.to_dict()
            >>> data['type']
            'financial_statement_item'
            >>> data['name']
            'Revenue'
        """
        return {
            "type": "financial_statement_item",
            "name": self.name,
            "values": self.values.copy(),
        }

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "FinancialStatementItemNode":
        """Create a FinancialStatementItemNode from a dictionary representation.

        Args:
            data: Dictionary containing the node's serialized data.
                Must include 'type', 'name', and 'values' fields.

        Returns:
            A new FinancialStatementItemNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.

        Examples:
            >>> data = {
            ...     'type': 'financial_statement_item',
            ...     'name': 'Revenue',
            ...     'values': {'2023': 1000.0}
            ... }
            >>> node = FinancialStatementItemNode.from_dict(data)
            >>> node.name
            'Revenue'
        """
        if data.get("type") != "financial_statement_item":
            raise ValueError(
                f"Invalid type for FinancialStatementItemNode: {data.get('type')}"
            )

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in FinancialStatementItemNode data")

        values = data.get("values", {})
        if not isinstance(values, dict):
            raise TypeError("'values' field must be a dictionary")

        return FinancialStatementItemNode(name, values)



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/__init__.py
================================================================================

"""Standard Node Definitions Package.

This package contains organized standard node definitions split into logical categories.
All definitions are automatically loaded into the standard_node_registry.
"""

import logging
from pathlib import Path
from typing import Optional

from fin_statement_model.core.nodes.standard_registry import standard_node_registry

logger = logging.getLogger(__name__)


def load_all_standard_nodes(base_path: Optional[Path] = None) -> int:
    """Load all standard node definitions from organized YAML files.

    Args:
        base_path: Base path to the standard_nodes_defn directory. If None, uses default.

    Returns:
        Total number of nodes loaded.
    """
    if base_path is None:
        base_path = Path(__file__).parent

    total_loaded = 0

    # Define the organized node files to load
    node_files = [
        # Balance sheet nodes
        "balance_sheet/assets.yaml",
        "balance_sheet/liabilities.yaml",
        "balance_sheet/equity.yaml",
        # Income statement nodes
        "income_statement/revenue_costs.yaml",
        "income_statement/operating.yaml",
        "income_statement/non_operating.yaml",
        "income_statement/shares.yaml",
        # Cash flow nodes
        "cash_flow/operating.yaml",
        "cash_flow/investing.yaml",
        "cash_flow/financing.yaml",
        # Calculated items
        "calculated/profitability.yaml",
        "calculated/liquidity.yaml",
        "calculated/leverage.yaml",
        "calculated/valuation.yaml",
        # Market data
        "market_data/market_data.yaml",
        # Real estate nodes
        "real_estate/property_operations.yaml",
        "real_estate/reit_specific.yaml",
        "real_estate/debt_financing.yaml",
        # Banking nodes
        "banking/assets.yaml",
        "banking/liabilities.yaml",
        "banking/income_statement.yaml",
        "banking/regulatory_capital.yaml",
        "banking/off_balance_sheet.yaml",
    ]

    for file_path in node_files:
        full_path = base_path / file_path
        if full_path.exists():
            try:
                count = standard_node_registry.load_from_yaml_file(full_path)
                total_loaded += count
                logger.debug(f"Loaded {count} nodes from {file_path}")
            except Exception:
                logger.exception(f"Failed to load {file_path}")
        else:
            logger.warning(f"Organized node file not found: {full_path}")

    logger.info(f"Loaded {total_loaded} total standard nodes from organized structure")
    return total_loaded


# Auto-load on import
try:
    load_all_standard_nodes()
except Exception as e:
    logger.warning(f"Failed to auto-load standard nodes: {e}")



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/balance_sheet/__init__.py
================================================================================

"""Balance Sheet Standard Node Definitions."""

# This package contains balance sheet node definitions split into:
# - assets.yaml: Current and non-current assets
# - liabilities.yaml: Current and non-current liabilities
# - equity.yaml: Equity components



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/banking/__init__.py
================================================================================

"""Banking Standard Node Definitions."""

# This package contains banking node definitions split into:
# - assets.yaml: Loans, securities, cash, and other banking assets
# - liabilities.yaml: Deposits, borrowings, and other banking liabilities
# - income_statement.yaml: Interest income/expense, fees, provisions
# - regulatory_capital.yaml: Tier 1/2 capital, risk-weighted assets
# - off_balance_sheet.yaml: Commitments, guarantees, derivatives



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/calculated/__init__.py
================================================================================

"""Calculated Standard Node Definitions."""

# This package contains calculated node definitions split into:
# - profitability.yaml: EBITDA, NOPAT, etc.
# - liquidity.yaml: Working capital measures
# - leverage.yaml: Net debt, leverage measures
# - valuation.yaml: Enterprise value, etc.



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/cash_flow/__init__.py
================================================================================

"""Cash Flow Statement Standard Node Definitions."""

# This package contains cash flow statement node definitions split into:
# - operating.yaml: Operating activities
# - investing.yaml: Investing activities
# - financing.yaml: Financing activities



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/income_statement/__init__.py
================================================================================

"""Income Statement Standard Node Definitions."""

# This package contains income statement node definitions split into:
# - revenue_costs.yaml: Revenue and direct costs
# - operating.yaml: Operating expenses and income
# - non_operating.yaml: Interest, other income, taxes
# - shares.yaml: Share-related items



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/market_data/__init__.py
================================================================================

"""Market Data Standard Node Definitions."""

# This package contains market data node definitions:
# - market_data.yaml: Market prices and per-share data



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/real_estate/__init__.py
================================================================================

"""Real Estate Standard Node Definitions."""

# This package contains real estate node definitions split into:
# - property_operations.yaml: Property income, expenses, and operational metrics
# - reit_specific.yaml: REIT-specific items like FFO, AFFO, and adjustments
# - property_metrics.yaml: Property-level metrics and measurements
# - debt_financing.yaml: Debt, financing, and loan-related nodes



================================================================================
File: fin_statement_model/core/nodes/standard_registry.py
================================================================================

"""Registry for standard node names.

This module provides a registry system for standardized node names,
ensuring consistency across financial models and enabling metrics to work properly.
"""

import logging
from pathlib import Path
from typing import Any, Optional
import yaml
from pydantic import BaseModel, ConfigDict

logger = logging.getLogger(__name__)


class StandardNodeDefinition(BaseModel):
    """Define a standard node with metadata.

    Attributes:
        category: The main category (e.g., balance_sheet_assets, income_statement)
        subcategory: The subcategory within the main category
        description: Human-readable description of the node
        alternate_names: List of alternate names that should map to this standard name
        sign_convention: Whether values are typically positive or negative
    """

    model_config = ConfigDict(str_strip_whitespace=True)

    category: str
    subcategory: str
    description: str
    alternate_names: list[str] = []
    sign_convention: str = "positive"  # 'positive' or 'negative'


class StandardNodeRegistry:
    """Registry for standard node definitions.

    This class manages loading and accessing standardized node definitions,
    providing validation and alternate name resolution capabilities.

    Attributes:
        _standard_nodes: Dict mapping standard node names to their definitions
        _alternate_to_standard: Dict mapping alternate names to standard names
    """

    def __init__(self) -> None:
        """Initialize an empty registry."""
        self._standard_nodes: dict[str, StandardNodeDefinition] = {}
        self._alternate_to_standard: dict[str, str] = {}
        self._categories: set[str] = set()
        self._initialized: bool = False  # Track initialization state
        self._loaded_from: Optional[str] = None  # Track source

    def _load_nodes_from_data(
        self,
        data: dict[str, Any],
        source_description: str,
        overwrite_existing: bool = False,
    ) -> int:
        """Process node definitions from parsed YAML data.

        Args:
            data: The parsed YAML data containing node definitions
            source_description: Description of the data source for logging
            overwrite_existing: Whether to overwrite existing node definitions

        Returns:
            Number of nodes successfully loaded

        Raises:
            ValueError: If node definitions are invalid or contain duplicates
        """
        if not isinstance(data, dict):
            raise TypeError(
                f"Expected dict at root of {source_description}, got {type(data)}"
            )

        nodes_loaded = 0

        # Load each node definition
        for node_name, node_data in data.items():
            if not isinstance(node_data, dict):
                logger.warning(
                    f"Skipping invalid node definition '{node_name}' "
                    f"in {source_description}: not a dict"
                )
                continue

            try:
                definition = StandardNodeDefinition(**node_data)

                # Handle duplicate standard names
                if node_name in self._standard_nodes:
                    if not overwrite_existing:
                        raise ValueError(f"Duplicate standard node name: {node_name}")
                    else:
                        logger.debug(
                            f"Overwriting existing standard node: {node_name} "
                            f"from {source_description}"
                        )

                # Add to main registry
                self._standard_nodes[node_name] = definition
                self._categories.add(definition.category)

                # Map alternate names
                for alt_name in definition.alternate_names:
                    if alt_name in self._alternate_to_standard:
                        existing_standard = self._alternate_to_standard[alt_name]
                        if not overwrite_existing and existing_standard != node_name:
                            raise ValueError(
                                f"Alternate name '{alt_name}' already maps to "
                                f"'{existing_standard}', cannot also map to '{node_name}'"
                            )
                        elif overwrite_existing and existing_standard != node_name:
                            logger.debug(
                                f"Alternate name '{alt_name}' already maps to "
                                f"'{existing_standard}', now mapping to '{node_name}'"
                            )
                    self._alternate_to_standard[alt_name] = node_name

                nodes_loaded += 1

            except Exception as e:
                logger.exception(
                    f"Error loading node '{node_name}' from {source_description}"
                )
                raise ValueError(
                    f"Invalid node definition for '{node_name}': {e}"
                ) from e

        return nodes_loaded

    def get_standard_name(self, name: str) -> str:
        """Get the standard name for a given node name.

        If the name is already standard, returns it unchanged.
        If it's an alternate name, returns the corresponding standard name.
        If it's not recognized, returns the original name.

        Args:
            name: The node name to standardize.

        Returns:
            The standardized node name.
        """
        # Check if it's already a standard name
        if name in self._standard_nodes:
            return name

        # Check if it's an alternate name
        if name in self._alternate_to_standard:
            return self._alternate_to_standard[name]

        # Not recognized, return as-is
        return name

    def is_standard_name(self, name: str) -> bool:
        """Check if a name is a recognized standard node name.

        Args:
            name: The node name to check.

        Returns:
            True if the name is a standard node name, False otherwise.
        """
        return name in self._standard_nodes

    def is_alternate_name(self, name: str) -> bool:
        """Check if a name is a recognized alternate name.

        Args:
            name: The node name to check.

        Returns:
            True if the name is an alternate name, False otherwise.
        """
        return name in self._alternate_to_standard

    def is_recognized_name(self, name: str) -> bool:
        """Check if a name is either standard or alternate.

        Args:
            name: The node name to check.

        Returns:
            True if the name is recognized, False otherwise.
        """
        return self.is_standard_name(name) or self.is_alternate_name(name)

    def get_definition(self, name: str) -> Optional[StandardNodeDefinition]:
        """Get the definition for a node name.

        Works with both standard and alternate names.

        Args:
            name: The node name to look up.

        Returns:
            The node definition if found, None otherwise.
        """
        standard_name = self.get_standard_name(name)
        return self._standard_nodes.get(standard_name)

    def list_standard_names(self, category: Optional[str] = None) -> list[str]:
        """List all standard node names, optionally filtered by category.

        Args:
            category: Optional category to filter by.

        Returns:
            Sorted list of standard node names.
        """
        if category:
            names = [
                name
                for name, defn in self._standard_nodes.items()
                if defn.category == category
            ]
        else:
            names = list(self._standard_nodes.keys())

        return sorted(names)

    def list_categories(self) -> list[str]:
        """List all available categories.

        Returns:
            Sorted list of categories.
        """
        return sorted(self._categories)

    def validate_node_name(self, name: str, strict: bool = False) -> tuple[bool, str]:
        """Validate a node name against standards.

        Args:
            name: The node name to validate.
            strict: If True, only standard names are valid.
                   If False, alternate names are also valid.

        Returns:
            Tuple of (is_valid, message).
        """
        if strict:
            if self.is_standard_name(name):
                return True, f"'{name}' is a standard node name"
            elif self.is_alternate_name(name):
                standard = self.get_standard_name(name)
                return (
                    False,
                    f"'{name}' is an alternate name. Use standard name '{standard}'",
                )
            else:
                return False, f"'{name}' is not a recognized node name"
        elif self.is_recognized_name(name):
            if self.is_alternate_name(name):
                standard = self.get_standard_name(name)
                return True, f"'{name}' is valid (alternate for '{standard}')"
            else:
                return True, f"'{name}' is a standard node name"
        else:
            return False, f"'{name}' is not a recognized node name"

    def get_sign_convention(self, name: str) -> Optional[str]:
        """Get the sign convention for a node.

        Args:
            name: The node name (standard or alternate).

        Returns:
            The sign convention ('positive' or 'negative') if found, None otherwise.
        """
        definition = self.get_definition(name)
        return definition.sign_convention if definition else None

    def initialize_default_nodes(
        self,
        organized_path: Optional[Path] = None,
        force_reload: bool = False,
    ) -> int:
        """Initialize the registry with default standard nodes from organized structure only.

        Args:
            organized_path: Base directory to load organized YAML files from.
                Defaults to standard_nodes_defn/ relative to this module.
            force_reload: If True, reload even if already initialized.

        Returns:
            Number of nodes loaded.
        """
        if self._initialized and not force_reload:
            logger.debug(
                f"Standard nodes already initialized from {self._loaded_from}, skipping reload."
            )
            return len(self._standard_nodes)

        if organized_path is None:
            organized_path = Path(__file__).parent / "standard_nodes_defn"

        count = self._load_from_organized_structure(organized_path)
        self._initialized = True
        self._loaded_from = f"organized structure at {organized_path}"
        logger.info(
            f"Successfully loaded {count} standard nodes from organized structure"
        )
        return count

    def _load_from_organized_structure(self, base_path: Path) -> int:
        """Load nodes from organized directory structure.

        Args:
            base_path: Base directory containing organized YAML files.

        Returns:
            Number of nodes loaded.

        Raises:
            Exception: If loading fails.
        """
        if not base_path.exists():
            raise FileNotFoundError(f"Organized structure path not found: {base_path}")

        # Check if it has the expected structure (contains __init__.py)
        init_file = base_path / "__init__.py"
        if not init_file.exists():
            raise ValueError(
                f"Invalid organized structure: missing __init__.py in {base_path}"
            )

        # Import and use the load_all_standard_nodes function
        import importlib.util

        spec = importlib.util.spec_from_file_location("standard_nodes_defn", init_file)
        if spec is None or spec.loader is None:
            raise ImportError(f"Failed to load module from {init_file}")

        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)

        # Clear existing data before loading
        self._standard_nodes.clear()
        self._alternate_to_standard.clear()
        self._categories.clear()

        # Call the loading function
        if hasattr(module, "load_all_standard_nodes"):
            loaded_count = module.load_all_standard_nodes(base_path)
            return int(loaded_count)
        else:
            raise AttributeError(
                f"Module at {init_file} missing load_all_standard_nodes function"
            )

    def is_initialized(self) -> bool:
        """Check if the registry has been initialized with default nodes."""
        return self._initialized

    def get_load_source(self) -> Optional[str]:
        """Get the source from which nodes were loaded."""
        return self._loaded_from

    def reload(self) -> int:
        """Force reload of standard nodes."""
        return self.initialize_default_nodes(force_reload=True)

    def __len__(self) -> int:
        """Return the number of standard nodes in the registry."""
        return len(self._standard_nodes)

    def load_from_yaml_file(self, yaml_path: Path) -> int:
        """Load standard node definitions from a single YAML file without clearing existing data.

        Args:
            yaml_path: Path to the YAML file containing node definitions.

        Returns:
            Number of nodes loaded from this file.

        Raises:
            ValueError: If the YAML file has invalid structure or duplicate names.
            FileNotFoundError: If the YAML file doesn't exist.
        """
        if not yaml_path.exists():
            raise FileNotFoundError(f"Standard nodes file not found: {yaml_path}")

        try:
            with open(yaml_path, encoding="utf-8") as f:
                data = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML in {yaml_path}: {e}") from e

        # Process the data without clearing existing
        nodes_loaded = self._load_nodes_from_data(
            data, str(yaml_path), overwrite_existing=True
        )
        if nodes_loaded:
            self._initialized = True
            self._loaded_from = str(yaml_path)
        logger.debug(f"Loaded {nodes_loaded} nodes from {yaml_path}")
        return nodes_loaded


# Global registry instance
standard_node_registry = StandardNodeRegistry()


# NOTE: Auto-loading is disabled to prevent conflicts with organized structure
# The nodes/__init__.py will handle loading from the appropriate source



================================================================================
File: fin_statement_model/core/nodes/stats_nodes.py
================================================================================

"""Provide nodes for statistical calculations on financial data across periods.

This module provides nodes for common time-series statistical analyses:
- `YoYGrowthNode`: Calculates year-over-year percentage growth.
- `MultiPeriodStatNode`: Computes statistics (mean, stddev, etc.) over a range of periods.
- `TwoPeriodAverageNode`: Calculates the simple average over two specific periods.
"""

import logging
import math
import statistics

# Use lowercase built-in types for annotations
from typing import Optional, Union, Any
from typing import Callable

# Use absolute imports
from fin_statement_model.core.nodes.base import Node
from fin_statement_model.core.errors import CalculationError

# Added logger instance
logger = logging.getLogger(__name__)

Numeric = Union[int, float]
StatFunc = Callable[
    ..., Any
]  # Widen callable type to accept any callable returning Numeric


class YoYGrowthNode(Node):
    """Calculate year-over-year (YoY) percentage growth.

    Compares the value of an input node between two specified periods
    (prior and current) and calculates the relative change.

    Growth = (Current Value - Prior Value) / Prior Value

    Attributes:
        name (str): The node's identifier.
        input_node (Node): The node providing the values for comparison.
        prior_period (str): Identifier for the earlier time period.
        current_period (str): Identifier for the later time period.

    Examples:
        >>> # Assume revenue_node holds {"2022": 100, "2023": 120}
        >>> revenue_node = FinancialStatementItemNode("revenue", {"2022": 100.0, "2023": 120.0})
        >>> yoy_growth = YoYGrowthNode(
        ...     "revenue_yoy",
        ...     input_node=revenue_node,
        ...     prior_period="2022",
        ...     current_period="2023"
        ... )
        >>> print(yoy_growth.calculate("any_period")) # Period arg is ignored
        0.2
    """

    def __init__(
        self, name: str, input_node: Node, prior_period: str, current_period: str
    ):
        """Initialize the YoY Growth node.

        Args:
            name (str): The identifier for this growth node.
            input_node (Node): The node whose values will be compared.
            prior_period (str): The identifier for the earlier period.
            current_period (str): The identifier for the later period.

        Raises:
            TypeError: If `input_node` is not a Node instance or periods are not strings.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError("YoYGrowthNode input_node must be a Node instance.")
        if not isinstance(prior_period, str) or not isinstance(current_period, str):
            raise TypeError(
                "YoYGrowthNode prior_period and current_period must be strings."
            )

        self.input_node = input_node
        self.prior_period = prior_period
        self.current_period = current_period

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the year-over-year growth rate.

        Retrieves values for the prior and current periods from the input node
        and computes the percentage growth. The `period` argument is ignored
        as the calculation periods are fixed during initialization.

        Args:
            period (Optional[str]): Ignored. The calculation uses the periods
                defined during initialization.

        Returns:
            float: The calculated growth rate (e.g., 0.2 for 20% growth).
                   Returns `float('nan')` if the prior period value is zero
                   or non-finite prior value.

        Raises:
            CalculationError: If the input node fails to provide numeric values
                for the required periods.
        """
        try:
            prior_value = self.input_node.calculate(self.prior_period)
            current_value = self.input_node.calculate(self.current_period)

            # Validate input types
            if not isinstance(prior_value, int | float):
                raise TypeError(
                    f"Prior period ('{self.prior_period}') value is non-numeric."
                )
            if not isinstance(current_value, int | float):
                raise TypeError(
                    f"Current period ('{self.current_period}') value is non-numeric."
                )

            # Handle division by zero or non-finite prior value
            if prior_value == 0 or not math.isfinite(prior_value):
                logger.warning(
                    f"YoYGrowthNode '{self.name}': Prior period '{self.prior_period}' value is zero or non-finite ({prior_value}). Returning NaN."
                )
                return float("nan")

            # Calculate growth
            growth = (float(current_value) - float(prior_value)) / float(prior_value)
            return growth

        except Exception as e:
            # Wrap any exception during calculation
            raise CalculationError(
                message=f"Failed to calculate YoY growth for node '{self.name}'",
                node_id=self.name,
                period=f"{self.prior_period}_to_{self.current_period}",  # Indicate period span
                details={
                    "input_node": self.input_node.name,
                    "prior_period": self.prior_period,
                    "current_period": self.current_period,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's type, name, and configuration.
        """
        return {
            "type": "yoy_growth",
            "name": self.name,
            "input_node_name": self.input_node.name,
            "prior_period": self.prior_period,
            "current_period": self.current_period,
        }

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "YoYGrowthNode":
        """Create a YoYGrowthNode from a dictionary representation.

        Args:
            data: Dictionary containing the node's serialized data.

        Returns:
            A new YoYGrowthNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: This method requires context (existing nodes) to resolve
                input dependencies. Use from_dict_with_context instead.
        """
        raise NotImplementedError(
            "YoYGrowthNode.from_dict() requires context to resolve input dependencies. "
            "Use NodeFactory.create_from_dict() or from_dict_with_context() instead."
        )

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "YoYGrowthNode":
        """Create a YoYGrowthNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new YoYGrowthNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        if data.get("type") != "yoy_growth":
            raise ValueError(f"Invalid type for YoYGrowthNode: {data.get('type')}")

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in YoYGrowthNode data")

        input_node_name = data.get("input_node_name")
        if not input_node_name:
            raise ValueError("Missing 'input_node_name' field in YoYGrowthNode data")

        if input_node_name not in context:
            raise ValueError(f"Input node '{input_node_name}' not found in context")

        input_node = context[input_node_name]
        prior_period = data.get("prior_period")
        current_period = data.get("current_period")

        if not prior_period:
            raise ValueError("Missing 'prior_period' field in YoYGrowthNode data")
        if not current_period:
            raise ValueError("Missing 'current_period' field in YoYGrowthNode data")

        return YoYGrowthNode(
            name=name,
            input_node=input_node,
            prior_period=prior_period,
            current_period=current_period,
        )


class MultiPeriodStatNode(Node):
    """Calculate a statistical measure across multiple periods.

    Applies a specified statistical function (e.g., mean, standard deviation)
    to the values of an input node over a list of periods.

    Attributes:
        name (str): The node's identifier.
        input_node (Node): The node providing the values for analysis.
        periods (List[str]): The list of period identifiers to include.
        stat_func (StatFunc): The statistical function to apply (e.g.,
            `statistics.mean`, `statistics.stdev`). Must accept a sequence
            of numbers and return a single number.

    Examples:
        >>> # Assume sales_node holds {"Q1": 10, "Q2": 12, "Q3": 11, "Q4": 13}
        >>> sales_node = FinancialStatementItemNode("sales", {"Q1": 10, "Q2": 12, "Q3": 11, "Q4": 13})
        >>> mean_sales = MultiPeriodStatNode(
        ...     "avg_quarterly_sales",
        ...     input_node=sales_node,
        ...     periods=["Q1", "Q2", "Q3", "Q4"],
        ...     stat_func=statistics.mean
        ... )
        >>> print(mean_sales.calculate()) # Period arg is ignored
        11.5
        >>> stddev_sales = MultiPeriodStatNode(
        ...     "sales_volatility",
        ...     input_node=sales_node,
        ...     periods=["Q1", "Q2", "Q3", "Q4"],
        ...     stat_func=statistics.stdev # Default
        ... )
        >>> print(round(stddev_sales.calculate(), 2))
        1.29
    """

    def __init__(
        self,
        name: str,
        input_node: Node,
        periods: list[str],
        stat_func: StatFunc = statistics.stdev,  # Default to standard deviation
    ):
        """Initialize the multi-period statistics node.

        Args:
            name (str): The identifier for this statistical node.
            input_node (Node): The node providing the source values.
            periods (List[str]): A list of period identifiers to analyze.
            stat_func (StatFunc): The statistical function to apply. Defaults to
                `statistics.stdev`. It must accept a sequence of numerics and
                return a numeric value.

        Raises:
            ValueError: If `periods` is not a list or is empty.
            TypeError: If `input_node` is not a Node, `periods` contains non-strings,
                or `stat_func` is not callable.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError("MultiPeriodStatNode input_node must be a Node instance.")
        if not isinstance(periods, list) or not periods:
            raise ValueError("MultiPeriodStatNode periods must be a non-empty list.")
        if not all(isinstance(p, str) for p in periods):
            raise TypeError("MultiPeriodStatNode periods must contain only strings.")
        if not callable(stat_func):
            raise TypeError(
                "MultiPeriodStatNode stat_func must be a callable function."
            )

        self.input_node = input_node
        self.periods = periods
        self.stat_func = stat_func

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the statistical measure across the specified periods.

        Retrieves values from the input node for each period in the configured list,
        then applies the `stat_func`. The `period` argument is ignored.

        Args:
            period (Optional[str]): Ignored. Calculation uses the periods defined
                during initialization.

        Returns:
            float: The result of the statistical function. Returns `float('nan')`
                   if the statistical function requires more data points than
                   available (e.g., standard deviation with < 2 values) or if
                   no valid numeric data is found.

        Raises:
            CalculationError: If retrieving input node values fails or if the
                statistical function itself raises an unexpected error.
        """
        values: list[Numeric] = []
        retrieval_errors = []
        try:
            for p in self.periods:
                try:
                    value = self.input_node.calculate(p)
                    if isinstance(value, int | float) and math.isfinite(value):
                        values.append(float(value))
                    else:
                        # Log non-numeric/non-finite values but continue if possible
                        logger.warning(
                            f"MultiPeriodStatNode '{self.name}': Input '{self.input_node.name}' gave non-numeric/non-finite value ({value}) for period '{p}'. Skipping."
                        )
                except Exception as node_err:
                    # Log error fetching data for a specific period but continue
                    logger.error(
                        f"MultiPeriodStatNode '{self.name}': Error getting value for period '{p}' from '{self.input_node.name}': {node_err}",
                        exc_info=True,
                    )
                    retrieval_errors.append(p)

            # If no valid numeric values were collected
            if not values:
                logger.warning(
                    f"MultiPeriodStatNode '{self.name}': No valid numeric data points found across periods {self.periods}. Returning NaN."
                )
                return float("nan")

            # Attempt the statistical calculation
            try:
                result = self.stat_func(values)
                # Ensure result is float, handle potential NaN from stat_func
                return float(result) if math.isfinite(result) else float("nan")
            except (statistics.StatisticsError, ValueError, TypeError) as stat_err:
                # Handle errors specific to statistical functions (e.g., stdev needs >= 2 points)
                logger.warning(
                    f"MultiPeriodStatNode '{self.name}': Stat function '{self.stat_func.__name__}' failed ({stat_err}). Values: {values}. Returning NaN."
                )
                return float("nan")

        except Exception as e:
            # Catch any other unexpected errors during the process
            raise CalculationError(
                message=f"Failed to calculate multi-period stat for node '{self.name}'",
                node_id=self.name,
                period="multi-period",  # Indicate calculation context
                details={
                    "input_node": self.input_node.name,
                    "periods": self.periods,
                    "stat_func": self.stat_func.__name__,
                    "collected_values_count": len(values),
                    "retrieval_errors_periods": retrieval_errors,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's type, name, and configuration.

        Note:
            The stat_func cannot be serialized, so a warning is included.
        """
        return {
            "type": "multi_period_stat",
            "name": self.name,
            "input_node_name": self.input_node.name,
            "periods": self.periods.copy(),
            "stat_func_name": self.stat_func.__name__,
            "serialization_warning": (
                "MultiPeriodStatNode uses a statistical function which may not be fully serializable. "
                "Manual reconstruction may be required for custom functions."
            ),
        }

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "MultiPeriodStatNode":
        """Create a MultiPeriodStatNode from a dictionary representation.

        Args:
            data: Dictionary containing the node's serialized data.

        Returns:
            A new MultiPeriodStatNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: This method requires context (existing nodes) to resolve
                input dependencies. Use from_dict_with_context instead.
        """
        raise NotImplementedError(
            "MultiPeriodStatNode.from_dict() requires context to resolve input dependencies. "
            "Use NodeFactory.create_from_dict() or from_dict_with_context() instead."
        )

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "MultiPeriodStatNode":
        """Create a MultiPeriodStatNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new MultiPeriodStatNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        if data.get("type") != "multi_period_stat":
            raise ValueError(
                f"Invalid type for MultiPeriodStatNode: {data.get('type')}"
            )

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in MultiPeriodStatNode data")

        input_node_name = data.get("input_node_name")
        if not input_node_name:
            raise ValueError(
                "Missing 'input_node_name' field in MultiPeriodStatNode data"
            )

        if input_node_name not in context:
            raise ValueError(f"Input node '{input_node_name}' not found in context")

        input_node = context[input_node_name]
        periods = data.get("periods", [])
        stat_func_name = data.get("stat_func_name", "stdev")

        if not periods:
            raise ValueError(
                "Missing or empty 'periods' field in MultiPeriodStatNode data"
            )

        # Map common statistical function names to their implementations
        stat_func_map: dict[str, StatFunc] = {
            "mean": statistics.mean,
            "stdev": statistics.stdev,
            "median": statistics.median,
            "variance": statistics.variance,
            "pstdev": statistics.pstdev,
            "pvariance": statistics.pvariance,
        }

        stat_func = stat_func_map.get(stat_func_name, statistics.stdev)
        if stat_func_name not in stat_func_map:
            logger.warning(
                f"Unknown stat_func_name '{stat_func_name}' for MultiPeriodStatNode '{name}'. "
                f"Using default statistics.stdev."
            )

        return MultiPeriodStatNode(
            name=name,
            input_node=input_node,
            periods=periods,
            stat_func=stat_func,
        )


class TwoPeriodAverageNode(Node):
    """Compute the simple average of an input node's value over two periods.

    Calculates (Value at Period 1 + Value at Period 2) / 2.

    Attributes:
        name (str): Identifier for this node.
        input_node (Node): Node providing the values to be averaged.
        period1 (str): Identifier for the first period.
        period2 (str): Identifier for the second period.

    Examples:
        >>> # Assume price_node holds {"Jan": 10.0, "Feb": 11.0}
        >>> price_node = FinancialStatementItemNode("price", {"Jan": 10.0, "Feb": 11.0})
        >>> avg_price = TwoPeriodAverageNode(
        ...     "jan_feb_avg_price",
        ...     input_node=price_node,
        ...     period1="Jan",
        ...     period2="Feb"
        ... )
        >>> print(avg_price.calculate()) # Period arg is ignored
        10.5
    """

    def __init__(self, name: str, input_node: Node, period1: str, period2: str):
        """Initialize the two-period average node.

        Args:
            name (str): The identifier for this node.
            input_node (Node): The node providing values.
            period1 (str): The identifier for the first period.
            period2 (str): The identifier for the second period.

        Raises:
            TypeError: If `input_node` is not a Node, or periods are not strings.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError(
                f"TwoPeriodAverageNode input_node must be a Node instance, got {type(input_node).__name__}"
            )
        if not isinstance(period1, str) or not isinstance(period2, str):
            raise TypeError("TwoPeriodAverageNode period1 and period2 must be strings.")

        self.input_node = input_node
        self.period1 = period1
        self.period2 = period2

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the average of the input node for the two fixed periods.

        Ignores the `period` argument, using `period1` and `period2` defined
        during initialization.

        Args:
            period (Optional[str]): Ignored.

        Returns:
            float: The average of the input node's values for `period1` and `period2`.
                   Returns `float('nan')` if either input value is non-numeric.

        Raises:
            CalculationError: If retrieving values from the input node fails.
        """
        try:
            val1 = self.input_node.calculate(self.period1)
            val2 = self.input_node.calculate(self.period2)

            # Ensure values are numeric and finite
            if not isinstance(val1, int | float) or not math.isfinite(val1):
                logger.warning(
                    f"TwoPeriodAverageNode '{self.name}': Value for period '{self.period1}' is non-numeric/non-finite ({val1}). Returning NaN."
                )
                return float("nan")
            if not isinstance(val2, int | float) or not math.isfinite(val2):
                logger.warning(
                    f"TwoPeriodAverageNode '{self.name}': Value for period '{self.period2}' is non-numeric/non-finite ({val2}). Returning NaN."
                )
                return float("nan")

            # Calculate the average
            return (float(val1) + float(val2)) / 2.0

        except Exception as e:
            # Wrap potential errors during input node calculation
            raise CalculationError(
                message=f"Failed to calculate two-period average for node '{self.name}'",
                node_id=self.name,
                period=f"{self.period1}_and_{self.period2}",  # Indicate context
                details={
                    "input_node": self.input_node.name,
                    "period1": self.period1,
                    "period2": self.period2,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's type, name, and configuration.
        """
        return {
            "type": "two_period_average",
            "name": self.name,
            "input_node_name": self.input_node.name,
            "period1": self.period1,
            "period2": self.period2,
        }

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "TwoPeriodAverageNode":
        """Create a TwoPeriodAverageNode from a dictionary representation.

        Args:
            data: Dictionary containing the node's serialized data.

        Returns:
            A new TwoPeriodAverageNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: This method requires context (existing nodes) to resolve
                input dependencies. Use from_dict_with_context instead.
        """
        raise NotImplementedError(
            "TwoPeriodAverageNode.from_dict() requires context to resolve input dependencies. "
            "Use NodeFactory.create_from_dict() or from_dict_with_context() instead."
        )

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "TwoPeriodAverageNode":
        """Create a TwoPeriodAverageNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new TwoPeriodAverageNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        if data.get("type") != "two_period_average":
            raise ValueError(
                f"Invalid type for TwoPeriodAverageNode: {data.get('type')}"
            )

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in TwoPeriodAverageNode data")

        input_node_name = data.get("input_node_name")
        if not input_node_name:
            raise ValueError(
                "Missing 'input_node_name' field in TwoPeriodAverageNode data"
            )

        if input_node_name not in context:
            raise ValueError(f"Input node '{input_node_name}' not found in context")

        input_node = context[input_node_name]
        period1 = data.get("period1")
        period2 = data.get("period2")

        if not period1:
            raise ValueError("Missing 'period1' field in TwoPeriodAverageNode data")
        if not period2:
            raise ValueError("Missing 'period2' field in TwoPeriodAverageNode data")

        return TwoPeriodAverageNode(
            name=name,
            input_node=input_node,
            period1=period1,
            period2=period2,
        )


__all__ = [
    "MultiPeriodStatNode",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]



================================================================================
File: fin_statement_model/extensions/__init__.py
================================================================================

"""Extensions package for fin_statement_model.

This package hosts optional in-repo extensions and third-party plugins discovered via entry-points under
`fin_statement_model.extensions`.
"""



================================================================================
File: fin_statement_model/extensions/llm/__init__.py
================================================================================

"""LLM extension subpackage for fin_statement_model.

Provides built-in OpenAI-based LLM client extension for generating and injecting content.
"""



================================================================================
File: fin_statement_model/extensions/llm/llm_client.py
================================================================================

"""LLM client module for OpenAI and backoff integration.

This module provides `LLMConfig` for client configuration and `LLMClient` for
asynchronous interactions with OpenAI's ChatCompletion API, including retry logic.
"""

import logging
import openai
from openai.api_resources.chat_completion import ChatCompletion
import backoff
from dataclasses import dataclass, field
from typing import Optional, Any, cast
from types import TracebackType

from fin_statement_model.config import cfg
from fin_statement_model.core.errors import FinancialModelError

logger = logging.getLogger(__name__)


@dataclass
class LLMConfig:
    """Configuration data for LLMClient.

    Attributes:
        api_key: API key for OpenAI authentication.
        model_name: Model to use (e.g., 'gpt-4o').
        temperature: Sampling temperature setting.
        max_tokens: Maximum tokens to generate.
        timeout: Request timeout in seconds.
        max_retries: Number of retries on failure.
    """

    api_key: str
    model_name: str = "gpt-4o"
    temperature: float = 0.7
    max_tokens: int = 1500
    timeout: int = field(default_factory=lambda: cfg("api.api_timeout", 30))
    max_retries: int = field(default_factory=lambda: cfg("api.api_retry_count", 3))
    # base_url is no longer needed as the openai library handles the endpoint configuration.


class LLMClientError(FinancialModelError):
    """Base exception for LLM client errors."""


class LLMTimeoutError(LLMClientError):
    """Exception for timeout errors."""


class LLMClient:
    """Asynchronous client for OpenAI ChatCompletion API with retry logic.

    Utilizes `LLMConfig` and supports retries on rate limits and timeouts.

    Methods:
        _make_api_call: Internal method for performing the API call with retry logic.
        get_completion: High-level method to obtain chat completions.
    """

    def __init__(self, config: Optional[LLMConfig] = None):
        """Initialize the async LLM client with configuration."""
        self.config = config or LLMConfig(api_key="")
        openai.api_key = self.config.api_key

    @backoff.on_exception(
        backoff.expo,
        (Exception, LLMTimeoutError),
        max_tries=3,
        giveup=lambda e: isinstance(e, LLMTimeoutError),
    )
    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=8)
    async def _make_api_call(self, messages: list[dict[str, str]]) -> dict[str, Any]:
        """Make the async API call to OpenAI with retry logic.

        Args:
            messages: List of message dicts for the ChatCompletion API

        Returns:
            Dict containing the API response

        Raises:
            LLMClientError: For any client-related errors, including timeout if applicable
        """
        try:
            logger.debug(
                f"Sending async request to OpenAI API with model {self.config.model_name}"
            )
            response = await ChatCompletion.acreate(
                model=self.config.model_name,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                timeout=self.config.timeout,
            )

            if not response.get("choices"):
                logger.error("No suggestions received from OpenAI API")
                raise LLMClientError("No suggestions received from API")

            return cast(dict[str, Any], response)
        except Exception as e:
            if "timeout" in str(e).lower():
                logger.exception("Async request timed out")
                raise LLMTimeoutError("Request timed out") from e
            logger.exception("OpenAI async API request failed")
            raise LLMClientError(f"API request failed: {e!s}") from e

    async def get_completion(self, messages: list[dict[str, str]]) -> str:
        """Get a completion result from the LLM using async API.

        Args:
            messages: List of message dictionaries for the ChatCompletion API

        Returns:
            str: The suggested completion from the LLM

        Raises:
            LLMClientError: For any client-related errors
        """
        try:
            logger.info("Requesting completion from OpenAI async API")
            response = await self._make_api_call(messages)
            completion = cast(str, response["choices"][0]["message"]["content"]).strip()
            logger.info("Successfully received completion")
            return completion
        except Exception as e:
            logger.exception("Error getting completion")
            raise LLMClientError(f"Failed to get completion: {e!s}") from e

    async def __aenter__(self) -> "LLMClient":
        """Enter the asynchronous context manager, returning the client."""
        return self

    async def __aexit__(
        self,
        exc_type: Optional[type[BaseException]],
        exc: Optional[BaseException],
        tb: Optional[TracebackType],
    ) -> None:
        """Exit the asynchronous context manager, performing cleanup."""
        # Ensure method has a body



================================================================================
File: fin_statement_model/forecasting/__init__.py
================================================================================

"""Forecasting sub-module for financial statement models.

This module provides comprehensive forecasting capabilities including:
- Multiple forecast methods (simple, curve, statistical, average, historical growth)
- Mutating and non-mutating forecast operations
- Extensible architecture for custom forecast methods
- Period management and validation utilities

Example:
    >>> from fin_statement_model.forecasting import StatementForecaster
    >>> forecaster = StatementForecaster(graph)
    >>>
    >>> # Mutating forecast - modifies the graph
    >>> forecaster.create_forecast(
    ...     forecast_periods=['2024', '2025'],
    ...     node_configs={
    ...         'revenue': {'method': 'simple', 'config': 0.05},
    ...         'costs': {'method': 'curve', 'config': [0.03, 0.04]}
    ...     }
    ... )
    >>>
    >>> # Non-mutating forecast - returns values without modifying graph
    >>> values = forecaster.forecast_value(
    ...     'revenue',
    ...     forecast_periods=['2024', '2025'],
    ...     forecast_config={'method': 'simple', 'config': 0.05}
    ... )
"""

# Main forecaster class
from .forecaster import StatementForecaster

# Forecast methods
from .methods import (
    ForecastMethod,
    BaseForecastMethod,
    SimpleForecastMethod,
    CurveForecastMethod,
    StatisticalForecastMethod,
    AverageForecastMethod,
    HistoricalGrowthForecastMethod,
)

# Registry and strategies
from .strategies import (
    ForecastMethodRegistry,
    forecast_registry,
    get_forecast_method,
    register_forecast_method,
)

# Utilities
from .period_manager import PeriodManager
from .validators import ForecastValidator

# Types
from .types import (
    ForecastMethodType,
    ForecastConfig,
    StatisticalConfig,
    ForecastResult,
)

# Error classes
from .errors import (
    ForecastingError,
    ForecastMethodError,
    ForecastConfigurationError,
    ForecastNodeError,
    ForecastResultError,
)

__all__ = [
    "AverageForecastMethod",
    "BaseForecastMethod",
    "CurveForecastMethod",
    "ForecastConfig",
    "ForecastConfigurationError",
    "ForecastMethod",
    "ForecastMethodError",
    "ForecastMethodRegistry",
    "ForecastMethodType",
    "ForecastNodeError",
    "ForecastResult",
    "ForecastResultError",
    "ForecastValidator",
    "ForecastingError",
    "HistoricalGrowthForecastMethod",
    "PeriodManager",
    "SimpleForecastMethod",
    "StatementForecaster",
    "StatisticalConfig",
    "StatisticalForecastMethod",
    "forecast_registry",
    "get_forecast_method",
    "register_forecast_method",
]



================================================================================
File: fin_statement_model/forecasting/errors.py
================================================================================

"""Custom Exception classes for the forecasting package.

These exceptions provide specific error information related to forecasting
operations, method configuration, and forecast result handling.
"""

from typing import Optional, Any
from fin_statement_model.core.errors import FinancialModelError

__all__ = [
    "ForecastConfigurationError",
    "ForecastMethodError",
    "ForecastNodeError",
    "ForecastResultError",
    "ForecastingError",
]


class ForecastingError(FinancialModelError):
    """Base exception for all forecasting-related errors."""


class ForecastMethodError(ForecastingError):
    """Exception raised for invalid or unsupported forecast methods.

    This includes unknown method names, invalid method parameters,
    or methods incompatible with the data type.
    """

    def __init__(
        self,
        message: str,
        method: Optional[str] = None,
        supported_methods: Optional[list[str]] = None,
        node_id: Optional[str] = None,
    ):
        """Initialize a ForecastMethodError.

        Args:
            message: The primary error message.
            method: Optional name of the invalid method.
            supported_methods: Optional list of supported methods.
            node_id: Optional ID of the node being forecasted.
        """
        self.method = method
        self.supported_methods = supported_methods or []
        self.node_id = node_id

        full_message = message
        if method:
            full_message = f"{message}: '{method}'"
        if node_id:
            full_message = f"{full_message} for node '{node_id}'"
        if supported_methods:
            full_message = (
                f"{full_message}. Supported methods: {', '.join(supported_methods)}"
            )

        super().__init__(full_message)


class ForecastConfigurationError(ForecastingError):
    """Exception raised for invalid forecast configuration.

    This includes missing required parameters, invalid parameter values,
    or incompatible configuration combinations.
    """

    def __init__(
        self,
        message: str,
        config: Optional[dict[str, Any]] = None,
        missing_params: Optional[list[str]] = None,
        invalid_params: Optional[dict[str, str]] = None,
    ):
        """Initialize a ForecastConfigurationError.

        Args:
            message: The primary error message.
            config: Optional configuration dictionary that caused the error.
            missing_params: Optional list of missing required parameters.
            invalid_params: Optional dict of parameter names to error descriptions.
        """
        self.config = config
        self.missing_params = missing_params or []
        self.invalid_params = invalid_params or {}

        details = []
        if missing_params:
            details.append(f"Missing parameters: {', '.join(missing_params)}")
        if invalid_params:
            param_errors = [f"{k}: {v}" for k, v in invalid_params.items()]
            details.append(f"Invalid parameters: {'; '.join(param_errors)}")

        full_message = message
        if details:
            full_message = f"{message} - {' | '.join(details)}"

        super().__init__(full_message)


class ForecastNodeError(ForecastingError):
    """Exception raised for node-related forecast errors.

    This includes nodes not found in the graph, nodes without historical data,
    or nodes that cannot be forecasted.
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str],
        available_nodes: Optional[list[str]] = None,
        reason: Optional[str] = None,
    ):
        """Initialize a ForecastNodeError.

        Args:
            message: The primary error message.
            node_id: The ID of the problematic node.
            available_nodes: Optional list of available node IDs.
            reason: Optional specific reason why the node cannot be forecasted.
        """
        self.node_id = node_id
        self.available_nodes = available_nodes or []
        self.reason = reason

        full_message = f"{message} for node '{node_id}'"
        if reason:
            full_message = f"{full_message}: {reason}"
        if available_nodes and len(available_nodes) < 10:  # Only show if list is small
            full_message = (
                f"{full_message}. Available nodes: {', '.join(available_nodes)}"
            )

        super().__init__(full_message)


class ForecastResultError(ForecastingError):
    """Exception raised for forecast result access or manipulation errors.

    This includes accessing results for non-existent periods, invalid result
    formats, or result validation failures.
    """

    def __init__(
        self,
        message: str,
        period: Optional[str] = None,
        available_periods: Optional[list[str]] = None,
        node_id: Optional[str] = None,
    ):
        """Initialize a ForecastResultError.

        Args:
            message: The primary error message.
            period: Optional period that caused the error.
            available_periods: Optional list of available periods.
            node_id: Optional ID of the node whose results are being accessed.
        """
        self.period = period
        self.available_periods = available_periods or []
        self.node_id = node_id

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if period:
            context.append(f"period '{period}'")

        full_message = message
        if context:
            full_message = f"{message} for {' and '.join(context)}"
        if available_periods and len(available_periods) < 10:
            full_message = (
                f"{full_message}. Available periods: {', '.join(available_periods)}"
            )

        super().__init__(full_message)



================================================================================
File: fin_statement_model/forecasting/forecaster.py
================================================================================

"""Forecasting operations dedicated to statement-level financial graphs.

This module provides the StatementForecaster class, which handles forecasting
operations for financial statement graphs. It offers both mutating operations
(that modify the graph) and non-mutating operations (that return forecast values
without changing the graph state).
"""

import logging
from typing import Any, Optional, cast
import numpy as np

# Core imports
from fin_statement_model.config import cfg
from fin_statement_model.core.nodes import Node
from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.forecasting.errors import (
    ForecastNodeError,
)

# Forecasting module imports
from .period_manager import PeriodManager
from .validators import ForecastValidator
from .strategies import get_forecast_method
from .types import ForecastConfig, ForecastResult
from .methods import BaseForecastMethod

logger = logging.getLogger(__name__)


class StatementForecaster:
    """Handles forecasting operations specifically for a FinancialStatementGraph.

    This class provides two main approaches to forecasting:

    1. **Mutating operations** (`create_forecast`): Modifies the graph by adding
       forecast periods and updating node values directly. This is useful when
       you want to extend the graph with forecast data for further analysis.

    2. **Non-mutating operations** (`forecast_value`): Returns forecast values
       without modifying the graph state. This is useful for what-if scenarios
       or when you need forecast values without altering the original data.

    The forecaster supports multiple forecasting methods:
    - simple: Simple growth rate
    - curve: Variable growth rates per period
    - statistical: Random sampling from distributions
    - average: Average of historical values
    - historical_growth: Based on historical growth patterns
    """

    def __init__(self, fsg: Any) -> None:
        """Initialize the forecaster.

        Args:
            fsg: The FinancialStatementGraph instance this forecaster will operate on.
        """
        self.fsg = fsg

    def create_forecast(
        self,
        forecast_periods: list[str],
        node_configs: Optional[dict[str, dict[str, Any]]] = None,
        historical_periods: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> None:
        """Create forecasts for financial statement items on the graph.

        **IMPORTANT**: This method MUTATES the graph by:
        - Adding new periods to the graph if they don't exist
        - Updating node values with forecast data
        - Clearing node caches after updates

        Use `forecast_value` instead if you need forecast values without
        modifying the graph.

        Args:
            forecast_periods: List of future periods to forecast.
            node_configs: Mapping of node names to their forecast configurations.
                Each config should contain:
                - 'method': Forecasting method ('simple', 'curve', 'statistical',
                           'average', 'historical_growth')
                - 'config': Method-specific parameters (growth rate, distribution, etc.)
            historical_periods: Optional list of historical periods to use as base.
                If not provided, will be inferred from the graph's existing periods.
            **kwargs: Additional arguments passed to the forecasting logic:
                add_missing_periods (bool): Whether to add missing forecast periods to the graph.

        Returns:
            None (modifies the graph in-place)

        Raises:
            ForecastNodeError: If no historical periods found, no forecast periods provided,
                              or invalid forecasting method/configuration.
        """
        logger.info(
            f"StatementForecaster: Creating forecast for periods {forecast_periods}"
        )
        try:
            # Use PeriodManager to infer historical periods
            historical_periods = PeriodManager.infer_historical_periods(
                self.fsg, forecast_periods, historical_periods
            )

            # Validate inputs using ForecastValidator
            ForecastValidator.validate_forecast_inputs(
                historical_periods, forecast_periods, node_configs
            )

            # Ensure forecast periods exist in the graph (override via add_missing_periods)
            add_missing = kwargs.get(
                "add_missing_periods", cfg("forecasting.add_missing_periods")
            )
            PeriodManager.ensure_periods_exist(
                self.fsg, forecast_periods, add_missing=add_missing
            )

            if node_configs is None:
                node_configs = {}

            for node_name, config in node_configs.items():
                node = self.fsg.get_node(node_name)
                if node is None:
                    raise ForecastNodeError(
                        f"Node {node_name} not found in graph",
                        node_id=node_name,
                        available_nodes=list(self.fsg.nodes.keys()),
                    )

                # Validate node can be forecasted
                forecast_config = ForecastValidator.validate_forecast_config(config)
                ForecastValidator.validate_node_for_forecast(
                    node, forecast_config.method
                )

                self._forecast_node(
                    node, historical_periods, forecast_periods, forecast_config
                )

            logger.info(
                f"Created forecast for {len(forecast_periods)} periods and {len(node_configs)} nodes"
            )
        except Exception as e:
            logger.error(f"Error creating forecast: {e}", exc_info=True)
            raise ForecastNodeError(
                f"Error creating forecast: {e}", node_id=None, reason=str(e)
            )

    def _forecast_node(
        self,
        node: Node,
        historical_periods: list[str],
        forecast_periods: list[str],
        forecast_config: ForecastConfig,
        **kwargs: Any,
    ) -> None:
        """Calculate forecast values and update the original node.

        **IMPORTANT**: This is an internal MUTATING method that:
        - Creates a temporary forecast node for calculations
        - Updates the original node's values dictionary with forecast results
        - Clears the original node's cache after updates

        This method should not be called directly. Use `create_forecast` for
        mutating operations or `forecast_value` for non-mutating operations.

        Args:
            node: The graph node to forecast. Must have a 'values' dictionary.
            historical_periods: List of historical periods for base values.
            forecast_periods: List of periods for which to calculate forecasts.
            forecast_config: Validated forecast configuration.
            **kwargs: Additional arguments passed to growth logic.

        Returns:
            None (modifies the node in-place)

        Raises:
            ForecastNodeError: If no historical periods provided or invalid method.
        """
        logger.debug(
            f"StatementForecaster: Forecasting node {node.name} using method {forecast_config.method}"
        )

        # Determine base period using PeriodManager
        base_period = PeriodManager.determine_base_period(node, historical_periods)

        # Get the forecast method from registry
        method = get_forecast_method(forecast_config.method)

        # Get normalized parameters for NodeFactory
        # All built-in methods extend BaseForecastMethod which has get_forecast_params
        base_method = cast(BaseForecastMethod, method)
        params = base_method.get_forecast_params(
            forecast_config.config, forecast_periods
        )

        # Create a temporary node to perform calculations
        tmp_node = NodeFactory.create_forecast_node(
            name=f"{node.name}_forecast_temp",
            base_node=node,
            base_period=base_period,
            forecast_periods=forecast_periods,
            forecast_type=params["forecast_type"],
            growth_params=params["growth_params"],
        )

        # Ensure the original node has a values dictionary
        if not hasattr(node, "values") or not isinstance(node.values, dict):
            logger.error(
                f"Cannot store forecast for node {node.name}: node does not have a 'values' dictionary."
            )
            return  # Cannot proceed

        # Calculate and update original node's values
        for period in forecast_periods:
            try:
                val = tmp_node.calculate(period)
                # Default fallback for bad forecasts, overrideable via bad_forecast_value kwarg
                bad_value = kwargs.get(
                    "bad_forecast_value", cfg("forecasting.default_bad_forecast_value")
                )
                if np.isnan(val) or np.isinf(val):
                    logger.warning(
                        f"Bad forecast {val} for {node.name}@{period}; defaulting to {bad_value}"
                    )
                    val = bad_value
                # Clamp negative values if disallowed
                allow_neg = kwargs.get(
                    "allow_negative_forecasts",
                    cfg("forecasting.allow_negative_forecasts"),
                )
                if not allow_neg and val < 0:
                    # Ensure bad_value is non-negative when clamping negative values
                    if bad_value < 0:
                        logger.warning(
                            f"bad_forecast_value ({bad_value}) is negative but allow_negative_forecasts is False. Using 0.0 instead."
                        )
                        bad_value = 0.0
                    logger.warning(
                        f"Negative forecast {val} for {node.name}@{period}; clamping to {bad_value}"
                    )
                    val = bad_value
                node.values[period] = float(val)  # Update the original node
            except Exception as e:
                logger.error(
                    f"Error forecasting {node.name}@{period}: {e}", exc_info=True
                )
                node.values[period] = bad_value  # Set default on error

        # Clear cache of the original node as its values have changed
        if hasattr(node, "clear_cache") and callable(node.clear_cache):
            node.clear_cache()
            logger.debug(f"Cleared cache for node {node.name} after forecast update.")

    def forecast_value(
        self,
        node_name: str,
        forecast_periods: list[str],
        base_period: Optional[str] = None,
        forecast_config: Optional[dict[str, Any]] = None,
        **kwargs: Any,
    ) -> dict[str, float]:
        """Forecast and return values for a node without mutating the graph.

        **IMPORTANT**: This is a NON-MUTATING method that:
        - Does NOT add periods to the graph
        - Does NOT modify any node values
        - Does NOT affect the graph state in any way
        - Returns forecast values as a separate dictionary

        This method is ideal for:
        - What-if analysis
        - Comparing different forecast scenarios
        - Getting forecast values without committing them to the graph
        - API responses where you don't want to modify server state

        Args:
            node_name: Name of the node to forecast.
            forecast_periods: List of future periods to forecast.
            base_period: Optional base period to use for forecasting.
                        If omitted, will be inferred from the node's historical data.
            forecast_config: Forecast configuration dict with:
                - 'method': Forecasting method ('simple', 'curve', 'statistical',
                           'average', 'historical_growth')
                - 'config': Method-specific parameters
                If not provided, uses global forecasting defaults.
            **kwargs: Additional arguments passed to the internal forecasting logic.
                bad_forecast_value (float): Default to use for NaN/Inf or errors (overrides config)
                allow_negative_forecasts (bool): Whether to allow negative forecast values (overrides config)

        Returns:
            A dictionary mapping forecast periods to their calculated values.
            Example: {'2024': 1050.0, '2025': 1102.5}

        Raises:
            ForecastNodeError: If node not found, no historical periods available,
                              or invalid forecast configuration.
        """
        # Optional override for bad forecast fallback
        bad_value = kwargs.get(
            "bad_forecast_value", cfg("forecasting.default_bad_forecast_value")
        )
        # Locate the node
        node = self.fsg.get_node(node_name)
        if node is None:
            raise ForecastNodeError(
                f"Node {node_name} not found in graph",
                node_id=node_name,
                available_nodes=list(self.fsg.nodes.keys()),
            )

        # Determine historical periods
        if base_period:
            historical_periods = [base_period]
        else:
            historical_periods = PeriodManager.infer_historical_periods(
                self.fsg, forecast_periods
            )

        # Validate inputs
        ForecastValidator.validate_forecast_inputs(historical_periods, forecast_periods)

        # Set default config if not provided, using global forecasting defaults
        if forecast_config is None:
            default_method = cfg("forecasting.default_method")
            # Use method-appropriate default config
            if default_method == "simple":
                forecast_config = {"method": default_method, "config": cfg("forecasting.default_growth_rate")}
            else:
                # For other methods, use empty config and let the method handle defaults
                forecast_config = {"method": default_method, "config": {}}

        # Validate and create ForecastConfig
        validated_config = ForecastValidator.validate_forecast_config(forecast_config)
        ForecastValidator.validate_node_for_forecast(node, validated_config.method)

        # Determine base period
        calc_base_period = PeriodManager.determine_base_period(
            node, historical_periods, base_period
        )

        # Get the forecast method from registry
        method = get_forecast_method(validated_config.method)

        # Get normalized parameters for NodeFactory
        # All built-in methods extend BaseForecastMethod which has get_forecast_params
        base_method = cast(BaseForecastMethod, method)
        params = base_method.get_forecast_params(
            validated_config.config, forecast_periods
        )

        # Create a temporary forecast node (DO NOT add to graph)
        try:
            temp_forecast_node = NodeFactory.create_forecast_node(
                name=f"{node_name}_temp_forecast",
                base_node=node,
                base_period=calc_base_period,
                forecast_periods=forecast_periods,
                forecast_type=params["forecast_type"],
                growth_params=params["growth_params"],
            )
        except Exception as e:
            logger.error(
                f"Failed to create temporary forecast node for '{node_name}': {e}",
                exc_info=True,
            )
            raise ForecastNodeError(
                f"Could not create temporary forecast node: {e}",
                node_id=node_name,
                reason=str(e),
            )

        # Calculate results using the temporary node
        results: dict[str, float] = {}
        for period in forecast_periods:
            try:
                value = temp_forecast_node.calculate(period)
                # Handle potential NaN/Inf results from calculation
                results[period] = bad_value if not np.isfinite(value) else float(value)
                # Clamp negative values if disallowed
                allow_neg = kwargs.get(
                    "allow_negative_forecasts",
                    cfg("forecasting.allow_negative_forecasts"),
                )
                if not allow_neg and results[period] < 0:
                    logger.warning(
                        f"Negative forecast {results[period]} for {node_name}@{period}; clamping to {bad_value}"
                    )
                    results[period] = bad_value
            except Exception as e:
                logger.warning(
                    f"Error calculating temporary forecast for {node_name}@{period}: {e}. Returning {bad_value}"
                )
                results[period] = bad_value

        # Validate results before returning
        ForecastValidator.validate_forecast_result(results, forecast_periods, node_name)

        return results

    def forecast_multiple(
        self,
        node_names: list[str],
        forecast_periods: list[str],
        forecast_configs: Optional[dict[str, dict[str, Any]]] = None,
        base_period: Optional[str] = None,
        **kwargs: Any,
    ) -> dict[str, ForecastResult]:
        """Forecast multiple nodes without mutating the graph.

        This is a convenience method that forecasts multiple nodes at once
        and returns structured results.

        Args:
            node_names: List of node names to forecast.
            forecast_periods: List of future periods to forecast.
            forecast_configs: Optional mapping of node names to their forecast configs.
                             If not provided, uses simple method with 0% growth for all.
            base_period: Optional base period to use for all nodes.
            **kwargs: Additional arguments passed to forecast_value.

        Returns:
            Dictionary mapping node names to ForecastResult objects.

        Example:
            >>> results = forecaster.forecast_multiple(
            ...     ['revenue', 'costs'],
            ...     ['2024', '2025'],
            ...     {'revenue': {'method': 'simple', 'config': 0.05}}
            ... )
            >>> print(results['revenue'].get_value('2024'))
        """
        # Determine error propagation strategy (override via continue_on_error kwarg)
        continue_on_err = kwargs.get(
            "continue_on_error", cfg("forecasting.continue_on_error")
        )
        results: dict[str, ForecastResult] = {}
        configs = forecast_configs or {}

        for node_name in node_names:
            try:
                # Get config for this node or use default
                node_config = configs.get(node_name)

                # Forecast the node
                values = self.forecast_value(
                    node_name, forecast_periods, base_period, node_config, **kwargs
                )

                # Determine actual base period used
                node = self.fsg.get_node(node_name)
                historical_periods = PeriodManager.infer_historical_periods(
                    self.fsg, forecast_periods
                )
                actual_base_period = PeriodManager.determine_base_period(
                    node, historical_periods, base_period
                )

                # Create ForecastResult
                default_method = cfg("forecasting.default_method")
                config = ForecastValidator.validate_forecast_config(
                    node_config
                    or {
                        "method": default_method,
                        "config": cfg("forecasting.default_growth_rate"),
                    }
                )

                results[node_name] = ForecastResult(
                    node_name=node_name,
                    periods=forecast_periods,
                    values=values,
                    method=config.method,
                    base_period=actual_base_period,
                )
            except Exception as e:
                logger.exception(f"Error forecasting node {node_name}")
                if continue_on_err:
                    continue
                raise ForecastNodeError(
                    f"Error forecasting node {node_name}: {e}",
                    node_id=node_name,
                    reason=str(e),
                )

        return results

    def forecast_node(
        self,
        node_name: str,
        config: ForecastConfig,
        historical_periods: Optional[int] = None,
    ) -> ForecastResult:
        """Forecast values for a specific node.

        Args:
            node_name: Name of the node to forecast
            config: Forecast configuration
            historical_periods: Number of historical periods to use

        Returns:
            ForecastResult containing the forecasted values

        Raises:
            ForecastNodeError: If node not found in graph
        """
        if node_name not in self.fsg.nodes:
            raise ForecastNodeError(
                f"Node {node_name} not found in graph",
                node_id=node_name,
                available_nodes=list(self.fsg.nodes.keys()),
            )
        # TODO: implement detailed forecasting logic for a single node
        raise NotImplementedError("forecast_node is not implemented")

    def forecast_all(
        self,
        default_config: ForecastConfig,
        node_configs: Optional[dict[str, ForecastConfig]] = None,
    ) -> dict[str, ForecastResult]:
        """Forecast all forecastable nodes in the graph.

        Args:
            default_config: Default configuration for nodes without specific config
            node_configs: Optional node-specific configurations

        Returns:
            Dictionary mapping node names to forecast results

        Raises:
            ForecastNodeError: If node not found in graph
        """
        node_configs = node_configs or {}
        results = {}

        for node_name, node in self.fsg.nodes.items():
            if self._is_forecastable(node):
                config = node_configs.get(node_name, default_config)
                try:
                    results[node_name] = self.forecast_node(node_name, config)
                except Exception as e:
                    logger.warning(f"Failed to forecast {node_name}: {e}")
                    continue

        return results

    def _is_forecastable(self, node: Node) -> bool:
        """Determine if a node is forecastable (has a 'values' dictionary)."""
        return hasattr(node, "values") and isinstance(node.values, dict)

    def create_forecast_node(
        self,
        base_node_name: str,
        forecast_name: str,
        config: ForecastConfig,
    ) -> str:
        """Create a new forecast node based on an existing node.

        Args:
            base_node_name: Name of the node to base forecast on
            forecast_name: Name for the new forecast node
            config: Forecast configuration

        Returns:
            Name of the created forecast node

        Raises:
            ForecastNodeError: If base node not found or forecast node creation fails
        """
        if base_node_name not in self.fsg.nodes:
            raise ForecastNodeError(
                f"Node {base_node_name} not found in graph",
                node_id=base_node_name,
                available_nodes=list(self.fsg.nodes.keys()),
            )

        base_node = self.fsg.nodes[base_node_name]

        # Create forecast node
        try:
            forecast_node = NodeFactory.create_forecast_node(  # type: ignore[call-arg]
                name=forecast_name,
                base_node=base_node,
                forecast_config=config,
            )
            self.fsg.add_node(forecast_node)
            return forecast_name
        except Exception as e:
            raise ForecastNodeError(
                f"Could not create temporary forecast node: {e}",
                node_id=forecast_name,
                reason=str(e),
            ) from e



================================================================================
File: fin_statement_model/forecasting/methods/__init__.py
================================================================================

"""Forecast method implementations.

This module contains all the built-in forecast methods available in the library.
"""

from .base import ForecastMethod, BaseForecastMethod
from .simple import SimpleForecastMethod
from .curve import CurveForecastMethod
from .statistical import StatisticalForecastMethod
from .average import AverageForecastMethod
from .historical_growth import HistoricalGrowthForecastMethod

__all__ = [
    "AverageForecastMethod",
    "BaseForecastMethod",
    "CurveForecastMethod",
    "ForecastMethod",
    "HistoricalGrowthForecastMethod",
    "SimpleForecastMethod",
    "StatisticalForecastMethod",
]



================================================================================
File: fin_statement_model/forecasting/methods/average.py
================================================================================

"""Average forecasting method based on historical values.

This method forecasts future values as the average of historical values.
"""

import logging
from typing import Any, Optional
import numpy as np

from fin_statement_model.core.nodes import Node
from .base import BaseForecastMethod

logger = logging.getLogger(__name__)


class AverageForecastMethod(BaseForecastMethod):
    """Historical average forecasting.

    This method calculates forecast values as the average of historical
    values. Useful for stable metrics or when expecting mean reversion.

    Configuration:
        - Not required (pass 0 or None)
        - The method will automatically use all available historical data

    Example:
        >>> method = AverageForecastMethod()
        >>> params = method.get_forecast_params(None, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'average', 'growth_params': None}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "average"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "average"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for average method.

        Args:
            config: Not used for average method, can be None or 0.

        Note:
            Average method doesn't require configuration as it uses
            historical data automatically.
        """
        # Average method doesn't need specific configuration
        # Accept None, 0, or any placeholder value

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Not used for average method.
            forecast_periods: List of periods to forecast (not used).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
            For average method, growth_params is None.
        """
        return {
            "forecast_type": self.internal_type,
            "growth_params": None,  # Average method doesn't use growth params
        }

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for average calculation.

        Args:
            node: The node to extract historical data from.
            historical_periods: List of historical periods.

        Returns:
            List of valid historical values.

        Raises:
            ValueError: If no valid historical data is available.
        """
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise ValueError(
                f"Node {node.name} cannot be calculated for average method"
            )

        if not hasattr(node, "values") or not isinstance(node.values, dict):
            raise ValueError(
                f"Node {node.name} does not have values dictionary for average method"
            )

        # Extract historical values
        historical_values = []
        for period in historical_periods:
            if period in node.values:
                try:
                    value = node.calculate(period)
                    if (
                        value is not None
                        and not np.isnan(value)
                        and not np.isinf(value)
                    ):
                        historical_values.append(float(value))
                except Exception as e:
                    # Log the exception and skip this period
                    logger.debug(
                        f"Skipping period {period} for node {node.name} in average calculation: {e}"
                    )
                    continue

        if not historical_values:
            raise ValueError(
                f"No valid historical data available for node {node.name} to compute average"
            )

        return historical_values



================================================================================
File: fin_statement_model/forecasting/methods/base.py
================================================================================

"""Base protocol and abstract class for forecast methods.

This module defines the interface that all forecast methods must implement.
"""

from typing import Protocol, Any, Optional, runtime_checkable
from abc import ABC, abstractmethod

from fin_statement_model.core.nodes import Node


@runtime_checkable
class ForecastMethod(Protocol):
    """Protocol that all forecast methods must implement."""

    @property
    def name(self) -> str:
        """Return the method name."""
        ...

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for this method.

        Args:
            config: The method-specific configuration to validate.

        Raises:
            ValueError: If configuration is invalid.
        """
        ...

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: The method-specific configuration.
            forecast_periods: List of periods to forecast.

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
        """
        ...

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for methods that need it.

        Args:
            node: The node to extract historical data from.
            historical_periods: List of historical periods.

        Returns:
            List of historical values or None if not needed.
        """
        ...


class BaseForecastMethod(ABC):
    """Abstract base class for forecast methods.

    Provides common functionality and enforces the interface.
    """

    @property
    @abstractmethod
    def name(self) -> str:
        """Return the method name."""

    @property
    @abstractmethod
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""

    @abstractmethod
    def validate_config(self, config: Any) -> None:
        """Validate the configuration for this method."""

    @abstractmethod
    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory."""

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for methods that need it.

        Default implementation returns None (not needed).
        Override in methods that require historical data.
        """
        return None

    def get_forecast_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Get complete forecast parameters.

        This is a convenience method that validates and normalizes in one call.

        Args:
            config: The method-specific configuration.
            forecast_periods: List of periods to forecast.

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.

        Raises:
            ValueError: If configuration is invalid.
        """
        self.validate_config(config)
        return self.normalize_params(config, forecast_periods)



================================================================================
File: fin_statement_model/forecasting/methods/curve.py
================================================================================

"""Curve forecasting method with variable growth rates.

This method applies different growth rates for each forecast period.
"""

from typing import Any

from .base import BaseForecastMethod


class CurveForecastMethod(BaseForecastMethod):
    """Variable growth rate forecasting.

    This method applies different growth rates for each forecast period,
    allowing for non-linear growth patterns.

    Configuration:
        - List of numeric values: One growth rate per forecast period
        - Single numeric value: Will be expanded to match forecast periods

    Example:
        >>> method = CurveForecastMethod()
        >>> params = method.get_forecast_params([0.05, 0.04, 0.03], ['2024', '2025', '2026'])
        >>> # Returns: {'forecast_type': 'curve', 'growth_params': [0.05, 0.04, 0.03]}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "curve"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "curve"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for curve method.

        Args:
            config: Should be a numeric value or a list of numeric values.

        Raises:
            ValueError: If config is empty list.
            TypeError: If config is not numeric or list of numerics.
        """
        if isinstance(config, list):
            if not config:
                raise ValueError("Curve method: empty list provided")
            for i, value in enumerate(config):
                if not isinstance(value, int | float):
                    raise TypeError(
                        f"Curve method: non-numeric value at index {i}: {type(value)}"
                    )
        elif not isinstance(config, int | float):
            raise TypeError(
                f"Curve method requires numeric or list of numeric values, got {type(config)}"
            )

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Growth rates (single value or list).
            forecast_periods: List of periods to forecast.

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.

        Raises:
            ValueError: If list length doesn't match forecast periods.
        """
        if not isinstance(config, list):
            # Single value - expand to match forecast periods
            growth_rates = [float(config)] * len(forecast_periods)
        else:
            # List of values - must match forecast periods length
            if len(config) != len(forecast_periods):
                raise ValueError(
                    f"Curve method: growth rate list length ({len(config)}) "
                    f"must match forecast periods ({len(forecast_periods)})"
                )
            growth_rates = [float(x) for x in config]

        return {"forecast_type": self.internal_type, "growth_params": growth_rates}



================================================================================
File: fin_statement_model/forecasting/methods/historical_growth.py
================================================================================

"""Historical growth forecasting method based on past growth patterns.

This method calculates future values based on the average historical growth rate.
"""

import logging
from typing import Any, Optional
import numpy as np

from fin_statement_model.core.nodes import Node
from .base import BaseForecastMethod
from fin_statement_model.config.helpers import cfg

logger = logging.getLogger(__name__)


class HistoricalGrowthForecastMethod(BaseForecastMethod):
    """Historical growth pattern forecasting.

    This method calculates the average historical growth rate and applies
    it to forecast future values. It's useful when past growth patterns
    are expected to continue.

    Configuration:
        - Not required (pass 0 or None)
        - The method will automatically calculate growth from historical data

    Example:
        >>> method = HistoricalGrowthForecastMethod()
        >>> params = method.get_forecast_params(None, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'historical_growth', 'growth_params': None}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "historical_growth"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "historical_growth"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for historical growth method.

        Args:
            config: Not used for historical growth method, can be None or 0.

        Note:
            Historical growth method doesn't require configuration as it
            calculates growth from historical data automatically.
        """
        # Historical growth method doesn't need specific configuration
        # Accept None, 0, or any placeholder value

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Not used for historical growth method.
            forecast_periods: List of periods to forecast (not used).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
            For historical growth method, growth_params is None.
        """
        return {
            "forecast_type": self.internal_type,
            "growth_params": None,  # Historical growth method calculates internally
        }

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for growth calculation.

        Args:
            node: The node to extract historical data from.
            historical_periods: List of historical periods.

        Returns:
            List of valid historical values (at least 2 needed for growth).

        Raises:
            ValueError: If insufficient historical data is available.
        """
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise ValueError(
                f"Node {node.name} cannot be calculated for historical growth method"
            )

        if not hasattr(node, "values") or not isinstance(node.values, dict):
            raise ValueError(
                f"Node {node.name} does not have values dictionary for historical growth method"
            )

        # Extract historical values in chronological order
        historical_values = []
        for period in historical_periods:
            if period in node.values:
                try:
                    value = node.calculate(period)
                    if (
                        value is not None
                        and not np.isnan(value)
                        and not np.isinf(value)
                    ):
                        historical_values.append(float(value))
                except Exception as e:
                    # Log the exception and skip this period
                    logger.debug(
                        f"Skipping period {period} for node {node.name} in historical growth calculation: {e}"
                    )
                    continue

        min_periods = cfg("forecasting.min_historical_periods")
        if len(historical_values) < min_periods:
            raise ValueError(
                f"Need at least {min_periods} historical data points for node {node.name} "
                f"to compute growth rate, found {len(historical_values)}"
            )

        return historical_values

    def calculate_average_growth_rate(self, historical_values: list[float]) -> float:
        """Calculate the average growth rate from historical values.

        Args:
            historical_values: List of historical values in chronological order.

        Returns:
            Average growth rate.

        Note:
            This is a helper method that can be used by the forecast node
            implementation to calculate the growth rate.
        """
        # Calculate period-over-period growth rates
        if len(historical_values) < 2:
            return 0.0

        growth_rates: list[float] = []
        for i in range(1, len(historical_values)):
            previous_value = historical_values[i - 1]
            if previous_value != 0:
                growth_rates.append((historical_values[i] - previous_value) / previous_value)

        if not growth_rates:
            return 0.0

        # Determine aggregation method: 'mean' or 'median'
        agg_method = cfg("forecasting.historical_growth_aggregation")
        if agg_method == "median":
            try:
                return float(np.median(growth_rates))
            except (ValueError, TypeError) as e:
                logger.warning(f"Failed to calculate median growth rate, falling back to mean: {e}")
                return float(np.mean(growth_rates))
        # Default to mean
        return float(np.mean(growth_rates))



================================================================================
File: fin_statement_model/forecasting/methods/simple.py
================================================================================

"""Simple growth rate forecasting method.

This method applies a constant growth rate to forecast future values.
"""

from typing import Any

from .base import BaseForecastMethod


class SimpleForecastMethod(BaseForecastMethod):
    """Simple growth rate forecasting.

    This method applies a constant growth rate to the base value
    for all forecast periods.

    Configuration:
        - Single numeric value: The growth rate (e.g., 0.05 for 5% growth)
        - List with single value: Will use the first value

    Example:
        >>> method = SimpleForecastMethod()
        >>> params = method.get_forecast_params(0.05, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'simple', 'growth_params': 0.05}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "simple"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "simple"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for simple method.

        Args:
            config: Should be a numeric value or a list containing a numeric value.

        Raises:
            ValueError: If config is empty list.
            TypeError: If config is not numeric or a list with numeric value.
        """
        if isinstance(config, list):
            if not config:
                raise ValueError("Simple method: empty list provided")
            if not isinstance(config[0], int | float):
                raise TypeError(
                    f"Simple method requires numeric growth rate, got {type(config[0])}"
                )
        elif not isinstance(config, int | float):
            raise TypeError(
                f"Simple method requires numeric growth rate, got {type(config)}"
            )

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: The growth rate (numeric or list with numeric).
            forecast_periods: List of periods to forecast (not used for simple method).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
        """
        # Handle list input - take first value
        growth_rate = float(config[0]) if isinstance(config, list) else float(config)

        return {"forecast_type": self.internal_type, "growth_params": growth_rate}



================================================================================
File: fin_statement_model/forecasting/methods/statistical.py
================================================================================

"""Statistical forecasting method using random sampling.

This method generates forecast values by sampling from statistical distributions.
"""

from typing import Any
import numpy as np
from pydantic import ValidationError
from fin_statement_model.config.helpers import cfg

from .base import BaseForecastMethod
from fin_statement_model.forecasting.types import StatisticalConfig


class StatisticalForecastMethod(BaseForecastMethod):
    """Statistical forecasting using random distributions.

    This method generates forecast values by sampling from specified
    statistical distributions, useful for Monte Carlo simulations
    and uncertainty analysis.

    Configuration:
        Dict with:
        - 'distribution': 'normal' or 'uniform'
        - 'params': Distribution-specific parameters
            - For 'normal': {'mean': float, 'std': float}
            - For 'uniform': {'low': float, 'high': float}

    Example:
        >>> method = StatisticalForecastMethod()
        >>> config = {
        ...     'distribution': 'normal',
        ...     'params': {'mean': 0.05, 'std': 0.02}
        ... }
        >>> params = method.get_forecast_params(config, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'statistical', 'growth_params': <callable>}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "statistical"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "statistical"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for statistical method.

        Args:
            config: Should be a dict with 'distribution' and 'params' keys.

        Raises:
            TypeError: If config is invalid.
        """
        if not isinstance(config, dict):
            raise TypeError(
                f"Statistical method requires dict configuration, got {type(config)}"
            )

        if "distribution" not in config:
            raise ValueError("Statistical method requires 'distribution' key")

        if "params" not in config:
            raise ValueError("Statistical method requires 'params' key")

        # Validate using StatisticalConfig model (raises ValidationError or ForecastConfigurationError)
        try:
            StatisticalConfig(
                distribution=config["distribution"], params=config["params"]
            )
        except (ValueError, TypeError, ValidationError) as e:
            raise ValueError(f"Invalid statistical configuration: {e}") from e

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Statistical distribution configuration.
            forecast_periods: List of periods to forecast (not used).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
            The 'growth_params' value is a callable that generates random values.
        """
        # Create validated config
        stat_config = StatisticalConfig(
            distribution=config["distribution"], params=config["params"]
        )

        # Seed RNG if configured
        seed = cfg("forecasting.random_seed")
        if seed is not None:
            rng = np.random.RandomState(seed)
        else:
            rng = np.random

        # Create generator function based on distribution
        def generator() -> float:
            """Generate a random growth rate from the specified distribution."""
            if stat_config.distribution == "normal":
                return float(
                    rng.normal(stat_config.params["mean"], stat_config.params["std"])
                )
            elif stat_config.distribution == "uniform":
                return float(
                    rng.uniform(stat_config.params["low"], stat_config.params["high"])
                )
            else:
                # This shouldn't happen due to validation, but just in case
                raise ValueError(
                    f"Unsupported distribution: {stat_config.distribution}"
                )

        return {"forecast_type": self.internal_type, "growth_params": generator}



================================================================================
File: fin_statement_model/forecasting/period_manager.py
================================================================================

"""Period inference and management utilities for forecasting.

This module handles the logic for determining historical periods,
base periods, and managing period-related operations.
"""

import logging
from typing import Any, Optional

from fin_statement_model.core.nodes import Node
from fin_statement_model.config.helpers import cfg

logger = logging.getLogger(__name__)


class PeriodManager:
    """Handles period inference and management for forecasting.

    This class provides utilities for:
    - Inferring historical periods from graph state
    - Determining base periods for forecasting
    - Validating period sequences
    - Managing period transitions
    """

    @staticmethod
    def infer_historical_periods(
        graph: Any,
        forecast_periods: list[str],
        provided_periods: Optional[list[str]] = None,
    ) -> list[str]:
        """Infer historical periods from graph state.

        Args:
            graph: The financial statement graph instance.
            forecast_periods: List of periods to forecast.
            provided_periods: Optional explicitly provided historical periods.

        Returns:
            List of historical periods.

        Raises:
            ValueError: If historical periods cannot be determined.
        """
        # If explicitly provided, use them
        if provided_periods is not None:
            logger.debug(
                f"Using explicitly provided historical periods: {provided_periods}"
            )
            return provided_periods

        # Check if graph has a custom method for getting historical periods
        if hasattr(graph, "get_historical_periods") and callable(
            graph.get_historical_periods
        ):
            historical = graph.get_historical_periods()
            logger.debug(f"Using graph's get_historical_periods method: {historical}")
            return historical  # type: ignore[no-any-return]

        # Otherwise, infer from graph periods and forecast periods
        if not hasattr(graph, "periods") or not graph.periods:
            raise ValueError(
                "Cannot infer historical periods: graph has no periods attribute"
            )

        if not forecast_periods:
            raise ValueError(
                "Cannot infer historical periods: no forecast periods provided"
            )

        # Try to find where forecast periods start
        first_forecast = forecast_periods[0]
        try:
            idx = graph.periods.index(first_forecast)
            historical_periods = graph.periods[:idx]
            logger.debug(
                f"Inferred historical periods by splitting at {first_forecast}: "
                f"{historical_periods}"
            )
        except ValueError:
            # First forecast period not in graph periods
            # Assume all current periods are historical
            historical_periods = list(graph.periods)
            logger.warning(
                f"First forecast period {first_forecast} not found in graph periods. "
                f"Using all existing periods as historical: {historical_periods}"
            )

        if not historical_periods:
            raise ValueError(
                "No historical periods found. Ensure graph has periods before "
                "the first forecast period."
            )

        return historical_periods  # type: ignore[no-any-return]

    @staticmethod
    def determine_base_period(
        node: Node,
        historical_periods: list[str],
        preferred_period: Optional[str] = None,
    ) -> str:
        """Determine the base period for forecasting a node.

        Args:
            node: The node to forecast.
            historical_periods: List of available historical periods.
            preferred_period: Optional preferred base period.

        Returns:
            The base period to use for forecasting.

        Raises:
            ValueError: If no valid base period can be determined.
        """
        if not historical_periods:
            raise ValueError("No historical periods provided")

        # Determine strategy for selecting base period
        strategy = cfg("forecasting.base_period_strategy")

        # Validate strategy
        valid_strategies = {"preferred_then_most_recent", "most_recent", "last_historical"}
        if strategy not in valid_strategies:
            logger.warning(
                f"Unknown base period strategy '{strategy}', falling back to 'preferred_then_most_recent'"
            )
            strategy = "preferred_then_most_recent"

        # 1. preferred_then_most_recent: check preferred first
        if strategy == "preferred_then_most_recent" and preferred_period:
            if preferred_period in historical_periods and hasattr(node, "values"):
                values = getattr(node, "values", {})
                if isinstance(values, dict) and preferred_period in values:
                    return preferred_period

        # 2. most_recent: pick most recent available data
        if strategy in ("preferred_then_most_recent", "most_recent"):
            if hasattr(node, "values") and isinstance(
                getattr(node, "values", None), dict
            ):
                values_dict = node.values  # type: ignore[attr-defined]
                available_periods = [p for p in historical_periods if p in values_dict]
                if available_periods:
                    return available_periods[-1]

        # 3. last_historical: always use last in historical_periods
        if strategy == "last_historical":
            return historical_periods[-1]

        # Final fallback: use last historical period
        base_period = historical_periods[-1]
        logger.info(
            f"Using last historical period as base for {node.name}: {base_period} "
            "(node may lack values)"
        )
        return base_period

    @staticmethod
    def validate_period_sequence(periods: list[str]) -> None:
        """Validate that a period sequence is valid.

        Args:
            periods: List of periods to validate.

        Raises:
            ValueError: If the period sequence is invalid.
        """
        if not periods:
            raise ValueError("Period sequence cannot be empty")

        if len(periods) != len(set(periods)):
            duplicates = [p for p in periods if periods.count(p) > 1]
            raise ValueError(f"Period sequence contains duplicates: {set(duplicates)}")

    @staticmethod
    def get_period_index(period: str, periods: list[str]) -> int:
        """Get the index of a period in a period list.

        Args:
            period: The period to find.
            periods: List of periods.

        Returns:
            The index of the period.

        Raises:
            ValueError: If period not found in list.
        """
        try:
            return periods.index(period)
        except ValueError:
            raise ValueError(f"Period '{period}' not found in period list") from None

    @staticmethod
    def ensure_periods_exist(
        graph: Any, periods: list[str], add_missing: bool = True
    ) -> list[str]:
        """Ensure periods exist in the graph.

        Args:
            graph: The financial statement graph instance.
            periods: List of periods that should exist.
            add_missing: Whether to add missing periods to the graph.

        Returns:
            List of periods that were added (empty if none).

        Raises:
            ValueError: If add_missing is False and periods are missing.
        """
        if not hasattr(graph, "periods"):
            raise ValueError("Graph does not have a periods attribute")

        existing_periods = set(graph.periods)
        missing_periods = [p for p in periods if p not in existing_periods]

        if missing_periods:
            if add_missing:
                # Add missing periods to graph
                if hasattr(graph, "add_periods") and callable(graph.add_periods):
                    graph.add_periods(missing_periods)
                    logger.info(f"Added missing periods to graph: {missing_periods}")
                else:
                    raise ValueError(
                        f"Graph is missing periods {missing_periods} but has no add_periods method"
                    )
            else:
                raise ValueError(
                    f"The following periods do not exist in the graph: {missing_periods}"
                )

        return missing_periods



================================================================================
File: fin_statement_model/forecasting/strategies.py
================================================================================

"""Forecast method registry and selection strategies.

This module provides a registry for forecast methods and handles method
selection and configuration.
"""

import logging
from typing import Any

from .methods import (
    ForecastMethod,
    SimpleForecastMethod,
    CurveForecastMethod,
    StatisticalForecastMethod,
    AverageForecastMethod,
    HistoricalGrowthForecastMethod,
)

logger = logging.getLogger(__name__)


class ForecastMethodRegistry:
    """Registry for forecast methods.

    This class manages the available forecast methods and provides
    a centralized way to access and register them.

    Example:
        >>> registry = ForecastMethodRegistry()
        >>> method = registry.get_method('simple')
        >>> print(registry.list_methods())
        ['simple', 'curve', 'statistical', 'average', 'historical_growth']
    """

    def __init__(self) -> None:
        """Initialize the registry with built-in methods."""
        self._methods: dict[str, ForecastMethod] = {}
        self._register_builtin_methods()

    def _register_builtin_methods(self) -> None:
        """Register all built-in forecast methods."""
        builtin_methods = [
            SimpleForecastMethod(),
            CurveForecastMethod(),
            StatisticalForecastMethod(),
            AverageForecastMethod(),
            HistoricalGrowthForecastMethod(),
        ]

        for method in builtin_methods:
            self.register(method)
            logger.debug(f"Registered built-in forecast method: {method.name}")

    def register(self, method: ForecastMethod) -> None:
        """Register a new forecast method.

        Args:
            method: The forecast method to register.

        Raises:
            ValueError: If a method with the same name is already registered.
        """
        if method.name in self._methods:
            raise ValueError(f"Forecast method '{method.name}' is already registered")

        self._methods[method.name] = method
        logger.info(f"Registered forecast method: {method.name}")

    def unregister(self, name: str) -> None:
        """Unregister a forecast method.

        Args:
            name: The name of the method to unregister.

        Raises:
            KeyError: If the method is not registered.
        """
        if name not in self._methods:
            raise KeyError(f"Forecast method '{name}' is not registered")

        del self._methods[name]
        logger.info(f"Unregistered forecast method: {name}")

    def get_method(self, name: str) -> ForecastMethod:
        """Get a forecast method by name.

        Args:
            name: The name of the method to retrieve.

        Returns:
            The requested forecast method.

        Raises:
            ValueError: If the method is not registered.
        """
        if name not in self._methods:
            available = ", ".join(sorted(self._methods.keys()))
            raise ValueError(
                f"Unknown forecast method: '{name}'. Available methods: {available}"
            )

        return self._methods[name]

    def list_methods(self) -> list[str]:
        """List all available forecast methods.

        Returns:
            Sorted list of registered method names.
        """
        return sorted(self._methods.keys())

    def has_method(self, name: str) -> bool:
        """Check if a method is registered.

        Args:
            name: The name of the method to check.

        Returns:
            True if the method is registered, False otherwise.
        """
        return name in self._methods

    def get_method_info(self, name: str) -> dict[str, Any]:
        """Get information about a forecast method.

        Args:
            name: The name of the method.

        Returns:
            Dictionary with method information including docstring.

        Raises:
            ValueError: If the method is not registered.
        """
        method = self.get_method(name)
        return {
            "name": method.name,
            "class": method.__class__.__name__,
            "description": method.__class__.__doc__ or "No description available",
            "module": method.__class__.__module__,
        }


# Global registry instance
forecast_registry = ForecastMethodRegistry()


def get_forecast_method(name: str) -> ForecastMethod:
    """Get a forecast method from the global registry.

    This is a convenience function that uses the global registry.

    Args:
        name: The name of the method to retrieve.

    Returns:
        The requested forecast method.

    Raises:
        ValueError: If the method is not registered.
    """
    return forecast_registry.get_method(name)


def register_forecast_method(method: ForecastMethod) -> None:
    """Register a custom forecast method in the global registry.

    This is a convenience function that uses the global registry.

    Args:
        method: The forecast method to register.

    Raises:
        ValueError: If a method with the same name is already registered.
    """
    forecast_registry.register(method)



================================================================================
File: fin_statement_model/forecasting/types.py
================================================================================

"""Type definitions and data structures for forecasting module.

This module contains all the type aliases, enums, and data structures
used throughout the forecasting sub-module.
"""

from typing import Any, Union, Literal
from collections.abc import Callable
import numpy as np

from pydantic import BaseModel, ConfigDict, model_validator, ValidationError

from fin_statement_model.forecasting.errors import (
    ForecastMethodError,
    ForecastConfigurationError,
    ForecastResultError,
)

# Type aliases for clarity
Numeric = Union[int, float, np.number[Any]]
GrowthRate = Union[float, list[float], Callable[[], float]]
PeriodValue = dict[str, float]

# Forecast method types
ForecastMethodType = Literal[
    "simple",
    "curve",
    "statistical",
    "average",
    "historical_growth",
]


class StatisticalConfig(BaseModel):
    """Configuration for statistical forecasting method."""

    distribution: str
    params: dict[str, float]

    model_config = ConfigDict(extra="forbid")

    @model_validator(mode="after")  # type: ignore[arg-type]
    def _validate_distribution(cls, values: "StatisticalConfig") -> "StatisticalConfig":
        distribution = values.distribution
        params = values.params

        if distribution == "normal":
            required = {"mean", "std"}
            missing = required - params.keys()
            if missing:
                raise ForecastConfigurationError(
                    "Normal distribution requires 'mean' and 'std' parameters",
                    config=params,
                    missing_params=list(missing),
                )
        elif distribution == "uniform":
            required = {"low", "high"}
            missing = required - params.keys()
            if missing:
                raise ForecastConfigurationError(
                    "Uniform distribution requires 'low' and 'high' parameters",
                    config=params,
                    missing_params=list(missing),
                )
        else:
            raise ForecastConfigurationError(
                f"Unsupported distribution: {distribution}",
                config=params,
                invalid_params={"distribution": f"'{distribution}' is not supported"},
            )

        return values


class ForecastConfig(BaseModel):
    """Configuration for a forecast operation."""

    method: ForecastMethodType
    config: Any  # Method-specific configuration

    model_config = ConfigDict(extra="forbid")

    @model_validator(mode="after")  # type: ignore[arg-type]
    def _validate_config(cls, values: "ForecastConfig") -> "ForecastConfig":
        method = values.method
        cfg = values.config or {}

        valid_methods = {
            "simple",
            "curve",
            "statistical",
            "average",
            "historical_growth",
        }

        if method not in valid_methods:
            raise ForecastMethodError(
                "Invalid forecast method",
                method=method,
                supported_methods=list(valid_methods),
            )

        if method == "statistical":
            # Delegate validation to StatisticalConfig for detailed checks
            try:
                (
                    StatisticalConfig(**cfg)
                    if isinstance(cfg, dict)
                    else StatisticalConfig.model_validate(cfg)
                )
            except (ForecastConfigurationError, ValidationError) as exc:
                # Re-raise as ForecastConfigurationError for consistency
                raise ForecastConfigurationError(
                    "Invalid statistical configuration",
                    config=cfg,
                ) from exc

        return values


class ForecastResult(BaseModel):
    """Result of a forecast operation."""

    node_name: str
    periods: list[str]
    values: PeriodValue
    method: ForecastMethodType
    base_period: str

    model_config = ConfigDict(extra="forbid")

    def get_value(self, period: str) -> float:
        """Get the forecast value for a specific period."""
        if period not in self.values:
            raise ForecastResultError(
                f"Period {period} not found in forecast results",
                period=period,
                available_periods=list(self.values.keys()),
            )
        return self.values[period]



================================================================================
File: fin_statement_model/forecasting/validators.py
================================================================================

"""Input validation and error checking for forecasting operations.

This module provides validation utilities to ensure forecast inputs
are valid before processing.
"""

import logging
from typing import Any, Optional

from fin_statement_model.core.nodes import Node
from .types import ForecastMethodType, ForecastConfig

logger = logging.getLogger(__name__)


class ForecastValidator:
    """Validates inputs for forecasting operations.

    This class provides methods to validate various aspects of forecast
    inputs including periods, node configurations, and method parameters.
    """

    @staticmethod
    def validate_forecast_inputs(
        historical_periods: list[str],
        forecast_periods: list[str],
        node_configs: Optional[dict[str, dict[str, Any]]] = None,
    ) -> None:
        """Validate basic forecast inputs.

        Args:
            historical_periods: List of historical periods.
            forecast_periods: List of periods to forecast.
            node_configs: Optional node configuration mapping.

        Raises:
            ValueError: If inputs are logically invalid.
            TypeError: If inputs are of wrong type.
        """
        # Validate historical periods
        if not historical_periods:
            raise ValueError("No historical periods provided for forecasting")

        if not isinstance(historical_periods, list):
            raise TypeError(
                f"Historical periods must be a list, got {type(historical_periods)}"
            )

        # Validate forecast periods
        if not forecast_periods:
            raise ValueError("No forecast periods provided")

        if not isinstance(forecast_periods, list):
            raise TypeError(
                f"Forecast periods must be a list, got {type(forecast_periods)}"
            )

        # Check for overlapping periods
        historical_set = set(historical_periods)
        forecast_set = set(forecast_periods)
        overlap = historical_set & forecast_set
        if overlap:
            logger.warning(
                f"Forecast periods overlap with historical periods: {overlap}. "
                f"This may overwrite historical data."
            )

        # Validate node configs if provided
        if node_configs is not None:
            if not isinstance(node_configs, dict):
                raise TypeError(
                    f"Node configs must be a dict, got {type(node_configs)}"
                )

            for node_name, config in node_configs.items():
                ForecastValidator.validate_node_config(node_name, config)

    @staticmethod
    def validate_node_config(node_name: str, config: dict[str, Any]) -> None:
        """Validate configuration for a single node.

        Args:
            node_name: Name of the node being configured.
            config: Configuration dictionary for the node.

        Raises:
            ValueError: If configuration is logically invalid.
            TypeError: If configuration is of wrong type.
        """
        if not isinstance(config, dict):
            raise TypeError(
                f"Configuration for node '{node_name}' must be a dict, got {type(config)}"
            )

        # Validate method
        if "method" not in config:
            raise ValueError(
                f"Configuration for node '{node_name}' missing required 'method' key"
            )

        method = config["method"]
        valid_methods: list[ForecastMethodType] = [
            "simple",
            "curve",
            "statistical",
            "average",
            "historical_growth",
        ]
        if method not in valid_methods:
            raise ValueError(
                f"Invalid forecast method '{method}' for node '{node_name}'. "
                f"Valid methods: {valid_methods}"
            )

        # Validate config exists (can be None for some methods)
        if "config" not in config:
            raise ValueError(
                f"Configuration for node '{node_name}' missing required 'config' key"
            )

    @staticmethod
    def validate_node_for_forecast(node: Node, method: str) -> None:
        """Validate that a node can be forecasted with the given method.

        Args:
            node: The node to validate.
            method: The forecast method to use.

        Raises:
            ValueError: If node cannot be forecasted.
        """
        # Check if node has values dictionary
        if not hasattr(node, "values") or not isinstance(node.values, dict):
            raise ValueError(
                f"Node '{node.name}' cannot be forecasted: missing or invalid "
                f"'values' attribute. Only nodes with values dictionaries can "
                f"be forecasted."
            )

        # Check if node has calculate method for certain forecast types
        if method in ["average", "historical_growth"] and (
            not hasattr(node, "calculate") or not callable(node.calculate)
        ):
            raise ValueError(
                f"Node '{node.name}' cannot use '{method}' forecast method: "
                f"missing calculate() method"
            )

    @staticmethod
    def validate_forecast_config(config: dict[str, Any]) -> ForecastConfig:
        """Validate and convert a forecast configuration dictionary.

        Args:
            config: Raw configuration dictionary.

        Returns:
            Validated ForecastConfig instance.

        Raises:
            ValueError: If configuration is logically invalid.
            TypeError: If configuration is of wrong type.
        """
        if not isinstance(config, dict):
            raise TypeError(f"Forecast config must be a dict, got {type(config)}")

        if "method" not in config:
            raise ValueError("Forecast config missing required 'method' key")

        if "config" not in config:
            raise ValueError("Forecast config missing required 'config' key")

        # Create and validate using dataclass
        return ForecastConfig(method=config["method"], config=config["config"])

    @staticmethod
    def validate_base_period(
        base_period: str, available_periods: list[str], node_name: str
    ) -> None:
        """Validate that a base period is valid for forecasting.

        Args:
            base_period: The proposed base period.
            available_periods: List of available periods.
            node_name: Name of the node (for error messages).

        Raises:
            ValueError: If base period is invalid.
        """
        if not base_period:
            raise ValueError(f"No base period determined for node '{node_name}'")

        if base_period not in available_periods:
            raise ValueError(
                f"Base period '{base_period}' for node '{node_name}' not found in available periods"
            )

    @staticmethod
    def validate_forecast_result(
        result: dict[str, float], expected_periods: list[str], node_name: str
    ) -> None:
        """Validate forecast results.

        Args:
            result: Dictionary of period -> value mappings.
            expected_periods: List of expected forecast periods.
            node_name: Name of the node (for error messages).

        Raises:
            ValueError: If results are logically invalid or incomplete.
            TypeError: If results are of wrong type.
        """
        if not isinstance(result, dict):
            raise TypeError(
                f"Forecast result for node '{node_name}' must be a dict, got {type(result)}"
            )

        # Check all expected periods are present
        missing_periods = set(expected_periods) - set(result.keys())
        if missing_periods:
            raise ValueError(
                f"Forecast result for node '{node_name}' missing periods: {missing_periods}"
            )

        # Validate all values are numeric
        for period, value in result.items():
            if not isinstance(value, int | float):
                raise TypeError(
                    f"Forecast value for node '{node_name}' period '{period}' "
                    f"must be numeric, got {type(value)}"
                )



================================================================================
File: fin_statement_model/io/__init__.py
================================================================================

"""Input/Output components for the Financial Statement Model.

This package provides a unified interface for reading and writing financial model
data from/to various formats using a registry-based approach.
"""

import logging

from .core import (
    DataReader,
    DataWriter,
    get_reader,
    get_writer,
    list_readers,
    list_writers,
    read_data,
    write_data,
)
from .exceptions import IOError, ReadError, WriteError, FormatNotSupportedError
from . import formats  # noqa: F401
from . import specialized  # noqa: F401

# Import specialized functions for convenience
from .specialized import (
    import_from_cells,
    load_adjustments_from_excel,
    export_adjustments_to_excel,
    list_available_builtin_configs,
    read_builtin_statement_config,
    write_statement_to_excel,
    write_statement_to_json,
)

# Configure logging for the io package
logger = logging.getLogger(__name__)

# --- Public API ---

__all__ = [
    # Base classes
    "DataReader",
    "DataWriter",
    # Exceptions
    "FormatNotSupportedError",
    "IOError",
    "ReadError",
    "WriteError",
    # Specialized functions
    "export_adjustments_to_excel",
    # Registry functions
    "get_reader",
    "get_writer",
    "import_from_cells",
    "list_available_builtin_configs",
    "list_readers",
    "list_writers",
    "load_adjustments_from_excel",
    "read_builtin_statement_config",
    # Facade functions
    "read_data",
    "write_data",
    "write_statement_to_excel",
    "write_statement_to_json",
]



================================================================================
File: fin_statement_model/io/config/models.py
================================================================================

"""Pydantic models for IO reader and writer configuration.

This module provides declarative schemas for validating configuration passed to IO readers.
"""

from __future__ import annotations

from typing import Optional, Literal, Any, Union
from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator

from fin_statement_model.config.helpers import cfg
from fin_statement_model.core.adjustments.models import (
    AdjustmentFilterInput,
)


# Define MappingConfig locally to avoid circular import
MappingConfig = Union[dict[str, str], dict[Optional[str], dict[str, str]]]


class BaseReaderConfig(BaseModel):
    """Base configuration for IO readers."""

    source: Any = Field(
        ..., description="URI or path to data source (file path, ticker, etc.)"
    )
    format_type: Literal["csv", "excel", "dataframe", "dict", "fmp"] = Field(
        ..., description="Type of reader (csv, excel, dataframe, dict, fmp)."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class CsvReaderConfig(BaseReaderConfig):
    """CSV reader options."""

    # Falls back to cfg("io.default_csv_delimiter") when not supplied
    delimiter: str = Field(
        default_factory=lambda: cfg("io.default_csv_delimiter"),
        description="Field delimiter for CSV files.",
    )
    header_row: int = Field(
        1, description="Row number containing column names (1-indexed)."
    )
    index_col: Optional[int] = Field(
        None, description="1-indexed column for row labels."
    )
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    # Runtime override options: these will override config defaults when provided at read-time
    statement_type: Optional[
        Literal["income_statement", "balance_sheet", "cash_flow"]
    ] = Field(
        None,
        description="Type of statement ('income_statement', 'balance_sheet', 'cash_flow') to select mapping scope.",
    )
    item_col: Optional[str] = Field(
        None, description="Name of the column containing item identifiers."
    )
    period_col: Optional[str] = Field(
        None, description="Name of the column containing period identifiers."
    )
    value_col: Optional[str] = Field(
        None, description="Name of the column containing numeric values."
    )
    pandas_read_csv_kwargs: dict[str, Any] = Field(
        default_factory=dict,
        description="Additional kwargs for pandas.read_csv, overriding config defaults.",
    )

    @model_validator(mode="after")  # type: ignore[arg-type]
    def check_header_row(cls, cfg: CsvReaderConfig) -> CsvReaderConfig:
        """Ensure header_row is at least 1."""
        if cfg.header_row < 1:
            raise ValueError("header_row must be >= 1")
        return cfg


class ExcelReaderConfig(BaseReaderConfig):
    """Excel reader options."""

    # Uses cfg("io.default_excel_sheet") unless caller overrides
    sheet_name: Optional[str] = Field(
        default_factory=lambda: cfg("io.default_excel_sheet"),
        description="Worksheet name or index.",
    )
    items_col: int = Field(1, description="1-indexed column where item names reside.")
    periods_row: int = Field(1, description="1-indexed row where periods reside.")
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    statement_type: Optional[
        Literal["income_statement", "balance_sheet", "cash_flow"]
    ] = Field(
        None,
        description="Type of statement ('income_statement', 'balance_sheet', 'cash_flow'). Used to select a mapping scope.",
    )
    header_row: Optional[int] = Field(
        None, description="1-indexed row for pandas header reading."
    )
    nrows: Optional[int] = Field(
        None, description="Number of rows to read from the sheet."
    )
    skiprows: Optional[int] = Field(
        None, description="Number of rows to skip at the beginning."
    )

    @model_validator(mode="after")  # type: ignore[arg-type]
    def check_indices(cls, cfg: ExcelReaderConfig) -> ExcelReaderConfig:
        """Ensure items_col and periods_row are at least 1."""
        if cfg.items_col < 1 or cfg.periods_row < 1:
            raise ValueError("items_col and periods_row must be >= 1")
        return cfg


class FmpReaderConfig(BaseReaderConfig):
    """Financial Modeling Prep API reader options."""

    statement_type: Literal["income_statement", "balance_sheet", "cash_flow"] = Field(
        ..., description="Type of financial statement to fetch."
    )
    period_type: Literal["FY", "QTR"] = Field(
        "FY", description="Period type: 'FY' or 'QTR'."
    )
    limit: int = Field(5, description="Number of periods to fetch.")
    # Caller value → env var → cfg("api.fmp_api_key")
    api_key: Optional[str] = Field(
        default=None,
        description="Financial Modeling Prep API key.",
    )
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    @field_validator("api_key", mode="before")
    def load_api_key_env(cls, value: Optional[str]) -> Optional[str]:
        """Cascade lookup: explicit param → env → global config."""
        if value:
            return value
        import os

        return os.getenv("FMP_API_KEY") or cfg("api.fmp_api_key", None)

    @model_validator(mode="after")  # type: ignore[arg-type]
    def check_api_key(cls, cfg: FmpReaderConfig) -> FmpReaderConfig:
        """Ensure an API key is provided."""
        if not cfg.api_key:
            raise ValueError("api_key is required (env var FMP_API_KEY or param)")
        return cfg


# --- New Reader Configs for DataFrame and Dict readers ---


class DataFrameReaderConfig(BaseReaderConfig):
    """Configuration for DataFrameReader.

    No additional reader-specific options are required at the moment because
    the reader consumes an in-memory :class:`pandas.DataFrame` supplied to
    :py:meth:`DataFrameReader.read`.  The `source` field therefore serves only
    to preserve a consistent registry-initialisation contract.
    """

    source: Any = Field(..., description="In-memory pandas DataFrame source")
    format_type: Literal["dataframe"] = "dataframe"

    # Runtime override for read-time periods selection
    periods: Optional[list[str]] = Field(
        None,
        description="Optional list of periods (columns) to include when reading a DataFrame.",
    )


class DictReaderConfig(BaseReaderConfig):
    """Configuration for DictReader.

    Mirrors :class:`DataFrameReaderConfig` - no custom options yet.  The
    placeholder keeps the IO registry symmetric and future-proof.
    """

    source: dict[str, dict[str, float]] = Field(
        ..., description="In-memory dictionary source"
    )
    format_type: Literal["dict"] = "dict"

    # Runtime override for read-time periods selection
    periods: Optional[list[str]] = Field(
        None, description="Optional list of periods to include when reading a dict."
    )


# --- Writer-side Pydantic configuration models ---
class BaseWriterConfig(BaseModel):
    """Base configuration for IO writers."""

    target: Optional[str] = Field(
        None,
        description="URI or path to data target (file path, in-memory target, etc.)",
    )
    format_type: Literal["excel", "dataframe", "dict", "markdown"] = Field(
        ..., description="Type of writer (excel, dataframe, dict, markdown)."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class ExcelWriterConfig(BaseWriterConfig):
    """Excel writer options."""

    # Default comes from cfg("io.default_excel_sheet")
    sheet_name: str = Field(
        default_factory=lambda: cfg("io.default_excel_sheet"),
        description="Name of the sheet to write to.",
    )
    recalculate: bool = Field(
        True, description="Whether to recalculate graph before export."
    )
    include_nodes: Optional[list[str]] = Field(
        None, description="Optional list of node names to include in export."
    )
    excel_writer_kwargs: dict[str, Any] = Field(
        default_factory=dict,
        description="Additional kwargs for pandas.DataFrame.to_excel.",
    )


class DataFrameWriterConfig(BaseWriterConfig):
    """DataFrame writer options."""

    target: Optional[str] = Field(
        None, description="Optional target path (ignored by DataFrameWriter)."
    )
    recalculate: bool = Field(
        True, description="Whether to recalculate graph before export."
    )
    include_nodes: Optional[list[str]] = Field(
        None, description="Optional list of node names to include in export."
    )


class DictWriterConfig(BaseWriterConfig):
    """Dict writer has no additional options."""

    target: Optional[str] = Field(
        None, description="Optional target (ignored by DictWriter)."
    )


class MarkdownWriterConfig(BaseWriterConfig):
    """Markdown writer options.

    The writer can be configured in multiple ways:

    1. ``statement_config_path`` – Path to a YAML file containing the statement
       definition.  This mirrors historical behaviour where configurations were
       stored on disk.
    2. ``raw_configs`` – An *in-memory* mapping of statement IDs to their
       configuration dictionaries.  This is the preferred mechanism in the new
       API and aligns with ``create_statement_dataframe`` and other
       in-memory workflows.

    Only **one** of these two options is required.  If both are supplied the
    explicit ``raw_configs`` takes precedence.
    """

    # Either a file-based config …
    statement_config_path: Optional[str] = Field(
        None, description="Path to the statement definition YAML file."
    )

    # … or in-memory configs.
    raw_configs: Optional[dict[str, dict[str, Any]]] = Field(
        None,
        description=(
            "Mapping of statement IDs to configuration dictionaries.  This allows "
            "fully in-memory operation without relying on the filesystem."
        ),
    )

    historical_periods: Optional[list[str]] = Field(
        None, description="List of historical period names."
    )
    forecast_periods: Optional[list[str]] = Field(
        None, description="List of forecast period names."
    )
    adjustment_filter: Optional[AdjustmentFilterInput] = Field(
        None, description="Adjustment filter to apply."
    )
    forecast_configs: Optional[dict[str, Any]] = Field(
        None,
        description="Dictionary mapping node IDs to forecast configurations for notes.",
    )
    indent_spaces: int = Field(4, description="Number of spaces per indentation level.")
    target: Optional[str] = Field(
        None, description="Optional target path (ignored by MarkdownWriter)."
    )

    # --- Validators ------------------------------------------------------

    @model_validator(mode="after")  # type: ignore[arg-type]
    def check_exclusive_sources(
        cls, cfg: "MarkdownWriterConfig"
    ) -> "MarkdownWriterConfig":
        """Ensure that exactly one of statement_config_path or raw_configs is provided."""
        if not cfg.statement_config_path and cfg.raw_configs is None:
            raise ValueError(
                "Must provide either 'statement_config_path' or 'raw_configs' to "
                "MarkdownWriterConfig."
            )
        if cfg.statement_config_path and cfg.raw_configs is not None:
            # Prefer raw_configs but emit warning via logging
            import logging

            logging.getLogger(__name__).warning(
                "Both 'statement_config_path' and 'raw_configs' supplied to "
                "MarkdownWriterConfig – 'raw_configs' will take precedence."
            )
        return cfg



================================================================================
File: fin_statement_model/io/core/__init__.py
================================================================================

"""Core IO components including base classes, registry, and utilities."""

from .base import DataReader, DataWriter
from .facade import read_data, write_data
from .mixins import (
    FileBasedReader,
    ConfigurationMixin,
    DataFrameBasedWriter,
    ValueExtractionMixin,
    BatchProcessingMixin,
    ValidationResultCollector,
    handle_read_errors,
    handle_write_errors,
)
from .registry import (
    HandlerRegistry,
    get_reader,
    get_writer,
    list_readers,
    list_writers,
    register_reader,
    register_writer,
)

__all__ = [
    # Mixins and utilities
    "BatchProcessingMixin",
    "ConfigurationMixin",
    "DataFrameBasedWriter",
    # Base classes
    "DataReader",
    "DataWriter",
    "FileBasedReader",
    # Registry
    "HandlerRegistry",
    "ValidationResultCollector",
    "ValueExtractionMixin",
    "get_reader",
    "get_writer",
    "handle_read_errors",
    "handle_write_errors",
    "list_readers",
    "list_writers",
    # Facade functions
    "read_data",
    "register_reader",
    "register_writer",
    "write_data",
]



================================================================================
File: fin_statement_model/io/core/base.py
================================================================================

"""Base classes for data readers and writers."""

from abc import ABC, abstractmethod
from typing import Any

# Use absolute import based on project structure
from fin_statement_model.core.graph import Graph


class DataReader(ABC):
    """Abstract base class for all data readers.

    Defines the interface for classes that read data from various sources
    and typically populate or return a Graph object.
    """

    @abstractmethod
    def read(self, source: Any, **kwargs: dict[str, Any]) -> Graph:
        """Read data from the specified source and return a Graph.

        Args:
            source: The data source. Type depends on the reader implementation
                (e.g., file path `str`, ticker `str`, `pd.DataFrame`, `dict`).
            **kwargs: Additional format-specific options for reading.

        Returns:
            A Graph object populated with the data from the source.

        Raises:
            ReadError: If an error occurs during the reading process.
            NotImplementedError: If the method is not implemented by a subclass.
        """
        raise NotImplementedError


class DataWriter(ABC):
    """Abstract base class for all data writers.

    Defines the interface for classes that write graph data to various targets.
    """

    @abstractmethod
    def write(self, graph: Graph, target: Any, **kwargs: dict[str, Any]) -> object:
        """Write data from the Graph object to the specified target.

        Args:
            graph: The Graph object containing the data to write.
            target: The destination target. Type depends on the writer implementation
                (e.g., file path `str`, or ignored if the writer returns an object).
            **kwargs: Additional format-specific options for writing.

        Raises:
            WriteError: If an error occurs during the writing process.
            NotImplementedError: If the method is not implemented by a subclass.
        """
        raise NotImplementedError



================================================================================
File: fin_statement_model/io/core/facade.py
================================================================================

"""Facade functions for simplified IO operations.

This module provides the main public API for reading and writing data,
abstracting away the complexity of the registry system.
"""

import logging
from typing import Union, Any

from fin_statement_model.core.graph import Graph
from .registry import get_reader, get_writer
from fin_statement_model.io.exceptions import (
    IOError,
    ReadError,
    WriteError,
    FormatNotSupportedError,
)

logger = logging.getLogger(__name__)


def read_data(
    format_type: str, source: Any, **kwargs: dict[str, Union[str, int, float, bool]]
) -> Graph:
    """Reads data from a source using the specified format.

    This function acts as a facade for the underlying reader implementations.
    It uses the `format_type` to look up the appropriate reader class in the registry.
    The `source` and `**kwargs` are combined and validated against the specific
    reader's Pydantic configuration model (e.g., `CsvReaderConfig`).

    The validated configuration is used to initialize the reader instance.
    The `source` (which might be the original object for dict/dataframe formats, or
    the validated string path/ticker otherwise) and the original `**kwargs` are then
    passed to the reader instance's `.read()` method, which handles format-specific
    read-time options.

    Args:
        format_type (str): The format identifier (e.g., 'excel', 'csv', 'fmp', 'dict').
        source (Any): The data source. Its type depends on `format_type`:
            - `str`: file path (for 'excel', 'csv'), ticker symbol (for 'fmp').
            - `pd.DataFrame`: for 'dataframe'.
            - `dict`: for 'dict'.
        **kwargs: Additional keyword arguments used for reader configuration (e.g.,
            `api_key`, `delimiter`, `sheet_name`, `mapping_config`) and potentially
            passed to the reader's `.read()` method (e.g., `periods`). Consult the
            specific reader's Pydantic config model and `.read()` docstring.

    Returns:
        Graph: A new Graph object populated with the read data.

    Raises:
        ReadError: If reading fails.
        FormatNotSupportedError: If the format_type is not registered.
        Exception: Other errors during reader initialization or reading.
    """
    logger.info(
        f"Attempting to read data using format '{format_type}' from source type '{type(source).__name__}'"
    )

    # Prepare kwargs for registry validation (includes source and format_type)
    config_kwargs = {**kwargs, "source": source, "format_type": format_type}
    # Keep separate kwargs for the read method itself (e.g., 'periods')
    # This assumes Pydantic configs *don't* capture read-time args.

    try:
        # Pass the config kwargs directly to get_reader
        reader = get_reader(**config_kwargs)

        # Determine the actual source object for the read method
        actual_source = (
            source if format_type in ("dict", "dataframe") else config_kwargs["source"]
        )

        # Pass the determined source and the original kwargs (excluding config keys potentially)
        # to the read method. Specific readers handle relevant kwargs.
        return reader.read(actual_source, **kwargs)
    except (IOError, FormatNotSupportedError):
        logger.exception("IO Error reading data")
        raise  # Re-raise specific IO errors
    except Exception as e:
        logger.exception(f"Unexpected error reading data with format '{format_type}'")
        # Wrap unexpected errors in ReadError for consistency?
        raise ReadError(
            "Unexpected error during read",
            source=str(source),
            reader_type=format_type,
            original_error=e,
        ) from e


def write_data(
    format_type: str,
    graph: Graph,
    target: Any,
    **kwargs: dict[str, Union[str, int, float, bool]],
) -> object:
    """Writes graph data to a target using the specified format.

    Similar to `read_data`, this acts as a facade for writer implementations.
    It uses `format_type` to find the writer class in the registry.
    The `target` and `**kwargs` are combined and validated against the specific
    writer's Pydantic configuration model (e.g., `ExcelWriterConfig`).

    The validated configuration initializes the writer instance.
    The original `graph`, `target`, and `**kwargs` are then passed to the writer
    instance's `.write()` method for format-specific write-time options.

    Args:
        format_type (str): The format identifier (e.g., 'excel', 'dataframe', 'dict').
        graph (Graph): The graph object containing data to write.
        target (Any): The destination target. Its type depends on `format_type`:
            - `str`: file path (usually required for file-based writers like 'excel').
            - Ignored: for writers that return objects (like 'dataframe', 'dict').
        **kwargs: Additional keyword arguments used for writer configuration (e.g.,
            `sheet_name`, `recalculate`) and potentially passed to the writer's
            `.write()` method. Consult the specific writer's Pydantic config model
            and `.write()` docstring.

    Returns:
        object: The result of the write operation. For writers like DataFrameWriter
                or DictWriter, this is the created object. For file writers, it's None.

    Raises:
        WriteError: If writing fails.
        FormatNotSupportedError: If the format_type is not registered.
        Exception: Other errors during writer initialization or writing.
    """
    logger.info(
        f"Attempting to write graph data using format '{format_type}' to target type '{type(target).__name__}'"
    )

    # Prepare kwargs for registry validation (includes target and format_type)
    config_kwargs = {**kwargs, "target": target, "format_type": format_type}

    # Pass the config kwargs directly to get_writer
    writer = get_writer(**config_kwargs)
    # Now call write with all writer-specific kwargs
    try:
        # Pass original graph, target, and non-config kwargs to write()
        result = writer.write(graph, target, **kwargs)

        # If the writer returns a string and target is a path, write it to the file.
        if isinstance(result, str) and isinstance(target, str):
            try:
                logger.debug(
                    f"Writing string result from writer '{type(writer).__name__}' to file: {target}"
                )
                with open(target, "w", encoding="utf-8") as f:
                    f.write(result)
                return None  # Consistent return for file writers
            except OSError as e:
                logger.exception(
                    f"Failed to write writer output to target file: {target}"
                )
                raise WriteError(
                    f"Failed to write writer output to file: {target}",
                    target=target,
                    writer_type=format_type,
                    original_error=e,
                ) from e
        else:
            # Otherwise, return the original result (e.g., DataFrame, dict)
            return result
    except (IOError, FormatNotSupportedError):
        logger.exception("IO Error writing data")
        raise  # Re-raise specific IO errors
    except Exception as e:
        logger.exception(f"Unexpected error writing data with format '{format_type}'")
        # Wrap unexpected errors
        raise WriteError(
            "Unexpected error during write",
            target=str(target),
            writer_type=format_type,
            original_error=e,
        ) from e


__all__ = ["read_data", "write_data"]



================================================================================
File: fin_statement_model/io/core/mixins.py
================================================================================

"""Reusable mixins and decorators for IO operations.

This module provides shared functionality for readers and writers including
error handling decorators and mixins for consistent behavior.
"""

import os
import functools
import logging
import importlib.resources
import yaml
from abc import abstractmethod
from typing import Any, TypeVar, Optional, ClassVar
from collections.abc import Callable

from fin_statement_model.core.graph import Graph
from .base import DataReader
from fin_statement_model.io.exceptions import ReadError, WriteError
from fin_statement_model.io.core.utils import normalize_mapping, MappingConfig

logger = logging.getLogger(__name__)

F = TypeVar("F", bound=Callable[..., Any])


# ===== Mapping Mixins =====


class MappingAwareMixin:
    """Mixin for readers that support name mapping functionality.

    Provides common methods for:
    - Loading default mappings from YAML files
    - Getting effective mappings based on context
    - Applying mappings to source names
    """

    # Class variable to store default mappings per reader type
    _default_mappings_cache: ClassVar[dict[str, MappingConfig]] = {}

    @classmethod
    def _get_default_mapping_path(cls) -> Optional[str]:
        """Get the path to default mapping YAML file for this reader.

        Subclasses should override this to specify their default mapping file.

        Returns:
            Path relative to fin_statement_model.io.config.mappings package,
            or None if no default mappings.
        """
        return None

    @classmethod
    def _load_default_mappings(cls) -> MappingConfig:
        """Load default mapping configurations from YAML file.

        Returns:
            Dictionary containing default mappings, empty if none found.
        """
        cache_key = cls.__name__

        # Return cached value if available
        if cache_key in cls._default_mappings_cache:
            return cls._default_mappings_cache[cache_key]

        mapping_path = cls._get_default_mapping_path()
        if not mapping_path:
            cls._default_mappings_cache[cache_key] = {}
            return {}

        try:
            yaml_content = (
                importlib.resources.files("fin_statement_model.io.config.mappings")
                .joinpath(mapping_path)
                .read_text(encoding="utf-8")
            )
            mappings = yaml.safe_load(yaml_content) or {}
            cls._default_mappings_cache[cache_key] = mappings
            logger.debug(f"Loaded default mappings for {cls.__name__}")
            return mappings
        except Exception:
            logger.exception(f"Failed to load default mappings for {cls.__name__}")
            cls._default_mappings_cache[cache_key] = {}
            return {}

    def _get_mapping(self, context_key: Optional[str] = None) -> dict[str, str]:
        """Get the effective mapping based on context and configuration.

        Args:
            context_key: Optional context (e.g., sheet name, statement type)
                        to select specific mapping scope.

        Returns:
            Flat dictionary mapping source names to canonical names.

        Raises:
            TypeError: If mapping configuration is invalid.
        """
        # Load defaults if not already loaded
        default_mappings = self._load_default_mappings()

        # Get user config
        user_config = self.get_config_value("mapping_config")

        # Start with defaults for the context
        mapping = normalize_mapping(default_mappings, context_key=context_key)

        # Overlay user mappings if provided
        if user_config:
            user_mapping = normalize_mapping(user_config, context_key=context_key)
            mapping.update(user_mapping)

        return mapping

    def _apply_mapping(self, source_name: str, mapping: dict[str, str]) -> str:
        """Apply mapping to convert source name to canonical name.

        Args:
            source_name: Original name from the data source
            mapping: Mapping dictionary

        Returns:
            Canonical name (mapped name or original if no mapping exists)
        """
        return mapping.get(source_name, source_name)

    def get_config_value(
        self,
        key: str,
        default: Any = None,
        required: bool = False,
        value_type: Optional[type] = None,
        validator: Optional[Callable[[Any], bool]] = None,
    ) -> Any:
        """Stub for configuration value retrieval; overridden by ConfigurationMixin."""
        ...


# ===== Validation Mixins =====


class ValidationMixin:
    """Mixin for readers that need comprehensive validation capabilities.

    Provides standardized validation methods for common data validation scenarios
    including data type validation, range validation, and custom validation rules.
    """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        """Initialize validation mixin."""
        super().__init__(*args, **kwargs)
        self._validation_context: dict[str, Any] = {}

    def set_validation_context(self, **context: Any) -> None:
        """Set validation context for error reporting.

        Args:
            **context: Key-value pairs to store as validation context.
        """
        self._validation_context.update(context)

    def get_validation_context(self) -> dict[str, Any]:
        """Get current validation context.

        Returns:
            Dictionary containing current validation context.
        """
        return self._validation_context.copy()

    def validate_required_columns(
        self,
        df: Any,  # pandas.DataFrame
        required_columns: list[str],
        source_identifier: str = "data source",
    ) -> None:
        """Validate that required columns exist in DataFrame.

        Args:
            df: DataFrame to validate.
            required_columns: List of required column names.
            source_identifier: Identifier for the data source (for error messages).

        Raises:
            ReadError: If required columns are missing.
        """
        if not hasattr(df, "columns"):
            raise ReadError(
                "Invalid data structure: expected DataFrame with columns attribute",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

        missing_columns = set(required_columns) - set(df.columns)
        if missing_columns:
            raise ReadError(
                f"Missing required columns in {source_identifier}: {missing_columns}",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

    def validate_column_bounds(
        self,
        df: Any,  # pandas.DataFrame
        column_index: int,
        source_identifier: str = "data source",
        context: str = "column",
    ) -> None:
        """Validate that column index is within DataFrame bounds.

        Args:
            df: DataFrame to validate.
            column_index: 0-based column index to validate.
            source_identifier: Identifier for the data source.
            context: Context description for error messages.

        Raises:
            ReadError: If column index is out of bounds.
        """
        if not hasattr(df, "columns"):
            raise ReadError(
                "Invalid data structure: expected DataFrame with columns attribute",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

        if column_index >= len(df.columns) or column_index < 0:
            raise ReadError(
                f"{context} index ({column_index + 1}) is out of bounds. "
                f"Found {len(df.columns)} columns.",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

    def validate_periods_exist(
        self,
        periods: list[str],
        source_identifier: str = "data source",
        min_periods: int = 1,
    ) -> None:
        """Validate that periods list is not empty and meets minimum requirements.

        Args:
            periods: List of period identifiers.
            source_identifier: Identifier for the data source.
            min_periods: Minimum number of periods required.

        Raises:
            ReadError: If periods validation fails.
        """
        if not periods:
            raise ReadError(
                f"No periods found in {source_identifier}",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

        if len(periods) < min_periods:
            raise ReadError(
                f"Insufficient periods in {source_identifier}. "
                f"Found {len(periods)}, minimum required: {min_periods}",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

    def validate_numeric_value(  # noqa: PLR0911
        self,
        value: Any,
        item_name: str,
        period: str,
        validator: Optional["ValidationResultCollector"] = None,
        allow_conversion: bool = True,
    ) -> tuple[bool, Optional[float]]:
        """Validate and optionally convert a value to numeric.

        Args:
            value: Value to validate.
            item_name: Name of the item (for error reporting).
            period: Period identifier (for error reporting).
            validator: Optional ValidationResultCollector to record errors.
            allow_conversion: Whether to attempt string-to-float conversion.

        Returns:
            Tuple of (is_valid, converted_value).
            If is_valid is False, converted_value will be None.
        """
        import pandas as pd

        # Skip NaN/None values
        if pd.isna(value) or value is None:
            return True, None

        # Already numeric
        if isinstance(value, int | float):
            if not pd.isfinite(value):
                error_msg = f"Non-finite numeric value '{value}' for period '{period}'"
                if validator:
                    validator.add_result(item_name, False, error_msg)
                return False, None
            return True, float(value)

        # Attempt conversion if allowed
        if allow_conversion:
            try:
                converted = float(value)
                if not pd.isfinite(converted):
                    error_msg = f"Converted to non-finite value '{converted}' for period '{period}'"
                    if validator:
                        validator.add_result(item_name, False, error_msg)
                    return False, None
                return True, converted
            except (ValueError, TypeError):
                error_msg = f"Non-numeric value '{value}' for period '{period}'"
                if validator:
                    validator.add_result(item_name, False, error_msg)
                return False, None

        # Not numeric and conversion not allowed
        error_msg = f"Non-numeric value '{value}' for period '{period}'"
        if validator:
            validator.add_result(item_name, False, error_msg)
        return False, None

    def validate_node_name(
        self,
        node_name: Any,
        source_name: str = "",
        allow_empty: bool = False,
    ) -> tuple[bool, Optional[str]]:
        """Validate and normalize a node name.

        Args:
            node_name: Raw node name to validate.
            source_name: Original source name (for error context).
            allow_empty: Whether to allow empty/None node names.

        Returns:
            Tuple of (is_valid, normalized_name).
            If is_valid is False, normalized_name will be None.
        """
        import pandas as pd

        if pd.isna(node_name) or node_name is None:
            return allow_empty, None

        if not node_name or (isinstance(node_name, str) and not node_name.strip()):
            return allow_empty, None

        # Normalize to string and strip whitespace
        normalized = str(node_name).strip()
        return True, normalized

    def create_validation_summary(
        self,
        validator: "ValidationResultCollector",
        source_identifier: str,
        operation: str = "processing",
    ) -> str:
        """Create a formatted validation summary message.

        Args:
            validator: ValidationResultCollector with results.
            source_identifier: Identifier for the data source.
            operation: Description of the operation being performed.

        Returns:
            Formatted summary message.
        """
        summary = validator.get_summary()

        if not validator.has_errors():
            return f"Successfully completed {operation} {source_identifier}"

        error_summary = (
            f"Validation errors occurred during {operation} {source_identifier}: "
        )
        error_details = "; ".join(summary["errors"][:5])  # Limit to first 5 errors

        if len(summary["errors"]) > 5:
            error_details += f" (and {len(summary['errors']) - 5} more errors)"

        return error_summary + error_details


# ===== Error Handling Decorators =====


def handle_read_errors(source_attr: str = "source") -> Callable[[F], F]:
    """Decorator to standardize error handling for readers.

    This decorator catches common exceptions during read operations and
    converts them to appropriate ReadError instances with consistent
    error messages and context.

    Args:
        source_attr: Name of the attribute containing the source identifier.
                    Defaults to "source".

    Returns:
        Decorated function that handles errors consistently.
    """

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(self: Any, source: Any, **kwargs: Any) -> Any:
            try:
                return func(self, source, **kwargs)
            except ReadError:
                raise  # Re-raise our own errors without modification
            except FileNotFoundError as e:
                raise ReadError(
                    f"File not found: {source}",
                    source=str(source),
                    reader_type=self.__class__.__name__,
                    original_error=e,
                )
            except ValueError as e:
                raise ReadError(
                    f"Invalid value encountered: {e}",
                    source=str(source),
                    reader_type=self.__class__.__name__,
                    original_error=e,
                )
            except Exception as e:
                logger.error(f"Failed to read from {source}: {e}", exc_info=True)
                raise ReadError(
                    f"Failed to process source: {e}",
                    source=str(source),
                    reader_type=self.__class__.__name__,
                    original_error=e,
                ) from e

        return wrapper  # type: ignore

    return decorator


def handle_write_errors(target_attr: str = "target") -> Callable[[F], F]:
    """Decorator to standardize error handling for writers.

    Similar to handle_read_errors but for write operations.

    Args:
        target_attr: Name of the attribute containing the target identifier.
                    Defaults to "target".

    Returns:
        Decorated function that handles errors consistently.
    """

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(self: Any, graph: Any, target: Any = None, **kwargs: Any) -> Any:
            try:
                return func(self, graph, target, **kwargs)
            except WriteError:
                raise  # Re-raise our own errors without modification
            except Exception as e:
                logger.error(f"Failed to write to {target}: {e}", exc_info=True)
                raise WriteError(
                    f"Failed to write data: {e}",
                    target=str(target) if target else "unknown",
                    writer_type=self.__class__.__name__,
                    original_error=e,
                ) from e

        return wrapper  # type: ignore

    return decorator


# ===== Reader Mixins =====


class FileBasedReader(DataReader):
    """Base class for file-based readers with common validation.

    This class provides common file validation methods and ensures
    consistent error handling for all file-based readers.

    Note: Subclasses should apply the @handle_read_errors() decorator
    to their read() method implementation for consistent error handling.
    """

    def validate_file_exists(self, path: str) -> None:
        """Validate that file exists.

        Args:
            path: Path to the file to validate.

        Raises:
            ReadError: If the file does not exist.
        """
        if not os.path.exists(path):
            raise ReadError(
                f"File not found: {path}",
                source=path,
                reader_type=self.__class__.__name__,
            )

    def validate_file_extension(
        self, path: str, valid_extensions: tuple[str, ...]
    ) -> None:
        """Validate file has correct extension.

        Args:
            path: Path to the file to validate.
            valid_extensions: Tuple of valid file extensions (e.g., ('.csv', '.txt')).

        Raises:
            ReadError: If the file extension is not valid.
        """
        if not path.lower().endswith(valid_extensions):
            raise ReadError(
                f"Invalid file extension. Expected one of {valid_extensions}, "
                f"got '{os.path.splitext(path)[1]}'",
                source=path,
                reader_type=self.__class__.__name__,
            )

    @abstractmethod
    def read(self, source: str, **kwargs: Any) -> Graph:
        """Read from file source.

        Subclasses must implement this method with their specific
        file reading logic. It's recommended to apply the @handle_read_errors()
        decorator to the implementation.

        Args:
            source: Path to the file to read.
            **kwargs: Additional reader-specific options.

        Returns:
            Graph populated with data from the file.
        """


class ConfigurationMixin:
    """Enhanced mixin for readers that use configuration objects.

    Provides comprehensive configuration management including:
    - Safe configuration value access with defaults
    - Configuration validation and type checking
    - Configuration inheritance and merging
    - Environment variable integration
    - Configuration context tracking
    """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        """Initialize configuration mixin."""
        super().__init__(*args, **kwargs)
        self._config_context: dict[str, Any] = {}
        self._config_overrides: dict[str, Any] = {}

    def set_config_context(self, **context: Any) -> None:
        """Set configuration context for enhanced error reporting.

        Args:
            **context: Key-value pairs to store as configuration context.
        """
        self._config_context.update(context)

    def get_config_context(self) -> dict[str, Any]:
        """Get current configuration context.

        Returns:
            Dictionary containing current configuration context.
        """
        return self._config_context.copy()

    def set_config_override(self, key: str, value: Any) -> None:
        """Set a configuration override for runtime customization.

        Args:
            key: Configuration key to override.
            value: Override value.
        """
        self._config_overrides[key] = value

    def clear_config_overrides(self) -> None:
        """Clear all configuration overrides."""
        self._config_overrides.clear()

    def get_config_value(
        self,
        key: str,
        default: Any = None,
        required: bool = False,
        value_type: Optional[type] = None,
        validator: Optional[Callable[[Any], bool]] = None,
    ) -> Any:
        """Get a configuration value with comprehensive validation and fallback.

        Args:
            key: Configuration key to retrieve.
            default: Default value if key is not found.
            required: Whether the configuration value is required.
            value_type: Expected type for the configuration value.
            validator: Optional validation function that takes the value and returns bool.

        Returns:
            Configuration value, override, or default.

        Raises:
            ReadError: If required value is missing or validation fails.
        """
        # Check for runtime overrides first
        if key in self._config_overrides:
            value = self._config_overrides[key]
        # Then check configuration object
        elif hasattr(self, "cfg") and self.cfg:
            value = getattr(self.cfg, key, default)
        else:
            value = default

        # Handle required values
        if required and value is None:
            raise ReadError(
                f"Required configuration value '{key}' is missing",
                reader_type=self.__class__.__name__,
            )

        # Type validation
        if (
            value is not None
            and value_type is not None
            and not isinstance(value, value_type)
        ):
            try:
                # Attempt type conversion
                value = value_type(value)
            except (ValueError, TypeError) as e:
                raise ReadError(
                    f"Configuration value '{key}' has invalid type. "
                    f"Expected {value_type.__name__}, got {type(value).__name__}",
                    reader_type=self.__class__.__name__,
                    original_error=e,
                )

        # Custom validation
        if value is not None and validator is not None:
            try:
                if not validator(value):
                    raise ReadError(
                        f"Configuration value '{key}' failed validation",
                        reader_type=self.__class__.__name__,
                    )
            except Exception as e:
                raise ReadError(
                    f"Configuration validation error for '{key}': {e}",
                    reader_type=self.__class__.__name__,
                    original_error=e,
                )

        return value

    def require_config_value(
        self,
        key: str,
        value_type: Optional[type] = None,
        validator: Optional[Callable[[Any], bool]] = None,
    ) -> Any:
        """Get a required configuration value with validation.

        Args:
            key: Configuration key to retrieve.
            value_type: Expected type for the configuration value.
            validator: Optional validation function.

        Returns:
            Configuration value.

        Raises:
            ReadError: If the configuration value is missing or invalid.
        """
        return self.get_config_value(
            key, required=True, value_type=value_type, validator=validator
        )

    def get_config_with_env_fallback(
        self,
        key: str,
        env_var: str,
        default: Any = None,
        value_type: Optional[type] = None,
    ) -> Any:
        """Get configuration value with environment variable fallback.

        Args:
            key: Configuration key to retrieve.
            env_var: Environment variable name to check as fallback.
            default: Default value if neither config nor env var is found.
            value_type: Expected type for the value.

        Returns:
            Configuration value, environment variable value, or default.
        """
        import os

        # First try configuration
        value = self.get_config_value(key)

        # If not found, try environment variable
        if value is None:
            env_value = os.getenv(env_var)
            if env_value is not None:
                value = env_value

        # Use default if still None
        if value is None:
            value = default

        # Type conversion if needed
        if (
            value is not None
            and value_type is not None
            and not isinstance(value, value_type)
        ):
            try:
                value = value_type(value)
            except (ValueError, TypeError):
                logger.warning(
                    f"Failed to convert {key} value '{value}' to {value_type.__name__}, using as-is"
                )

        return value

    def validate_configuration(self) -> "ValidationResultCollector":
        """Validate the entire configuration object.

        Returns:
            ValidationResultCollector with validation results.
        """
        validator = ValidationResultCollector(context=self._config_context)

        if not hasattr(self, "cfg") or not self.cfg:
            validator.add_result(
                "configuration", False, "Missing configuration object", "structure"
            )
            return validator

        # Validate configuration object using Pydantic if available
        try:
            if hasattr(self.cfg, "model_validate"):
                # It's a Pydantic model, validation already happened during creation
                validator.add_result(
                    "configuration", True, "Configuration object is valid", "structure"
                )
            else:
                validator.add_result(
                    "configuration",
                    True,
                    "Configuration object exists (non-Pydantic)",
                    "structure",
                )
        except Exception as e:
            validator.add_result(
                "configuration",
                False,
                f"Configuration validation failed: {e}",
                "validation",
            )

        return validator

    def get_effective_configuration(self) -> dict[str, Any]:
        """Get the effective configuration including overrides.

        Returns:
            Dictionary containing all configuration values with overrides applied.
        """
        config_dict = {}

        # Start with base configuration
        if hasattr(self, "cfg") and self.cfg:
            if hasattr(self.cfg, "model_dump"):
                # Pydantic model
                config_dict = self.cfg.model_dump()
            elif hasattr(self.cfg, "__dict__"):
                # Regular object
                config_dict = vars(self.cfg).copy()

        # Apply overrides
        config_dict.update(self._config_overrides)

        return config_dict

    def merge_configurations(self, *configs: Any) -> dict[str, Any]:
        """Merge multiple configuration objects with precedence.

        Args:
            *configs: Configuration objects to merge (later ones take precedence).

        Returns:
            Merged configuration dictionary.
        """
        merged = {}

        for config in configs:
            if config is None:
                continue

            if hasattr(config, "model_dump"):
                # Pydantic model
                config_dict = config.model_dump()
            elif hasattr(config, "__dict__"):
                # Regular object
                config_dict = vars(config).copy()
            elif isinstance(config, dict):
                # Dictionary
                config_dict = config.copy()
            else:
                logger.warning(f"Unsupported configuration type: {type(config)}")
                continue

            merged.update(config_dict)

        return merged


# ===== Writer Mixins =====


class ValueExtractionMixin:
    """Mixin for consistent value extraction from nodes.

    This mixin provides a standardized way to extract values from nodes,
    handling both calculated values and stored values with proper error
    handling.
    """

    def extract_node_value(
        self,
        node: Any,  # Avoid circular import with Node type
        period: str,
        calculate: bool = True,
    ) -> Optional[float]:
        """Extract value from node with consistent error handling.

        Args:
            node: The node to extract value from.
            period: The period to get the value for.
            calculate: If True, attempt to calculate the value using node.calculate().
                      If False, only look for stored values.

        Returns:
            The extracted value as a float, or None if no value could be extracted.
        """
        try:
            # First try calculation if enabled and method exists
            if calculate and hasattr(node, "calculate") and callable(node.calculate):
                value = node.calculate(period)
                if isinstance(value, int | float):
                    return float(value)

            # Fall back to stored values
            if hasattr(node, "values") and isinstance(node.values, dict):
                value = node.values.get(period)
                if isinstance(value, int | float):
                    return float(value)

            return None

        except Exception as e:
            logger.debug(
                f"Failed to extract value from node '{getattr(node, 'name', 'unknown')}' "
                f"for period '{period}': {e}"
            )
            return None


class DataFrameBasedWriter(ValueExtractionMixin):
    """Base class for writers that convert to DataFrame format.

    This base class provides common functionality for writers that
    need to extract data from a graph into a tabular format.

    Note: Subclasses should apply the @handle_write_errors() decorator
    to their write() method implementation for consistent error handling.
    """

    def extract_graph_data(
        self,
        graph: Graph,
        include_nodes: Optional[list[str]] = None,
        calculate: bool = True,
    ) -> dict[str, dict[str, float]]:
        """Extract data from graph nodes into a dictionary format.

        Args:
            graph: The graph to extract data from.
            include_nodes: Optional list of node names to include.
                          If None, includes all nodes.
            calculate: Whether to calculate values or just use stored values.

        Returns:
            Dictionary mapping node names to period-value dictionaries.
        """
        import numpy as np

        periods = sorted(graph.periods) if graph.periods else []
        data: dict[str, dict[str, float]] = {}

        # Determine which nodes to process
        nodes_to_process = include_nodes if include_nodes else list(graph.nodes.keys())

        # Validate requested nodes exist
        if include_nodes:
            missing_nodes = [n for n in include_nodes if n not in graph.nodes]
            if missing_nodes:
                logger.warning(f"Requested nodes not found in graph: {missing_nodes}")
                nodes_to_process = [n for n in include_nodes if n in graph.nodes]

        # Extract data for each node
        for node_id in nodes_to_process:
            node = graph.nodes[node_id]
            row: dict[str, float] = {}

            for period in periods:
                # Use the mixin's extract method for consistent value extraction
                value = self.extract_node_value(node, period, calculate=calculate)

                # Convert None to NaN for DataFrame compatibility
                if (
                    value is None
                    or not isinstance(value, int | float | np.number)
                    or not np.isfinite(value)
                ):
                    value = np.nan

                row[period] = float(value)

            data[node_id] = row

        return data

    @abstractmethod
    def write(self, graph: Graph, target: Any = None, **kwargs: Any) -> Any:
        """Write graph data to target.

        Subclasses must implement this method with their specific
        writing logic. It's recommended to apply the @handle_write_errors()
        decorator to the implementation.

        Args:
            graph: Graph containing data to write.
            target: Target for the output (file path, etc.).
            **kwargs: Additional writer-specific options.

        Returns:
            Writer-specific return value.
        """


# ===== Utility Classes =====


class BatchProcessingMixin:
    """Mixin for readers/writers that process data in batches.

    Provides utilities for chunking data and progress reporting.
    """

    def __init__(self, batch_size: int = 1000):
        """Initialize with batch size.

        Args:
            batch_size: Number of items to process in each batch.
        """
        self.batch_size = batch_size
        self._processed_count = 0
        self._total_count = 0

    def process_in_batches(
        self,
        items: list[Any],
        process_func: Callable[[list[Any]], list[Any]],
        progress_callback: Optional[Callable[[int, int], Any]] = None,
    ) -> list[Any]:
        """Process items in batches.

        Args:
            items: List of items to process.
            process_func: Function to apply to each batch.
            progress_callback: Optional callback for progress updates.

        Returns:
            List of results from processing all batches.
        """
        results = []
        self._total_count = len(items)
        self._processed_count = 0

        for i in range(0, len(items), self.batch_size):
            batch = items[i : i + self.batch_size]
            batch_results = process_func(batch)
            results.extend(batch_results)

            self._processed_count += len(batch)

            if progress_callback:
                progress_callback(self._processed_count, self._total_count)

            # Log progress
            if self._processed_count % (self.batch_size * 10) == 0:
                logger.info(
                    f"Processed {self._processed_count}/{self._total_count} items "
                    f"({self._processed_count / self._total_count * 100:.1f}%)"
                )

        return results

    def get_progress(self) -> tuple[int, int]:
        """Get current progress.

        Returns:
            Tuple of (processed_count, total_count).
        """
        return self._processed_count, self._total_count


class ValidationResultCollector:
    """Enhanced utility class for collecting and summarizing validation results.

    Useful for batch operations where you want to collect all validation
    results and report them together. Supports categorization, context tracking,
    and detailed metrics.
    """

    def __init__(self, context: Optional[dict[str, Any]] = None):
        """Initialize the validation result collector.

        Args:
            context: Optional context information for validation.
        """
        self.results: list[tuple[str, bool, str, str]] = (
            []
        )  # item, valid, message, category
        self.errors: list[str] = []
        self.warnings: list[str] = []
        self.context = context or {}
        self._categories: dict[str, int] = {}

    def add_result(
        self,
        item_name: str,
        is_valid: bool,
        message: str,
        category: str = "general",
    ) -> None:
        """Add a validation result with optional categorization.

        Args:
            item_name: Name/identifier of the item being validated.
            is_valid: Whether the validation passed.
            message: Validation message or error description.
            category: Category of validation (e.g., 'data_type', 'range', 'format').
        """
        self.results.append((item_name, is_valid, message, category))

        # Track categories
        if category not in self._categories:
            self._categories[category] = 0
        if not is_valid:
            self._categories[category] += 1

        if not is_valid:
            self.errors.append(f"{item_name}: {message}")
        elif "warning" in message.lower():
            self.warnings.append(f"{item_name}: {message}")

    def add_warning(
        self, item_name: str, message: str, category: str = "warning"
    ) -> None:
        """Add a warning (non-blocking validation issue).

        Args:
            item_name: Name/identifier of the item.
            message: Warning message.
            category: Category of the warning.
        """
        self.warnings.append(f"{item_name}: {message}")
        self.results.append((item_name, True, f"WARNING: {message}", category))

    def has_errors(self) -> bool:
        """Check if any errors were collected."""
        return len(self.errors) > 0

    def has_warnings(self) -> bool:
        """Check if any warnings were collected."""
        return len(self.warnings) > 0

    def get_error_count_by_category(self) -> dict[str, int]:
        """Get error counts grouped by category.

        Returns:
            Dictionary mapping category names to error counts.
        """
        return self._categories.copy()

    def get_items_with_errors(self) -> list[str]:
        """Get list of item names that had validation errors.

        Returns:
            List of item names with errors.
        """
        return [item for item, valid, _, _ in self.results if not valid]

    def get_summary(self) -> dict[str, Any]:
        """Get a comprehensive summary of all validation results.

        Returns:
            Dictionary containing validation metrics and summaries.
        """
        total = len(self.results)
        valid = sum(1 for _, is_valid, _, _ in self.results if is_valid)

        # Calculate category-specific metrics
        category_summary = {}
        for category, error_count in self._categories.items():
            category_total = sum(1 for _, _, _, cat in self.results if cat == category)
            category_summary[category] = {
                "total": category_total,
                "errors": error_count,
                "success_rate": (
                    (category_total - error_count) / category_total
                    if category_total > 0
                    else 1.0
                ),
            }

        return {
            "total": total,
            "valid": valid,
            "invalid": total - valid,
            "errors": self.errors.copy(),
            "warnings": self.warnings.copy(),
            "error_rate": (total - valid) / total if total > 0 else 0.0,
            "warning_count": len(self.warnings),
            "categories": category_summary,
            "context": self.context.copy(),
            "items_with_errors": self.get_items_with_errors(),
        }

    def clear(self) -> None:
        """Clear all collected results."""
        self.results.clear()
        self.errors.clear()
        self.warnings.clear()
        self._categories.clear()

    def merge(self, other: "ValidationResultCollector") -> None:
        """Merge results from another collector.

        Args:
            other: Another ValidationResultCollector to merge from.
        """
        for item, valid, message, category in other.results:
            self.add_result(item, valid, message, category)

    def get_detailed_report(self) -> str:
        """Generate a detailed text report of validation results.

        Returns:
            Formatted string report.
        """
        summary = self.get_summary()

        report_lines = [
            "=== Validation Report ===",
            f"Total items processed: {summary['total']}",
            f"Valid items: {summary['valid']}",
            f"Invalid items: {summary['invalid']}",
            f"Warnings: {summary['warning_count']}",
            f"Overall success rate: {(1 - summary['error_rate']) * 100:.1f}%",
        ]

        if summary["categories"]:
            report_lines.append("\n--- Category Breakdown ---")
            for category, stats in summary["categories"].items():
                report_lines.append(
                    f"{category}: {stats['total']} items, {stats['errors']} errors "
                    f"({stats['success_rate'] * 100:.1f}% success)"
                )

        if self.has_errors():
            report_lines.append("\n--- First 10 Errors ---")
            report_lines.extend(f"  • {error}" for error in self.errors[:10])
            if len(self.errors) > 10:
                report_lines.append(f"  ... and {len(self.errors) - 10} more errors")

        if self.has_warnings():
            report_lines.append("\n--- First 5 Warnings ---")
            report_lines.extend(f"  • {warning}" for warning in self.warnings[:5])
            if len(self.warnings) > 5:
                report_lines.append(f"  ... and {len(self.warnings) - 5} more warnings")

        return "\n".join(report_lines)



================================================================================
File: fin_statement_model/io/core/registry.py
================================================================================

"""Registry system for managing IO format handlers.

This module provides a generic registry implementation and specific registries
for readers and writers, along with registration decorators and access functions.
"""

import logging
from typing import TypeVar, Generic, Optional, Any, Union, cast
from collections.abc import Callable

from pydantic import ValidationError

from fin_statement_model.io.core.base import DataReader, DataWriter
from fin_statement_model.io.exceptions import (
    FormatNotSupportedError,
    ReadError,
    WriteError,
)
from fin_statement_model.io.config.models import (
    CsvReaderConfig,
    ExcelReaderConfig,
    FmpReaderConfig,
    DataFrameReaderConfig,
    DictReaderConfig,
    ExcelWriterConfig,
    DataFrameWriterConfig,
    DictWriterConfig,
    MarkdownWriterConfig,
)

logger = logging.getLogger(__name__)

# Type variable for the handler type (DataReader or DataWriter)
T = TypeVar("T")


# ===== Generic Registry Implementation =====


class HandlerRegistry(Generic[T]):
    """Generic registry for managing format handlers (readers or writers).

    This class provides a reusable registry pattern for registering and
    retrieving handler classes by format type.

    Attributes:
        _registry: Internal dictionary mapping format types to handler classes.
        _handler_type: String describing the handler type ('reader' or 'writer').
    """

    def __init__(self, handler_type: str):
        """Initialize the registry.

        Args:
            handler_type: Type of handlers ('reader' or 'writer') for error messages.
        """
        self._registry: dict[str, type[T]] = {}
        self._handler_type = handler_type

    def register(self, format_type: str) -> Callable[[type[T]], type[T]]:
        """Create a decorator to register a handler class for a format type.

        Args:
            format_type: The format identifier (e.g., 'excel', 'csv').

        Returns:
            A decorator function that registers the class.

        Raises:
            ValueError: If the format is already registered to a different class.
        """

        def decorator(cls: type[T]) -> type[T]:
            if format_type in self._registry:
                # Allow re-registration of the same class (idempotent)
                if self._registry[format_type] is not cls:
                    raise ValueError(
                        f"{self._handler_type.capitalize()} format type '{format_type}' "
                        f"already registered to {self._registry[format_type]}."
                    )
                logger.debug(
                    f"Re-registering {self._handler_type} format type '{format_type}' "
                    f"to {cls.__name__}"
                )
            else:
                logger.debug(
                    f"Registering {self._handler_type} format type '{format_type}' "
                    f"to {cls.__name__}"
                )

            self._registry[format_type] = cls
            return cls

        return decorator

    def get(self, format_type: str) -> type[T]:
        """Get the registered handler class for a format type.

        Args:
            format_type: The format identifier.

        Returns:
            The registered handler class.

        Raises:
            FormatNotSupportedError: If no handler is registered for the format.
        """
        if format_type not in self._registry:
            raise FormatNotSupportedError(
                format_type=format_type, operation=f"{self._handler_type} operations"
            )

        return self._registry[format_type]

    def list_formats(self) -> dict[str, type[T]]:
        """Return a copy of all registered format handlers.

        Returns:
            Dictionary mapping format types to handler classes.
        """
        return self._registry.copy()

    def is_registered(self, format_type: str) -> bool:
        """Check if a format type is registered.

        Args:
            format_type: The format identifier to check.

        Returns:
            True if the format is registered, False otherwise.
        """
        return format_type in self._registry

    def unregister(self, format_type: str) -> Optional[type[T]]:
        """Remove a format handler from the registry.

        This method is primarily useful for testing.

        Args:
            format_type: The format identifier to remove.

        Returns:
            The removed handler class, or None if not found.
        """
        return self._registry.pop(format_type, None)

    def clear(self) -> None:
        """Clear all registered handlers.

        This method is primarily useful for testing.
        """
        self._registry.clear()

    def __contains__(self, format_type: str) -> bool:
        """Check if a format type is registered using 'in' operator.

        Args:
            format_type: The format identifier to check.

        Returns:
            True if the format is registered, False otherwise.
        """
        return format_type in self._registry

    def __len__(self) -> int:
        """Return the number of registered formats.

        Returns:
            Number of registered format handlers.
        """
        return len(self._registry)


# ===== Registry Instances =====

# Create registry instances for readers and writers
_reader_registry = HandlerRegistry[DataReader]("reader")
_writer_registry = HandlerRegistry[DataWriter]("writer")

# Schema mappings for configuration validation
_READER_SCHEMA_MAP = {
    "csv": CsvReaderConfig,
    "excel": ExcelReaderConfig,
    "fmp": FmpReaderConfig,
    "dataframe": DataFrameReaderConfig,
    "dict": DictReaderConfig,
}

_WRITER_SCHEMA_MAP = {
    "excel": ExcelWriterConfig,
    "dataframe": DataFrameWriterConfig,
    "dict": DictWriterConfig,
    "markdown": MarkdownWriterConfig,
}


# ===== Registration Decorators =====


def register_reader(format_type: str) -> Callable[[type[DataReader]], type[DataReader]]:
    """Decorator to register a DataReader class for a specific format type.

    Args:
        format_type: The string identifier for the format (e.g., 'excel', 'csv').

    Returns:
        A decorator function that registers the class and returns it unmodified.

    Raises:
        ValueError: If the format_type is already registered for a reader.
    """
    return _reader_registry.register(format_type)


def register_writer(format_type: str) -> Callable[[type[DataWriter]], type[DataWriter]]:
    """Decorator to register a DataWriter class for a specific format type.

    Args:
        format_type: The string identifier for the format (e.g., 'excel', 'json').

    Returns:
        A decorator function that registers the class and returns it unmodified.

    Raises:
        ValueError: If the format_type is already registered for a writer.
    """
    return _writer_registry.register(format_type)


# ===== Generic Handler Function =====


def _get_handler(
    format_type: str,
    registry: HandlerRegistry[Any],
    schema_map: dict[str, Any],
    handler_type: str,
    error_class: type[Union[ReadError, WriteError]],
    **kwargs: Any,
) -> Union[DataReader, DataWriter]:
    """Generic handler instantiation logic.

    This function encapsulates the common pattern for instantiating
    readers and writers, including configuration validation and error handling.

    Args:
        format_type: The format identifier (e.g., 'excel', 'csv').
        registry: The registry instance containing handler classes.
        schema_map: Mapping of format types to Pydantic config schemas.
        handler_type: Either 'read' or 'write' for error messages.
        error_class: Either ReadError or WriteError class.
        **kwargs: Configuration parameters for the handler.

    Returns:
        An initialized handler instance.

    Raises:
        FormatNotSupportedError: If format_type is not in registry.
        ReadError/WriteError: If configuration validation or instantiation fails.
    """
    # Get handler class from registry (may raise FormatNotSupportedError)
    handler_class = registry.get(format_type)

    schema = schema_map.get(format_type)

    # Prepare error context based on handler type
    error_context = {}
    if handler_type == "read":
        error_context["source"] = kwargs.get("source")
        error_context["reader_type"] = format_type
    else:  # write
        error_context["target"] = kwargs.get("target")
        error_context["writer_type"] = format_type

    if schema:
        # Validate configuration using Pydantic schema
        try:
            cfg = schema.model_validate({**kwargs, "format_type": format_type})
        except ValidationError as ve:
            raise error_class(
                message=f"Invalid {handler_type}er configuration",
                original_error=ve,
                **error_context,
            ) from ve

        # Instantiate handler with validated config
        try:
            return cast(Union[DataReader, DataWriter], handler_class(cfg))
        except Exception as e:
            logger.error(
                f"Failed to instantiate {handler_type}er for format '{format_type}' "
                f"({handler_class.__name__}): {e}",
                exc_info=True,
            )
            raise error_class(
                message=f"Failed to initialize {handler_type}er",
                original_error=e,
                **error_context,
            ) from e

    # Only Pydantic-schema path is supported
    from fin_statement_model.core.errors import ConfigurationError

    raise ConfigurationError(
        f"No configuration schema available for format '{format_type}'"
    )


# ===== Registry Access Functions =====


def get_reader(format_type: str, **kwargs: Any) -> DataReader:
    """Get an instance of the registered DataReader for the given format type.

    Args:
        format_type: The string identifier for the format.
        **kwargs: Keyword arguments to pass to the reader's constructor.

    Returns:
        An initialized DataReader instance.

    Raises:
        FormatNotSupportedError: If no reader is registered for the format type.
        ReadError: If validation fails for known reader types.
    """
    return cast(
        DataReader,
        _get_handler(
            format_type=format_type,
            registry=_reader_registry,
            schema_map=_READER_SCHEMA_MAP,
            handler_type="read",
            error_class=ReadError,
            **kwargs,
        ),
    )


def get_writer(format_type: str, **kwargs: Any) -> DataWriter:
    """Get an instance of the registered DataWriter for the given format type.

    Args:
        format_type: The string identifier for the format.
        **kwargs: Keyword arguments to pass to the writer's constructor.

    Returns:
        An initialized DataWriter instance.

    Raises:
        FormatNotSupportedError: If no writer is registered for the format type.
        WriteError: If validation fails for known writer types.
    """
    return cast(
        DataWriter,
        _get_handler(
            format_type=format_type,
            registry=_writer_registry,
            schema_map=_WRITER_SCHEMA_MAP,
            handler_type="write",
            error_class=WriteError,
            **kwargs,
        ),
    )


def list_readers() -> dict[str, type[DataReader]]:
    """Return a copy of the registered reader classes."""
    return _reader_registry.list_formats()


def list_writers() -> dict[str, type[DataWriter]]:
    """Return a copy of the registered writer classes."""
    return _writer_registry.list_formats()


__all__ = [
    "HandlerRegistry",
    "get_reader",
    "get_writer",
    "list_readers",
    "list_writers",
    "register_reader",
    "register_writer",
]



================================================================================
File: fin_statement_model/io/core/utils.py
================================================================================

"""Utility functions for IO operations."""

from typing import Optional, Union, cast

# Type alias for mapping configurations
MappingConfig = Union[dict[str, str], dict[Optional[str], dict[str, str]]]


def normalize_mapping(
    mapping_config: Optional[MappingConfig] = None, context_key: Optional[str] = None
) -> dict[str, str]:
    """Turn a scoped MappingConfig into a unified flat dict with a required default mapping under None.

    Args:
        mapping_config: MappingConfig object defining name mappings.
        context_key: Optional key (e.g., sheet name or statement type) to select
            a scoped mapping within a scoped config.

    Returns:
        A flat dict mapping original names to canonical names.

    Raises:
        TypeError: If the provided mapping_config is not of a supported structure.
    """
    if mapping_config is None:
        return {}
    if not isinstance(mapping_config, dict):
        raise TypeError(
            f"mapping_config must be a dict, got {type(mapping_config).__name__}"
        )
    if None not in mapping_config:
        # Flat mapping: keys are source names
        return cast(dict[str, str], mapping_config)
    # Scoped mapping: mapping_config keys include Optional[str]
    scoped = cast(dict[Optional[str], dict[str, str]], mapping_config)
    # Default scope under None
    default_mapping = scoped[None]
    # Overlay context-specific mappings if provided
    if context_key and context_key in scoped:
        context_mapping = scoped[context_key]
        merged: dict[str, str] = {**default_mapping, **context_mapping}
        return merged
    return default_mapping


__all__ = ["MappingConfig", "normalize_mapping"]



================================================================================
File: fin_statement_model/io/exceptions.py
================================================================================

"""IO specific exceptions."""

from typing import Optional

# Use absolute import based on project structure
from fin_statement_model.core.errors import FinancialModelError


class IOError(FinancialModelError):
    """Base exception for all Input/Output errors in the IO package."""

    def __init__(
        self,
        message: str,
        source_or_target: Optional[str] = None,
        format_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the IOError.

        Args:
            message: The base error message.
            source_or_target: Optional identifier for the source (read) or target (write).
            format_type: Optional name of the format or handler involved.
            original_error: Optional underlying exception that caused the failure.
        """
        self.source_or_target = source_or_target
        self.format_type = format_type
        self.original_error = original_error

        context = []
        if source_or_target:
            context.append(f"source/target '{source_or_target}'")
        if format_type:
            context.append(f"format '{format_type}'")

        full_message = (
            f"{message} involving {' and '.join(context)}" if context else message
        )

        if original_error:
            full_message = f"{full_message}: {original_error!s}"

        super().__init__(full_message)


class ReadError(IOError):
    """Exception raised specifically for errors during data read/import operations."""

    def __init__(
        self,
        message: str,
        source: Optional[str] = None,
        reader_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the ReadError.

        Args:
            message: The base error message.
            source: Optional identifier for the data source (e.g., file path, URL).
            reader_type: Optional name of the reader class used for importing.
            original_error: Optional underlying exception that caused the import failure.
        """
        super().__init__(
            message=message,
            source_or_target=source,
            format_type=reader_type,
            original_error=original_error,
        )


class WriteError(IOError):
    """Exception raised specifically for errors during data write/export operations."""

    def __init__(
        self,
        message: str,
        target: Optional[str] = None,
        writer_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the WriteError.

        Args:
            message: The base error message.
            target: Optional identifier for the export destination (e.g., file path).
            writer_type: Optional name of the writer class being used.
            original_error: Optional underlying exception that caused the export failure.
        """
        super().__init__(
            message=message,
            source_or_target=target,
            format_type=writer_type,
            original_error=original_error,
        )


class FormatNotSupportedError(IOError):
    """Exception raised when a requested IO format is not registered or supported."""

    def __init__(self, format_type: str, operation: str = "read/write"):
        """Initializes the FormatNotSupportedError.

        Args:
            format_type: The requested format identifier (e.g., 'excel', 'json').
            operation: The operation being attempted ('read' or 'write').
        """
        message = f"Format '{format_type}' is not supported for {operation} operations."
        super().__init__(message=message, format_type=format_type)



================================================================================
File: fin_statement_model/io/formats/__init__.py
================================================================================

"""Format-specific IO implementations.

This module contains readers and writers for various data formats.
Each format is organized in its own submodule.
"""

# Import all format handlers to ensure they're registered
from .csv import CsvReader
from .dataframe import DataFrameReader, DataFrameWriter
from .dict import DictReader, DictWriter
from .excel import ExcelReader, ExcelWriter
from .api import FmpReader
from .markdown import MarkdownWriter

__all__ = [
    # CSV
    "CsvReader",
    # DataFrame
    "DataFrameReader",
    "DataFrameWriter",
    # Dict
    "DictReader",
    "DictWriter",
    # Excel
    "ExcelReader",
    "ExcelWriter",
    # API
    "FmpReader",
    # Markdown
    "MarkdownWriter",
]



================================================================================
File: fin_statement_model/io/formats/api/__init__.py
================================================================================

"""API format IO operations."""

from .fmp import FmpReader

__all__ = ["FmpReader"]



================================================================================
File: fin_statement_model/io/formats/api/fmp.py
================================================================================

"""Data reader for the Financial Modeling Prep (FMP) API."""

import logging
import requests
from typing import Optional, Any
import numpy as np


from fin_statement_model.config import cfg
from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.base import DataReader
from fin_statement_model.io.core.mixins import (
    MappingAwareMixin,
    ConfigurationMixin,
)
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError

from fin_statement_model.io.config.models import FmpReaderConfig

logger = logging.getLogger(__name__)


@register_reader("fmp")
class FmpReader(DataReader, ConfigurationMixin, MappingAwareMixin):
    """Reads financial statement data from the FMP API into a Graph.

    Fetches data for a specific ticker and statement type.
    Requires an API key, either passed directly or via the FMP_API_KEY env var.

    Supports a `mapping_config` constructor parameter for mapping API field names to canonical node names,
    accepting either a flat mapping or a statement-type keyed mapping.

    Configuration (api_key, statement_type, period_type, limit, mapping_config)
    is passed via an `FmpReaderConfig` object during initialization (typically by
    the `read_data` facade). The `.read()` method currently takes no specific
    keyword arguments beyond the `source` (ticker).

    Stateful Use:
        For advanced use cases involving repeated API calls, consider instantiating
        and reusing a single `FmpReader` instance to avoid redundant API key
        validations and improve performance.
    """

    BASE_URL = "https://financialmodelingprep.com/api/v3"

    @classmethod
    def _get_default_mapping_path(cls) -> Optional[str]:
        """Specify the default mapping file for FMP."""
        return "fmp_default_mappings.yaml"

    def __init__(self, cfg: FmpReaderConfig) -> None:
        """Initialize the FmpReader with validated configuration.

        Args:
            cfg: A validated `FmpReaderConfig` instance containing parameters like
                 `source` (ticker), `api_key`, `statement_type`, `period_type`,
                 `limit`, and `mapping_config`.
        """
        # Initialise mixins to set up configuration context and mapping caches.
        ConfigurationMixin.__init__(self)
        MappingAwareMixin.__init__(self)  # currently a no-op but future-proof

        self.cfg = cfg

    def _validate_api_key(self, api_key: str) -> None:
        """Perform a simple check if the provided API key is valid."""
        if not api_key:
            raise ReadError(
                "FMP API key is required for reading.",
                source="FMP API",
                reader_type="FmpReader",
            )
        try:
            # Use a cheap endpoint for validation
            test_url = f"{self.BASE_URL}/profile/AAPL?apikey={api_key}"  # Example
            response = requests.get(test_url, timeout=cfg("api.api_timeout"))
            response.raise_for_status()
            if not response.json():
                raise ReadError(
                    "API key validation returned empty response.",
                    source="FMP API",
                    reader_type="FmpReader",
                )
            logger.debug("FMP API key validated successfully.")
        except requests.exceptions.RequestException as e:
            logger.error(f"FMP API key validation failed: {e}", exc_info=True)
            raise ReadError(
                f"FMP API key validation failed: {e}",
                source="FMP API",
                reader_type="FmpReader",
                original_error=e,
            )

    def read(self, source: str, **kwargs: Any) -> Graph:
        """Fetch data from FMP API and return a Graph.

        Args:
            source (str): The stock ticker symbol (e.g., "AAPL").
            **kwargs: Optional runtime arguments overriding config defaults:
                statement_type (str): Type of statement to fetch ('income_statement', 'balance_sheet', 'cash_flow').
                period_type (str): Period type ('FY' or 'QTR').
                limit (int): Number of periods to fetch.
                api_key (str): API key for FMP, overrides env or config value.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If API key is missing/invalid, API request fails, or data format is unexpected.
        """
        ticker = source

        # Set configuration context for better error reporting
        self.set_config_context(ticker=ticker, operation="api_read")

        # Runtime overrides: kwargs override configuration defaults
        statement_type = kwargs.get("statement_type", self.cfg.statement_type)
        period_type_arg = kwargs.get("period_type", self.cfg.period_type)
        limit = kwargs.get("limit", self.cfg.limit)
        # Handle API key override or fallback to validated config
        api_key = (
            kwargs.get("api_key")
            if kwargs.get("api_key") is not None
            else self.cfg.api_key
        )

        # --- Validate Inputs ---
        if not ticker or not isinstance(ticker, str):
            raise ReadError(
                "Invalid source (ticker) provided. Expected a non-empty string.",
                source=ticker,
                reader_type="FmpReader",
            )
        # statement_type and period_type are validated by FmpReaderConfig

        # Validate API key (using any override)
        self._validate_api_key(api_key)

        # Determine mapping for this operation, allowing override via kwargs
        try:
            mapping = self._get_mapping(statement_type)
        except TypeError as te:
            raise ReadError(
                "Invalid mapping_config provided.",
                source=ticker,
                reader_type="FmpReader",
                original_error=te,
            )
        logger.debug(f"Using mapping for {ticker} {statement_type}: {mapping}")

        # --- Fetch API Data ---
        # Correct endpoint construction based on FMP v3 docs
        # e.g., /income-statement/AAPL, not /income_statement-statement/AAPL
        endpoint_path = statement_type.replace("_", "-")
        endpoint = f"{self.BASE_URL}/{endpoint_path}/{ticker}"
        params = {"apikey": api_key, "limit": limit}
        if period_type_arg == "QTR":
            params["period"] = "quarter"

        try:
            logger.info(
                f"Fetching {period_type_arg} {statement_type} for {ticker} from FMP API (limit={limit})."
            )
            response = requests.get(
                endpoint, params=params, timeout=cfg("api.api_timeout")
            )
            response.raise_for_status()  # Check for HTTP errors
            api_data = response.json()

            if not isinstance(api_data, list):
                raise ReadError(
                    f"Unexpected API response format. Expected list, got {type(api_data)}. Response: {str(api_data)[:100]}...",
                    source=f"FMP API ({ticker})",
                    reader_type="FmpReader",
                )
            if not api_data:
                logger.warning(
                    f"FMP API returned empty list for {ticker} {statement_type}."
                )
                # Return empty graph or raise? Returning empty for now.
                return Graph(periods=[])

        except requests.exceptions.RequestException as e:
            logger.error(
                f"FMP API request failed for {ticker} {statement_type}: {e}",
                exc_info=True,
            )
            raise ReadError(
                f"FMP API request failed: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            )
        except Exception as e:
            logger.error(f"Failed to process FMP API response: {e}", exc_info=True)
            raise ReadError(
                f"Failed to process FMP API response: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            )

        # --- Process Data and Populate Graph ---
        try:
            # FMP data is usually newest first, reverse to process chronologically
            api_data.reverse()

            # Extract periods (e.g., 'date' or 'fillingDate')
            # Using 'date' as it usually represents the period end date
            periods = [item.get("date") for item in api_data if item.get("date")]
            if not periods:
                raise ReadError(
                    "Could not extract periods ('date' field) from FMP API response.",
                    source=f"FMP API ({ticker})",
                    reader_type="FmpReader",
                )

            graph = Graph(periods=periods)
            all_item_data: dict[str, dict[str, float]] = {}

            # Collect data for all items across all periods
            for period_data in api_data:
                period = period_data.get("date")
                if not period:
                    continue  # Skip records without a date

                for api_field, value in period_data.items():
                    node_name = self._apply_mapping(api_field, mapping)

                    # Initialize node data dict if first time seeing this node
                    if node_name not in all_item_data:
                        all_item_data[node_name] = {
                            p: np.nan for p in periods
                        }  # Pre-fill with NaN

                    # Store value for this period
                    if isinstance(value, int | float):
                        all_item_data[node_name][period] = float(value)

            # Create nodes from collected data
            nodes_added = 0
            for node_name, period_values in all_item_data.items():
                # Filter out periods that only have NaN
                valid_period_values = {
                    p: v for p, v in period_values.items() if not np.isnan(v)
                }
                if valid_period_values:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=valid_period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

            logger.info(
                f"Successfully created graph with {nodes_added} nodes from FMP API for {ticker} {statement_type}."
            )
            return graph

        except Exception as e:
            logger.error(
                f"Failed to parse FMP data and build graph: {e}", exc_info=True
            )
            raise ReadError(
                message=f"Failed to parse FMP data: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            ) from e



================================================================================
File: fin_statement_model/io/formats/csv/__init__.py
================================================================================

"""CSV format IO operations."""

from .reader import CsvReader

__all__ = ["CsvReader"]



================================================================================
File: fin_statement_model/io/formats/csv/reader.py
================================================================================

"""Data reader for CSV files."""

import logging
from typing import Any, cast

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.mixins import (
    FileBasedReader,
    ConfigurationMixin,
    MappingAwareMixin,
    ValidationMixin,
    handle_read_errors,
    ValidationResultCollector,
)
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import CsvReaderConfig

logger = logging.getLogger(__name__)


@register_reader("csv")
class CsvReader(
    FileBasedReader, ConfigurationMixin, MappingAwareMixin, ValidationMixin
):
    """Reads financial statement data from a CSV file into a Graph.

    Assumes a 'long' format where each row represents a single data point
    (item, period, value).
    Requires specifying the columns containing item names, period identifiers,
    and values.

    Supports a `mapping_config` constructor parameter for name mapping,
    accepting either a flat mapping or a statement-type scoped mapping.

    Configuration (delimiter, header_row, index_col, mapping_config) is passed
    via a `CsvReaderConfig` object during initialization (typically by the `read_data` facade).
    Method-specific options (`item_col`, `period_col`, `value_col`, `pandas_read_csv_kwargs`)
    are passed as keyword arguments to the `read()` method.
    """

    def __init__(self, cfg: CsvReaderConfig) -> None:
        """Initialize the CsvReader with validated configuration.

        Args:
            cfg: A validated `CsvReaderConfig` instance containing parameters like
                 `source`, `delimiter`, `header_row`, `index_col`, and `mapping_config`.
        """
        self.cfg = cfg

    @handle_read_errors()
    def read(self, source: str, **kwargs: Any) -> Graph:
        """Read data from a CSV file into a new Graph.

        Args:
            source (str): Path to the CSV file.
            **kwargs: Optional runtime arguments overriding config defaults:
                statement_type (str): Statement type ('income_statement', 'balance_sheet', 'cash_flow').
                item_col (str): Name of the column containing item identifiers.
                period_col (str): Name of the column containing period identifiers.
                value_col (str): Name of the column containing numeric values.
                pandas_read_csv_kwargs (dict): Additional kwargs for pandas.read_csv.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the file cannot be read or required columns are missing.
        """
        file_path = source
        logger.info(f"Starting import from CSV file: {file_path}")

        # Set configuration context for better error reporting
        self.set_config_context(file_path=file_path, operation="read")

        # Use base class validation
        self.validate_file_exists(file_path)
        self.validate_file_extension(file_path, (".csv", ".txt"))

        # Runtime overrides: kwargs override configured defaults (statement_type handled in _process_dataframe)
        item_col = kwargs.get("item_col", self.cfg.item_col)
        period_col = kwargs.get("period_col", self.cfg.period_col)
        value_col = kwargs.get("value_col", self.cfg.value_col)
        pandas_read_csv_kwargs = (
            kwargs.get("pandas_read_csv_kwargs")
            or self.cfg.pandas_read_csv_kwargs
            or {}
        )

        if not all([item_col, period_col, value_col]):
            raise ReadError(
                "Missing required arguments: 'item_col', 'period_col', 'value_col' must be provided.",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        # Cast columns to str after validation
        item_col_str = cast(str, item_col)
        period_col_str = cast(str, period_col)
        value_col_str = cast(str, value_col)
        # Read CSV Data
        df = self._read_csv_file(file_path, pandas_read_csv_kwargs)

        # Validate columns
        self._validate_columns(
            df, item_col_str, period_col_str, value_col_str, file_path
        )

        # Process data
        return self._process_dataframe(
            df, item_col_str, period_col_str, value_col_str, file_path, kwargs
        )

    def _read_csv_file(
        self, file_path: str, user_options: dict[str, Any]
    ) -> pd.DataFrame:
        """Read CSV file with configuration options."""
        # Use configuration from self.cfg with enhanced validation
        from fin_statement_model.config.helpers import cfg

        delimiter = self.get_config_value(
            "delimiter",
            default=cfg("io.default_csv_delimiter"),
            value_type=str,
            validator=lambda x: len(x) >= 1,
        )
        header_row = self.get_config_value(
            "header_row", default=1, value_type=int, validator=lambda x: x >= 1
        )

        read_options = {
            "delimiter": delimiter,
            "header": header_row - 1,  # Convert to 0-indexed
        }

        # Handle optional index_col with validation
        index_col = self.get_config_value(
            "index_col",
            value_type=int,
            validator=lambda x: x is None or x >= 1,
        )
        if index_col is not None:
            read_options["index_col"] = index_col - 1  # Convert to 0-indexed

        # Merge user-provided kwargs, allowing them to override config
        read_options.update(user_options)

        return pd.read_csv(file_path, **read_options)

    def _validate_columns(
        self,
        df: pd.DataFrame,
        item_col: str,
        period_col: str,
        value_col: str,
        file_path: str,
    ) -> None:
        """Validate that required columns exist in the DataFrame."""
        # Use ValidationMixin for column validation
        self.validate_required_columns(df, [item_col, period_col, value_col], file_path)

    def _process_dataframe(
        self,
        df: pd.DataFrame,
        item_col: str,
        period_col: str,
        value_col: str,
        file_path: str,
        kwargs: dict[str, Any],
    ) -> Graph:
        """Process the DataFrame and create a Graph."""
        # Convert period column to string
        df[period_col] = df[period_col].astype(str)
        all_periods = sorted(df[period_col].unique().tolist())

        # Use ValidationMixin for periods validation
        self.validate_periods_exist(all_periods, file_path)

        logger.info(f"Identified periods: {all_periods}")
        graph = Graph(periods=all_periods)

        # Use validation collector for better error reporting
        validator = ValidationResultCollector()

        # Group data by item name
        grouped = df.groupby(item_col)
        nodes_added = 0

        # Determine mapping context
        context_key = kwargs.get("statement_type", self.cfg.statement_type)
        mapping = self._get_mapping(context_key)

        for item_name_csv, group in grouped:
            if pd.isna(item_name_csv) or not item_name_csv:
                logger.debug("Skipping group with empty item name.")
                continue

            item_name_csv_str = str(item_name_csv).strip()
            node_name = self._apply_mapping(item_name_csv_str, mapping)

            period_values = self._extract_period_values(
                group, period_col, value_col, item_name_csv_str, node_name, validator
            )

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' (from CSV item '{item_name_csv_str}') already exists. "
                        "Overwriting data is not standard for readers."
                    )
                else:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

        # Check for validation errors
        if validator.has_errors():
            summary = validator.get_summary()
            raise ReadError(
                f"Validation errors occurred while reading {file_path}: {'; '.join(summary['errors'])}",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        logger.info(
            f"Successfully created graph with {nodes_added} nodes from {file_path}."
        )
        return graph

    def _extract_period_values(
        self,
        group: pd.DataFrame,
        period_col: str,
        value_col: str,
        item_name_csv: str,
        node_name: str,
        validator: ValidationResultCollector,
    ) -> dict[str, float]:
        """Extract period values from a group with validation."""
        period_values: dict[str, float] = {}

        for _, row in group.iterrows():
            period = row[period_col]
            value = row[value_col]

            if pd.isna(value):
                continue  # Skip missing values

            # Use ValidationMixin for numeric validation
            is_valid, converted_value = self.validate_numeric_value(
                value, item_name_csv, period, validator, allow_conversion=True
            )

            if not is_valid or converted_value is None:
                continue

            value = converted_value

            if period in period_values:
                logger.warning(
                    f"Duplicate value found for node '{node_name}' "
                    f"(from CSV item '{item_name_csv}') period '{period}'. "
                    "Using the last one found."
                )

            period_values[period] = float(value)

        return period_values



================================================================================
File: fin_statement_model/io/formats/dataframe/__init__.py
================================================================================

"""DataFrame format IO operations."""

from .reader import DataFrameReader
from .writer import DataFrameWriter

__all__ = ["DataFrameReader", "DataFrameWriter"]



================================================================================
File: fin_statement_model/io/formats/dataframe/reader.py
================================================================================

"""Data reader for pandas DataFrames."""

import logging
import pandas as pd
import numpy as np
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.base import DataReader
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import DataFrameReaderConfig

logger = logging.getLogger(__name__)


@register_reader("dataframe")
class DataFrameReader(DataReader):
    """Reads data from a pandas DataFrame into a Graph.

    Assumes the DataFrame index contains node names and columns contain periods.
    Values should be numeric.
    """

    def __init__(self, cfg: Optional[DataFrameReaderConfig] = None) -> None:
        """Initialize the DataFrameReader.

        Args:
            cfg: Optional validated `DataFrameReaderConfig` instance.
                 Currently unused but kept for registry symmetry and future options.
        """
        self.cfg = cfg  # For future use; currently no configuration options.

    def read(self, source: pd.DataFrame, **kwargs: Any) -> Graph:
        """Read data from a pandas DataFrame into a new Graph.

        Assumes DataFrame index = node names, columns = periods.

        Args:
            source (pd.DataFrame): The DataFrame to read data from.
            **kwargs: Optional runtime argument overriding config defaults:
                periods (list[str], optional): List of periods (columns) to include. Overrides `cfg.periods`.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the source is not a DataFrame or has invalid structure.
        """
        df = source
        logger.info("Starting import from DataFrame.")

        # --- Validate Inputs ---
        if not isinstance(df, pd.DataFrame):
            raise ReadError(
                "Source is not a pandas DataFrame.",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        if df.index.name is None and df.index.empty:
            logger.warning(
                "DataFrame index is unnamed and empty, assuming columns are nodes if periods kwarg is provided."
            )
            # Handle case where DF might be oriented differently if periods kwarg is present?
            # For now, stick to index=nodes assumption.

        # Determine periods: runtime kwargs override config, else config defaults, else infer
        graph_periods_arg = kwargs.get(
            "periods", self.cfg.periods if self.cfg else None
        )
        if graph_periods_arg:
            if not isinstance(graph_periods_arg, list):
                raise ReadError("'periods' argument must be a list of column names.")
            missing_cols = [p for p in graph_periods_arg if p not in df.columns]
            if missing_cols:
                raise ReadError(
                    f"Specified periods (columns) not found in DataFrame: {missing_cols}"
                )
            graph_periods = sorted(graph_periods_arg)
            df_subset = df[graph_periods]  # Select only specified period columns
        else:
            # No explicit periods provided; infer from columns
            graph_periods = sorted(df.columns.astype(str).tolist())
            df_subset = df

        if not graph_periods:
            raise ReadError(
                "No periods identified in DataFrame columns.",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        logger.info(f"Using periods (columns): {graph_periods}")
        graph = Graph(periods=graph_periods)

        # --- Populate Graph ---
        validation_errors = []
        nodes_added = 0
        for node_name_df, row in df_subset.iterrows():
            if pd.isna(node_name_df) or not node_name_df:
                logger.debug("Skipping row with empty index name.")
                continue

            node_name = str(node_name_df).strip()
            period_values: dict[str, float] = {}
            for period in graph_periods:
                value = row[period]
                if pd.isna(value):
                    continue  # Skip NaN values

                if not isinstance(value, int | float | np.number):
                    try:
                        value = float(value)
                        logger.warning(
                            f"Converted non-numeric value '{row[period]}' to float for node '{node_name}' period '{period}'"
                        )
                    except (ValueError, TypeError):
                        validation_errors.append(
                            f"Node '{node_name}': Non-numeric value '{value}' for period '{period}'"
                        )
                        continue  # Skip invalid value

                period_values[period] = float(value)

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' already exists. Overwriting data is not standard for readers."
                    )
                    # Update existing? Log for now.
                else:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

        if validation_errors:
            raise ReadError(
                f"Validation errors occurred while reading DataFrame: {'; '.join(validation_errors)}",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        logger.info(
            f"Successfully created graph with {nodes_added} nodes from DataFrame."
        )
        return graph

        # No specific file operations, so less need for broad Exception catch
        # Specific errors handled above (TypeError, ValueError from float conversion)



================================================================================
File: fin_statement_model/io/formats/dataframe/writer.py
================================================================================

"""Data writer for pandas DataFrames."""

import logging
from typing import Optional, Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.mixins import (
    DataFrameBasedWriter,
    ConfigurationMixin,
    handle_write_errors,
)
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.io.config.models import DataFrameWriterConfig

logger = logging.getLogger(__name__)


@register_writer("dataframe")  # type: ignore[arg-type]
class DataFrameWriter(DataFrameBasedWriter, ConfigurationMixin):
    """Writes graph data to a pandas DataFrame.

    Converts the graph to a DataFrame with node names as index and periods as columns.

    Configuration options `recalculate` and `include_nodes` are controlled by
    the `DataFrameWriterConfig` object passed during initialization.
    """

    def __init__(self, cfg: Optional[DataFrameWriterConfig] = None) -> None:
        """Initialize the DataFrameWriter.

        Args:
            cfg: Optional validated `DataFrameWriterConfig` instance.
        """
        super().__init__()
        self.cfg = cfg

    @handle_write_errors()
    def write(self, graph: Graph, target: Any = None, **kwargs: Any) -> pd.DataFrame:
        """Convert the graph data to a pandas DataFrame based on instance configuration.

        Args:
            graph (Graph): The Graph instance to export.
            target (Any): Ignored by this writer; the DataFrame is returned directly.
            **kwargs: Optional runtime overrides of configured defaults:
                recalculate (bool): Whether to recalculate graph before export.
                include_nodes (list[str]): List of node names to include in export.

        Returns:
            pd.DataFrame: DataFrame with node names as index and periods as columns.

        Raises:
            WriteError: If an error occurs during conversion.
        """
        # Runtime overrides: kwargs override configured defaults
        recalculate = kwargs.get(
            "recalculate", self.cfg.recalculate if self.cfg else True
        )
        include_nodes = kwargs.get(
            "include_nodes", self.cfg.include_nodes if self.cfg else None
        )

        logger.info("Exporting graph to DataFrame format.")

        # Handle recalculation if requested
        if recalculate:
            self._recalculate_graph(graph)

        # Extract data using base class method
        data = self.extract_graph_data(
            graph, include_nodes=include_nodes, calculate=True
        )

        # Convert to DataFrame
        periods = sorted(graph.periods) if graph.periods else []
        df = pd.DataFrame.from_dict(data, orient="index", columns=periods)
        df.index.name = "node_name"

        logger.info(f"Successfully exported {len(df)} nodes to DataFrame.")
        return df

    def _recalculate_graph(self, graph: Graph) -> None:
        """Recalculate the graph if it has periods defined.

        Args:
            graph: The graph to recalculate.
        """
        try:
            if graph.periods:
                graph.recalculate_all(periods=graph.periods)
                logger.info("Recalculated graph before exporting to DataFrame.")
            else:
                logger.warning("Graph has no periods defined, skipping recalculation.")
        except Exception as e:
            logger.error(
                f"Error during recalculation for DataFrame export: {e}",
                exc_info=True,
            )
            logger.warning(
                "Proceeding to export DataFrame without successful recalculation."
            )



================================================================================
File: fin_statement_model/io/formats/dict/__init__.py
================================================================================

"""Dictionary format IO operations."""

from .reader import DictReader
from .writer import DictWriter

__all__ = ["DictReader", "DictWriter"]



================================================================================
File: fin_statement_model/io/formats/dict/reader.py
================================================================================

"""Data reader for Python dictionaries."""

import logging
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.base import DataReader
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import DictReaderConfig

logger = logging.getLogger(__name__)


@register_reader("dict")
class DictReader(DataReader):
    """Reads data from a Python dictionary to create a new Graph.

    Expects a dictionary format: {node_name: {period: value, ...}, ...}
    Creates FinancialStatementItemNode instances for each entry.

    Note:
        Configuration is handled via `DictReaderConfig` during initialization.
        The `read()` method takes the source dictionary directly and an optional
        `periods` keyword argument.
    """

    def __init__(self, cfg: Optional[DictReaderConfig] = None) -> None:
        """Initialize the DictReader.

        Args:
            cfg: Optional validated `DictReaderConfig` instance.
                 Currently unused but kept for registry symmetry and future options.
        """
        self.cfg = cfg

    def read(self, source: dict[str, dict[str, float]], **kwargs: Any) -> Graph:
        """Create a new Graph from a dictionary.

        Args:
            source: Dictionary mapping node names to period-value dictionaries.
                    Format: {node_name: {period: value, ...}, ...}
            **kwargs: Optional runtime argument overriding config defaults:
                periods (list[str], optional): List of periods to include. Overrides `cfg.periods`.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the source data format is invalid or processing fails.
            # DataValidationError: If data values are not numeric.
        """
        logger.info("Starting import from dictionary to create a new graph.")

        if not isinstance(source, dict):
            raise ReadError(
                message="Invalid source type for DictReader. Expected dict.",
                source="dict_input",
                reader_type="DictReader",
            )

        # Validate data structure and collect all periods
        all_periods = set()
        validation_errors: list[str] = []
        try:
            for node_name, period_values in source.items():
                if not isinstance(period_values, dict):
                    validation_errors.append(
                        f"Node '{node_name}': Invalid format - expected dict, got {type(period_values).__name__}"
                    )
                    continue  # Skip further checks for this node
                for period, value in period_values.items():
                    # Basic type checks - can be expanded
                    if not isinstance(period, str):
                        validation_errors.append(
                            f"Node '{node_name}': Invalid period format '{period}' - expected string."
                        )
                    if not isinstance(value, int | float):
                        validation_errors.append(
                            f"Node '{node_name}' period '{period}': Invalid value type {type(value).__name__} - expected number."
                        )
                    all_periods.add(str(period))

            if validation_errors:
                # Use core DataValidationError if it exists and is suitable
                # Otherwise, stick to ReadError or a specific IOValidationError
                # raise DataValidationError(
                #     message="Input dictionary failed validation",
                #     validation_errors=validation_errors
                # )
                raise ReadError(
                    f"Input dictionary failed validation: {'; '.join(validation_errors)}",
                    source="dict_input",
                    reader_type="DictReader",
                )

        except Exception as e:
            # Catch unexpected validation errors
            raise ReadError(
                message=f"Error validating input dictionary: {e}",
                source="dict_input",
                reader_type="DictReader",
                original_error=e,
            ) from e

        # Determine graph periods: runtime kwargs override config defaults
        graph_periods = kwargs.get("periods", self.cfg.periods if self.cfg else None)
        if graph_periods is None:
            graph_periods = sorted(list(all_periods))
            logger.debug(f"Inferred graph periods from data: {graph_periods}")
        # Optional: Validate if all data periods are within the provided list
        elif not all_periods.issubset(set(graph_periods)):
            missing = all_periods - set(graph_periods)
            logger.warning(
                f"Data contains periods not in specified graph periods: {missing}"
            )
            # Decide whether to error or just ignore extra data

        # Create graph and add nodes
        try:
            graph = Graph(periods=graph_periods)
            for node_name, period_values in source.items():
                # Filter values to only include those matching graph_periods
                filtered_values = {
                    p: v for p, v in period_values.items() if p in graph_periods
                }
                if filtered_values:
                    # Create FinancialStatementItemNode directly
                    # Assumes FinancialStatementItemNode takes name and values dict
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=filtered_values.copy()
                    )
                    graph.add_node(new_node)
                else:
                    logger.debug(
                        f"Node '{node_name}' has no data for specified graph periods. Skipping."
                    )

            logger.info(
                f"Successfully created graph with {len(graph.nodes)} nodes from dictionary."
            )
            return graph

        except Exception as e:
            # Catch errors during graph/node creation
            logger.error(f"Failed to create graph from dictionary: {e}", exc_info=True)
            raise ReadError(
                message="Failed to build graph from dictionary data",
                source="dict_input",
                reader_type="DictReader",
                original_error=e,
            ) from e



================================================================================
File: fin_statement_model/io/formats/dict/writer.py
================================================================================

"""Data writer for Python dictionaries."""

import logging
from typing import Any, Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.mixins import (
    DataFrameBasedWriter,
    ConfigurationMixin,
    handle_write_errors,
)
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.io.config.models import DictWriterConfig

logger = logging.getLogger(__name__)


@register_writer("dict")  # type: ignore[arg-type]
class DictWriter(DataFrameBasedWriter, ConfigurationMixin):
    """Writes graph data to a Python dictionary.

    Extracts values for each node and period in the graph, attempting to
    calculate values where possible.

    Initialized via `DictWriterConfig` (typically by the `write_data` facade),
    although the config currently has no options.
    """

    def __init__(self, cfg: Optional[DictWriterConfig] = None) -> None:
        """Initialize the DictWriter.

        Args:
            cfg: Optional validated `DictWriterConfig` instance.
                 Currently unused but kept for registry symmetry.
        """
        super().__init__()
        self.cfg = cfg

    @handle_write_errors()
    def write(
        self, graph: Graph, target: Any = None, **kwargs: dict[str, Any]
    ) -> dict[str, dict[str, float]]:
        """Export calculated data from all graph nodes to a dictionary.

        Args:
            graph (Graph): The Graph instance to export data from.
            target (Any): Ignored by this writer; the dictionary is returned directly.
            **kwargs: Currently unused.

        Returns:
            Dict[str, Dict[str, float]]: Mapping node names to period-value dicts.
                                         Includes values for all nodes in the graph
                                         for all defined periods. NaN represents
                                         uncalculable values.

        Raises:
            WriteError: If an unexpected error occurs during export.
        """
        logger.info(f"Starting export of graph '{graph}' to dictionary format.")

        if not graph.periods:
            logger.warning(
                "Graph has no periods defined. Exported dictionary will be empty."
            )
            return {}

        # Use base class method to extract all data
        # This handles calculation attempts and error handling consistently
        result = self.extract_graph_data(graph, include_nodes=None, calculate=True)

        logger.info(f"Successfully exported {len(result)} nodes to dictionary.")
        return result



================================================================================
File: fin_statement_model/io/formats/excel/__init__.py
================================================================================

"""Excel format IO operations."""

from .reader import ExcelReader
from .writer import ExcelWriter

__all__ = ["ExcelReader", "ExcelWriter"]



================================================================================
File: fin_statement_model/io/formats/excel/reader.py
================================================================================

"""Data reader for Excel files."""

import logging
from typing import Optional, Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.mixins import (
    FileBasedReader,
    ConfigurationMixin,
    MappingAwareMixin,
    ValidationMixin,
    handle_read_errors,
    ValidationResultCollector,
)
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError

from fin_statement_model.io.config.models import ExcelReaderConfig

logger = logging.getLogger(__name__)


@register_reader("excel")
class ExcelReader(
    FileBasedReader, ConfigurationMixin, MappingAwareMixin, ValidationMixin
):
    """Reads financial statement data from an Excel file into a Graph.

    Expects data in a tabular format where rows typically represent items
    and columns represent periods, or vice-versa.
    Requires specifying sheet name, period identification, and item identification.

    Configuration (sheet_name, items_col, periods_row, mapping_config) is passed
    via an `ExcelReaderConfig` object during initialization (typically by the `read_data` facade).
    Method-specific options (`statement_type`, `header_row`, `nrows`, `skiprows`)
    are passed as keyword arguments to the `read()` method.
    """

    def __init__(self, cfg: ExcelReaderConfig) -> None:
        """Initialize the ExcelReader with validated configuration.

        Args:
            cfg: A validated `ExcelReaderConfig` instance containing parameters like
                 `source`, `sheet_name`, `items_col`, `periods_row`, and `mapping_config`.
        """
        self.cfg = cfg

    @handle_read_errors()
    def read(self, source: str, **kwargs: Any) -> Graph:
        """Read data from an Excel file sheet into a new Graph based on instance config.

        Args:
            source (str): Path to the Excel file.
            **kwargs: Optional runtime keyword arguments overriding configured defaults:
                statement_type (str): Type of statement ('income_statement', 'balance_sheet', 'cash_flow').
                header_row (int): 1-based index for pandas header reading.
                nrows (int): Number of rows to read from the sheet.
                skiprows (int): Number of rows to skip at the beginning.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the file cannot be read or the configuration is invalid.
        """
        file_path = source
        logger.info(f"Starting import from Excel file: {file_path}")

        # Use base class validation
        self.validate_file_exists(file_path)
        self.validate_file_extension(file_path, (".xls", ".xlsx", ".xlsm"))

        # Set configuration context for better error reporting
        self.set_config_context(file_path=file_path, operation="read")

        # Get configuration values with validation
        sheet_name = self.require_config_value("sheet_name", value_type=str)
        periods_row = self.require_config_value(
            "periods_row", value_type=int, validator=lambda x: x >= 1
        )
        items_col = self.require_config_value(
            "items_col", value_type=int, validator=lambda x: x >= 1
        )

        # Runtime options: kwargs override config defaults
        statement_type = kwargs.get("statement_type", self.cfg.statement_type)
        header_row = kwargs.get("header_row", self.cfg.header_row or periods_row)
        nrows = kwargs.get("nrows", self.cfg.nrows)
        skiprows = kwargs.get("skiprows", self.cfg.skiprows)

        # Get mapping
        mapping = self._get_mapping(statement_type)
        logger.debug(f"Using mapping for statement type '{statement_type}': {mapping}")

        # Read Excel data
        df, period_headers = self._read_excel_data(
            file_path, sheet_name, periods_row, items_col, header_row, nrows, skiprows
        )

        # Extract periods
        graph_periods = self._extract_periods(period_headers, items_col)

        # Create and populate graph
        return self._create_graph(
            df, graph_periods, items_col, mapping, file_path, sheet_name
        )

    def _read_excel_data(
        self,
        file_path: str,
        sheet_name: str,
        periods_row: int,
        items_col: int,
        header_row: int,
        nrows: Optional[int],
        skiprows: Optional[int],
    ) -> tuple[pd.DataFrame, list[str]]:
        """Read Excel file and extract data and period headers."""
        # Convert to 0-based indices for pandas
        periods_row_0idx = periods_row - 1
        items_col_0idx = items_col - 1
        header_row_0idx = header_row - 1

        # Read the main data
        df = pd.read_excel(
            file_path,
            sheet_name=sheet_name,
            header=header_row_0idx,
            skiprows=skiprows,
            nrows=nrows,
        )

        # Get period headers
        if header_row_0idx != periods_row_0idx:
            # Read periods row separately if different from header
            periods_df = pd.read_excel(
                file_path,
                sheet_name=sheet_name,
                header=None,
                skiprows=periods_row_0idx,
                nrows=1,
            )
            period_headers = periods_df.iloc[0].astype(str).tolist()
        else:
            # Periods are in the main header row
            period_headers = df.columns.astype(str).tolist()

        # Validate items column index using ValidationMixin
        self.validate_column_bounds(
            df, items_col_0idx, file_path, f"items_col ({items_col})"
        )

        return df, period_headers

    def _extract_periods(self, period_headers: list[str], items_col: int) -> list[str]:
        """Extract valid period names from headers."""
        items_col_0idx = items_col - 1

        # Filter period headers: exclude the item column and empty values
        graph_periods = [
            p
            for i, p in enumerate(period_headers)
            if i > items_col_0idx and p and p.strip()
        ]

        # Validate periods using ValidationMixin
        self.validate_periods_exist(graph_periods, "Excel file")

        logger.info(f"Identified periods: {graph_periods}")
        return graph_periods

    def _create_graph(
        self,
        df: pd.DataFrame,
        graph_periods: list[str],
        items_col: int,
        mapping: dict[str, str],
        file_path: str,
        sheet_name: str,
    ) -> Graph:
        """Create and populate the graph from DataFrame."""
        items_col_0idx = items_col - 1
        graph = Graph(periods=graph_periods)

        # Use validation collector
        validator = ValidationResultCollector()
        nodes_added = 0

        for index, row in df.iterrows():
            # Get item name
            item_name_excel = row.iloc[items_col_0idx]
            if pd.isna(item_name_excel) or not item_name_excel:
                continue

            item_name_excel = str(item_name_excel).strip()
            node_name = self._apply_mapping(item_name_excel, mapping)

            # Extract values for all periods
            period_values = self._extract_row_values(
                row, df, graph_periods, node_name, item_name_excel, index, validator
            )

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' (from Excel item '{item_name_excel}') already exists. "
                        "Overwriting data is not standard for readers."
                    )
                else:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

        # Check for validation errors
        if validator.has_errors():
            summary = validator.get_summary()
            raise ReadError(
                f"Validation errors occurred while reading {file_path} sheet '{sheet_name}': "
                f"{'; '.join(summary['errors'])}",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        logger.info(
            f"Successfully created graph with {nodes_added} nodes from {file_path} sheet '{sheet_name}'."
        )
        return graph

    def _extract_row_values(
        self,
        row: pd.Series,
        df: pd.DataFrame,
        graph_periods: list[str],
        node_name: str,
        item_name_excel: str,
        row_index: int,
        validator: ValidationResultCollector,
    ) -> dict[str, float]:
        """Extract period values from a row with validation."""
        period_values: dict[str, float] = {}

        for period in graph_periods:
            if period not in df.columns:
                logger.warning(
                    f"Period header '{period}' not found in DataFrame columns for row {row_index}."
                )
                continue

            value = row[period]

            if pd.isna(value):
                continue  # Skip NaN values

            # Use ValidationMixin for numeric validation
            is_valid, converted_value = self.validate_numeric_value(
                value, node_name, period, validator, allow_conversion=True
            )

            if is_valid and converted_value is not None:
                period_values[period] = converted_value

        return period_values



================================================================================
File: fin_statement_model/io/formats/excel/writer.py
================================================================================

"""Data writer for Excel files."""

import logging
from pathlib import Path
from typing import Any, Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.base import DataWriter
from fin_statement_model.io.core.mixins import (
    ConfigurationMixin,
    handle_write_errors,
)
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.io.formats.dataframe.writer import DataFrameWriter
from fin_statement_model.io.config.models import (
    ExcelWriterConfig,
    DataFrameWriterConfig,
)

logger = logging.getLogger(__name__)


@register_writer("excel")
class ExcelWriter(DataWriter, ConfigurationMixin):
    """Writes graph data to an Excel file.

    Converts the graph data to a pandas DataFrame first (using `DataFrameWriter`),
    then writes that DataFrame to an Excel file using `pandas.to_excel()`.

    Configuration (sheet_name, recalculate, include_nodes, excel_writer_kwargs) is
    provided via an `ExcelWriterConfig` object during initialization.
    """

    def __init__(self, cfg: Optional[ExcelWriterConfig] = None) -> None:
        """Initialize the ExcelWriter.

        Args:
            cfg: Optional validated `ExcelWriterConfig` instance.
        """
        super().__init__()
        self.cfg = cfg

    @handle_write_errors()
    def write(self, graph: Graph, target: str, **kwargs: Any) -> None:
        """Write graph data to an Excel file, converting via DataFrame first.

        Args:
            graph (Graph): The Graph object containing the data to write.
            target (str): Path to the target Excel file.
            **kwargs: Optional runtime overrides of configured defaults:
                sheet_name (str): Excel sheet name.
                recalculate (bool): Whether to recalculate graph before export.
                include_nodes (list[str]): List of node names to include in export.
                excel_writer_kwargs (dict): Additional kwargs for pandas.DataFrame.to_excel.

        Raises:
            WriteError: If an error occurs during the writing process.
        """
        file_path = target

        # Runtime overrides: kwargs override configured defaults
        if self.cfg is None:
            raise ValueError("ExcelWriter configuration is required but not provided")

        sheet_name = kwargs.get("sheet_name", self.cfg.sheet_name)
        recalculate = kwargs.get("recalculate", self.cfg.recalculate)
        include_nodes = kwargs.get("include_nodes", self.cfg.include_nodes)
        excel_writer_options = kwargs.get(
            "excel_writer_kwargs", self.cfg.excel_writer_kwargs
        )

        logger.info(f"Exporting graph to Excel file: {file_path}, sheet: {sheet_name}")

        # Convert graph to DataFrame
        df = self._create_dataframe(graph, recalculate, include_nodes)

        # Write DataFrame to Excel
        self._write_to_excel(df, file_path, sheet_name, excel_writer_options)

        logger.info(f"Successfully exported graph to {file_path}, sheet '{sheet_name}'")

    def _create_dataframe(
        self, graph: Graph, recalculate: bool, include_nodes: Optional[list[str]]
    ) -> Any:
        """Convert graph to DataFrame using DataFrameWriter.

        Args:
            graph: The graph to convert.
            recalculate: Whether to recalculate before export.
            include_nodes: Optional list of nodes to include.

        Returns:
            pandas DataFrame with the graph data.
        """
        # Create a config for DataFrameWriter
        df_config = DataFrameWriterConfig(
            target=None,
            format_type="dataframe",
            recalculate=recalculate,
            include_nodes=include_nodes,
        )

        df_writer = DataFrameWriter(df_config)
        return df_writer.write(graph=graph, target=None)

    def _write_to_excel(
        self,
        df: Any,
        file_path: str,
        sheet_name: str,
        excel_writer_options: dict[str, Any],
    ) -> None:
        """Write DataFrame to Excel file.

        Args:
            df: The pandas DataFrame to write.
            file_path: Path to the output file.
            sheet_name: Name of the Excel sheet.
            excel_writer_options: Additional options for pandas.to_excel().
        """
        output_path = Path(file_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        df.to_excel(
            output_path,
            sheet_name=sheet_name,
            index=True,  # Keep node names as index column
            **excel_writer_options,
        )



================================================================================
File: fin_statement_model/io/formats/markdown/__init__.py
================================================================================

"""Markdown format IO operations."""

from .writer import MarkdownWriter
from .renderer import MarkdownStatementRenderer
from .formatter import MarkdownTableFormatter
from .notes import MarkdownNotesBuilder
from .models import MarkdownStatementItem

__all__ = [
    "MarkdownNotesBuilder",
    "MarkdownStatementItem",
    "MarkdownStatementRenderer",
    "MarkdownTableFormatter",
    "MarkdownWriter",
]



================================================================================
File: fin_statement_model/io/formats/markdown/formatter.py
================================================================================

"""Formatter for converting statement items to markdown table format."""

import logging
from typing import Any, Optional, Union

from fin_statement_model.io.formats.markdown.models import MarkdownStatementItem

logger = logging.getLogger(__name__)


class MarkdownTableFormatter:
    """Formats statement items into markdown table format.

    This class handles the table layout, column width calculations,
    and markdown-specific formatting like bold text for subtotals.
    """

    def __init__(self, indent_spaces: int = 4):
        """Initialize the formatter.

        Args:
            indent_spaces: Number of spaces per indentation level.
        """
        self.indent_spaces = indent_spaces

    def format_table(
        self,
        items: list[MarkdownStatementItem],
        periods: list[str],
        historical_periods: Optional[list[str]] = None,
        forecast_periods: Optional[list[str]] = None,
    ) -> list[str]:
        """Format items into markdown table lines.

        Args:
            items: List of MarkdownStatementItem objects to format.
            periods: List of all periods in order.
            historical_periods: List of historical period names.
            forecast_periods: List of forecast period names.

        Returns:
            List of strings representing the markdown table lines.
        """
        if not items:
            logger.warning("No items to format into table")
            return []

        # Convert to sets for faster lookup
        historical_set = set(historical_periods or [])
        forecast_set = set(forecast_periods or [])

        # Calculate column widths and format data
        max_desc_width, period_max_widths, formatted_lines = (
            self._calculate_widths_and_format(items, periods)
        )

        # Build the table
        output_lines = []

        # Build header row
        header_parts = ["Description".ljust(max_desc_width)]
        for period in periods:
            period_label = period
            if period in historical_set:
                period_label += " (H)"
            elif period in forecast_set:
                period_label += " (F)"
            header_parts.append(period_label.rjust(period_max_widths[period]))

        output_lines.append(f"| {' | '.join(header_parts)} |")

        # Add separator line
        separator_parts = ["-" * max_desc_width]
        separator_parts.extend("-" * period_max_widths[period] for period in periods)
        output_lines.append(f"| {' | '.join(separator_parts)} |")

        # Build data rows
        for line_data in formatted_lines:
            row_parts = [line_data["name"].ljust(max_desc_width)]
            for period in periods:
                value = line_data["values"].get(period, "")
                row_parts.append(value.rjust(period_max_widths[period]))
            output_lines.append(f"| {' | '.join(row_parts)} |")

        return output_lines

    def _calculate_widths_and_format(
        self, items: list[MarkdownStatementItem], periods: list[str]
    ) -> tuple[int, dict[str, int], list[dict[str, Any]]]:
        """Calculate column widths and format all data.

        Args:
            items: List of items to format.
            periods: List of periods.

        Returns:
            Tuple of (max_desc_width, period_max_widths, formatted_lines).
        """
        max_desc_width = 0
        period_max_widths = {p: 0 for p in periods}
        formatted_lines = []

        # First pass: format data and calculate max widths
        for item in items:
            indent = " " * (item["level"] * self.indent_spaces)
            name = f"{indent}{item['name']}"
            is_subtotal = item["is_subtotal"]
            is_contra = item.get("is_contra", False)
            values_formatted = {}

            # Apply markdown formatting for subtotals
            if is_subtotal:
                name = f"**{name}**"

            # Apply contra formatting if needed
            if is_contra:
                name = f"_{name}_"  # Italic for contra items

            max_desc_width = max(max_desc_width, len(name))

            # Format values for each period
            for period in periods:
                raw_value = item["values"].get(period)
                value_str = self._format_value(raw_value, item)

                # Apply markdown formatting for subtotals
                if is_subtotal:
                    value_str = f"**{value_str}**"
                elif is_contra:
                    value_str = f"_{value_str}_"  # Italic for contra items

                values_formatted[period] = value_str
                period_max_widths[period] = max(
                    period_max_widths[period], len(value_str)
                )

            formatted_lines.append(
                {
                    "name": name,
                    "values": values_formatted,
                    "is_subtotal": is_subtotal,
                    "is_contra": is_contra,
                }
            )

        return max_desc_width, period_max_widths, formatted_lines

    def _format_value(
        self, value: Union[float, int, str, None], item: MarkdownStatementItem
    ) -> str:
        """Format a single value for display in the table.

        Args:
            value: The value to format.
            item: The item containing formatting information.

        Returns:
            Formatted string representation of the value.
        """
        if value is None:
            return ""

        if isinstance(value, str):
            # Keep error strings and other text as-is
            return value

        if isinstance(value, float | int):
            # Use custom format if specified
            display_format = item.get("display_format")
            if display_format:
                try:
                    return format(value, display_format)
                except (ValueError, TypeError) as e:
                    logger.warning(f"Invalid display format '{display_format}': {e}")
                    # Fall back to default formatting

            # Default number formatting
            if isinstance(value, float):
                return f"{value:,.2f}"
            else:
                return f"{value:,}"

        # Fallback for any other type
        return str(value)



================================================================================
File: fin_statement_model/io/formats/markdown/models.py
================================================================================

"""Data models for markdown formatting."""

from typing import Optional, TypedDict, Union


class MarkdownStatementItem(TypedDict):
    """Enhanced representation of a line item for Markdown output.

    This extends the basic StatementItem concept from the old implementation
    with additional formatting and display properties.
    """

    name: str
    values: dict[str, Union[float, int, str, None]]  # Values per period
    level: int
    is_subtotal: bool  # Indicates if the row is a subtotal or section header
    sign_convention: int  # 1 for normal, -1 for inverted
    display_format: Optional[str]  # Optional number format string
    units: Optional[str]  # Unit description
    display_scale_factor: float  # Factor to scale values for display
    is_contra: bool  # Whether this is a contra item for special formatting



================================================================================
File: fin_statement_model/io/formats/markdown/notes.py
================================================================================

"""Builder for markdown notes sections (forecast and adjustment notes)."""

import logging
from typing import Any, Optional

from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentFilter,
    DEFAULT_SCENARIO,
)
from fin_statement_model.core.graph import Graph

logger = logging.getLogger(__name__)


class MarkdownNotesBuilder:
    """Builds notes sections for markdown output.

    This class handles the creation of forecast notes and adjustment notes
    that are appended to the main statement table.
    """

    def build_notes(
        self,
        graph: Graph,
        forecast_configs: Optional[dict[str, Any]] = None,
        adjustment_filter: Optional[Any] = None,
    ) -> list[str]:
        """Build forecast and adjustment notes.

        Args:
            graph: The Graph object containing financial data.
            forecast_configs: Dictionary mapping node IDs to forecast configurations.
            adjustment_filter: Filter for adjustments to include.

        Returns:
            List of strings representing the notes sections.
        """
        lines = []

        # Add forecast notes
        if forecast_configs:
            forecast_lines = self._build_forecast_notes(forecast_configs)
            if forecast_lines:
                lines.extend(forecast_lines)

        # Add adjustment notes
        adjustment_lines = self._build_adjustment_notes(graph, adjustment_filter)
        if adjustment_lines:
            lines.extend(adjustment_lines)

        return lines

    def _build_forecast_notes(self, forecast_configs: dict[str, Any]) -> list[str]:
        """Build forecast notes section.

        Args:
            forecast_configs: Dictionary mapping node IDs to forecast configurations.

        Returns:
            List of strings for the forecast notes section.
        """
        if not forecast_configs:
            return []

        notes = ["", "## Forecast Notes"]  # Add blank line before header

        for node_id, config in forecast_configs.items():
            method = config.get("method", "N/A")
            cfg_details = config.get("config")
            desc = f"- **{node_id}**: Forecasted using method '{method}'"

            # Add method-specific details
            if method == "simple" and cfg_details is not None:
                desc += f" (e.g., simple growth rate: {cfg_details:.1%})."
            elif method == "curve" and cfg_details:
                rates_str = ", ".join([f"{r:.1%}" for r in cfg_details])
                desc += f" (e.g., specific growth rates: [{rates_str}])."
            elif method == "historical_growth":
                desc += " (based on average historical growth)."
            elif method == "average":
                desc += " (based on historical average value)."
            elif method == "statistical":
                dist_name = cfg_details.get("distribution", "unknown")
                params_dict = cfg_details.get("params", {})
                params_str = ", ".join(
                    [
                        f"{k}={v:.3f}" if isinstance(v, float) else f"{k}={v}"
                        for k, v in params_dict.items()
                    ]
                )
                desc += (
                    f" (using '{dist_name}' distribution with params: {params_str})."
                )
            else:
                desc += "."

            notes.append(desc)

        return notes

    def _build_adjustment_notes(
        self, graph: Graph, adjustment_filter: Optional[Any] = None
    ) -> list[str]:
        """Build adjustment notes section.

        Args:
            graph: The Graph object containing adjustments.
            adjustment_filter: Filter for adjustments to include.

        Returns:
            List of strings for the adjustment notes section.
        """
        all_adjustments: list[Adjustment] = graph.list_all_adjustments()
        if not all_adjustments:
            return []

        # Apply filter to adjustments
        filtered_adjustments = self._filter_adjustments(
            all_adjustments, adjustment_filter
        )
        if not filtered_adjustments:
            return []

        lines = [
            "",
            "## Adjustment Notes (Matching Filter)",
        ]  # Add blank line before header

        # Sort adjustments for consistent output
        sorted_adjustments = sorted(
            filtered_adjustments,
            key=lambda x: (x.node_name, x.period, x.priority, x.timestamp),
        )

        for adj in sorted_adjustments:
            tags_str = ", ".join(sorted(adj.tags)) if adj.tags else "None"
            details = (
                f"- **{adj.node_name}** ({adj.period}, Scenario: {adj.scenario}, "
                f"Prio: {adj.priority}): {adj.type.name.capitalize()} adjustment of {adj.value:.2f}. "
                f"Reason: {adj.reason}. Tags: [{tags_str}]. (ID: {adj.id})"
            )
            lines.append(details)

        return lines

    def _filter_adjustments(
        self, all_adjustments: list[Adjustment], adjustment_filter: Optional[Any]
    ) -> list[Adjustment]:
        """Filter adjustments based on the provided filter.

        Args:
            all_adjustments: List of all adjustments.
            adjustment_filter: Filter to apply.

        Returns:
            List of filtered adjustments.
        """
        # Create a filter instance based on the input
        filt: AdjustmentFilter

        if isinstance(adjustment_filter, AdjustmentFilter):
            filt = adjustment_filter.model_copy(
                update={"period": None}
            )  # Ignore period context
        elif isinstance(adjustment_filter, set):
            filt = AdjustmentFilter(
                include_tags=adjustment_filter,
                include_scenarios={
                    DEFAULT_SCENARIO
                },  # Assume default scenario for tag shorthand
                period=None,
            )
        else:  # Includes None or other types
            filt = AdjustmentFilter(include_scenarios={DEFAULT_SCENARIO}, period=None)

        # Apply the filter
        return [adj for adj in all_adjustments if filt.matches(adj)]



================================================================================
File: fin_statement_model/io/formats/markdown/renderer.py
================================================================================

"""Helper class for rendering StatementStructure to markdown format."""

import logging
from typing import Optional, Union

from fin_statement_model.core.graph import Graph
from fin_statement_model.statements.structure import (
    Section,
    StatementItem,
    StatementItemType,
    StatementStructure,
    LineItem,
    CalculatedLineItem,
    SubtotalLineItem,
    MetricLineItem,
)
from fin_statement_model.io.formats.markdown.models import MarkdownStatementItem

logger = logging.getLogger(__name__)


class MarkdownStatementRenderer:
    """Helper class that renders StatementStructure to markdown format.

    This class handles the recursive traversal of a StatementStructure and
    extracts values from the graph to create formatted items for markdown output.
    """

    def __init__(self, graph: Graph, indent_spaces: int = 4):
        """Initialize the renderer.

        Args:
            graph: The Graph object containing financial data.
            indent_spaces: Number of spaces per indentation level.
        """
        self.graph = graph
        self.indent_spaces = indent_spaces
        self.periods = sorted(list(graph.periods))

    def render_structure(
        self,
        structure: StatementStructure,
        historical_periods: Optional[set[str]] = None,
        forecast_periods: Optional[set[str]] = None,
    ) -> list[MarkdownStatementItem]:
        """Traverse StatementStructure and extract formatted items.

        Args:
            structure: The StatementStructure to render.
            historical_periods: Set of historical period names.
            forecast_periods: Set of forecast period names.

        Returns:
            List of MarkdownStatementItem objects ready for formatting.
        """
        logger.debug(f"Rendering statement structure: {structure.id}")
        items = []

        for section in structure.sections:
            items.extend(self._render_section(section, level=0))

        logger.debug(f"Rendered {len(items)} items from structure")
        return items

    def _render_section(
        self, section: Section, level: int
    ) -> list[MarkdownStatementItem]:
        """Recursively render a section and its contents.

        Args:
            section: The Section to render.
            level: Current indentation level.

        Returns:
            List of rendered items from this section.
        """
        items = []

        # Process items within the section
        for item in section.items:
            if isinstance(item, Section):
                # Nested section - recurse
                items.extend(self._render_section(item, level + 1))
            else:
                # Statement item (LineItem, CalculatedLineItem, etc.)
                rendered_item = self._render_item(item, level + 1)
                if rendered_item:
                    items.append(rendered_item)

        # Process section subtotal if exists
        if hasattr(section, "subtotal") and section.subtotal:
            rendered_subtotal = self._render_item(section.subtotal, level + 1)
            if rendered_subtotal:
                items.append(rendered_subtotal)

        return items

    def _render_item(
        self, item: StatementItem, level: int
    ) -> Optional[MarkdownStatementItem]:
        """Render a single statement item with values from graph.

        Args:
            item: The StatementItem to render.
            level: Current indentation level.

        Returns:
            MarkdownStatementItem or None if item couldn't be rendered.
        """
        try:
            # Extract values based on item type
            values = self._extract_values(item)

            return MarkdownStatementItem(
                name=item.name,
                values=values,
                level=level,
                is_subtotal=(item.item_type == StatementItemType.SUBTOTAL),
                sign_convention=getattr(item, "sign_convention", 1),
                display_format=item.display_format,
                units=item.units,
                display_scale_factor=item.display_scale_factor,
                is_contra=item.is_contra,
            )
        except Exception as e:
            logger.warning(f"Failed to render item {item.id}: {e}")
            return None

    def _extract_values(
        self, item: StatementItem
    ) -> dict[str, Union[float, int, str, None]]:
        """Extract values for an item from the graph.

        Args:
            item: The StatementItem to extract values for.

        Returns:
            Dictionary mapping periods to values.
        """
        values = {}

        # Get the node ID based on item type
        node_id = self._get_node_id(item)
        if not node_id:
            logger.warning(f"Could not determine node ID for item: {item.id}")
            return {period: None for period in self.periods}

        try:
            node = self.graph.get_node(node_id)
            if node is None:
                logger.warning(f"Node '{node_id}' returned None for item: {item.id}")
                return {period: None for period in self.periods}

            for period in self.periods:
                raw_value = None

                # Calculation items use graph.calculate, pure line items use node.get_value
                if isinstance(
                    item, (CalculatedLineItem, SubtotalLineItem, MetricLineItem)
                ):
                    # Calculate value for derived items
                    try:
                        raw_value = self.graph.calculate(node_id, period)
                    except Exception as calc_error:
                        logger.warning(
                            f"Calculation failed for node '{node_id}' period '{period}': {calc_error}"
                        )
                        raw_value = "CALC_ERR"  # type: ignore[assignment]
                elif isinstance(item, LineItem):
                    # Direct value from node for basic line items
                    raw_value = node.get_value(period)
                else:
                    logger.warning(
                        f"Unsupported item type: {type(item)} for item: {item.id}"
                    )
                    raw_value = None

                # Apply sign convention and scaling
                values[period] = self._apply_formatting(raw_value, item)

        except KeyError:
            logger.warning(f"Node '{node_id}' not found in graph for item: {item.id}")
            values = {period: None for period in self.periods}
        except Exception:
            logger.exception(f"Error extracting values for item {item.id}")
            values = {period: "ERROR" for period in self.periods}

        return values

    def _get_node_id(self, item: StatementItem) -> Optional[str]:
        """Get the node ID for a statement item.

        Args:
            item: The StatementItem to get node ID for.

        Returns:
            The node ID or None if it couldn't be determined.
        """
        if isinstance(item, LineItem):
            # For LineItem, try to resolve node_id or standard_node_ref
            if item.node_id:
                return item.node_id
            elif item.standard_node_ref:
                return item.get_resolved_node_id()
            else:
                return None
        elif isinstance(item, CalculatedLineItem | SubtotalLineItem | MetricLineItem):
            # For calculated items, use the item ID as node ID
            return item.id
        else:
            logger.warning(f"Unknown item type for node ID resolution: {type(item)}")
            return None

    def _apply_formatting(
        self, raw_value: Union[float, int, str, None], item: StatementItem
    ) -> Union[float, int, str, None]:
        """Apply sign convention and scaling to a raw value.

        Args:
            raw_value: The raw value from the graph.
            item: The StatementItem containing formatting info.

        Returns:
            The formatted value.
        """
        if raw_value is None or isinstance(raw_value, str):
            # Keep None and error strings as-is
            return raw_value

        if isinstance(raw_value, int | float):
            # Apply sign convention
            sign_convention = getattr(item, "sign_convention", 1)
            formatted_value = raw_value * sign_convention

            # Apply display scale factor
            scale_factor = item.display_scale_factor
            if scale_factor != 1.0:
                formatted_value = formatted_value * scale_factor

            return formatted_value

        return raw_value



================================================================================
File: fin_statement_model/io/formats/markdown/writer.py
================================================================================

"""Writes a financial statement graph to a Markdown table."""

import logging
from typing import Any, Optional, TypedDict, Union

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.base import DataWriter
from fin_statement_model.io.config.models import BaseWriterConfig
from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.io.formats.markdown.renderer import MarkdownStatementRenderer
from fin_statement_model.io.formats.markdown.formatter import MarkdownTableFormatter
from fin_statement_model.io.formats.markdown.notes import MarkdownNotesBuilder

logger = logging.getLogger(__name__)


class MarkdownWriterConfig(BaseWriterConfig):
    """Configuration specific to the Markdown writer."""

    indent_spaces: int = 4  # Number of spaces per indentation level
    # Add other Markdown-specific config options here if needed


# Legacy structure for backward compatibility
class StatementItem(TypedDict):
    """Represents a line item with its values for Markdown output."""

    name: str
    # value: Union[float, int, str, None] # Replaced single value
    values: dict[str, Union[float, int, str, None]]  # Values per period
    level: int
    is_subtotal: bool  # Indicates if the row is a subtotal or section header


@register_writer("markdown")
class MarkdownWriter(DataWriter):
    """Writes a financial statement structure to a Markdown table."""

    def __init__(self, config: Optional[MarkdownWriterConfig] = None):
        """Initializes the MarkdownWriter."""
        self.config = config or MarkdownWriterConfig(
            format_type="markdown", target=None
        )
        logger.debug(f"Initialized MarkdownWriter with config: {self.config}")

    def _format_value(self, value: Union[float, int, str, None]) -> str:
        """Formats the value for display in the table."""
        if value is None:
            return ""
        if isinstance(value, float | int):
            # Basic number formatting, could be enhanced (e.g., commas)
            return f"{value:,.2f}" if isinstance(value, float) else str(value)
        return str(value)

    def write(self, graph: Graph, target: Any = None, **kwargs: Any) -> str:
        """Write financial statement to markdown.

        Args:
            graph: The Graph object containing the financial data.
            target: Ignored by this writer (returns string).
            **kwargs: Additional options including:
                - statement_structure: The StatementStructure to render

        Returns:
            String containing the formatted statement in Markdown.

        Raises:
            WriteError: If 'statement_structure' is not provided.
        """
        logger.info(
            f"Writing graph to Markdown format (target ignored: {target}) using kwargs: {kwargs.keys()}"
        )

        try:
            statement_structure = kwargs.get("statement_structure")

            # Fallback: build StatementStructure on-the-fly from raw_configs
            if statement_structure is None and "raw_configs" in kwargs:
                raw_configs = kwargs["raw_configs"]
                try:
                    from fin_statement_model.statements.structure.builder import (
                        StatementStructureBuilder,
                    )
                    from fin_statement_model.statements.orchestration.loader import (
                        load_build_register_statements,
                    )
                    from fin_statement_model.statements.registry import (
                        StatementRegistry,
                    )

                    # Build and register statement(s)
                    registry = StatementRegistry()
                    builder = StatementStructureBuilder(
                        enable_node_validation=False, node_validation_strict=False
                    )
                    loaded_ids = load_build_register_statements(
                        raw_configs,
                        registry,
                        builder,
                        enable_node_validation=False,
                        node_validation_strict=False,
                    )

                    if not loaded_ids:
                        raise WriteError(
                            "No valid statements could be built from raw_configs."
                        )

                    # Use the *first* statement by default – users can supply a
                    # pre-built structure via 'statement_structure' for more
                    # complex scenarios or when specific statement selection is needed.
                    # TODO: Consider adding a 'statement_id' parameter to select which statement to use
                    statement_structure = registry.get(loaded_ids[0])
                except Exception as build_err:
                    logger.exception(
                        "Failed to build statement structure from raw_configs"
                    )
                    raise WriteError(
                        message="Error building statement structure from raw_configs",
                        target=target,
                        writer_type="markdown",
                        original_error=build_err,
                    ) from build_err

            # If still None, we cannot proceed.
            if statement_structure is None:
                raise WriteError(
                    "Must provide 'statement_structure' argument or 'raw_configs'."
                )

            filtered_kwargs = {
                k: v
                for k, v in kwargs.items()
                if k not in ("statement_structure", "raw_configs")
            }
            return self._write_with_structure(
                graph, statement_structure, **filtered_kwargs
            )
        except NotImplementedError as nie:
            logger.exception("Markdown write failed")
            raise WriteError(
                message=f"Markdown writer requires graph traversal logic: {nie}",
                target=target,
                writer_type="markdown",
                original_error=nie,
            ) from nie
        except Exception as e:
            logger.exception("Error writing Markdown for graph", exc_info=True)
            raise WriteError(
                message=f"Failed to generate Markdown table: {e}",
                target=target,
                writer_type="markdown",
                original_error=e,
            ) from e

    def _write_with_structure(
        self, graph: Graph, statement_structure: StatementStructure, **kwargs: Any
    ) -> str:
        """Write using the new StatementStructure approach.

        Args:
            graph: The Graph object containing financial data.
            statement_structure: The StatementStructure to render.
            **kwargs: Additional options.

        Returns:
            Formatted markdown string.
        """
        # Use renderer to process structure
        renderer = MarkdownStatementRenderer(graph, self.config.indent_spaces)
        items = renderer.render_structure(
            statement_structure,
            historical_periods=set(kwargs.get("historical_periods", [])),
            forecast_periods=set(kwargs.get("forecast_periods", [])),
        )

        if not items:
            logger.warning("No statement items generated from structure.")
            return ""

        # Format into markdown table
        formatter = MarkdownTableFormatter(self.config.indent_spaces)
        table_lines = formatter.format_table(
            items,
            periods=renderer.periods,
            historical_periods=kwargs.get("historical_periods"),
            forecast_periods=kwargs.get("forecast_periods"),
        )

        # Add notes sections
        notes_builder = MarkdownNotesBuilder()
        notes_lines = notes_builder.build_notes(
            graph=graph,
            forecast_configs=kwargs.get("forecast_configs"),
            adjustment_filter=kwargs.get("adjustment_filter"),
        )

        return "\n".join(table_lines + notes_lines)



================================================================================
File: fin_statement_model/io/specialized/__init__.py
================================================================================

"""Specialized IO operations for domain-specific functionality."""

# Adjustments
from .adjustments import (
    read_excel,
    write_excel,
    load_adjustments_from_excel,
    export_adjustments_to_excel,
)

# Cells
from .cells import import_from_cells

# Graph serialization
from .graph import (
    GraphDefinitionReader,
    GraphDefinitionWriter,
    save_graph_definition,
    load_graph_definition,
)

# Statement utilities
from .statements import (
    list_available_builtin_configs,
    read_builtin_statement_config,
    write_statement_to_excel,
    write_statement_to_json,
)

__all__ = [
    # Graph
    "GraphDefinitionReader",
    "GraphDefinitionWriter",
    # Adjustments
    "export_adjustments_to_excel",
    # Cells
    "import_from_cells",
    # Statements
    "list_available_builtin_configs",
    "read_builtin_statement_config",
    "write_statement_to_excel",
    "write_statement_to_json",
    # Graph serialization
    "save_graph_definition",
    "load_graph_definition",
    # File readers/writers for data sources
    "read_excel",
    "write_excel",
    "load_adjustments_from_excel",
]



================================================================================
File: fin_statement_model/io/specialized/adjustments.py
================================================================================

"""Functions for bulk import and export of adjustments via Excel files."""

import logging
from typing import Optional, Any, cast
from pathlib import Path
from collections import defaultdict

import pandas as pd

from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentType,
    AdjustmentTag,
    DEFAULT_SCENARIO,
)
from fin_statement_model.core.graph import Graph  # Needed for Graph convenience methods
from fin_statement_model.io.exceptions import ReadError, WriteError

logger = logging.getLogger(__name__)

# Define expected column names (case-insensitive matching during read)
# Required columns per spec:
REQ_COLS = {"node_name", "period", "value", "reason"}
# Optional columns per spec:
OPT_COLS = {
    "type",
    "tags",
    "scale",
    "scenario",
    "start_period",
    "end_period",
    "priority",
    "user",
    "id",
}
ALL_COLS = REQ_COLS.union(OPT_COLS)

# Map DataFrame column names (lowercase) to Adjustment model fields
COL_TO_FIELD_MAP = {
    "node_name": "node_name",
    "period": "period",
    "value": "value",
    "reason": "reason",
    "type": "type",
    "tags": "tags",
    "scale": "scale",
    "scenario": "scenario",
    "start_period": "start_period",
    "end_period": "end_period",
    "priority": "priority",
    "user": "user",
    "id": "id",
    # Note: timestamp is not expected in input file, generated on creation
}


def _parse_tags(tag_str: Optional[str]) -> set[AdjustmentTag]:
    """Parse a comma-separated tag string into a set."""
    if pd.isna(tag_str) or not isinstance(tag_str, str) or not tag_str.strip():
        return set()
    return set(tag.strip() for tag in tag_str.split(",") if tag.strip())


def read_excel(path: str | Path) -> tuple[list[Adjustment], pd.DataFrame]:
    """Read adjustments from an Excel file.

    Expects the first sheet to contain adjustment data.
    Validates required columns and attempts to parse each row into an Adjustment object.
    Rows that fail validation are collected into an error report DataFrame.

    Args:
        path: Path to the Excel file.

    Returns:
        A tuple containing:
            - list[Adjustment]: A list of successfully parsed Adjustment objects.
            - pd.DataFrame: A DataFrame containing rows that failed validation,
                          with an added 'error' column explaining the issue.

    Raises:
        ReadError: If the file cannot be read, sheet is missing, or required
                   columns are not found.
    """
    file_path = Path(path)
    logger.info(f"Reading adjustments from Excel file: {file_path}")

    try:
        # Read the first sheet by default
        df = pd.read_excel(file_path, sheet_name=0)
    except FileNotFoundError:
        raise ReadError(
            f"Adjustment Excel file not found: {file_path}", source=str(file_path)
        )
    except Exception as e:
        # Catch other potential pandas read errors (e.g., bad format, permissions)
        raise ReadError(
            f"Failed to read Excel file {file_path}: {e}",
            source=str(file_path),
            original_error=e,
        )

    # Normalize column names to lowercase for case-insensitive matching
    df.columns = [str(col).lower().strip() for col in df.columns]
    actual_cols = set(df.columns)

    # Check for required columns
    missing_req_cols = REQ_COLS - actual_cols
    if missing_req_cols:
        raise ReadError(
            f"Missing required columns in adjustment Excel file: {missing_req_cols}",
            source=str(file_path),
        )

    valid_adjustments: list[Adjustment] = []
    error_rows: list[dict[str, Any]] = []

    for index, row in df.iterrows():
        adj_data: dict[str, Any] = {}
        parse_errors: list[str] = []

        # Map columns to Adjustment fields
        for col_name, field_name in COL_TO_FIELD_MAP.items():
            if col_name in df.columns:
                value = row[col_name]
                # Handle potential NaNs from Excel
                if pd.isna(value):
                    adj_data[field_name] = None
                else:
                    # Specific type conversions / parsing
                    try:
                        if field_name == "tags":
                            adj_data[field_name] = _parse_tags(str(value))
                        elif field_name == "type":
                            adj_data[field_name] = AdjustmentType(str(value).lower())
                        elif field_name == "priority":
                            adj_data[field_name] = int(value)
                        elif field_name in {"scale", "value"}:
                            adj_data[field_name] = float(value)
                        elif field_name == "id":
                            # Allow specific UUIDs from input
                            from uuid import UUID  # Local import

                            adj_data[field_name] = UUID(str(value))
                        else:
                            # Default to string conversion for others (node_name, period, etc.)
                            adj_data[field_name] = str(value)
                    except ValueError as e:
                        parse_errors.append(
                            f"Column '{col_name}': Invalid value '{value}' ({e})"
                        )
                    except Exception as e:
                        parse_errors.append(
                            f"Column '{col_name}': Error parsing value '{value}' ({e})"
                        )
            else:
                # Optional field not present
                adj_data[field_name] = None

        # Fill defaults for optional fields if not provided/mapped
        adj_data.setdefault("scenario", DEFAULT_SCENARIO)
        adj_data.setdefault("scale", 1.0)
        adj_data.setdefault("priority", 0)
        adj_data.setdefault("type", AdjustmentType.ADDITIVE)
        adj_data.setdefault("tags", set())

        # Remove None values for fields that shouldn't be None if missing (handled by Pydantic defaults later)
        # This is mainly for fields where None might cause issues if explicitly passed to Pydantic
        # e.g., pydantic might handle default factories better if key is absent vs. key=None
        # Let's be explicit for required ones:
        if adj_data.get("node_name") is None:
            parse_errors.append("Column 'node_name': Missing value")
        if adj_data.get("period") is None:
            parse_errors.append("Column 'period': Missing value")
        if adj_data.get("value") is None:
            parse_errors.append("Column 'value': Missing value")
        if adj_data.get("reason") is None:
            parse_errors.append("Column 'reason': Missing value")

        if parse_errors:
            error_detail = "; ".join(parse_errors)
            error_row = row.to_dict()
            error_row["error"] = error_detail
            error_rows.append(error_row)
            logger.debug(f"Row {index + 2}: Validation failed - {error_detail}")
            continue

        # Attempt to create the Adjustment object (final validation)
        try:
            # Filter out keys with None values before passing to Adjustment, let Pydantic handle defaults
            final_adj_data = {k: v for k, v in adj_data.items() if v is not None}
            adjustment = Adjustment(**final_adj_data)
            valid_adjustments.append(adjustment)
        except Exception as e:
            error_detail = f"Pydantic validation failed: {e}"
            error_row = row.to_dict()
            error_row["error"] = error_detail
            error_rows.append(error_row)
            logger.debug(f"Row {index + 2}: Pydantic validation failed - {e}")

    error_report_df = pd.DataFrame(error_rows)
    if not error_report_df.empty:
        logger.warning(
            f"Completed reading adjustments from {file_path}. Found {len(valid_adjustments)} valid adjustments and {len(error_rows)} errors."
        )
    else:
        logger.info(
            f"Successfully read {len(valid_adjustments)} adjustments from {file_path} with no errors."
        )

    return valid_adjustments, error_report_df


def write_excel(adjustments: list[Adjustment], path: str | Path) -> None:
    """Write a list of adjustments to an Excel file.

    Writes adjustments to separate sheets based on their scenario.
    The columns will match the optional fields defined for reading.

    Args:
        adjustments: A list of Adjustment objects to write.
        path: Path for the output Excel file.

    Raises:
        WriteError: If writing to the file fails.
    """
    file_path = Path(path)
    logger.info(f"Writing {len(adjustments)} adjustments to Excel file: {file_path}")

    # Group adjustments by scenario
    grouped_by_scenario: dict[str, list[dict[str, Any]]] = defaultdict(list)
    for adj in adjustments:
        # Use model_dump for serialization, exclude fields we don't usually export
        adj_dict = adj.model_dump(exclude={"timestamp"})  # Exclude timestamp by default
        # Convert complex types to simple types for Excel
        adj_dict["id"] = str(adj_dict.get("id"))
        raw_type = adj_dict.get("type")
        # Cast to AdjustmentType to access .value safely if present
        adj_dict["type"] = cast(AdjustmentType, raw_type).value if raw_type else None
        adj_dict["tags"] = ",".join(sorted(adj_dict.get("tags", set())))
        grouped_by_scenario[adj.scenario].append(adj_dict)

    if not grouped_by_scenario:
        logger.warning("No adjustments provided to write_excel. Creating empty file.")
        # Create an empty file or handle as desired
        try:
            pd.DataFrame().to_excel(file_path, index=False)
        except Exception as e:
            raise WriteError(
                f"Failed to write empty Excel file {file_path}: {e}",
                target=str(file_path),
                original_error=e,
            )
        return

    try:
        with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
            for scenario, scenario_adjustments in grouped_by_scenario.items():
                df = pd.DataFrame(scenario_adjustments)
                # Reorder columns for consistency
                cols_ordered = [c for c in COL_TO_FIELD_MAP if c in df.columns]
                cols_ordered += [c for c in df.columns if c not in cols_ordered]
                df = df[cols_ordered]
                # Sheet names must be valid
                safe_scenario_name = (
                    scenario.replace(":", "-").replace("/", "-").replace("\\", "-")[:31]
                )
                df.to_excel(writer, sheet_name=safe_scenario_name, index=False)
        logger.info(f"Successfully wrote adjustments to {file_path}")
    except Exception as e:
        logger.error(
            f"Failed to write adjustments to Excel file {file_path}: {e}", exc_info=True
        )
        raise WriteError(
            f"Failed to write adjustments to Excel: {e}",
            target=str(file_path),
            original_error=e,
        )


# --- Graph Convenience Methods ---


def load_adjustments_from_excel(
    graph: Graph, path: str | Path, replace: bool = False
) -> pd.DataFrame:
    """Reads adjustments from Excel and adds them to the graph.

    Args:
        graph: The Graph instance to add adjustments to.
        path: Path to the Excel file.
        replace: If True, clear existing adjustments in the manager before adding new ones.

    Returns:
        pd.DataFrame: The error report DataFrame from `read_excel`.
                   Empty if no errors occurred.
    """
    logger.info(
        f"Loading adjustments from Excel ({path}) into graph. Replace={replace}"
    )
    valid_adjustments, error_report_df = read_excel(path)

    if replace:
        logger.debug("Clearing existing adjustments before loading.")
        graph.adjustment_manager.clear_all()

    added_count = 0
    for adj in valid_adjustments:
        try:
            graph.adjustment_manager.add_adjustment(adj)
            added_count += 1
        except Exception as e:
            logger.error(
                f"Failed to add valid adjustment {adj.id} to graph: {e}", exc_info=True
            )
            # Optionally add this failure to the error report?
            error_row = adj.model_dump(mode="json")
            error_row["error"] = f"Failed to add to graph: {e}"
            # Need to handle DataFrame append carefully if modifying during iteration
            # Simplest is to report read errors, log add errors.

    logger.info(f"Added {added_count} adjustments to the graph from {path}.")
    if not error_report_df.empty:
        logger.warning(
            f"Encountered {len(error_report_df)} errors during Excel read process."
        )

    return error_report_df


def export_adjustments_to_excel(graph: Graph, path: str | Path) -> None:
    """Exports all adjustments from the graph to an Excel file.

    Args:
        graph: The Graph instance containing adjustments.
        path: Path for the output Excel file.
    """
    logger.info(f"Exporting all adjustments from graph to Excel ({path}).")
    all_adjustments = graph.list_all_adjustments()
    write_excel(all_adjustments, path)


# Add convenience methods to Graph class directly?
# This uses module patching which can sometimes be debated, but keeps the Graph API clean.
# Alternatively, users would call fin_statement_model.io.adjustments_excel.load_adjustments_from_excel(graph, path)
setattr(Graph, "load_adjustments_from_excel", load_adjustments_from_excel)
setattr(Graph, "export_adjustments_to_excel", export_adjustments_to_excel)



================================================================================
File: fin_statement_model/io/specialized/cells.py
================================================================================

"""Importer module for reading cell-based financial statement data into a Graph."""

from typing import Any

# from fin_statement_model.statements.graph.financial_graph import FinancialStatementGraph # Removed
from fin_statement_model.core.graph import Graph  # Added

__all__ = ["import_from_cells"]


def import_from_cells(cells_info: list[dict[str, Any]]) -> Graph:  # Changed return type
    """Import a list of cell dictionaries into a core Graph.

    Each cell dict should include at minimum:
    - 'row_name': identifier for the line item (becomes node ID)
    - 'column_name': the period label
    - 'value': the numeric value

    Args:
        cells_info: List of cell metadata dictionaries.

    Returns:
        A core Graph populated with detected periods and data nodes.
    """
    # Group cells by row_name to aggregate values per financial statement item
    items: dict[str, dict[str, Any]] = {}
    unique_periods: set[str] = set()

    for cell in cells_info:
        # Clean the item name and period
        item_name = cell.get("row_name", "").strip()
        period = cell.get("column_name", "").strip()
        value = cell.get("value")

        if not item_name or not period:
            continue

        unique_periods.add(period)
        if item_name not in items:
            items[item_name] = {}
        items[item_name][period] = value

    # Sort periods and create the graph
    sorted_periods = sorted(list(unique_periods))  # Ensure list for Graph constructor
    graph = Graph(periods=sorted_periods)  # Changed to Graph

    # Add each financial statement item as a data node to the graph
    for name, values in items.items():
        # Use add_financial_statement_item based on Graph API
        graph.add_financial_statement_item(name, values)

    return graph



================================================================================
File: fin_statement_model/io/specialized/graph.py
================================================================================

"""Graph definition serialization and deserialization.

This module provides functionality to save and load complete graph definitions,
including all nodes, periods, and adjustments.
"""

import logging
from typing import Any, Optional, cast

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.adjustments.models import Adjustment
from fin_statement_model.core.nodes import (
    Node,
)
from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.io.core import (
    DataReader,
    DataWriter,
    register_reader,
    register_writer,
)
from fin_statement_model.io.exceptions import ReadError, WriteError
from fin_statement_model.io.config.models import BaseWriterConfig

logger = logging.getLogger(__name__)

# Define a type for the serialized node dictionary for clarity
SerializedNode = dict[str, Any]


# ===== Reader Implementation =====


@register_reader("graph_definition_dict")
class GraphDefinitionReader(DataReader):
    """Reads a graph definition dictionary to reconstruct a Graph object.

    Handles reconstructing nodes based on their serialized type and configuration,
    and loads adjustments.
    """

    def __init__(self, cfg: Optional[Any] = None) -> None:
        """Initialize the GraphDefinitionReader. Config currently unused."""
        self.cfg = cfg

    def _get_node_dependencies(self, node_def: dict[str, Any]) -> list[str]:
        """Extract dependency names from a node definition.

        Args:
            node_def: Dictionary containing node definition.

        Returns:
            List of dependency node names.
        """
        node_type = node_def.get("type")

        if node_type == "financial_statement_item":
            return []  # No dependencies
        elif node_type in ["calculation", "formula_calculation"]:
            return cast(list[str], node_def.get("inputs", []))
        elif node_type == "forecast":
            base_node_name = cast(Optional[str], node_def.get("base_node_name"))
            return [base_node_name] if base_node_name is not None else []
        elif node_type == "custom_calculation":
            return cast(list[str], node_def.get("inputs", []))
        else:
            # Default: inputs should be list[str]
            return cast(list[str], node_def.get("inputs", []))

    def read(self, source: dict[str, Any], **kwargs: Any) -> Graph:
        """Reconstruct a Graph instance from its definition dictionary.

        Args:
            source: Dictionary containing the graph definition (periods, nodes, adjustments).
            **kwargs: Currently unused.

        Returns:
            A new Graph instance populated from the definition.

        Raises:
            ReadError: If the source format is invalid or graph reconstruction fails.
        """
        logger.info("Starting graph reconstruction from definition dictionary.")

        if (
            not isinstance(source, dict)
            or "periods" not in source
            or "nodes" not in source
        ):
            raise ReadError(
                message="Invalid source format for GraphDefinitionReader. Expected dict with 'periods' and 'nodes' keys.",
                source="graph_definition_dict",
                reader_type="GraphDefinitionReader",
            )

        try:
            # 1. Initialize Graph with Periods
            periods = source.get("periods", [])
            if not isinstance(periods, list):
                raise ReadError("Invalid format: 'periods' must be a list.")
            graph = Graph(periods=periods)

            # 2. Reconstruct Nodes using topological sort of definitions
            nodes_dict = source.get("nodes", {})
            if not isinstance(nodes_dict, dict):
                raise ReadError("Invalid format: 'nodes' must be a dictionary.")
            # Build dependency graph from definitions
            dep_graph: dict[str, list[str]] = {
                name: self._get_node_dependencies(defn)
                for name, defn in nodes_dict.items()
            }
            # Perform topological sort
            in_degree = {n: 0 for n in dep_graph}
            adjacency: dict[str, list[str]] = {n: [] for n in dep_graph}
            for n, deps in dep_graph.items():
                for dep in deps:
                    if dep not in dep_graph:
                        raise ReadError(
                            f"Dependency '{dep}' for node '{n}' not found in definitions.",
                            source="graph_definition_dict",
                        )
                    adjacency[dep].append(n)
                    in_degree[n] += 1
            queue = [n for n, d in in_degree.items() if d == 0]
            sorted_names: list[str] = []
            while queue:
                current = queue.pop()
                sorted_names.append(current)
                for neighbor in adjacency[current]:
                    in_degree[neighbor] -= 1
                    if in_degree[neighbor] == 0:
                        queue.append(neighbor)
            if len(sorted_names) != len(dep_graph):
                raise ReadError(
                    f"Circular dependency detected among nodes: {set(dep_graph) - set(sorted_names)}",
                    source="graph_definition_dict",
                )
            # Create and add nodes in sorted order
            for node_name in sorted_names:
                node_def = nodes_dict[node_name]
                existing_nodes = {name: graph.nodes[name] for name in graph.nodes}
                node = NodeFactory.create_from_dict(node_def, context=existing_nodes)
                graph.add_node(node)

            # 3. Load Adjustments
            adjustments_list = source.get("adjustments")  # Optional
            if adjustments_list is not None:
                if not isinstance(adjustments_list, list):
                    raise ReadError(
                        "Invalid format: 'adjustments' must be a list if present."
                    )

                deserialized_adjustments = []
                for i, adj_dict in enumerate(adjustments_list):
                    try:
                        # Use model_validate for Pydantic V2
                        adj = Adjustment.model_validate(adj_dict)
                        deserialized_adjustments.append(adj)
                    except Exception:
                        # Log error but try to continue with other nodes
                        logger.exception(
                            f"Failed to deserialize adjustment at index {i}: {adj_dict}. Skipping."
                        )
                        # Optionally raise ReadError here to fail fast

                if deserialized_adjustments:
                    graph.adjustment_manager.load_adjustments(deserialized_adjustments)
                    logger.info(
                        f"Loaded {len(deserialized_adjustments)} adjustments into the graph."
                    )

            logger.info(
                f"Successfully reconstructed graph with {len(graph.nodes)} nodes."
            )
            return graph

        except ReadError:  # Re-raise ReadErrors directly
            raise
        except Exception as e:
            logger.error(
                f"Failed to reconstruct graph from definition: {e}", exc_info=True
            )
            raise ReadError(
                message=f"Failed to reconstruct graph from definition: {e}",
                source="graph_definition_dict",
                reader_type="GraphDefinitionReader",
                original_error=e,
            ) from e


# ===== Writer Implementation =====


@register_writer("graph_definition_dict")
class GraphDefinitionWriter(DataWriter):
    """Writes the full graph definition (nodes, periods, adjustments) to a dictionary.

    This writer serializes the structure and configuration of the graph, suitable
    for saving and reloading the entire model state.
    """

    def __init__(self, cfg: Optional[BaseWriterConfig] = None) -> None:
        """Initialize the GraphDefinitionWriter."""
        self.cfg = cfg

    def _serialize_node(self, node: Node) -> Optional[SerializedNode]:
        """Serialize a single node using its to_dict() method.

        Args:
            node: The node to serialize.

        Returns:
            Dictionary representation of the node, or None if serialization fails.
        """
        try:
            return node.to_dict()
        except Exception:
            logger.exception(f"Failed to serialize node '{node.name}'")
            logger.warning(f"Skipping node '{node.name}' due to serialization error.")
            return None

    def write(
        self, graph: Graph, target: Any = None, **kwargs: dict[str, Any]
    ) -> dict[str, Any]:
        """Export the full graph definition to a dictionary.

        Args:
            graph (Graph): The Graph instance to serialize.
            target (Any): Ignored by this writer; the dictionary is returned directly.
            **kwargs: Currently unused.

        Returns:
            Dict[str, Any]: Dictionary representing the graph definition, including
                            periods, node definitions, and adjustments.

        Raises:
            WriteError: If an unexpected error occurs during export.
        """
        logger.info(f"Starting export of graph definition for: {graph!r}")
        graph_definition: dict[str, Any] = {
            "periods": [],
            "nodes": {},
            "adjustments": [],
        }

        try:
            # 1. Serialize Periods
            graph_definition["periods"] = list(graph.periods)

            # 2. Serialize Nodes using their to_dict() methods
            serialized_nodes: dict[str, SerializedNode] = {}
            for node_name, node in graph.nodes.items():
                node_dict = self._serialize_node(node)
                if node_dict:
                    serialized_nodes[node_name] = node_dict
                else:
                    logger.warning(
                        f"Node '{node_name}' was not serialized due to errors."
                    )
            graph_definition["nodes"] = serialized_nodes

            # 3. Serialize Adjustments
            adjustments = graph.list_all_adjustments()
            serialized_adjustments = []
            for adj in adjustments:
                try:
                    # Use model_dump for Pydantic V2, ensure mode='json' for types like UUID/datetime
                    serialized_adjustments.append(adj.model_dump(mode="json"))
                except Exception as e:
                    logger.warning(
                        f"Failed to serialize adjustment {adj.id}: {e}. Skipping."
                    )
            graph_definition["adjustments"] = serialized_adjustments

            logger.info(
                f"Successfully created graph definition dictionary with {len(serialized_nodes)} nodes and {len(serialized_adjustments)} adjustments."
            )
            return graph_definition

        except Exception as e:
            logger.error(
                f"Failed to create graph definition dictionary: {e}", exc_info=True
            )
            raise WriteError(
                message=f"Failed to create graph definition dictionary: {e}",
                target="graph_definition_dict",
                writer_type="GraphDefinitionWriter",
                original_error=e,
            ) from e


# ===== Convenience Functions =====


def save_graph_definition(graph: Graph, filepath: str) -> None:
    """Save a graph definition to a JSON file.

    Args:
        graph: The graph to save.
        filepath: Path to the output JSON file.
    """
    import json

    writer = GraphDefinitionWriter()
    definition = writer.write(graph)

    with open(filepath, "w") as f:
        json.dump(definition, f, indent=2)

    logger.info(f"Saved graph definition to {filepath}")


def load_graph_definition(filepath: str) -> Graph:
    """Load a graph definition from a JSON file.

    Args:
        filepath: Path to the JSON file containing the graph definition.

    Returns:
        The reconstructed Graph object.
    """
    import json

    with open(filepath) as f:
        definition = json.load(f)

    reader = GraphDefinitionReader()
    graph = reader.read(definition)

    logger.info(f"Loaded graph definition from {filepath}")
    return graph


__all__ = [
    "GraphDefinitionReader",
    "GraphDefinitionWriter",
    "load_graph_definition",
    "save_graph_definition",
]



================================================================================
File: fin_statement_model/io/specialized/statements.py
================================================================================

"""Statement-related IO utilities for built-in configs and writing formatted statement data."""

import json
import yaml
import logging
import importlib.resources
from pathlib import Path
from typing import Any

import pandas as pd

from fin_statement_model.io.exceptions import ReadError, WriteError

logger = logging.getLogger(__name__)

# Built-in config package path constant (directory containing YAML/JSON configs)
# Default base path is "fin_statement_model.statements.configs" – end-users can
# drop YAML or JSON files in that package (or a sub-package added to
# ``__init__.py``) and they will be discovered automatically.

_BUILTIN_CONFIG_PACKAGE = "fin_statement_model.statements.configs"


def list_available_builtin_configs() -> list[str]:
    """List names of all built-in statement configuration mappings."""
    package_path = _BUILTIN_CONFIG_PACKAGE
    try:
        resource_path = importlib.resources.files(package_path)
        if not resource_path.is_dir():
            logger.warning(
                f"Built-in config package path is not a directory: {package_path}"
            )
            return []
        names = [
            Path(res.name).stem
            for res in resource_path.iterdir()
            if res.is_file()
            and Path(res.name).suffix.lower() in (".yaml", ".yml", ".json")
        ]
        return sorted(names)
    except (ModuleNotFoundError, FileNotFoundError):
        logger.warning(f"Built-in statement config path not found: {package_path}")
        return []


def read_builtin_statement_config(name: str) -> dict[str, Any]:
    """Read and parse a built-in statement config by name."""
    package_path = _BUILTIN_CONFIG_PACKAGE
    found = None
    content = None
    for ext in (".yaml", ".yml", ".json"):
        try:
            path = importlib.resources.files(package_path).joinpath(f"{name}{ext}")
            if path.is_file():
                content = path.read_text(encoding="utf-8")
                found = ext
                break
        except Exception:
            continue
    if not content or not found:
        raise ReadError(
            message=f"Built-in statement config '{name}' not found in {package_path}",
            source=package_path,
        )
    try:
        if found == ".json":
            return json.loads(content)
        return yaml.safe_load(content)
    except Exception as e:
        raise ReadError(
            message=f"Failed to parse built-in statement config '{name}'",
            source=f"{package_path}/{name}{found}",
            original_error=e,
        ) from e


# ===== Statement Writing =====


def write_statement_to_excel(
    statement_df: pd.DataFrame, file_path: str, **kwargs: Any
) -> None:
    """Write a statement DataFrame to an Excel file."""
    try:
        kwargs.setdefault("index", False)
        statement_df.to_excel(file_path, **kwargs)
    except Exception as e:
        raise WriteError(
            message="Failed to export statement DataFrame to Excel",
            target=file_path,
            writer_type="excel",
            original_error=e,
        ) from e


def write_statement_to_json(
    statement_df: pd.DataFrame,
    file_path: str,
    orient: str = "columns",
    **kwargs: Any,
) -> None:
    """Write a statement DataFrame to a JSON file."""
    try:
        statement_df.to_json(file_path, orient=orient, **kwargs)
    except Exception as e:
        raise WriteError(
            message="Failed to export statement DataFrame to JSON",
            target=file_path,
            writer_type="json",
            original_error=e,
        ) from e


__all__ = [
    # Built-in configs
    "list_available_builtin_configs",
    "read_builtin_statement_config",
    # Statement writing
    "write_statement_to_excel",
    "write_statement_to_json",
]



================================================================================
File: fin_statement_model/io/validation.py
================================================================================

"""Unified node name validation and standardization utilities.

This module provides a comprehensive validator that combines basic validation
with context-aware pattern recognition for financial statement nodes.
"""

import logging
import re
from typing import Optional, ClassVar, Any
from dataclasses import dataclass, field

from fin_statement_model.core.nodes import Node, standard_node_registry

logger = logging.getLogger(__name__)


@dataclass
class ValidationResult:
    """Result of a node name validation."""

    original_name: str
    standardized_name: str
    is_valid: bool
    message: str
    category: str
    confidence: float = 1.0
    suggestions: list[str] = field(default_factory=list)


class UnifiedNodeValidator:
    """Unified validator for node names with pattern recognition and standardization.

    This validator combines the functionality of NodeNameValidator and
    ContextAwareNodeValidator into a single, more efficient implementation.
    """

    # Common sub-node patterns
    SUBNODE_PATTERNS: ClassVar[list[tuple[str, str]]] = [
        (r"^(.+)_(q[1-4])$", "quarterly"),
        (r"^(.+)_(fy\d{4})$", "fiscal_year"),
        (r"^(.+)_(\d{4})$", "annual"),
        (r"^(.+)_(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)$", "monthly"),
        (r"^(.+)_(actual|budget|forecast)$", "scenario"),
    ]

    # Generic segment pattern - must be checked separately
    SEGMENT_PATTERN = r"^(.+)_([a-z_]+)$"

    # Formula patterns - check exact endings
    FORMULA_ENDINGS: ClassVar[list[str]] = [
        "_margin",
        "_ratio",
        "_growth",
        "_change",
        "_pct",
    ]

    def __init__(
        self,
        strict_mode: Optional[bool] = None,
        auto_standardize: Optional[bool] = None,
        warn_on_non_standard: Optional[bool] = None,
        enable_patterns: bool = True,
    ):
        """Initialize the unified validator.

        Args:
            strict_mode: If True, only standard names are allowed.
                        If None, uses config.validation.strict_mode.
            auto_standardize: If True, convert alternate names to standard.
                            If None, uses config.validation.auto_standardize_names.
            warn_on_non_standard: If True, log warnings for non-standard names.
                                If None, uses config.validation.warn_on_non_standard.
            enable_patterns: If True, recognize sub-node and formula patterns.
        """
        from fin_statement_model import get_config

        config = get_config()

        # Use config defaults if not explicitly provided
        self.strict_mode = (
            strict_mode if strict_mode is not None else config.validation.strict_mode
        )
        self.auto_standardize = (
            auto_standardize
            if auto_standardize is not None
            else config.validation.auto_standardize_names
        )
        self.warn_on_non_standard = (
            warn_on_non_standard
            if warn_on_non_standard is not None
            else config.validation.warn_on_non_standard
        )
        self.enable_patterns = enable_patterns
        self._validation_cache: dict[str, ValidationResult] = {}

    def validate(
        self,
        name: str,
        node_type: Optional[str] = None,
        parent_nodes: Optional[list[str]] = None,
        use_cache: bool = True,
    ) -> ValidationResult:
        """Validate a node name with full context awareness.

        Args:
            name: The node name to validate.
            node_type: Optional node type hint.
            parent_nodes: Optional list of parent node names.
            use_cache: Whether to use cached results.

        Returns:
            ValidationResult with all validation details.
        """
        # Check cache first
        cache_key = f"{name}:{node_type}:{','.join(parent_nodes or [])}"
        if use_cache and cache_key in self._validation_cache:
            return self._validation_cache[cache_key]

        # Start validation
        result = self._perform_validation(name, node_type, parent_nodes)

        # Cache result
        if use_cache:
            self._validation_cache[cache_key] = result

        # Log warnings if configured
        if self.warn_on_non_standard and result.category in ["custom", "invalid"]:
            logger.warning(f"{result.message}")
            if result.suggestions:
                logger.info(
                    f"Suggestions for '{name}': {'; '.join(result.suggestions)}"
                )

        return result

    def _perform_validation(
        self,
        name: str,
        node_type: Optional[str],
        parent_nodes: Optional[list[str]],
    ) -> ValidationResult:
        """Perform the actual validation logic."""
        # Normalize name to lowercase for registry checks
        normalized_name = name.lower()

        # Check standard names first (using normalized name)
        if standard_node_registry.is_standard_name(normalized_name):
            # If original name is different case, standardize to lowercase
            standardized = normalized_name if name != normalized_name else name
            return ValidationResult(
                original_name=name,
                standardized_name=standardized,
                is_valid=True,
                message=f"Standard node: {normalized_name}",
                category="standard",
                confidence=1.0,
            )

        # Check alternate names (using normalized name)
        if standard_node_registry.is_alternate_name(normalized_name):
            standard_name = standard_node_registry.get_standard_name(normalized_name)
            return ValidationResult(
                original_name=name,
                standardized_name=standard_name if self.auto_standardize else name,
                is_valid=True,
                message=f"{'Standardized' if self.auto_standardize else 'Alternate name for'} '{standard_name}'",
                category="alternate",
                confidence=1.0,
            )

        # Pattern recognition if enabled
        if self.enable_patterns:
            pattern_result = self._check_pattern_validations(
                name, node_type, parent_nodes
            )
            if pattern_result:
                return pattern_result

        # Generate suggestions for unrecognized names
        suggestions = self._generate_suggestions(name)

        # Default to custom/invalid
        return ValidationResult(
            original_name=name,
            standardized_name=name,
            is_valid=not self.strict_mode,
            message=f"{'Non-standard' if self.strict_mode else 'Custom'} node: '{name}'",
            category="invalid" if self.strict_mode else "custom",
            confidence=0.5,
            suggestions=suggestions,
        )

    def _check_pattern_validations(
        self,
        name: str,
        node_type: Optional[str],
        parent_nodes: Optional[list[str]],
    ) -> Optional[ValidationResult]:
        """Check all pattern-based validations."""
        # Check formula patterns first (more specific)
        if node_type in ["calculation", "formula", None]:
            pattern_result = self._check_formula_ending(name)
            if pattern_result:
                base_name, formula_type = pattern_result

                return ValidationResult(
                    original_name=name,
                    standardized_name=name,
                    is_valid=True,
                    message=f"Formula node: {formula_type} of '{base_name}'",
                    category="formula",
                    confidence=0.85,
                )

        # Check parent relationships before sub-nodes
        if parent_nodes and self._check_parent_relationship(name, parent_nodes):
            return ValidationResult(
                original_name=name,
                standardized_name=name,
                is_valid=True,
                message="Derived from parent nodes",
                category="derived",
                confidence=0.8,
            )

        # Check specific sub-node patterns
        subnode_match = self._check_patterns(name, self.SUBNODE_PATTERNS, "subnode")
        if subnode_match:
            base_name, suffix, pattern_type = subnode_match
            # Normalize base name for registry check
            is_base_standard = standard_node_registry.is_recognized_name(
                base_name.lower()
            )

            return ValidationResult(
                original_name=name,
                standardized_name=name,
                is_valid=not self.strict_mode or is_base_standard,
                message=f"{'Valid' if is_base_standard else 'Non-standard'} sub-node of '{base_name}' ({pattern_type})",
                category="subnode" if is_base_standard else "subnode_nonstandard",
                confidence=0.9 if is_base_standard else 0.7,
            )

        # Check generic segment pattern last
        match = re.match(self.SEGMENT_PATTERN, name.lower())
        if match and "_" in name:
            base_name = match.group(1)
            suffix = match.group(2)

            # Only treat as segment if it doesn't match other patterns
            # and has a reasonable structure (geographic/business segment)
            segment_keywords = [
                "america",
                "europe",
                "asia",
                "pacific",
                "africa",
                "region",
                "domestic",
                "international",
                "global",
                "local",
                "retail",
                "wholesale",
                "online",
                "digital",
                "services",
                "products",
                "solutions",
                "segment",
                "division",
                "unit",
            ]

            if len(suffix) > 2 and any(
                keyword in suffix.lower() for keyword in segment_keywords
            ):
                # Normalize base name for registry check
                is_base_standard = standard_node_registry.is_recognized_name(
                    base_name.lower()
                )

                return ValidationResult(
                    original_name=name,
                    standardized_name=name,
                    is_valid=not self.strict_mode or is_base_standard,
                    message=f"{'Valid' if is_base_standard else 'Non-standard'} sub-node of '{base_name}' (segment)",
                    category="subnode" if is_base_standard else "subnode_nonstandard",
                    confidence=0.85 if is_base_standard else 0.65,
                )

        return None

    def _check_formula_ending(self, name: str) -> Optional[tuple[str, str]]:
        """Check if name ends with a formula pattern."""
        name_lower = name.lower()

        for ending in self.FORMULA_ENDINGS:
            if name_lower.endswith(ending):
                base_name = name[: -len(ending)]
                formula_type = ending[1:]  # Remove underscore
                return base_name, formula_type

        return None

    def _check_patterns(
        self,
        name: str,
        patterns: list[tuple[str, str]],
        pattern_category: str,
    ) -> Optional[tuple[str, str, str]]:
        """Check if name matches any pattern in the list."""
        name_lower = name.lower()

        for pattern, pattern_type in patterns:
            match = re.match(pattern, name_lower)
            if match:
                base_name = match.group(1)
                suffix = (
                    match.group(2)
                    if (match.lastindex is not None and match.lastindex > 1)
                    else ""
                )
                return base_name, suffix, pattern_type

        return None

    def _check_parent_relationship(self, name: str, parent_nodes: list[str]) -> bool:
        """Check if node name is related to its parents."""
        name_lower = name.lower()

        # Check if we have enough parents to establish a relationship
        if len(parent_nodes) < 2:
            return False

        # Check if any parent name is contained in this node name
        parent_match_count = 0
        for parent in parent_nodes:
            parent_lower = parent.lower()

            # Direct containment or similarity
            if parent_lower in name_lower or self._is_similar(
                name_lower, parent_lower, threshold=0.5
            ):
                parent_match_count += 1

            # Check standard name relationships
            if standard_node_registry.is_standard_name(parent_lower):
                definition = standard_node_registry.get_definition(parent_lower)
                if definition:
                    for alt in definition.alternate_names:
                        if alt.lower() in name_lower:
                            parent_match_count += 1
                            break

        # Consider it derived if it matches at least one parent
        return parent_match_count >= 1

    def _generate_suggestions(self, name: str) -> list[str]:
        """Generate improvement suggestions for non-standard names."""
        suggestions_with_scores = []
        name_lower = name.lower()

        # Find similar standard names
        for std_name in standard_node_registry.list_standard_names():
            std_lower = std_name.lower()

            # Calculate similarity score
            score = 0.0

            # Exact prefix match gets highest score
            if std_lower.startswith(name_lower) or name_lower.startswith(std_lower):
                score = 0.9
            # Check character overlap
            elif self._is_similar(name_lower, std_lower):
                overlap = len(set(name_lower) & set(std_lower))
                min_len = min(len(name_lower), len(std_lower))
                score = overlap / min_len * 0.8

            if score > 0:
                suggestions_with_scores.append(
                    (score, f"Consider using standard name: '{std_name}'")
                )

        # Sort by score (highest first) and take top suggestions
        suggestions_with_scores.sort(key=lambda x: x[0], reverse=True)
        suggestions = [msg for _, msg in suggestions_with_scores[:3]]

        # Check for pattern improvements
        if "_" in name and len(suggestions) < 3:
            parts = name.split("_", 1)
            base = parts[0]

            # Suggest standardizing the base
            for std_name in standard_node_registry.list_standard_names():
                if self._is_similar(base.lower(), std_name.lower()):
                    suggestions.append(
                        f"Consider using '{std_name}_{parts[1]}' for consistency"
                    )
                    break

        # Generic suggestions if nothing specific found
        if not suggestions:
            if any(
                suffix in name for suffix in ["_margin", "_ratio", "_growth", "_pct"]
            ):
                suggestions.append(
                    "Formula node detected - ensure base name follows standard conventions"
                )
            else:
                suggestions.append(
                    "Consider using a standard node name for better metric compatibility"
                )

        return suggestions[:3]  # Return top 3 suggestions

    def _is_similar(self, str1: str, str2: str, threshold: float = 0.6) -> bool:
        """Check if two strings are similar enough."""
        if len(str1) < 3 or len(str2) < 3:
            return False

        # Check if one is a prefix of the other
        if str1.startswith(str2) or str2.startswith(str1):
            return True

        # Check containment
        if str1 in str2 or str2 in str1:
            return True

        # Check character overlap
        overlap = len(set(str1) & set(str2))
        min_len = min(len(str1), len(str2))

        return overlap / min_len >= threshold

    def validate_batch(
        self,
        names: list[str],
        node_types: Optional[dict[str, str]] = None,
        parent_map: Optional[dict[str, list[str]]] = None,
    ) -> dict[str, ValidationResult]:
        """Validate multiple node names efficiently.

        Args:
            names: List of node names to validate.
            node_types: Optional mapping of names to node types.
            parent_map: Optional mapping of names to parent node lists.

        Returns:
            Dictionary mapping names to ValidationResults.
        """
        results = {}
        node_types = node_types or {}
        parent_map = parent_map or {}

        for name in names:
            result = self.validate(
                name,
                node_type=node_types.get(name),
                parent_nodes=parent_map.get(name),
            )
            results[name] = result

        return results

    def validate_graph(self, nodes: list[Node]) -> dict[str, Any]:
        """Validate all nodes in a graph with full context.

        Args:
            nodes: List of Node objects from the graph.

        Returns:
            Comprehensive validation report.
        """
        # Build context maps
        node_types = {}
        parent_map = {}

        for node in nodes:
            # Determine node type
            class_name = node.__class__.__name__
            if "Formula" in class_name:
                node_types[node.name] = "formula"
            elif "Calculation" in class_name:
                node_types[node.name] = "calculation"
            elif "Forecast" in class_name:
                node_types[node.name] = "forecast"
            else:
                node_types[node.name] = "data"

            # Extract parent nodes
            if hasattr(node, "inputs"):
                if isinstance(node.inputs, dict):
                    parent_map[node.name] = [p.name for p in node.inputs.values()]
                elif isinstance(node.inputs, list):
                    parent_map[node.name] = [p.name for p in node.inputs]

        # Validate all nodes
        node_names = [node.name for node in nodes]
        results = self.validate_batch(node_names, node_types, parent_map)

        # Build summarized report
        by_category: dict[str, list[str]] = {}
        by_validity: dict[str, int] = {"valid": 0, "invalid": 0}
        suggestions: dict[str, list[str]] = {}
        # Populate report sections
        for name, result in results.items():
            # Count by category
            cat = result.category
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(name)
            # Count by validity
            if result.is_valid:
                by_validity["valid"] += 1
            else:
                by_validity["invalid"] += 1
            # Collect suggestions
            if result.suggestions:
                suggestions[name] = result.suggestions
        return {
            "total": len(results),
            "by_category": by_category,
            "by_validity": by_validity,
            "suggestions": suggestions,
            "details": results,
        }

    def clear_cache(self) -> None:
        """Clear the validation cache."""
        self._validation_cache.clear()


def create_validator(**kwargs: Any) -> UnifiedNodeValidator:
    """Create a validator instance with the given configuration."""
    return UnifiedNodeValidator(**kwargs)



================================================================================
File: fin_statement_model/logging_config.py
================================================================================

"""Centralized logging configuration for the fin_statement_model library.

This module provides consistent logging configuration across the entire package.
It sets up appropriate formatters, handlers, and logging levels for different
components of the library.
"""

import logging
import logging.handlers
import os
import sys
from typing import Optional


# Default format for log messages
DEFAULT_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
DETAILED_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(funcName)s() - %(message)s"

# Environment variable to control logging level
LOG_LEVEL_ENV = "FSM_LOG_LEVEL"
LOG_FORMAT_ENV = "FSM_LOG_FORMAT"


def get_logger(name: str) -> logging.Logger:
    """Get a logger instance with the given name.

    This is a convenience function that ensures all loggers are properly
    namespaced under the fin_statement_model package.

    Args:
        name: The name of the logger, typically __name__

    Returns:
        A configured logger instance
    """
    if not name.startswith("fin_statement_model") and name.startswith("."):
        name = f"fin_statement_model{name}"
    return logging.getLogger(name)


def setup_logging(
    level: Optional[str] = None,
    format_string: Optional[str] = None,
    log_file_path: Optional[str] = None,
    detailed: bool = False,
) -> None:
    """Configure logging for the fin_statement_model library.

    This function should be called once at application startup to configure
    logging behavior. If not called, the library will use a NullHandler to
    avoid "no handler" warnings.

    Args:
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).
               If None, uses FSM_LOG_LEVEL env var or defaults to WARNING.
        format_string: Custom format string for log messages.
                      If None, uses FSM_LOG_FORMAT env var or default format.
        log_file_path: Optional file path to write logs to.
        detailed: If True, uses detailed format with file/line information.

    Example:
        >>> from fin_statement_model import logging_config
        >>> logging_config.setup_logging(level="INFO", detailed=True)
    """
    # Determine log level
    if level is None:
        level = os.environ.get(LOG_LEVEL_ENV, "WARNING")

    # Convert string level to logging constant
    numeric_level = getattr(logging, level.upper(), logging.WARNING)

    # Determine format
    if format_string is None:
        format_string = os.environ.get(LOG_FORMAT_ENV)
        if format_string is None:
            format_string = DETAILED_FORMAT if detailed else DEFAULT_FORMAT

    # Get the root logger for fin_statement_model
    root_logger = logging.getLogger("fin_statement_model")
    root_logger.setLevel(numeric_level)

    # Remove any existing handlers to avoid duplicates
    root_logger.handlers.clear()

    # Create formatter
    formatter = logging.Formatter(format_string)

    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    # Optional file handler
    if log_file_path:
        file_handler = logging.handlers.RotatingFileHandler(
            log_file_path,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5,
        )
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)

    # Set specific log levels for noisy sub-modules if needed
    # For example, reduce verbosity of certain components during normal operation
    logging.getLogger("fin_statement_model.io.formats").setLevel(
        max(numeric_level, logging.INFO)
    )
    logging.getLogger("fin_statement_model.core.graph.traverser").setLevel(
        max(numeric_level, logging.INFO)
    )


def configure_library_logging() -> None:
    """Configure default logging for library usage.

    This is called automatically when the library is imported and sets up
    a NullHandler to prevent "no handler" warnings. Users should call
    setup_logging() to enable actual logging output.
    """
    # Attach a NullHandler to the base fin_statement_model logger so that
    # all child loggers inherit it and avoid 'No handler' warnings by default.
    base_logger = logging.getLogger("fin_statement_model")

    # Only add NullHandler if no handlers exist
    if not base_logger.handlers:
        base_logger.addHandler(logging.NullHandler())

    # Prevent propagation to root logger by default
    base_logger.propagate = False


# Configure library logging on import
configure_library_logging()


# Logging best practices for fin_statement_model:
#
# 1. Always use logger = logging.getLogger(__name__) in modules
# 2. Use appropriate log levels:
#    - DEBUG: Detailed information for diagnosing problems
#    - INFO: General informational messages
#    - WARNING: Something unexpected happened but the app is still working
#    - ERROR: A serious problem occurred, function cannot proceed
#    - CRITICAL: A very serious error occurred, program may be unable to continue
#
# 3. Include context in log messages:
#    - Good: logger.info(f"Loaded {count} metrics from {filepath}")
#    - Bad: logger.info("Metrics loaded")
#
# 4. Use logger.exception() in except blocks to capture stack traces:
#    try:
#        risky_operation()
#    except Exception:
#        logger.exception("Failed to perform risky operation")
#
# 5. For performance-sensitive code, check log level before expensive operations:
#    if logger.isEnabledFor(logging.DEBUG):
#        logger.debug(f"Expensive debug info: {expensive_function()}")



================================================================================
File: fin_statement_model/preprocessing/__init__.py
================================================================================

"""Export DataTransformer, CompositeTransformer, and TransformerFactory for preprocessing.

This module exposes core transformer interfaces and factory for the preprocessing layer.
"""

from .base_transformer import DataTransformer, CompositeTransformer
from .transformer_service import TransformerFactory, TransformationService
from .errors import (
    PreprocessingError,
    TransformerRegistrationError,
    TransformerConfigurationError,
    PeriodConversionError,
    NormalizationError,
    TimeSeriesError,
)

## Trigger transformer discovery on package import
TransformerFactory.discover_transformers(
    "fin_statement_model.preprocessing.transformers"
)

__all__ = [
    "CompositeTransformer",
    "DataTransformer",
    "NormalizationError",
    "PeriodConversionError",
    "PreprocessingError",
    "TimeSeriesError",
    "TransformationService",
    "TransformerConfigurationError",
    "TransformerFactory",
    "TransformerRegistrationError",
]



================================================================================
File: fin_statement_model/preprocessing/base_transformer.py
================================================================================

"""Define base DataTransformer interface for preprocessing layer.

This module provides the DataTransformer abstract base class and CompositeTransformer.
"""

from abc import ABC, abstractmethod
import pandas as pd
from typing import Optional, Union
import logging

from fin_statement_model.core.errors import TransformationError

logger = logging.getLogger(__name__)


class DataTransformer(ABC):
    """Define base class for data transformers.

    Data transformers convert data between formats and apply business rules.

    This separation follows the Single Responsibility Principle for maintainability.
    """

    def __init__(self, config: Optional[dict[str, object]] = None):
        """Initialize the transformer with optional configuration.

        Args:
            config: Optional configuration dictionary for the transformer
        """
        self.config = config or {}
        logger.debug(
            f"Initialized {self.__class__.__name__} with config: {self.config}"
        )

    def transform(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Transform the input data.

        Args:
            data: The data to transform.

        Returns:
            The transformed data.

        Raises:
            TransformationError: If transformation fails.
        """
        try:
            logger.debug(f"Transforming data with {self.__class__.__name__}")
            return self._transform_impl(data)
        except Exception as e:
            logger.exception(f"Error transforming data with {self.__class__.__name__}")
            raise TransformationError(
                "Error transforming data",
                transformer_type=self.__class__.__name__,
            ) from e

    @abstractmethod
    def _transform_impl(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Apply the transformation logic.

        This method must be implemented by subclasses to define the specific
        transformation logic.

        Args:
            data: The data to transform.

        Returns:
            The transformed data.

        Raises:
            TransformationError: If transformation fails.
        """

    def validate_input(self, data: object) -> bool:
        """Validate that the input data is a pandas DataFrame by default.

        This performs a basic DataFrame type check and can be overridden by subclasses with more specific validation logic.

        Args:
            data (object): The input data to validate.

        Returns:
            bool: True if data is a pandas.DataFrame, False otherwise.
        """
        return isinstance(data, pd.DataFrame)

    def _pre_transform_hook(self, data: object) -> object:
        """Hook method called before transformation.

        Args:
            data: The input data

        Returns:
            Processed data to be passed to the transform method

        This method can be overridden by subclasses to add pre-processing steps.
        """
        return data

    def _post_transform_hook(self, data: object) -> object:
        """Hook method called after transformation.

        Args:
            data: The transformed data

        Returns:
            Final processed data

        This method can be overridden by subclasses to add post-processing steps.
        """
        return data

    def execute(self, data: object) -> object:
        """Execute the complete transformation pipeline.

        Args:
            data: The input data to transform

        Returns:
            Transformed data

        Raises:
            ValueError: If the data is invalid or cannot be transformed
        """
        if not self.validate_input(data):
            raise ValueError(f"Invalid input data for {self.__class__.__name__}")

        try:
            # Apply pre-transform hook
            processed_data = self._pre_transform_hook(data)

            # Perform transformation
            result = self.transform(processed_data)
            result = self._post_transform_hook(result)
            logger.debug(
                f"Successfully transformed data with {self.__class__.__name__}"
            )
        except Exception as e:
            logger.exception(f"Error transforming data with {self.__class__.__name__}")
            raise ValueError("Error transforming data") from e
        else:
            return result

    def validate_config(self) -> None:
        """Validate the transformer configuration.

        This method can be overridden by subclasses to add specific validation logic.

        Raises:
            TransformationError: If the configuration is invalid.
        """
        if self.config is None:
            raise TransformationError(
                f"Invalid input data for {self.__class__.__name__}",
                transformer_type=self.__class__.__name__,
            )


class CompositeTransformer(DataTransformer):
    """Compose multiple transformers into a pipeline.

    This allows building complex transformation chains from simple steps.
    """

    def __init__(
        self,
        transformers: list[DataTransformer],
        config: Optional[dict[str, object]] = None,
    ):
        """Initialize with a list of transformers.

        Args:
            transformers: List of transformers to apply in sequence
            config: Optional configuration dictionary
        """
        super().__init__(config)
        self.transformers = transformers

    def _transform_impl(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Apply each transformer in sequence.

        Args:
            data: The input data to transform

        Returns:
            Data transformed by the pipeline
        """
        result = data
        for transformer in self.transformers:
            result = transformer.execute(result)
        return result

    def add_transformer(self, transformer: DataTransformer) -> None:
        """Add a transformer to the pipeline.

        Args:
            transformer: The transformer to add
        """
        self.transformers.append(transformer)

    def remove_transformer(self, index: int) -> Optional[DataTransformer]:
        """Remove a transformer from the pipeline.

        Args:
            index: Index of the transformer to remove

        Returns:
            The removed transformer or None if index is invalid
        """
        if 0 <= index < len(self.transformers):
            return self.transformers.pop(index)
        return None

    def validate_input(self, data: object) -> bool:
        """Validate input for the composite transformer.

        If the pipeline is empty, accepts any data; otherwise, delegates validation to the first transformer.

        Args:
            data (object): Input data to validate.

        Returns:
            bool: True if input is valid for the pipeline.
        """
        if not hasattr(self, "transformers") or not self.transformers:
            return True
        return self.transformers[0].validate_input(data)



================================================================================
File: fin_statement_model/preprocessing/config.py
================================================================================

"""Configuration models and enums for preprocessing transformers.

This module contains Pydantic models and Enums for configuring preprocessing transformations.
"""

from enum import Enum
from typing import Optional
from pydantic import BaseModel


class NormalizationType(Enum):
    """Available normalization types for NormalizationTransformer."""

    PERCENT_OF = "percent_of"
    MINMAX = "minmax"
    STANDARD = "standard"
    SCALE_BY = "scale_by"


class TransformationType(Enum):
    """Available transformation types for TimeSeriesTransformer."""

    GROWTH_RATE = "growth_rate"
    MOVING_AVG = "moving_avg"
    CAGR = "cagr"
    YOY = "yoy"
    QOQ = "qoq"


class ConversionType(Enum):
    """Available conversion types for PeriodConversionTransformer."""

    QUARTERLY_TO_ANNUAL = "quarterly_to_annual"
    MONTHLY_TO_QUARTERLY = "monthly_to_quarterly"
    MONTHLY_TO_ANNUAL = "monthly_to_annual"
    QUARTERLY_TO_TTM = "quarterly_to_ttm"


class StatementType(Enum):
    """Available statement types for StatementFormattingTransformer."""

    INCOME_STATEMENT = "income_statement"
    BALANCE_SHEET = "balance_sheet"
    CASH_FLOW = "cash_flow"


class NormalizationConfig(BaseModel):
    """Configuration for normalization transformations."""

    normalization_type: Optional[str] = None
    reference: Optional[str] = None
    scale_factor: Optional[float] = None


class TimeSeriesConfig(BaseModel):
    """Configuration for time series transformations."""

    transformation_type: Optional[str] = None
    periods: Optional[int] = None
    window_size: Optional[int] = None


class PeriodConversionConfig(BaseModel):
    """Configuration for period conversion transformations."""

    conversion_type: Optional[str] = None
    aggregation: Optional[str] = None


class StatementFormattingConfig(BaseModel):
    """Configuration for formatting statement output."""

    statement_type: Optional[str] = None
    add_subtotals: Optional[bool] = None
    apply_sign_convention: Optional[bool] = None


__all__ = [
    "ConversionType",
    "NormalizationConfig",
    "NormalizationType",
    "PeriodConversionConfig",
    "StatementFormattingConfig",
    "StatementType",
    "TimeSeriesConfig",
    "TransformationType",
]



================================================================================
File: fin_statement_model/preprocessing/errors.py
================================================================================

"""Custom Exception classes for the preprocessing package.

These exceptions provide specific error information related to data
preprocessing, transformation, and validation operations.
"""

from typing import Optional, Any
from fin_statement_model.core.errors import (
    FinancialModelError,
    TransformationError,
)

__all__ = [
    "NormalizationError",
    "PeriodConversionError",
    "PreprocessingError",
    "TimeSeriesError",
    "TransformerConfigurationError",
    "TransformerRegistrationError",
]


class PreprocessingError(FinancialModelError):
    """Base exception for all preprocessing-related errors."""


class TransformerRegistrationError(PreprocessingError):
    """Exception raised for transformer registration issues.

    This includes attempts to register duplicate transformers or
    register invalid transformer classes.
    """

    def __init__(
        self,
        message: str,
        transformer_name: Optional[str] = None,
        existing_class: Optional[type] = None,
    ):
        """Initialize a TransformerRegistrationError.

        Args:
            message: The primary error message.
            transformer_name: Optional name of the transformer.
            existing_class: Optional existing class that's already registered.
        """
        self.transformer_name = transformer_name
        self.existing_class = existing_class

        full_message = message
        if transformer_name:
            full_message = f"{message} for transformer '{transformer_name}'"
        if existing_class:
            full_message = (
                f"{full_message} (already registered as {existing_class.__name__})"
            )

        super().__init__(full_message)


class TransformerConfigurationError(PreprocessingError):
    """Exception raised for invalid transformer configuration.

    This includes missing required parameters, invalid parameter values,
    or incompatible configuration options.
    """

    def __init__(
        self,
        message: str,
        transformer_name: Optional[str] = None,
        config: Optional[dict[str, Any]] = None,
        missing_params: Optional[list[str]] = None,
    ):
        """Initialize a TransformerConfigurationError.

        Args:
            message: The primary error message.
            transformer_name: Optional name of the transformer.
            config: Optional configuration dictionary that caused the error.
            missing_params: Optional list of missing required parameters.
        """
        self.transformer_name = transformer_name
        self.config = config
        self.missing_params = missing_params or []

        details = []
        if transformer_name:
            details.append(f"Transformer: {transformer_name}")
        if missing_params:
            details.append(f"Missing params: {', '.join(missing_params)}")

        full_message = message
        if details:
            full_message = f"{message} ({'; '.join(details)})"

        super().__init__(full_message)


class PeriodConversionError(TransformationError):
    """Exception raised for period conversion failures.

    This includes invalid period formats, unsupported conversion types,
    or missing date/period columns.
    """

    def __init__(
        self,
        message: str,
        source_period: Optional[str] = None,
        target_period: Optional[str] = None,
        date_column: Optional[str] = None,
    ):
        """Initialize a PeriodConversionError.

        Args:
            message: The primary error message.
            source_period: Optional source period type.
            target_period: Optional target period type.
            date_column: Optional name of the date column.
        """
        self.source_period = source_period
        self.target_period = target_period
        self.date_column = date_column

        details: dict[str, Any] = {}
        if source_period and target_period:
            details["conversion"] = f"{source_period} -> {target_period}"
        if date_column:
            details["date_column"] = date_column

        super().__init__(
            message,
            transformer_type="PeriodConversionTransformer",
            parameters=details,
        )


class NormalizationError(TransformationError):
    """Exception raised for normalization failures.

    This includes missing reference columns, invalid normalization methods,
    or data type incompatibilities.
    """

    def __init__(
        self,
        message: str,
        method: Optional[str] = None,
        reference_field: Optional[str] = None,
        scale_factor: Optional[float] = None,
    ):
        """Initialize a NormalizationError.

        Args:
            message: The primary error message.
            method: Optional normalization method.
            reference_field: Optional reference field for percent_of method.
            scale_factor: Optional scale factor for scale_by method.
        """
        self.method = method
        self.reference_field = reference_field
        self.scale_factor = scale_factor

        params: dict[str, Any] = {}
        if method:
            params["method"] = method
        if reference_field:
            params["reference"] = reference_field
        if scale_factor is not None:
            params["scale_factor"] = scale_factor

        super().__init__(
            message,
            transformer_type="NormalizationTransformer",
            parameters=params,
        )


class TimeSeriesError(TransformationError):
    """Exception raised for time series transformation failures.

    This includes invalid window sizes, missing columns, or
    incompatible aggregation methods.
    """

    def __init__(
        self,
        message: str,
        operation: Optional[str] = None,
        window_size: Optional[int] = None,
        column: Optional[str] = None,
    ):
        """Initialize a TimeSeriesError.

        Args:
            message: The primary error message.
            operation: Optional operation type (e.g., 'rolling_mean', 'lag').
            window_size: Optional window size parameter.
            column: Optional column being processed.
        """
        self.operation = operation
        self.window_size = window_size
        self.column = column

        params: dict[str, Any] = {}
        if operation:
            params["operation"] = operation
        if window_size is not None:
            params["window_size"] = window_size
        if column:
            params["column"] = column

        super().__init__(
            message,
            transformer_type="TimeSeriesTransformer",
            parameters=params,
        )



================================================================================
File: fin_statement_model/preprocessing/transformer_service.py
================================================================================

"""Provide TransformerFactory and TransformationService for preprocessing.

This module merges the transformer factory and transformation service into a single module for simplicity.
"""

import importlib
import inspect
import logging
import pkgutil
import re
from typing import Any, ClassVar, Optional, Union

import pandas as pd

from fin_statement_model.preprocessing.base_transformer import (
    DataTransformer,
    CompositeTransformer,
)
from fin_statement_model.preprocessing.errors import (
    TransformerRegistrationError,
    TransformerConfigurationError,
)
from fin_statement_model.config.helpers import cfg

logger = logging.getLogger(__name__)


class TransformerFactory:
    """Create and manage transformer instances.

    Centralizes transformer registration, discovery, and instantiation.
    """

    _transformers: ClassVar[dict[str, type[DataTransformer]]] = {}

    @classmethod
    def register_transformer(
        cls, name: str, transformer_class: type[DataTransformer]
    ) -> None:
        """Register a transformer class with the factory."""
        if name in cls._transformers:
            raise TransformerRegistrationError(
                f"Transformer name '{name}' is already registered",
                transformer_name=name,
                existing_class=cls._transformers[name],
            )
        if not issubclass(transformer_class, DataTransformer):
            raise TransformerRegistrationError(
                "Transformer class must be a subclass of DataTransformer",
                transformer_name=name,
            )
        cls._transformers[name] = transformer_class
        logger.info(f"Registered transformer '{name}'")

    @classmethod
    def create_transformer(cls, name: str, **kwargs: Any) -> DataTransformer:
        """Create a transformer instance by name."""
        if name not in cls._transformers:
            raise TransformerConfigurationError(
                f"No transformer registered with name '{name}'",
                transformer_name=name,
            )
        transformer_class = cls._transformers[name]
        transformer = transformer_class(**kwargs)
        logger.debug(f"Created transformer '{name}'")
        return transformer

    @classmethod
    def list_transformers(cls) -> list[str]:
        """List all registered transformer names."""
        return list(cls._transformers.keys())

    @classmethod
    def get_transformer_class(cls, name: str) -> type[DataTransformer]:
        """Get a transformer class by name."""
        if name not in cls._transformers:
            raise TransformerConfigurationError(
                f"No transformer registered with name '{name}'",
                transformer_name=name,
            )
        return cls._transformers[name]

    @classmethod
    def discover_transformers(cls, package_name: str) -> None:
        """Discover and register all transformers in a package."""
        try:
            package = importlib.import_module(package_name)
            package_path = package.__path__
            for _, module_name, _ in pkgutil.iter_modules(package_path):
                full_module_name = f"{package_name}.{module_name}"
                module = importlib.import_module(full_module_name)
                for obj_name, obj in inspect.getmembers(module):
                    if (
                        inspect.isclass(obj)
                        and issubclass(obj, DataTransformer)
                        and obj is not DataTransformer
                    ):
                        cls.register_transformer(obj_name, obj)
                        snake = re.sub(r"(.)([A-Z][a-z]+)", r"\1_\2", obj_name)
                        snake = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", snake).lower()
                        alias = snake.replace("_transformer", "")
                        if alias not in cls._transformers:
                            cls.register_transformer(alias, obj)
            logger.info(f"Discovered transformers from package '{package_name}'")
        except ImportError:
            logger.exception(
                f"Error discovering transformers from package '{package_name}'"
            )

    @classmethod
    def create_composite_transformer(
        cls, transformer_names: list[str], **kwargs: Any
    ) -> DataTransformer:
        """Create a composite transformer from a list of transformer names."""
        transformers = [
            cls.create_transformer(name, **kwargs) for name in transformer_names
        ]
        return CompositeTransformer(transformers)


class TransformationService:
    """Service for managing and applying data transformations."""

    def __init__(self) -> None:
        """Initialize the transformation service."""
        logger.info("TransformationService initialized")

    def normalize_data(
        self,
        data: Union[pd.DataFrame, dict[str, Any]],
        normalization_type: Optional[str] = None,
        reference: Optional[str] = None,
        scale_factor: Optional[float] = None,
    ) -> Union[pd.DataFrame, dict[str, Any]]:
        """Normalize financial data."""
        # Determine normalization type default
        default_norm_type = cfg("preprocessing.default_normalization_type")
        norm_type = (
            normalization_type if normalization_type is not None else default_norm_type
        ) or "percent_of"
        transformer = TransformerFactory.create_transformer(
            "normalization",
            normalization_type=norm_type,
            reference=reference,
            scale_factor=scale_factor,
        )
        return transformer.execute(data)

    def transform_time_series(
        self,
        data: Union[pd.DataFrame, dict[str, Any]],
        transformation_type: Optional[str] = None,
        periods: Optional[int] = None,
        window_size: Optional[int] = None,
    ) -> Union[pd.DataFrame, dict[str, Any]]:
        """Apply time series transformations to financial data."""
        # Determine defaults from config
        default_transform_type = cfg("preprocessing.default_transformation_type")
        transform_type = (
            transformation_type
            if transformation_type is not None
            else default_transform_type
        )
        default_periods = cfg("preprocessing.default_time_series_periods")
        num_periods = periods if periods is not None else default_periods
        default_window = cfg("preprocessing.default_time_series_window_size")
        win_size = window_size if window_size is not None else default_window
        transformer = TransformerFactory.create_transformer(
            "time_series",
            transformation_type=transform_type,
            periods=num_periods,
            window_size=win_size,
        )
        return transformer.execute(data)

    def convert_periods(
        self,
        data: pd.DataFrame,
        conversion_type: str,
        aggregation: Optional[str] = None,
    ) -> pd.DataFrame:
        """Convert data between different period types."""
        default_agg = cfg("preprocessing.default_conversion_aggregation")
        agg = aggregation if aggregation is not None else default_agg
        transformer = TransformerFactory.create_transformer(
            "period_conversion",
            conversion_type=conversion_type,
            aggregation=agg,
        )
        return transformer.execute(data)

    def format_statement(
        self,
        data: pd.DataFrame,
        statement_type: Optional[str] = None,
        add_subtotals: Optional[bool] = None,
        apply_sign_convention: Optional[bool] = None,
    ) -> pd.DataFrame:
        """Format a financial statement DataFrame."""
        # Determine defaults from preprocessing config
        stmt_cfg = cfg("preprocessing.statement_formatting")
        default_stmt_type = stmt_cfg.statement_type or "income_statement"
        stmt_type = statement_type if statement_type is not None else default_stmt_type
        default_add = (
            stmt_cfg.add_subtotals if hasattr(stmt_cfg, "add_subtotals") else True
        )
        sub = add_subtotals if add_subtotals is not None else default_add
        default_sign = (
            stmt_cfg.apply_sign_convention
            if hasattr(stmt_cfg, "apply_sign_convention")
            else True
        )
        sign = (
            apply_sign_convention if apply_sign_convention is not None else default_sign
        )
        transformer = TransformerFactory.create_transformer(
            "statement_formatting",
            statement_type=stmt_type,
            add_subtotals=sub,
            apply_sign_convention=sign,
        )
        return transformer.execute(data)

    def create_transformation_pipeline(
        self, transformers_config: list[dict[str, Any]]
    ) -> DataTransformer:
        """Create a composite transformer from configurations."""
        transformers = []
        for config in transformers_config:
            if "name" not in config:
                raise ValueError(
                    "Each transformer configuration must have a 'name' field"
                )
            name = config.pop("name")
            transformer = TransformerFactory.create_transformer(name, **config)
            transformers.append(transformer)
        return CompositeTransformer(transformers)

    def apply_transformation_pipeline(
        self, data: object, transformers_config: list[dict[str, Any]]
    ) -> object:
        """Apply a transformation pipeline to data."""
        pipeline = self.create_transformation_pipeline(transformers_config)
        return pipeline.execute(data)

    def register_custom_transformer(
        self, name: str, transformer_class: type[DataTransformer]
    ) -> None:
        """Register a custom transformer with the factory."""
        TransformerFactory.register_transformer(name, transformer_class)
        logger.info(f"Registered custom transformer: {name}")

    def list_available_transformers(self) -> list[str]:
        """List all available transformer types."""
        return TransformerFactory.list_transformers()



================================================================================
File: fin_statement_model/preprocessing/transformers/__init__.py
================================================================================

"""Package for preprocessing transformers.

This package exports built-in data transformer classes for the preprocessing layer.
"""

from .normalization import NormalizationTransformer
from .time_series import TimeSeriesTransformer
from .period_conversion import PeriodConversionTransformer

__all__ = [
    "NormalizationTransformer",
    "PeriodConversionTransformer",
    "TimeSeriesTransformer",
]



================================================================================
File: fin_statement_model/preprocessing/transformers/normalization.py
================================================================================

"""Provide a NormalizationTransformer to normalize financial data.

Transforms data by percent_of, minmax, standard, or scale_by methods.

This module implements the NormalizationTransformer for the preprocessing layer.
"""

from typing import Optional, Union, ClassVar
import logging

import numpy as np
import pandas as pd

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.config import (
    NormalizationConfig,
    NormalizationType,
)
from fin_statement_model.preprocessing.errors import NormalizationError
from fin_statement_model.core.errors import DataValidationError

logger = logging.getLogger(__name__)


class NormalizationTransformer(DataTransformer):
    """Transformer that normalizes financial data using various methods.

    This transformer provides multiple normalization strategies commonly used in
    financial analysis to make data comparable across different scales or to
    express values as percentages of a reference metric.

    Supported normalization types:
        - **percent_of**: Express values as percentages of a reference column
          (e.g., all items as % of revenue)
        - **minmax**: Scale values to [0, 1] range based on min/max values
        - **standard**: Standardize using (x - mean) / std deviation
        - **scale_by**: Multiply all values by a fixed scale factor

    Examples:
        Express all income statement items as percentage of revenue:

        >>> import pandas as pd
        >>> from fin_statement_model.preprocessing.transformers import NormalizationTransformer
        >>>
        >>> # Sample income statement data
        >>> data = pd.DataFrame({
        ...     'revenue': [1000, 1100, 1200],
        ...     'cogs': [600, 650, 700],
        ...     'operating_expenses': [200, 220, 250]
        ... }, index=['2021', '2022', '2023'])
        >>>
        >>> # Create transformer to express as % of revenue
        >>> normalizer = NormalizationTransformer(
        ...     normalization_type='percent_of',
        ...     reference='revenue'
        ... )
        >>>
        >>> # Transform the data
        >>> normalized = normalizer.transform(data)
        >>> print(normalized)
        #       revenue  cogs  operating_expenses
        # 2021    100.0  60.0               20.0
        # 2022    100.0  59.1               20.0
        # 2023    100.0  58.3               20.8

        Scale financial data to millions:

        >>> # Scale values to millions (divide by 1,000,000)
        >>> scaler = NormalizationTransformer(
        ...     normalization_type='scale_by',
        ...     scale_factor=0.000001
        ... )
        >>> scaled = scaler.transform(data)

    Note:
        For 'percent_of' normalization, if a reference value is 0 or NaN,
        the corresponding output for that row will be NaN to avoid division
        by zero errors.
    """

    NORMALIZATION_TYPES: ClassVar[list[str]] = [t.value for t in NormalizationType]

    def __init__(
        self,
        normalization_type: Union[
            str, NormalizationType
        ] = NormalizationType.PERCENT_OF,
        reference: Optional[str] = None,
        scale_factor: Optional[float] = None,
        config: Optional[NormalizationConfig] = None,
    ):
        """Initialize the normalizer with specified parameters.

        Args:
            normalization_type: Type of normalization to apply. Can be either
                a string or NormalizationType enum value:
                - 'percent_of': Express values as percentage of reference column
                - 'minmax': Scale to [0,1] range
                - 'standard': Apply z-score normalization
                - 'scale_by': Multiply by scale_factor
            reference: Name of the reference column for 'percent_of' normalization.
                Required when normalization_type is 'percent_of'.
            scale_factor: Multiplication factor for 'scale_by' normalization.
                Required when normalization_type is 'scale_by'.
                Common values: 0.001 (to thousands), 0.000001 (to millions)
            config: Optional NormalizationConfig object containing configuration.
                If provided, overrides other parameters.

        Raises:
            NormalizationError: If normalization_type is invalid, or if required
                parameters are missing for the selected normalization type.
        """
        super().__init__(config.model_dump() if config else None)
        # Normalize enum to string
        if isinstance(normalization_type, NormalizationType):
            norm_type = normalization_type.value
        else:
            norm_type = normalization_type
        if norm_type not in self.NORMALIZATION_TYPES:
            raise NormalizationError(
                f"Invalid normalization type: {norm_type}. "
                f"Must be one of {self.NORMALIZATION_TYPES}",
                method=norm_type,
            )
        self.normalization_type = norm_type

        self.reference = reference
        self.scale_factor = scale_factor

        # Validation
        if (
            self.normalization_type == NormalizationType.PERCENT_OF.value
            and not reference
        ):
            raise NormalizationError(
                "Reference field must be provided for percent_of normalization",
                method=self.normalization_type,
            )

        if (
            self.normalization_type == NormalizationType.SCALE_BY.value
            and scale_factor is None
        ):
            raise NormalizationError(
                "Scale factor must be provided for scale_by normalization",
                method=self.normalization_type,
            )

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Normalize the data based on the configured normalization type.

        Args:
            data: DataFrame containing financial data to normalize.
                All columns will be normalized except the reference column
                in 'percent_of' normalization.

        Returns:
            DataFrame with normalized values. Original column names are preserved
            for all normalization types.

        Raises:
            DataValidationError: If data is not a pandas DataFrame.
            NormalizationError: If reference column is not found in DataFrame
                (for 'percent_of' normalization).
        """
        if not isinstance(data, pd.DataFrame):
            raise DataValidationError(
                f"Unsupported data type: {type(data)}. Expected pandas.DataFrame",
                validation_errors=[f"Got type: {type(data).__name__}"],
            )
        return super().transform(data)

    def validate_config(self) -> None:
        """Validate the transformer configuration.

        Raises:
            NormalizationError: If the configuration is invalid.
        """
        super().validate_config()

        if self.normalization_type not in self.NORMALIZATION_TYPES:
            raise NormalizationError(
                f"Unknown normalization method: {self.normalization_type}. "
                f"Supported methods are: {self.NORMALIZATION_TYPES}",
                method=self.normalization_type,
            )

        if (
            self.normalization_type == NormalizationType.PERCENT_OF.value
            and not self.reference
        ):
            raise NormalizationError(
                "Reference field must be provided for percent_of normalization",
                method=self.normalization_type,
            )

        if (
            self.normalization_type == NormalizationType.SCALE_BY.value
            and self.scale_factor is None
        ):
            raise NormalizationError(
                "Scale factor must be provided for scale_by normalization",
                method=self.normalization_type,
            )

    def _transform_impl(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Apply the normalization transformation.

        Args:
            data: The data to transform.

        Returns:
            The normalized data.

        Raises:
            DataValidationError: If the data type is not supported.
            NormalizationError: If there are issues during normalization.
        """
        if not isinstance(data, pd.DataFrame | pd.Series):
            raise DataValidationError(
                f"Unsupported data type: {type(data)}. Expected pandas.DataFrame or pandas.Series",
                validation_errors=[f"Got type: {type(data).__name__}"],
            )

        # Handle Series by converting to DataFrame temporarily
        if isinstance(data, pd.Series):
            temp_df = data.to_frame()
            result_df = self._normalize_dataframe(temp_df)
            return result_df.iloc[:, 0]  # Return as Series
        else:
            return self._normalize_dataframe(data)

    def _normalize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply normalization to a DataFrame.

        Args:
            df: DataFrame to normalize.

        Returns:
            Normalized DataFrame.

        Raises:
            NormalizationError: If reference column is not found or other normalization issues.
        """
        result = df.copy()

        if self.normalization_type == NormalizationType.PERCENT_OF.value:
            if self.reference not in df.columns:
                raise NormalizationError(
                    f"Reference column '{self.reference}' not found in DataFrame",
                    method=self.normalization_type,
                    reference_field=self.reference,
                )

            for col in df.columns:
                if col != self.reference:
                    # Replace 0 with NaN in the denominator to ensure division by zero results in NaN
                    reference_series = df[self.reference].replace(0, np.nan)
                    if (
                        reference_series.isnull().all()
                    ):  # If all reference values are NaN (or were 0)
                        result[col] = np.nan
                        logger.warning(
                            f"All reference values for '{self.reference}' are zero or NaN. '{col}' will be NaN."
                        )
                    else:
                        result[col] = (df[col] / reference_series) * 100

        elif (
            self.normalization_type == NormalizationType.MINMAX.value
        ):  # pragma: no cover
            for col in df.columns:
                min_val = df[col].min()
                max_val = df[col].max()

                if max_val > min_val:
                    result[col] = (df[col] - min_val) / (
                        max_val - min_val
                    )  # pragma: no cover
                elif max_val == min_val:  # Handles constant columns
                    result[col] = (
                        0.0  # Or np.nan, depending on desired behavior for constant series
                    )
                # else: max_val < min_val (should not happen with .min()/.max())

        elif self.normalization_type == NormalizationType.STANDARD.value:
            for col in df.columns:
                mean = df[col].mean()
                std = df[col].std()

                if std > 0:
                    result[col] = (df[col] - mean) / std
                elif std == 0:  # Handles constant columns
                    result[col] = 0.0  # Or np.nan, depending on desired behavior
                # else: std < 0 (not possible)

        elif self.normalization_type == NormalizationType.SCALE_BY.value:
            for col in df.columns:
                result[col] = df[col] * self.scale_factor

        return result



================================================================================
File: fin_statement_model/preprocessing/transformers/period_conversion.py
================================================================================

"""Financial data transformers for the Financial Statement Model.

This module provides the PeriodConversionTransformer for converting between period types:
quarterly_to_annual, monthly_to_quarterly, monthly_to_annual, and quarterly_to_ttm.
"""

import logging
import pandas as pd
from typing import Optional, Union, ClassVar

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.config import (
    ConversionType,
    PeriodConversionConfig,
)
from fin_statement_model.core.errors import DataValidationError

# Configure logging
logger = logging.getLogger(__name__)


class PeriodConversionTransformer(DataTransformer):
    """Transformer for converting between different financial reporting periods.

    This transformer aggregates financial data from higher-frequency periods
    (e.g., monthly, quarterly) to lower-frequency periods (e.g., quarterly, annual)
    or calculates trailing metrics like TTM (Trailing Twelve Months).

    Supported conversion types:
        - **quarterly_to_annual**: Aggregate 4 quarters into annual data
        - **monthly_to_quarterly**: Aggregate 3 months into quarterly data
        - **monthly_to_annual**: Aggregate 12 months into annual data
        - **quarterly_to_ttm**: Calculate trailing twelve months from quarterly data

    Input Data Requirements:
        - Data must have a DatetimeIndex or an index convertible to datetime
        - The index should represent the period-end dates
        - Data frequency should match the conversion type (e.g., quarterly data
          for quarterly_to_annual conversion)

    Aggregation Methods:
        - **sum**: Total values (default) - use for flow items like revenue, expenses
        - **mean**: Average values - use for rates, ratios, or average balances
        - **last**: Take last value - use for balance sheet items (point-in-time)
        - **first**: Take first value - use for opening balances
        - **max/min**: Maximum/minimum values - use for peak/trough analysis

    Examples:
        Convert quarterly revenue to annual totals:

        >>> import pandas as pd
        >>> from fin_statement_model.preprocessing.transformers import PeriodConversionTransformer
        >>>
        >>> # Quarterly revenue and expense data
        >>> quarterly_data = pd.DataFrame({
        ...     'revenue': [100, 110, 120, 130, 140, 150, 160, 170],
        ...     'expenses': [80, 85, 90, 95, 100, 105, 110, 115]
        ... }, index=pd.date_range('2022-03-31', periods=8, freq='Q'))
        >>>
        >>> # Convert to annual data (sum 4 quarters)
        >>> annual_converter = PeriodConversionTransformer(
        ...     conversion_type='quarterly_to_annual',
        ...     aggregation='sum'
        ... )
        >>> annual_data = annual_converter.transform(quarterly_data)
        >>> print(annual_data)
        #       revenue  expenses
        # 2022      460       350
        # 2023      620       430

        Convert monthly balance sheet to quarterly (taking last value):

        >>> # Monthly balance sheet data
        >>> monthly_bs = pd.DataFrame({
        ...     'total_assets': [1000, 1020, 1050, 1080, 1100, 1150],
        ...     'total_equity': [600, 610, 620, 630, 640, 650]
        ... }, index=pd.date_range('2023-01-31', periods=6, freq='M'))
        >>>
        >>> # Convert to quarterly, taking last month's value
        >>> quarterly_converter = PeriodConversionTransformer(
        ...     conversion_type='monthly_to_quarterly',
        ...     aggregation='last'
        ... )
        >>> quarterly_bs = quarterly_converter.transform(monthly_bs)
        >>> print(quarterly_bs)
        #                  total_assets  total_equity
        # (2023, 1)              1050           620
        # (2023, 2)              1150           650

        Calculate trailing twelve months (TTM) from quarterly data:

        >>> # Quarterly earnings data
        >>> quarterly_earnings = pd.DataFrame({
        ...     'net_income': [25, 30, 35, 40, 45, 50, 55, 60]
        ... }, index=pd.date_range('2022-03-31', periods=8, freq='Q'))
        >>>
        >>> # Calculate TTM (rolling 4-quarter sum)
        >>> ttm_converter = PeriodConversionTransformer(
        ...     conversion_type='quarterly_to_ttm',
        ...     aggregation='sum'
        ... )
        >>> ttm_data = ttm_converter.transform(quarterly_earnings)
        >>> print(ttm_data.iloc[3:])  # First 3 periods will be NaN
        #             net_income
        # 2023-03-31       130.0
        # 2023-06-30       150.0
        # 2023-09-30       170.0
        # 2023-12-31       190.0
        # 2024-03-31       210.0

    Note:
        - The resulting index format depends on the conversion type
        - Annual conversions group by year (integer index)
        - Quarterly conversions group by (year, quarter) tuple
        - TTM conversions maintain the original datetime index
        - Ensure your aggregation method matches the financial item type
    """

    # All valid conversion types
    CONVERSION_TYPES: ClassVar[list[str]] = [t.value for t in ConversionType]

    def __init__(
        self,
        conversion_type: Union[
            str, ConversionType
        ] = ConversionType.QUARTERLY_TO_ANNUAL,
        aggregation: str = "sum",
        config: Optional[PeriodConversionConfig] = None,
    ):
        """Initialize the period conversion transformer.

        Args:
            conversion_type: Type of period conversion to apply. Can be either
                a string or ConversionType enum value:
                - 'quarterly_to_annual': Convert 4 quarters to 1 year
                - 'monthly_to_quarterly': Convert 3 months to 1 quarter
                - 'monthly_to_annual': Convert 12 months to 1 year
                - 'quarterly_to_ttm': Calculate trailing twelve months
            aggregation: How to aggregate data within each period:
                - 'sum': Add up all values (default) - for flow items
                - 'mean': Calculate average - for rates/ratios
                - 'last': Take last value - for balance sheet items
                - 'first': Take first value - for opening balances
                - 'max': Take maximum value
                - 'min': Take minimum value
                - 'std': Calculate standard deviation
                - 'count': Count non-null values
            config: Optional PeriodConversionConfig object containing configuration.
                If provided, overrides other parameters.

        Raises:
            ValueError: If conversion_type is invalid.

        Examples:
            >>> # Annual totals from quarterly data
            >>> converter = PeriodConversionTransformer('quarterly_to_annual', 'sum')
            >>>
            >>> # Quarter-end balances from monthly data
            >>> converter = PeriodConversionTransformer('monthly_to_quarterly', 'last')
            >>>
            >>> # TTM revenue from quarterly data
            >>> converter = PeriodConversionTransformer('quarterly_to_ttm', 'sum')
        """
        super().__init__(config.model_dump() if config else None)
        # Normalize enum to string
        if isinstance(conversion_type, ConversionType):
            ctype = conversion_type.value
        else:
            ctype = conversion_type
        if ctype not in self.CONVERSION_TYPES:
            raise ValueError(
                f"Invalid conversion type: {ctype}. Must be one of {self.CONVERSION_TYPES}"
            )
        self.conversion_type = ctype
        self.aggregation = aggregation

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform data by converting between period types.

        Args:
            data: DataFrame with time-based data to convert. Must have either:
                - A DatetimeIndex
                - An index containing date/time strings parsable by pd.to_datetime
                - Period labels that can be converted to datetime

                The data frequency should match the source period type (e.g.,
                quarterly data for 'quarterly_to_annual' conversion).

        Returns:
            DataFrame with converted periods:
            - For annual conversions: Index will be years (integers)
            - For quarterly conversions: Index will be (year, quarter) tuples
            - For TTM conversions: Original datetime index is preserved

            All columns are aggregated according to the specified method.

        Raises:
            TypeError: If data is not a pandas DataFrame.
            ValueError: If index cannot be converted to datetime or if
                aggregation='sum' is used with 'quarterly_to_ttm' and a
                different aggregation method is specified.

        Examples:
            >>> # Convert quarterly data to annual
            >>> df = pd.DataFrame({
            ...     'revenue': [100, 110, 120, 130],
            ...     'costs': [60, 65, 70, 75]
            ... }, index=['2023-Q1', '2023-Q2', '2023-Q3', '2023-Q4'])
            >>>
            >>> converter = PeriodConversionTransformer('quarterly_to_annual')
            >>> annual = converter.transform(df)
            >>> print(annual)
            #       revenue  costs
            # 2023      460    270
        """
        # Ensure we have a DataFrame
        if not isinstance(data, pd.DataFrame):
            raise TypeError("Period conversion requires a pandas DataFrame")

        return super().transform(data)

    def _transform_impl(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Apply the period conversion transformation.

        Args:
            data: The data to transform.

        Returns:
            The transformed data.

        Raises:
            DataValidationError: If data is not a DataFrame or Series.
            ValueError: If conversion fails.
        """
        if not isinstance(data, pd.DataFrame | pd.Series):
            raise DataValidationError(
                "Period conversion requires a pandas DataFrame or Series",
                validation_errors=[f"Got type: {type(data).__name__}"],
            )

        # Handle Series by converting to DataFrame temporarily
        if isinstance(data, pd.Series):
            temp_df = data.to_frame()
            result_df = self._convert_periods(temp_df)
            return result_df.iloc[:, 0]  # Return as Series
        else:
            return self._convert_periods(data)

    def _convert_periods(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply period conversion to a DataFrame.

        Args:
            df: DataFrame to convert.

        Returns:
            Converted DataFrame.
        """
        df_copy = df.copy()

        # Try to convert index to datetime if it's not already
        if not isinstance(df_copy.index, pd.DatetimeIndex):
            try:
                df_copy.index = pd.to_datetime(df_copy.index)
                logger.debug("Successfully converted DataFrame index to DatetimeIndex.")
            except Exception as e:
                logger.exception(
                    "Failed to convert DataFrame index to DatetimeIndex. Ensure index contains standard date/time strings or is already a DatetimeIndex."
                )
                raise ValueError(
                    f"Index must be convertible to datetime for period conversion: {e}"
                )

        if self.conversion_type == ConversionType.QUARTERLY_TO_ANNUAL.value:
            # Group by year and aggregate
            return df_copy.groupby(df_copy.index.year).agg(self.aggregation)

        elif self.conversion_type == ConversionType.MONTHLY_TO_QUARTERLY.value:
            # Group by year and quarter
            return df_copy.groupby([df_copy.index.year, df_copy.index.quarter]).agg(
                self.aggregation
            )

        elif self.conversion_type == ConversionType.MONTHLY_TO_ANNUAL.value:
            # Group by year
            return df_copy.groupby(df_copy.index.year).agg(self.aggregation)

        elif self.conversion_type == ConversionType.QUARTERLY_TO_TTM.value:
            # Implement TTM as rolling sum with window=4 for quarterly data
            if self.aggregation == "sum":
                return df_copy.rolling(window=4, min_periods=4).sum()
            else:
                # For other aggregation methods, we need custom logic
                raise ValueError(
                    "QUARTERLY_TO_TTM conversion currently only supports 'sum' aggregation for TTM. "
                    "TTM typically represents the sum of the last 4 quarters for flow items like revenue."
                )
        else:
            raise NotImplementedError(
                f"Conversion type '{self.conversion_type}' is defined in ConversionType enum but not implemented in PeriodConversionTransformer."
            )



================================================================================
File: fin_statement_model/preprocessing/transformers/time_series.py
================================================================================

"""Financial data transformers for the Financial Statement Model.

This module provides the TimeSeriesTransformer which applies growth rates,
moving averages, CAGR, year-over-year, and quarter-over-quarter conversions.
"""

import logging
import numpy as np
import pandas as pd
from typing import Union, Optional, ClassVar

from fin_statement_model.preprocessing.config import (
    TimeSeriesConfig,
    TransformationType,
)
from fin_statement_model.preprocessing.base_transformer import DataTransformer

logger = logging.getLogger(__name__)


class TimeSeriesTransformer(DataTransformer):
    """Transformer for time series financial data analysis.

    This transformer provides common time series transformations used in financial
    analysis to identify trends, growth patterns, and period-over-period changes.

    Supported transformation types:
        - **growth_rate**: Calculate period-to-period growth rates (%)
        - **moving_avg**: Calculate moving averages over specified window
        - **cagr**: Compute compound annual growth rate
        - **yoy**: Year-over-year comparison (%)
        - **qoq**: Quarter-over-quarter comparison (%)

    Data Frequency Assumptions:
        The transformer makes no assumptions about the frequency of your data.
        You must specify the appropriate 'periods' parameter based on your data:

        - For **monthly data**:
            - YoY: use periods=12 (compare to same month last year)
            - QoQ: use periods=3 (compare to same month last quarter)

        - For **quarterly data**:
            - YoY: use periods=4 (compare to same quarter last year)
            - QoQ: use periods=1 (compare to previous quarter)

        - For **annual data**:
            - YoY: use periods=1 (compare to previous year)

    Examples:
        Calculate year-over-year growth for quarterly revenue data:

        >>> import pandas as pd
        >>> from fin_statement_model.preprocessing.transformers import TimeSeriesTransformer
        >>>
        >>> # Quarterly revenue data
        >>> data = pd.DataFrame({
        ...     'revenue': [100, 105, 110, 115, 120, 125, 130, 135],
        ...     'costs': [60, 62, 65, 68, 70, 73, 75, 78]
        ... }, index=pd.date_range('2022-Q1', periods=8, freq='Q'))
        >>>
        >>> # Calculate YoY growth (comparing to same quarter previous year)
        >>> yoy_transformer = TimeSeriesTransformer(
        ...     transformation_type='yoy',
        ...     periods=4  # 4 quarters back for quarterly data
        ... )
        >>> yoy_growth = yoy_transformer.transform(data)
        >>> print(yoy_growth[['revenue_yoy', 'costs_yoy']].iloc[4:])  # First 4 periods will be NaN
        #             revenue_yoy  costs_yoy
        # 2023-Q1           20.0      16.67
        # 2023-Q2           19.05     17.74
        # 2023-Q3           18.18     15.38
        # 2023-Q4           17.39     14.71

        Calculate 3-month moving average for monthly data:

        >>> # Monthly sales data
        >>> monthly_data = pd.DataFrame({
        ...     'sales': [100, 95, 105, 110, 108, 115, 120, 118, 125]
        ... }, index=pd.date_range('2023-01', periods=9, freq='M'))
        >>>
        >>> # Calculate 3-month moving average
        >>> ma_transformer = TimeSeriesTransformer(
        ...     transformation_type='moving_avg',
        ...     window_size=3
        ... )
        >>> ma_result = ma_transformer.transform(monthly_data)
        >>> print(ma_result['sales_ma3'].round(2))
        # 2023-01-31       NaN
        # 2023-02-28       NaN
        # 2023-03-31    100.00
        # 2023-04-30    103.33
        # 2023-05-31    107.67
        # 2023-06-30    111.00
        # 2023-07-31    114.33
        # 2023-08-31    117.67
        # 2023-09-30    121.00

    Note:
        - Growth rate calculations will return NaN for periods without valid
          comparison data (e.g., first 4 periods for YoY with quarterly data)
        - CAGR requires at least 2 data points and positive starting values
        - Moving averages will have NaN values for the first (window_size - 1) periods
    """

    TRANSFORMATION_TYPES: ClassVar[list[str]] = [t.value for t in TransformationType]

    def __init__(
        self,
        transformation_type: Union[
            str, TransformationType
        ] = TransformationType.GROWTH_RATE,
        periods: int = 1,
        window_size: int = 3,
        config: Optional[TimeSeriesConfig] = None,
    ):
        """Initialize the time series transformer.

        Args:
            transformation_type: Type of transformation to apply. Can be either
                a string or TransformationType enum value:
                - 'growth_rate': Period-to-period growth rate
                - 'moving_avg': Rolling window average
                - 'cagr': Compound annual growth rate
                - 'yoy': Year-over-year growth rate
                - 'qoq': Quarter-over-quarter growth rate
            periods: Number of periods for lag calculations. Critical for YoY/QoQ:
                - For YoY with quarterly data: use periods=4
                - For YoY with monthly data: use periods=12
                - For QoQ with quarterly data: use periods=1
                - For QoQ with monthly data: use periods=3
                - For growth_rate: use periods=1 for consecutive period growth
            window_size: Size of the moving average window (only used for 'moving_avg').
                Default is 3.
            config: Optional TimeSeriesConfig object containing configuration.
                If provided, overrides other parameters.

        Raises:
            ValueError: If transformation_type is invalid.

        Examples:
            >>> # YoY for quarterly data
            >>> transformer = TimeSeriesTransformer('yoy', periods=4)
            >>>
            >>> # 3-month moving average
            >>> transformer = TimeSeriesTransformer('moving_avg', window_size=3)
            >>>
            >>> # Quarter-over-quarter for monthly data
            >>> transformer = TimeSeriesTransformer('qoq', periods=3)
        """
        super().__init__(config.model_dump() if config else None)
        # Normalize to string
        if isinstance(transformation_type, TransformationType):
            ttype = transformation_type.value
        else:
            ttype = transformation_type
        if ttype not in self.TRANSFORMATION_TYPES:
            raise ValueError(
                f"Invalid transformation type: {ttype}. Must be one of {self.TRANSFORMATION_TYPES}"
            )
        self.transformation_type = ttype

        self.periods = periods
        self.window_size = window_size

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform time series data based on the configured transformation type.

        Args:
            data: DataFrame containing time series financial data. The DataFrame
                should have a time-based index (DatetimeIndex, PeriodIndex, or
                sequential numeric index) for meaningful time series analysis.

        Returns:
            DataFrame with new columns containing transformed values:
            - For 'growth_rate': adds '{column}_growth' columns
            - For 'moving_avg': adds '{column}_ma{window_size}' columns
            - For 'cagr': adds '{column}_cagr' columns (single value repeated)
            - For 'yoy': adds '{column}_yoy' columns
            - For 'qoq': adds '{column}_qoq' columns

            Original columns are preserved in all cases.

        Raises:
            TypeError: If data is not a pandas DataFrame.

        Examples:
            >>> df = pd.DataFrame({'revenue': [100, 110, 120, 130]})
            >>> transformer = TimeSeriesTransformer('growth_rate')
            >>> result = transformer.transform(df)
            >>> print(result)
            #    revenue  revenue_growth
            # 0      100             NaN
            # 1      110            10.0
            # 2      120            9.09
            # 3      130            8.33
        """
        if not isinstance(data, pd.DataFrame):
            raise TypeError(
                f"Unsupported data type: {type(data)}. Expected pandas.DataFrame"
            )
        return super().transform(data)

    def _transform_impl(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Transform time series data.

        Internal method that performs the actual transformation based on
        the configured transformation type.

        Args:
            data: DataFrame or Series containing time series data.

        Returns:
            Transformed data with the same type as input.
        """
        # Handle Series by converting to DataFrame temporarily
        if isinstance(data, pd.Series):
            temp_df = data.to_frame()
            result_df = self._transform_dataframe(temp_df)
            return result_df.iloc[:, 0]  # Return as Series
        else:
            return self._transform_dataframe(data)

    def _transform_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform a DataFrame with time series data.

        Internal method that performs the actual transformation on DataFrames.
        """
        result = df.copy()

        if self.transformation_type == "growth_rate":
            for col in df.columns:
                result[f"{col}_growth"] = df[col].pct_change(periods=self.periods) * 100

        elif self.transformation_type == "moving_avg":
            for col in df.columns:
                result[f"{col}_ma{self.window_size}"] = (
                    df[col].rolling(window=self.window_size).mean()
                )

        elif self.transformation_type == "cagr":
            # Assuming the index represents time periods
            n_periods_for_cagr = len(df) - 1

            if n_periods_for_cagr < 1:
                logger.warning(
                    "CAGR requires at least 2 periods. Returning NaN for all columns."
                )
                for col in df.columns:
                    result[f"{col}_cagr"] = pd.NA
            else:
                for col in df.columns:
                    start_val = df[col].iloc[0]
                    end_val = df[col].iloc[-1]

                    if pd.isna(start_val) or pd.isna(end_val) or start_val == 0:
                        result[f"{col}_cagr"] = pd.NA
                        continue

                    ratio = end_val / start_val
                    # Check for negative base with fractional exponent leading to complex numbers
                    if ratio < 0 and (1 / n_periods_for_cagr) % 1 != 0:
                        result[f"{col}_cagr"] = pd.NA
                    else:
                        try:
                            # Ensure result is float, np.power can handle negative base if exponent is integer
                            power_val = np.power(ratio, (1 / n_periods_for_cagr))
                            if np.iscomplex(
                                power_val
                            ):  # Should be caught by above, but defensive
                                result[f"{col}_cagr"] = pd.NA
                            else:
                                result[f"{col}_cagr"] = (float(power_val) - 1) * 100
                        except (
                            ValueError,
                            TypeError,
                            ZeroDivisionError,
                        ):  # Catch any math errors
                            result[f"{col}_cagr"] = pd.NA

        elif self.transformation_type == "yoy":
            if self.periods not in [
                4,
                12,
            ]:  # Assuming YoY is typically for quarterly (lag 4) or monthly (lag 12)
                logger.warning(
                    f"For YoY transformation, 'periods' parameter is {self.periods}. "
                    f"This will calculate change over {self.periods} periods. "
                    f"Commonly, 4 (for quarterly data) or 12 (for monthly data) is used for YoY."
                )
            for col in df.columns:
                result[f"{col}_yoy"] = df[col].pct_change(periods=self.periods) * 100

        elif self.transformation_type == "qoq":
            if self.periods not in [
                1,
                3,
            ]:  # Assuming QoQ is typically for quarterly (lag 1) or monthly (lag 3)
                logger.warning(
                    f"For QoQ transformation, 'periods' parameter is {self.periods}. "
                    f"This will calculate change over {self.periods} periods. "
                    f"Commonly, 1 (for quarterly data) or 3 (for monthly data) is used for QoQ."
                )
            for col in df.columns:
                result[f"{col}_qoq"] = df[col].pct_change(periods=self.periods) * 100

        else:
            # This case should ideally be caught by the __init__ validation,
            # but as a safeguard during development:
            raise NotImplementedError(
                f"Transformation type '{self.transformation_type}' is defined in TransformationType enum but not implemented in TimeSeriesTransformer."
            )

        return result



================================================================================
File: fin_statement_model/statements/__init__.py
================================================================================

"""Financial Statements Layer (`fin_statement_model.statements`).

This package provides domain-specific abstractions for defining, building,
managing, and presenting financial statements (like Income Statement,
Balance Sheet, Cash Flow Statement) based on underlying configurations.

It sits above the `core` layer and orchestrates the use of core components
(like `Graph`, `Node`) within the context of financial statement structures.
It utilizes configurations (often YAML files) to define the layout, items,
and calculations of a statement.

Key functionalities include:
  - Defining statement structure (`StatementStructure`, `Section`, `LineItem` etc.)
  - Loading and validating statement configurations (`StatementConfig`).
  - Building `StatementStructure` objects from configurations
    (`StatementStructureBuilder`).
  - Managing multiple loaded statements (`StatementRegistry`).
  - Populating a `core.graph.Graph` with calculation nodes based on statement
    definitions (`populate_graph_from_statement`).
  - Formatting statement data retrieved from a graph into user-friendly formats,
    primarily pandas DataFrames (`StatementFormatter`).
  - High-level functions to streamline common workflows like generating a
    statement DataFrame or exporting statements to files (`create_statement_dataframe`,
    `export_statements_to_excel`).
  - Centralizing ID resolution logic between statement items and graph nodes
    (`IDResolver`).

This package imports from `core` and `io` (indirectly via `factory`), but should
not be imported by `core`.
"""

# Core statement structure components
from .structure import (
    StatementStructure,
    Section,
    LineItem,
    CalculatedLineItem,
    SubtotalLineItem,
    StatementItemType,
    StatementItem,  # Added base item type if needed
)

# Configuration related classes
from .configs.validator import StatementConfig
from .configs.models import AdjustmentFilterSpec

# Building
from .structure.builder import StatementStructureBuilder

# Registry
from .registry import StatementRegistry

# ID Resolution
from .population.id_resolver import IDResolver

# Data Fetching
from .formatting.data_fetcher import DataFetcher, FetchResult, NodeData

# Item Processors
from .population.item_processors import (
    ProcessorResult,
    ItemProcessor,
    MetricItemProcessor,
    CalculatedItemProcessor,
    SubtotalItemProcessor,
    ItemProcessorManager,
)

# Result Types for Error Handling
from .utilities.result_types import (
    Result,
    Success,
    Failure,
    ErrorDetail,
    ErrorSeverity,
    ErrorCollector,
    OperationResult,
    ValidationResult,
    ProcessingResult,
    combine_results,
)

# Retry Handler
from .utilities.retry_handler import (
    RetryHandler,
    RetryConfig,
    RetryStrategy,
    RetryResult,
    BackoffStrategy,
    ExponentialBackoff,
    LinearBackoff,
    ConstantBackoff,
    retry_with_exponential_backoff,
    retry_on_specific_errors,
)

# Populator
from .population.populator import populate_graph_from_statement

# Formatting
from .formatting.formatter import StatementFormatter

# High-level orchestration functions
from .orchestration.orchestrator import create_statement_dataframe
from .orchestration.exporter import (
    export_statements_to_excel,
    export_statements_to_json,
)

# Errors specific to statements
from .errors import StatementError, ConfigurationError
from typing import Any, Optional

# Import UnifiedNodeValidator for convenience
from fin_statement_model.io.validation import UnifiedNodeValidator


# Node validation convenience functions
def create_validated_statement_config(
    config_data: dict[str, Any],
    enable_node_validation: bool = True,
    strict_mode: bool = False,
    node_validator: Optional[UnifiedNodeValidator] = None,
) -> StatementConfig:
    """Create a StatementConfig with optional node validation enabled.

    Args:
        config_data: Dictionary containing the raw configuration data.
        enable_node_validation: If True, validates node IDs using UnifiedNodeValidator.
        strict_mode: If True, treats node validation failures as errors.
        node_validator: Optional pre-configured UnifiedNodeValidator instance.

    Returns:
        StatementConfig instance with validation configured.

    Example:
        >>> config_data = {...}  # Your YAML/JSON config as dict
        >>> config = create_validated_statement_config(
        ...     config_data,
        ...     enable_node_validation=True,
        ...     strict_mode=True
        ... )
        >>> errors = config.validate_config()
        >>> if errors:
        ...     print("Validation failed:", errors)
    """
    return StatementConfig(
        config_data=config_data,
        enable_node_validation=enable_node_validation,
        node_validation_strict=strict_mode,
        node_validator=node_validator,
    )


def create_validated_statement_builder(
    enable_node_validation: bool = True,
    strict_mode: bool = False,
    node_validator: Optional[UnifiedNodeValidator] = None,
) -> StatementStructureBuilder:
    """Create a StatementStructureBuilder with optional node validation enabled.

    Args:
        enable_node_validation: If True, validates node IDs during build.
        strict_mode: If True, treats node validation failures as errors.
        node_validator: Optional pre-configured UnifiedNodeValidator instance.

    Returns:
        StatementStructureBuilder instance with validation configured.

    Example:
        >>> builder = create_validated_statement_builder(
        ...     enable_node_validation=True,
        ...     strict_mode=False  # Warnings only
        ... )
        >>> statement = builder.build(validated_config)
    """
    return StatementStructureBuilder(
        enable_node_validation=enable_node_validation,
        node_validation_strict=strict_mode,
        node_validator=node_validator,
    )


def validate_statement_config_with_nodes(
    config_path_or_data: str | dict[str, Any],
    strict_mode: bool = False,
    auto_standardize: bool = True,
) -> tuple[StatementConfig, list[str]]:
    """Validate a statement configuration with comprehensive node validation.

    This is a high-level convenience function that handles the entire validation
    process including node ID validation.

    Args:
        config_path_or_data: Path to config file or config data dict.
        strict_mode: If True, treats node validation failures as errors.
        auto_standardize: If True, auto-standardize alternate node names.

    Returns:
        Tuple of (StatementConfig, validation_errors).
        If validation_errors is empty, validation was successful.

    Example:
        >>> config, errors = validate_statement_config_with_nodes(
        ...     "path/to/income_statement.yaml",
        ...     strict_mode=True
        ... )
        >>> if errors:
        ...     print("Validation failed:", errors)
        >>> else:
        ...     print("Validation passed!")
    """
    # File-based loading is no longer supported; only in-memory dicts
    if not isinstance(config_path_or_data, dict):
        raise ConfigurationError(
            message="File-based loading of statement configs is no longer supported; please pass a configuration dictionary."
        )
    config_data = config_path_or_data

    # Create validator
    node_validator = UnifiedNodeValidator(
        strict_mode=strict_mode,
        auto_standardize=auto_standardize,
        warn_on_non_standard=True,
        enable_patterns=True,
    )

    # Create and validate config
    config = StatementConfig(
        config_data=config_data,
        enable_node_validation=True,
        node_validation_strict=strict_mode,
        node_validator=node_validator,
    )

    errors = config.validate_config()
    return config, errors


def build_validated_statement_from_config(
    config_path_or_data: str | dict[str, Any],
    strict_mode: bool = False,
    auto_standardize: bool = True,
) -> StatementStructure:
    """Build a complete validated StatementStructure from configuration.

    This is the highest-level convenience function that handles the entire
    process from config to built statement with comprehensive validation.

    Args:
        config_path_or_data: Path to config file or config data dict.
        strict_mode: If True, treats node validation failures as errors.
        auto_standardize: If True, auto-standardize alternate node names.

    Returns:
        StatementStructure instance.

    Raises:
        ConfigurationError: If validation fails in strict mode.
        ValueError: If config validation fails.

    Example:
        >>> try:
        ...     statement = build_validated_statement_from_config(
        ...         "path/to/income_statement.yaml",
        ...         strict_mode=True
        ...     )
        ...     print(f"Built statement: {statement.name}")
        ... except ConfigurationError as e:
        ...     print(f"Validation failed: {e}")
    """
    # Validate config
    config, errors = validate_statement_config_with_nodes(
        config_path_or_data, strict_mode, auto_standardize
    )

    if errors:
        raise ConfigurationError(
            message="Statement configuration validation failed",
            errors=errors,
        )

    # Create builder with validation
    builder = create_validated_statement_builder(
        enable_node_validation=True,
        strict_mode=strict_mode,
    )

    # Build statement
    return builder.build(config)


# Public API definition
__all__ = [
    # Core components
    "AdjustmentFilterSpec",
    "BackoffStrategy",
    "CalculatedItemProcessor",
    "CalculatedLineItem",
    "ConfigurationError",
    "ConstantBackoff",
    "DataFetcher",
    "ErrorCollector",
    "ErrorDetail",
    "ErrorSeverity",
    "ExponentialBackoff",
    "Failure",
    "FetchResult",
    "IDResolver",
    "ItemProcessor",
    "ItemProcessorManager",
    "LineItem",
    "LinearBackoff",
    "MetricItemProcessor",
    "MetricLineItem",
    "NodeData",
    "OperationResult",
    "ProcessingResult",
    "ProcessorResult",
    "Result",
    "RetryConfig",
    "RetryHandler",
    "RetryResult",
    "RetryStrategy",
    "Section",
    "StatementConfig",
    "StatementError",
    "StatementFormatter",
    "StatementItem",
    "StatementItemType",
    "StatementRegistry",
    "StatementStructure",
    "StatementStructureBuilder",
    "SubtotalItemProcessor",
    "SubtotalLineItem",
    "Success",
    "UnifiedNodeValidator",
    "ValidationResult",
    # High-level functions
    "build_validated_statement_from_config",
    "combine_results",
    "create_statement_dataframe",
    "create_validated_statement_builder",
    "create_validated_statement_config",
    "export_statements_to_excel",
    "export_statements_to_json",
    "populate_graph_from_statement",
    "retry_on_specific_errors",
    "retry_with_exponential_backoff",
    "validate_statement_config_with_nodes",
]

# Note: FinancialStatementGraph removed as part of refactor, assuming its
# responsibilities are covered by core Graph and statement-specific components.



================================================================================
File: fin_statement_model/statements/configs/__init__.py
================================================================================

"""Configuration handling for financial statements.

This package provides:
- Pydantic models for configuration validation
- Configuration file loading utilities
- StatementConfig class for managing configurations
"""

from .models import (
    AdjustmentFilterSpec,
    BaseItemModel,
    CalculatedItemModel,
    CalculationSpec,
    LineItemModel,
    MetricItemModel,
    SectionModel,
    StatementModel,
    SubtotalModel,
)
from .validator import StatementConfig

__all__ = [
    # Models
    "AdjustmentFilterSpec",
    "BaseItemModel",
    "CalculatedItemModel",
    "CalculationSpec",
    "LineItemModel",
    "MetricItemModel",
    "SectionModel",
    # Validator
    "StatementConfig",
    "StatementModel",
    "SubtotalModel",
]



================================================================================
File: fin_statement_model/statements/configs/models.py
================================================================================

"""Define Pydantic models for statement configuration.

This module defines Pydantic models for validating statement configuration data,
including statements, sections, line items, calculations, and subtotals.
"""

from __future__ import annotations

from typing import Any, Optional, Union, Literal

from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator


class CalculationSpec(BaseModel):
    """Define a calculation specification.

    Args:
        type: Type identifier for the calculation (e.g., 'addition', 'subtraction').
        inputs: List of input node or line item IDs referenced by this calculation.
    """

    type: str = Field(
        ...,
        description="Type identifier for the calculation (e.g., 'addition', 'subtraction').",
    )
    inputs: list[str] = Field(
        ...,
        description="List of input node or line item IDs referenced by this calculation.",
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class AdjustmentFilterSpec(BaseModel):
    """Define an adjustment filter specification for configuration.

    This model represents the adjustment filter options that can be specified
    in configuration files. It maps to the core AdjustmentFilter model but
    uses serializable types suitable for YAML/JSON.

    Args:
        include_scenarios: Only include adjustments from these scenarios.
        exclude_scenarios: Exclude adjustments from these scenarios.
        include_tags: Include adjustments matching any of these tag prefixes.
        exclude_tags: Exclude adjustments matching any of these tag prefixes.
        require_all_tags: Include only adjustments having all these exact tags.
        include_types: Only include adjustments of these types.
        exclude_types: Exclude adjustments of these types.
        period: The specific period context for effective window checks.
    """

    include_scenarios: Optional[list[str]] = Field(
        None, description="Only include adjustments from these scenarios."
    )
    exclude_scenarios: Optional[list[str]] = Field(
        None, description="Exclude adjustments from these scenarios."
    )
    include_tags: Optional[list[str]] = Field(
        None, description="Include adjustments matching any of these tag prefixes."
    )
    exclude_tags: Optional[list[str]] = Field(
        None, description="Exclude adjustments matching any of these tag prefixes."
    )
    require_all_tags: Optional[list[str]] = Field(
        None, description="Include only adjustments having all these exact tags."
    )
    include_types: Optional[list[str]] = Field(
        None,
        description="Only include adjustments of these types (additive, multiplicative, replacement).",
    )
    exclude_types: Optional[list[str]] = Field(
        None,
        description="Exclude adjustments of these types (additive, multiplicative, replacement).",
    )
    period: Optional[str] = Field(
        None, description="The specific period context for effective window checks."
    )

    @field_validator("include_types", "exclude_types", mode="before")
    def validate_adjustment_types(cls, value: list[str] | None) -> list[str] | None:
        """Validate adjustment types are valid."""
        if value is not None:
            valid_types = {"additive", "multiplicative", "replacement"}
            for adj_type in value:
                if adj_type not in valid_types:
                    raise ValueError(
                        f"Invalid adjustment type '{adj_type}'. Must be one of: {valid_types}"
                    )
        return value

    model_config = ConfigDict(extra="forbid", frozen=True)


class BaseItemModel(BaseModel):
    """Define common fields for all statement items.

    Args:
        id: Unique identifier for the item. Must not contain spaces.
        name: Human-readable name of the item.
        description: Optional description for the item.
        metadata: Optional metadata dictionary for the item.
        sign_convention: Sign convention for the item (1 or -1).
        default_adjustment_filter: Optional default adjustment filter for this item.
        display_format: Optional specific number format string (e.g., ",.2f", ",.0f").
        hide_if_all_zero: Whether to hide this item if all values are zero.
        css_class: Optional CSS class name for HTML/web outputs.
        notes_references: List of footnote/note IDs referenced by this item.
        units: Optional unit description (e.g., "USD Thousands", "Percentage").
        display_scale_factor: Factor to scale values for display (e.g., 0.001 for thousands).
    """

    id: str = Field(
        ...,
        description="Unique identifier for the item. Must not contain spaces.",
    )
    name: str = Field(..., description="Human-readable name of the item.")
    description: Optional[str] = Field(
        "", description="Optional description for the item."
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Optional metadata for the item."
    )
    sign_convention: int = Field(
        1, description="Sign convention for the item (1 or -1)."
    )
    default_adjustment_filter: Optional[Union[AdjustmentFilterSpec, list[str]]] = Field(
        None,
        description="Optional default adjustment filter for this item. Can be a filter specification or list of tags.",
    )

    # Enhanced Display Control Fields
    display_format: Optional[str] = Field(
        None,
        description="Specific number format string for this item (e.g., ',.2f', ',.0f', '.1%').",
    )
    hide_if_all_zero: bool = Field(
        False,
        description="Whether to hide this item from display if all values are zero or null.",
    )
    css_class: Optional[str] = Field(
        None,
        description="CSS class name to apply to this item in HTML/web outputs.",
    )
    notes_references: list[str] = Field(
        default_factory=list,
        description="List of footnote or note IDs that reference this item.",
    )

    # Contra Item Support
    is_contra: bool = Field(
        False,
        description="Whether this is a contra item (e.g., Accumulated Depreciation, Treasury Stock, Sales Returns) that naturally reduces the balance of its category for display purposes.",
    )

    # Units and Scaling Fields
    units: Optional[str] = Field(
        None,
        description="Unit description for this item (e.g., 'USD Thousands', 'Percentage', 'Days').",
    )
    display_scale_factor: float = Field(
        1.0,
        description="Factor to scale values for display purposes (e.g., 0.001 to show in thousands).",
    )

    @field_validator("id", mode="before")
    def id_must_not_contain_spaces(cls, value: str) -> str:
        """Ensure that 'id' does not contain spaces."""
        if " " in value:
            raise ValueError("must not contain spaces")
        return value

    @field_validator("display_scale_factor", mode="before")
    def validate_display_scale_factor(cls, value: float) -> float:
        """Ensure display_scale_factor is positive and non-zero."""
        if value <= 0:
            raise ValueError("display_scale_factor must be positive and non-zero")
        return value

    @field_validator("display_format", mode="before")
    def validate_display_format(cls, value: Optional[str]) -> Optional[str]:
        """Validate that display_format is a reasonable format string."""
        if value is not None:
            # Basic validation - try to format a test number
            try:
                test_format = f"{12345.67:{value}}"
                # Basic sanity check that it produced something reasonable
                if not test_format or len(test_format) > 50:
                    raise ValueError("Invalid or problematic format string")
            except (ValueError, TypeError) as e:
                raise ValueError(f"Invalid display_format '{value}': {e}") from e
        return value

    model_config = ConfigDict(extra="forbid", frozen=True)


class LineItemModel(BaseItemModel):
    """Define a basic line item configuration model.

    Args:
        type: Must be 'line_item' for this model.
        node_id: ID of the core node this line item maps to.
        standard_node_ref: Optional reference to a standard node name from the registry.
    """

    type: Literal["line_item"] = Field(
        "line_item", description="Discriminator for basic line items."
    )
    node_id: Optional[str] = Field(
        None, description="ID of the core node this line item maps to."
    )
    standard_node_ref: Optional[str] = Field(
        None,
        description="Reference to a standard node name from the standard_node_registry.",
    )

    @model_validator(mode="before")
    def exactly_one_node_reference(cls, values: dict[str, Any]) -> dict[str, Any]:
        """Ensure exactly one of 'node_id' or 'standard_node_ref' is provided."""
        node_id = values.get("node_id")
        standard_ref = values.get("standard_node_ref")

        if not node_id and not standard_ref:
            raise ValueError("must provide either 'node_id' or 'standard_node_ref'")
        if node_id and standard_ref:
            raise ValueError(
                "cannot provide both 'node_id' and 'standard_node_ref' - use only one"
            )
        return values

    model_config = ConfigDict(extra="forbid", frozen=True)


class MetricItemModel(BaseItemModel):
    """Define a metric-based line item configuration model.

    Args:
        type: Must be 'metric' for this model.
        metric_id: ID of the metric in the core registry.
        inputs: Mapping of metric input names to statement item IDs.
    """

    type: Literal["metric"] = Field(
        "metric", description="Discriminator for metric-based items."
    )
    metric_id: str = Field(
        ..., description="ID of the metric in the core.metrics.registry."
    )
    inputs: dict[str, str] = Field(
        ..., description="Mapping of metric input names to statement item IDs."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class CalculatedItemModel(BaseItemModel):
    """Define a calculated line item configuration model.

    Args:
        type: Must be 'calculated' for this model.
        calculation: Calculation specification for the calculated item.
    """

    type: Literal["calculated"] = Field(
        "calculated", description="Discriminator for calculated items."
    )
    calculation: CalculationSpec = Field(
        ..., description="Calculation specification for the calculated item."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class SubtotalModel(BaseItemModel):
    """Define a subtotal configuration model.

    Args:
        type: Must be 'subtotal' for this model.
        calculation: Optional calculation specification for the subtotal.
        items_to_sum: Optional list of item IDs to sum for the subtotal.
    """

    type: Literal["subtotal"] = Field(
        "subtotal", description="Discriminator for subtotal items."
    )
    calculation: Optional[CalculationSpec] = Field(
        None, description="Calculation specification for the subtotal."
    )
    items_to_sum: Optional[list[str]] = Field(
        None, description="List of item IDs to sum for the subtotal."
    )

    @model_validator(mode="before")
    def exactly_one_of_calculation_or_items(
        cls, values: dict[str, Any]
    ) -> dict[str, Any]:
        """Ensure exactly one of 'calculation' or 'items_to_sum' is provided."""
        calc, items = values.get("calculation"), values.get("items_to_sum")
        if bool(calc) == bool(items):
            raise ValueError(
                "must provide exactly one of 'calculation' or 'items_to_sum'"
            )
        return values

    model_config = ConfigDict(extra="forbid", frozen=True)


class SectionModel(BaseItemModel):
    """Define a nested section within the statement configuration.

    Args:
        type: Must be 'section' for this model.
        items: List of line items, calculated items, subtotals, or nested sections.
        subsections: List of nested sections.
        subtotal: Optional subtotal configuration for this section.
        default_adjustment_filter: Optional default adjustment filter for this section.
    """

    type: Literal["section"] = Field(
        "section", description="Discriminator for nested sections."
    )
    items: list[
        Union[
            LineItemModel,
            CalculatedItemModel,
            MetricItemModel,
            SubtotalModel,
            SectionModel,
        ]
    ] = Field(
        default_factory=list,
        description=(
            "List of line items, calculated items, subtotals, or nested sections."
        ),
    )
    subsections: list[SectionModel] = Field(
        default_factory=list,
        description="List of nested sections.",
    )
    subtotal: Optional[SubtotalModel] = Field(
        None, description="Optional subtotal configuration for this section."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)

    @model_validator(mode="after")
    def check_unique_item_ids(section: SectionModel) -> SectionModel:
        """Ensure that item and subsection IDs within a section are unique and subtotal refs valid."""
        ids = [item.id for item in section.items] + [
            sub.id for sub in section.subsections
        ]
        duplicates = {item_id for item_id in ids if ids.count(item_id) > 1}
        if duplicates:
            raise ValueError(
                f"Duplicate item id(s) in section '{section.id}': {', '.join(duplicates)}"
            )
        if section.subtotal and section.subtotal.items_to_sum is not None:
            valid_ids = [item.id for item in section.items]
            missing = [i for i in section.subtotal.items_to_sum if i not in valid_ids]
            if missing:
                raise ValueError(
                    f"Section '{section.id}' subtotal references undefined ids: {', '.join(missing)}"
                )
        return section


SectionModel.model_rebuild(force=True)


class StatementModel(BaseModel):
    """Define the top-level statement configuration model.

    Args:
        id: Unique identifier for the statement. Must not contain spaces.
        name: Human-readable name of the statement.
        description: Optional description of the statement.
        metadata: Optional metadata dictionary.
        sections: List of top-level sections in the statement.
        units: Optional default unit description for the entire statement.
        display_scale_factor: Optional default scale factor for the entire statement.
    """

    id: str = Field(
        ..., description="Unique statement identifier. Must not contain spaces."
    )
    name: str = Field(..., description="Human-readable statement name.")
    description: Optional[str] = Field(
        "", description="Optional statement description."
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Optional metadata dictionary."
    )
    sections: list[SectionModel] = Field(
        ..., description="List of top-level sections in the statement."
    )

    # Statement-level units and scaling
    units: Optional[str] = Field(
        None,
        description="Default unit description for the statement (e.g., 'USD Thousands').",
    )
    display_scale_factor: float = Field(
        1.0,
        description="Default scale factor for displaying values in this statement.",
    )

    @field_validator("id", mode="before")
    def id_must_not_contain_spaces(cls, value: str) -> str:
        """Ensure that statement 'id' does not contain spaces."""
        if " " in value:
            raise ValueError("must not contain spaces")
        return value

    @field_validator("display_scale_factor", mode="before")
    def validate_display_scale_factor(cls, value: float) -> float:
        """Ensure display_scale_factor is positive and non-zero."""
        if value <= 0:
            raise ValueError("display_scale_factor must be positive and non-zero")
        return value

    model_config = ConfigDict(extra="forbid", frozen=True)

    @model_validator(mode="after")
    def check_unique_section_ids(model: StatementModel) -> StatementModel:
        """Ensure that top-level section IDs are unique."""
        ids = [section.id for section in model.sections]
        duplicates = {sec_id for sec_id in ids if ids.count(sec_id) > 1}
        if duplicates:
            raise ValueError(f"Duplicate section id(s): {', '.join(duplicates)}")
        return model



================================================================================
File: fin_statement_model/statements/configs/validator.py
================================================================================

"""Statement configuration handling for Financial Statement Model.

This module provides utilities for parsing and validating statement configuration data
(provided as a dictionary) and building StatementStructure objects.
"""

# Removed json, yaml, Path imports as file loading moved to IO
import logging
from typing import Any, Optional

# Use absolute imports
# Import Pydantic models for building from validated configuration
from fin_statement_model.statements.configs.models import (
    StatementModel,
    BaseItemModel,
    LineItemModel,
    CalculatedItemModel,
    MetricItemModel,
    SubtotalModel,
    SectionModel,
    CalculationSpec,
)
from pydantic import ValidationError  # Import directly

# Import UnifiedNodeValidator for node ID validation
from fin_statement_model.io.validation import UnifiedNodeValidator

# Import Result types for enhanced error handling
from fin_statement_model.statements.utilities.result_types import (
    ErrorCollector,
    ErrorSeverity,
)

# Configure logging
logger = logging.getLogger(__name__)


class StatementConfig:
    """Manages configuration parsing and building for financial statement structures.

    This class handles validating statement configuration data (provided as a dictionary)
    and building StatementStructure objects from these configurations.
    It does NOT handle file loading.
    """

    def __init__(
        self,
        config_data: dict[str, Any],
        enable_node_validation: bool = False,
        node_validation_strict: bool = False,
        node_validator: Optional[UnifiedNodeValidator] = None,
    ):
        """Initialize a statement configuration processor.

        Args:
            config_data: Dictionary containing the raw configuration data.
            enable_node_validation: If True, validates node IDs using UnifiedNodeValidator.
            node_validation_strict: If True, treats node validation failures as errors.
                                   If False, treats them as warnings.
            node_validator: Optional pre-configured UnifiedNodeValidator instance.
                           If None and enable_node_validation is True, creates a default instance.

        Raises:
            ValueError: If config_data is not a non-empty dictionary.
        """
        if not config_data or not isinstance(config_data, dict):
            raise ValueError("config_data must be a non-empty dictionary.")

        self.config_data = config_data
        self.model: Optional[StatementModel] = None  # Store validated model

        # Node validation configuration
        self.enable_node_validation = enable_node_validation
        self.node_validation_strict = node_validation_strict

        # Initialize node_validator attribute
        self.node_validator: Optional[UnifiedNodeValidator] = None
        if enable_node_validation:
            if node_validator is not None:
                self.node_validator = node_validator
            else:
                # Create default validator
                self.node_validator = UnifiedNodeValidator(
                    strict_mode=node_validation_strict,
                    auto_standardize=True,
                    warn_on_non_standard=True,
                    enable_patterns=True,
                )

    def validate_config(self) -> list[str]:
        """Validate the configuration data using Pydantic models and optional node validation.

        Returns:
            list[str]: List of validation errors, or empty list if valid.
                     Stores the validated model in self.model on success.
        """
        error_collector = ErrorCollector()

        try:
            # First perform Pydantic validation
            self.model = StatementModel.model_validate(self.config_data)

            # If node validation is enabled, perform additional validation
            if self.enable_node_validation and self.node_validator:
                self._validate_node_ids(self.model, error_collector)

            # Convert warnings and errors to string list for backward compatibility
            validation_errors: list[str] = []

            # Add errors (always included)
            validation_errors.extend(
                str(error) for error in error_collector.get_errors()
            )

            # Add warnings if in strict mode
            if self.node_validation_strict:
                validation_errors.extend(
                    str(warning) for warning in error_collector.get_warnings()
                )
            else:
                # Log warnings but don't include in errors list
                for warning in error_collector.get_warnings():
                    logger.warning(f"Node validation warning: {warning}")

            return validation_errors

        except ValidationError as ve:
            # Convert Pydantic errors to list of strings
            errors: list[str] = []
            for err in ve.errors():
                loc = ".".join(str(x) for x in err.get("loc", []))
                msg = err.get("msg", "")
                errors.append(f"{loc}: {msg}")
            self.model = None  # Ensure model is not set on validation error
            return errors
        except Exception as e:
            # Catch other potential validation issues
            logger.exception("Unexpected error during configuration validation")
            self.model = None
            return [f"Unexpected validation error: {e}"]

    def _validate_node_ids(
        self, model: StatementModel, error_collector: ErrorCollector
    ) -> None:
        """Validate all node IDs in the statement model using UnifiedNodeValidator.

        Args:
            model: The validated StatementModel to check.
            error_collector: ErrorCollector to accumulate validation issues.
        """
        logger.debug(f"Starting node ID validation for statement '{model.id}'")

        # Validate statement ID itself
        self._validate_single_node_id(
            model.id, "statement", "statement.id", error_collector
        )

        # Validate all sections recursively
        for section in model.sections:
            self._validate_section_node_ids(
                section, error_collector, f"statement.{model.id}"
            )

    def _validate_section_node_ids(
        self,
        section: SectionModel,
        error_collector: ErrorCollector,
        parent_context: str,
    ) -> None:
        """Validate node IDs within a section and its items.

        Args:
            section: The section model to validate.
            error_collector: ErrorCollector to accumulate validation issues.
            parent_context: Context string for error reporting.
        """
        section_context = f"{parent_context}.section.{section.id}"

        # Validate section ID
        self._validate_single_node_id(
            section.id, "section", f"{section_context}.id", error_collector
        )

        # Validate all items in the section
        for item in section.items:
            self._validate_item_node_ids(item, error_collector, section_context)

        # Validate subsections recursively
        for subsection in section.subsections:
            self._validate_section_node_ids(
                subsection, error_collector, section_context
            )

        # Validate section subtotal if present
        if section.subtotal:
            self._validate_item_node_ids(
                section.subtotal, error_collector, section_context
            )

    def _validate_item_node_ids(
        self, item: BaseItemModel, error_collector: ErrorCollector, parent_context: str
    ) -> None:
        """Validate node IDs within a specific item.

        Args:
            item: The item model to validate.
            error_collector: ErrorCollector to accumulate validation issues.
            parent_context: Context string for error reporting.
        """
        item_context = f"{parent_context}.item.{item.id}"

        # Validate the item ID itself
        self._validate_single_node_id(
            item.id, "item", f"{item_context}.id", error_collector
        )

        # Type-specific validation
        if isinstance(item, LineItemModel):
            # Validate node_id if present
            if item.node_id:
                self._validate_single_node_id(
                    item.node_id, "node", f"{item_context}.node_id", error_collector
                )

            # Validate standard_node_ref if present
            if item.standard_node_ref:
                self._validate_single_node_id(
                    item.standard_node_ref,
                    "standard_node",
                    f"{item_context}.standard_node_ref",
                    error_collector,
                )

        elif isinstance(item, CalculatedItemModel):
            # Validate calculation inputs
            self._validate_calculation_inputs(
                item.calculation, error_collector, item_context
            )

        elif isinstance(item, MetricItemModel):
            # Validate metric inputs (the values, not the keys)
            for input_key, input_id in item.inputs.items():
                self._validate_single_node_id(
                    input_id,
                    "metric_input",
                    f"{item_context}.inputs.{input_key}",
                    error_collector,
                )

        elif isinstance(item, SubtotalModel):
            # Validate items_to_sum if present
            if item.items_to_sum:
                for i, input_id in enumerate(item.items_to_sum):
                    self._validate_single_node_id(
                        input_id,
                        "subtotal_input",
                        f"{item_context}.items_to_sum[{i}]",
                        error_collector,
                    )

            # Validate calculation inputs if present
            if item.calculation:
                self._validate_calculation_inputs(
                    item.calculation, error_collector, item_context
                )

        elif isinstance(item, SectionModel):
            # Recursive validation for nested sections
            self._validate_section_node_ids(item, error_collector, parent_context)

    def _validate_calculation_inputs(
        self,
        calculation: CalculationSpec,
        error_collector: ErrorCollector,
        parent_context: str,
    ) -> None:
        """Validate inputs within a calculation specification.

        Args:
            calculation: The calculation specification to validate.
            error_collector: ErrorCollector to accumulate validation issues.
            parent_context: Context string for error reporting.
        """
        for i, input_id in enumerate(calculation.inputs):
            self._validate_single_node_id(
                input_id,
                "calculation_input",
                f"{parent_context}.calculation.inputs[{i}]",
                error_collector,
            )

    def _validate_single_node_id(
        self,
        node_id: str,
        node_type: str,
        context: str,
        error_collector: ErrorCollector,
    ) -> None:
        """Validate a single node ID using the UnifiedNodeValidator.

        Args:
            node_id: The node ID to validate.
            node_type: Type description for error messages.
            context: Context string for error reporting.
            error_collector: ErrorCollector to accumulate validation issues.
        """
        if not self.node_validator:
            return

        try:
            validation_result = self.node_validator.validate(
                node_id,
                node_type=node_type,
                parent_nodes=None,  # Could be enhanced to track parent context
                use_cache=True,
            )

            # Determine severity based on validation result and configuration
            if not validation_result.is_valid:
                severity = (
                    ErrorSeverity.ERROR
                    if self.node_validation_strict
                    else ErrorSeverity.WARNING
                )
                message = (
                    f"Invalid {node_type} ID '{node_id}': {validation_result.message}"
                )
                if severity == ErrorSeverity.ERROR:
                    error_collector.add_error(
                        code="invalid_node_id",
                        message=message,
                        context=context,
                        source=node_id,
                    )
                else:
                    error_collector.add_warning(
                        code="invalid_node_id",
                        message=message,
                        context=context,
                        source=node_id,
                    )

            elif validation_result.category in [
                "alternate",
                "subnode_nonstandard",
                "custom",
            ]:
                # These are valid but could be improved
                error_collector.add_warning(
                    code="non_standard_node_id",
                    message=f"Non-standard {node_type} ID '{node_id}': {validation_result.message}",
                    context=context,
                    source=node_id,
                )

            # Add suggestions if available
            if validation_result.suggestions:
                suggestion_msg = f"Suggestions for {node_type} ID '{node_id}': {'; '.join(validation_result.suggestions)}"
                error_collector.add_warning(
                    code="node_id_suggestions",
                    message=suggestion_msg,
                    context=context,
                    source=node_id,
                )

        except Exception as e:
            logger.exception(
                f"Error validating node ID '{node_id}' in context '{context}'"
            )
            error_collector.add_warning(
                code="node_validation_error",
                message=f"Failed to validate {node_type} ID '{node_id}': {e}",
                context=context,
                source=node_id,
            )



================================================================================
File: fin_statement_model/statements/errors.py
================================================================================

"""Custom Exception classes for the `fin_statement_model.statements` package.

These exceptions provide more specific error information related to statement
definition, configuration, building, and processing, inheriting from the base
`FinancialModelError` defined in `fin_statement_model.core.errors`.
"""

from typing import Optional
from fin_statement_model.core.errors import (
    StatementError,
    ConfigurationError,
)

__all__ = [
    "ConfigurationError",
    "StatementBuilderError",
    "StatementError",
    "StatementValidationError",
]


class StatementBuilderError(StatementError):
    """Exception raised during statement structure building.

    This includes errors encountered while constructing nodes from statement
    definitions or creating graph relationships.
    """

    def __init__(
        self,
        message: str,
        item_id: Optional[str] = None,
        statement_type: Optional[str] = None,
    ):
        """Initialize a StatementBuilderError.

        Args:
            message: The primary error message.
            item_id: Optional ID of the item causing the error.
            statement_type: Optional type of statement being built.
        """
        self.item_id = item_id
        self.statement_type = statement_type

        details = []
        if statement_type:
            details.append(f"Statement type: {statement_type}")
        if item_id:
            details.append(f"Item ID: {item_id}")

        full_message = message
        if details:
            full_message = f"{message} ({', '.join(details)})"

        super().__init__(full_message)


class StatementValidationError(StatementError):
    """Exception raised during statement validation.

    This includes structural validation errors, consistency checks,
    and statement-specific rule violations.
    """

    def __init__(
        self,
        message: str,
        validation_errors: Optional[list[str]] = None,
        statement_id: Optional[str] = None,
    ):
        """Initialize a StatementValidationError.

        Args:
            message: The primary error message.
            validation_errors: Optional list of specific validation failures.
            statement_id: Optional ID of the statement being validated.
        """
        self.validation_errors = validation_errors or []

        full_message = message
        if statement_id:
            full_message = f"{message} for statement '{statement_id}'"
        if validation_errors:
            full_message = f"{full_message}:\n" + "\n".join(
                f"  - {error}" for error in validation_errors
            )

        super().__init__(full_message, statement_id=statement_id)



================================================================================
File: fin_statement_model/statements/formatting/__init__.py
================================================================================

"""Formatting and data fetching for financial statements.

This package provides tools for:
- Fetching data from graphs for statement display
- Formatting statements as DataFrames
- Applying formatting rules and conventions
"""

from .data_fetcher import DataFetcher, FetchResult, NodeData
from .formatter import StatementFormatter

__all__ = [
    # Data Fetching
    "DataFetcher",
    "FetchResult",
    "NodeData",
    # Formatting
    "StatementFormatter",
]



================================================================================
File: fin_statement_model/statements/formatting/_formatting_utils.py
================================================================================

"""Utility functions for formatting statement DataFrames."""

import pandas as pd
from typing import Optional, Any  # Keep necessary imports
from pandas.api.types import is_numeric_dtype


def apply_sign_convention(df: pd.DataFrame, period_columns: list[str]) -> pd.DataFrame:
    """Apply sign conventions to the statement values across periods."""
    result = df.copy()
    if "sign_convention" in result.columns:
        for col in period_columns:
            if col in result.columns and is_numeric_dtype(result[col]):
                mask = result[col].notna()
                # Ensure sign_convention is treated as numeric if needed
                sign_col = pd.to_numeric(
                    result.loc[mask, "sign_convention"], errors="coerce"
                ).fillna(1)
                result.loc[mask, col] = result.loc[mask, col] * sign_col
    return result


def format_numbers(
    df: pd.DataFrame,
    default_formats: dict[str, Any],  # Pass defaults needed
    number_format: Optional[str] = None,
    period_columns: Optional[list[str]] = None,
) -> pd.DataFrame:
    """Format numeric values in the statement.

    Args:
        df: DataFrame to format numbers in
        default_formats: Dictionary containing default formatting options
                         (e.g., 'precision', 'use_thousands_separator').
        number_format: Optional format string
        period_columns: List of columns containing period data to format.
                        If None, attempts to format all numeric columns
                        except metadata/indicators.

    Returns:
        pd.DataFrame: DataFrame with formatted numbers
    """
    result = df.copy()

    if period_columns:
        numeric_cols = [
            col
            for col in period_columns
            if col in result.columns and is_numeric_dtype(result[col])
        ]
    else:
        # Original logic if period_columns not specified
        numeric_cols = [
            col
            for col in result.columns
            if is_numeric_dtype(result[col])
            and col not in ("sign_convention", "depth", "ID")  # Added ID
            and not col.startswith("meta_")
            and col != "Line Item"  # Ensure Line Item name is not formatted
        ]

    # Get defaults from the passed dictionary
    precision = default_formats.get("precision", 2)  # Provide fallback default
    use_thousands = default_formats.get("use_thousands_separator", True)

    if number_format:
        # Use provided format string
        for col in numeric_cols:
            # Check if column exists before applying format
            if col in result.columns:
                result[col] = result[col].apply(
                    lambda x: f"{x:{number_format}}" if pd.notna(x) else ""
                )
    else:
        # Use default formatting based on passed defaults
        for col in numeric_cols:
            # Check if column exists before applying format
            if col in result.columns:
                result[col] = result[col].apply(
                    lambda x: (
                        (f"{x:,.{precision}f}" if pd.notna(x) else "")
                        if use_thousands
                        else (f"{x:.{precision}f}" if pd.notna(x) else "")
                    )
                )

    return result



================================================================================
File: fin_statement_model/statements/formatting/data_fetcher.py
================================================================================

"""Data fetching functionality for financial statements.

This module provides the DataFetcher class that handles retrieving data from
the graph for statement formatting. It encapsulates the logic for resolving
item IDs to node IDs and fetching values with proper error handling.
"""

import logging
from dataclasses import dataclass
from typing import Optional, cast

import numpy as np
import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import NodeError, CalculationError
from fin_statement_model.core.adjustments.models import AdjustmentFilterInput
from fin_statement_model.statements.structure import (
    StatementStructure,
    Section,
    StatementItem,
)
from fin_statement_model.statements.population.id_resolver import IDResolver
from fin_statement_model.statements.utilities.result_types import (
    Result,
    Success,
    Failure,
    ErrorDetail,
    ErrorSeverity,
    ErrorCollector,
)

logger = logging.getLogger(__name__)

__all__ = ["DataFetcher", "FetchResult", "NodeData"]


@dataclass
class NodeData:
    """Data for a single node across all periods.

    Attributes:
        node_id: The graph node ID
        values: Dict mapping period to value
        is_adjusted: Dict mapping period to bool indicating if adjusted
        errors: Any errors encountered during fetching
    """

    node_id: str
    values: dict[str, float]
    is_adjusted: dict[str, bool]
    errors: list[ErrorDetail]

    @property
    def has_data(self) -> bool:
        """Check if any non-NaN values exist."""
        return any(pd.notna(v) for v in self.values.values())


@dataclass
class FetchResult:
    """Result of fetching data for a statement.

    Attributes:
        data: Dict mapping node_id to period values
        errors: ErrorCollector with any errors/warnings
        node_count: Number of nodes successfully fetched
        missing_nodes: List of node IDs that couldn't be found
    """

    data: dict[str, dict[str, float]]
    errors: ErrorCollector
    node_count: int
    missing_nodes: list[str]

    def to_result(self) -> Result[dict[str, dict[str, float]]]:
        """Convert to Result type."""
        if self.errors.has_errors():
            return Failure(errors=self.errors.get_errors())
        return Success(value=self.data)


class DataFetcher:
    """Fetches data from graph for statement formatting.

    This class encapsulates the logic for:
    - Resolving statement item IDs to graph node IDs
    - Fetching values from the graph with error handling
    - Applying adjustment filters if specified
    - Collecting errors and warnings during the process
    """

    def __init__(self, statement: StatementStructure, graph: Graph):
        """Initialize the data fetcher.

        Args:
            statement: The statement structure to fetch data for
            graph: The graph containing the data
        """
        self.statement = statement
        self.graph = graph
        self.id_resolver = IDResolver(statement)

    def _resolve_adjustment_filter(
        self,
        item: StatementItem,
        global_filter: Optional[AdjustmentFilterInput] = None,
    ) -> Optional[AdjustmentFilterInput]:
        """Resolve which adjustment filter to use for an item.

        Precedence order:
        1. Global filter passed to fetch method (highest priority)
        2. Item's default adjustment filter
        3. Parent section's default adjustment filter
        4. None (no filter)

        Args:
            item: The statement item to get the filter for.
            global_filter: Optional global filter that overrides everything.

        Returns:
            The resolved adjustment filter to use, or None.
        """
        # Global filter has highest priority
        if global_filter is not None:
            return global_filter

        # Check item's own default filter
        if (
            hasattr(item, "default_adjustment_filter")
            and item.default_adjustment_filter is not None
        ):
            return cast(AdjustmentFilterInput, item.default_adjustment_filter)

        # Check parent section's default filter
        # We need to find which section contains this item
        parent_section = self._find_parent_section(item)
        if (
            parent_section
            and hasattr(parent_section, "default_adjustment_filter")
            and parent_section.default_adjustment_filter is not None
        ):
            return cast(AdjustmentFilterInput, parent_section.default_adjustment_filter)

        # No filter
        return None

    def _find_parent_section(self, target_item: StatementItem) -> Optional[Section]:
        """Find the parent section that contains the given item.

        Args:
            target_item: The item to find the parent section for.

        Returns:
            The parent Section object, or None if not found.
        """

        def search_in_section(section: Section) -> Optional[Section]:
            # Check direct items
            for item in section.items:
                if item is target_item or (
                    hasattr(item, "id")
                    and hasattr(target_item, "id")
                    and item.id == target_item.id
                ):
                    return section
                # Check nested sections
                if isinstance(item, Section):
                    result = search_in_section(item)
                    if result:
                        return result

            # Check subtotal
            if hasattr(section, "subtotal") and section.subtotal is target_item:
                return section

            return None

        # Search through all top-level sections
        for section in self.statement.sections:
            result = search_in_section(section)
            if result:
                return result

        return None

    def fetch_all_data(
        self,
        adjustment_filter: Optional[AdjustmentFilterInput] = None,
        include_missing: bool = False,
    ) -> FetchResult:
        """Fetch data for all items in the statement.

        Args:
            adjustment_filter: Optional global filter for adjustments (overrides item defaults)
            include_missing: If True, include nodes that don't exist in graph
                           with NaN values

        Returns:
            FetchResult containing the fetched data and any errors
        """
        error_collector = ErrorCollector()
        data: dict[str, dict[str, float]] = {}
        missing_nodes: list[str] = []

        # Check if graph has periods
        periods = self.graph.periods
        if not periods:
            error_collector.add_error(
                code="no_periods",
                message=f"Graph has no periods defined for statement '{self.statement.id}'",
                source=self.statement.id,
            )
            return FetchResult(
                data={}, errors=error_collector, node_count=0, missing_nodes=[]
            )

        logger.debug(
            f"Fetching data for statement '{self.statement.id}' across {len(periods)} periods"
        )

        # Get all items and resolve their node IDs
        all_items = self.statement.get_all_items()
        processed_node_ids = set()

        for item in all_items:
            # Resolve item ID to node ID
            node_id = self.id_resolver.resolve(item.id, self.graph)

            if not node_id:
                error_collector.add_warning(
                    code="unresolvable_item",
                    message=f"Cannot resolve item '{item.id}' to a node ID",
                    source=item.id,
                    context="IDResolver.resolve",
                )
                continue

            if node_id in processed_node_ids:
                continue  # Skip already processed nodes

            processed_node_ids.add(node_id)

            # Resolve adjustment filter for this specific item
            item_filter = self._resolve_adjustment_filter(item, adjustment_filter)

            # Fetch data for this node
            node_result = self._fetch_node_data(
                node_id, periods, item_filter, item_id=item.id
            )

            if node_result.is_success():
                node_data = cast(NodeData, node_result.get_value())
                if node_data.has_data or include_missing:
                    data[node_id] = node_data.values

                # Add any warnings from node fetching
                for error in node_data.errors:
                    if error.severity == ErrorSeverity.WARNING:
                        error_collector.add_warning(
                            error.code,
                            error.message,
                            error.context,
                            error.source or item.id,
                        )
            else:
                # Node doesn't exist in graph
                missing_nodes.append(node_id)
                if include_missing:
                    # Fill with NaN values
                    data[node_id] = {period: np.nan for period in periods}

                error_collector.add_from_result(node_result, source=item.id)

        logger.info(
            f"Fetched data for {len(data)} nodes from statement '{self.statement.id}'. "
            f"Missing: {len(missing_nodes)}, Warnings: {len(error_collector.get_warnings())}"
        )

        return FetchResult(
            data=data,
            errors=error_collector,
            node_count=len(data),
            missing_nodes=missing_nodes,
        )

    def _fetch_node_data(
        self,
        node_id: str,
        periods: list[str],
        adjustment_filter: Optional[AdjustmentFilterInput],
        item_id: Optional[str] = None,
    ) -> Result[NodeData]:
        """Fetch data for a single node across all periods.

        Args:
            node_id: The graph node ID to fetch
            periods: List of periods to fetch
            adjustment_filter: Optional adjustment filter
            item_id: Optional statement item ID for error context

        Returns:
            Result containing NodeData or error details
        """
        # Check if node exists
        if not self.graph.has_node(node_id):
            return Failure(
                [
                    ErrorDetail(
                        code="node_not_found",
                        message=f"Node '{node_id}' not found in graph",
                        source=item_id or node_id,
                        severity=ErrorSeverity.WARNING,
                    )
                ]
            )

        values = {}
        is_adjusted = {}
        errors = []

        for period in periods:
            try:
                # Fetch value with optional adjustments
                value = self.graph.get_adjusted_value(
                    node_id,
                    period,
                    filter_input=adjustment_filter,
                    return_flag=False,  # Only need the value
                )
                # Ensure value is float or NaN
                values[period] = float(value) if pd.notna(value) else np.nan
                is_adjusted[period] = bool(value)

            except (NodeError, CalculationError) as e:
                # Expected errors - log as warning
                logger.warning(
                    f"Error calculating node '{node_id}' for period '{period}': {e}"
                )
                values[period] = np.nan
                is_adjusted[period] = False
                errors.append(
                    ErrorDetail(
                        code="calculation_error",
                        message=f"Failed to calculate value: {e}",
                        context=f"period={period}",
                        severity=ErrorSeverity.WARNING,
                        source=item_id or node_id,
                    )
                )

            except TypeError as e:
                # Filter/adjustment errors
                logger.warning(
                    f"Type error for node '{node_id}', period '{period}': {e}"
                )
                values[period] = np.nan
                is_adjusted[period] = False
                errors.append(
                    ErrorDetail(
                        code="filter_error",
                        message=f"Invalid adjustment filter: {e}",
                        context=f"period={period}",
                        severity=ErrorSeverity.WARNING,
                        source=item_id or node_id,
                    )
                )

            except Exception as e:
                # Unexpected errors - log as error
                logger.error(
                    f"Unexpected error for node '{node_id}', period '{period}': {e}",
                    exc_info=True,
                )
                values[period] = np.nan
                is_adjusted[period] = False
                errors.append(
                    ErrorDetail(
                        code="unexpected_error",
                        message=f"Unexpected error: {e}",
                        context=f"period={period}, error_type={type(e).__name__}",
                        severity=ErrorSeverity.ERROR,
                        source=item_id or node_id,
                    )
                )

        return Success(
            NodeData(
                node_id=node_id, values=values, is_adjusted=is_adjusted, errors=errors
            )
        )

    def check_adjustments(
        self,
        node_ids: list[str],
        periods: list[str],
        adjustment_filter: Optional[AdjustmentFilterInput] = None,
    ) -> dict[str, dict[str, bool]]:
        """Check which node/period combinations have adjustments.

        Args:
            node_ids: List of node IDs to check
            periods: List of periods to check
            adjustment_filter: Filter to check adjustments against

        Returns:
            Dict mapping node_id -> period -> was_adjusted boolean
        """
        results = {}

        for node_id in node_ids:
            if not self.graph.has_node(node_id):
                results[node_id] = {period: False for period in periods}
                continue

            period_results = {}
            for period in periods:
                try:
                    was_adjusted = self.graph.was_adjusted(
                        node_id, period, adjustment_filter
                    )
                    period_results[period] = bool(was_adjusted)
                except Exception as e:
                    logger.warning(
                        f"Error checking adjustments for {node_id}/{period}: {e}"
                    )
                    period_results[period] = False

            results[node_id] = period_results

        return results



================================================================================
File: fin_statement_model/statements/formatting/formatter.py
================================================================================

"""Formatter for financial statements.

This module provides functionality for formatting financial statements
for display or reporting, including applying formatting rules, adding subtotals,
and applying sign conventions with enhanced display control.
"""

import pandas as pd
import numpy as np  # Added numpy for NaN handling
import warnings  # Added for suppressing dtype warnings
from typing import Optional, Any, Union
from collections.abc import Callable
import logging
import re
from dataclasses import dataclass, field

from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.statements.structure import (
    Section,
    CalculatedLineItem,
    SubtotalLineItem,
    StatementItem,
)

# Add core Graph and errors
from fin_statement_model.core.graph import Graph

# Import adjustment types for filtering
from fin_statement_model.core.adjustments.models import AdjustmentFilterInput

# Import the ID resolver
from fin_statement_model.statements.population.id_resolver import IDResolver

# Import the data fetcher
from fin_statement_model.statements.formatting.data_fetcher import DataFetcher

# Import the new formatting utils
from ._formatting_utils import format_numbers
from ._formatting_utils import apply_sign_convention as apply_sign_convention_func

# Configure logging
logger = logging.getLogger(__name__)


@dataclass
class FormattingContext:
    """Encapsulates all formatting configuration and state for statement generation.

    This dataclass holds all the configuration options and runtime state needed
    for formatting financial statements, providing a clean interface for passing
    formatting parameters between methods.
    """

    # Core formatting options
    should_apply_signs: bool = True
    include_empty_items: bool = False
    number_format: Optional[str] = None
    include_metadata_cols: bool = False

    # Adjustment options
    adjustment_filter: Optional[AdjustmentFilterInput] = None
    add_is_adjusted_column: bool = False

    # Enhanced display options
    include_units_column: bool = False
    include_css_classes: bool = False
    include_notes_column: bool = False
    apply_item_scaling: bool = True
    apply_item_formatting: bool = True
    respect_hide_flags: Optional[bool] = None

    # Contra item options
    contra_display_style: Optional[str] = None
    apply_contra_formatting: bool = True
    add_contra_indicator_column: bool = False

    # Runtime state (populated during processing)
    all_periods: list[str] = field(default_factory=list)
    items_to_hide: set[str] = field(default_factory=set)
    default_formats: dict[str, Any] = field(default_factory=dict)

    # Derived flags (computed after initialization)
    should_include_enhanced_metadata: bool = field(init=False)

    def __post_init__(self) -> None:
        """Compute derived flags after initialization."""
        self.should_include_enhanced_metadata = any(
            [
                self.include_units_column,
                self.include_css_classes,
                self.include_notes_column,
                self.add_contra_indicator_column,
            ]
        )


class StatementFormatter:
    """Formats financial statements for display or reporting.

    This class provides methods to transform raw financial data into
    formatted financial statements with proper headers, indentation,
    subtotals, sign conventions, and enhanced display control.
    """

    def __init__(self, statement: StatementStructure):
        """Initialize a statement formatter.

        Args:
            statement: The statement structure to format
        """
        self.statement = statement

        # --- Build default display formats from global config ---
        from fin_statement_model import get_config  # Local import to avoid circular dep

        cfg_display = get_config().display
        num_format = cfg_display.default_number_format or ",.2f"
        # Detect precision from format string like ',.2f' or '.3f'
        precision_match = re.search(r"\.([0-9]+)f$", num_format)
        precision = int(precision_match.group(1)) if precision_match else 2
        use_thousands_sep = "," in num_format.split(".")[0]

        self.default_formats = {
            "precision": precision,
            "use_thousands_separator": use_thousands_sep,
            "show_zero_values": not cfg_display.hide_zero_rows,
            "show_negative_sign": cfg_display.show_negative_sign,
            "indent_character": cfg_display.indent_character,
            "subtotal_style": cfg_display.subtotal_style,
            "total_style": cfg_display.total_style,
            "header_style": cfg_display.header_style,
            # Contra item display options
            "contra_display_style": cfg_display.contra_display_style,
            "contra_css_class": cfg_display.contra_css_class,
        }

    def _resolve_hierarchical_attribute(
        self,
        item: Union[StatementItem, Section],
        attribute_name: str,
        default_value: Any = None,
        config_path: Optional[str] = None,
        skip_default_check: Optional[Callable[[Any], bool]] = None,
    ) -> Any:
        """Resolve an attribute value using hierarchical lookup.

        Precedence: Item > Parent Section > Statement > Config/Default

        Args:
            item: The item or section to resolve the attribute for
            attribute_name: Name of the attribute to look up
            default_value: Default value if not found anywhere
            config_path: Optional config path to check before using default_value
            skip_default_check: Optional function to determine if a value should be
                              considered "default" and skipped (e.g., scale_factor == 1.0)

        Returns:
            The resolved attribute value
        """
        # Check item-specific attribute
        if hasattr(item, attribute_name):
            item_value = getattr(item, attribute_name)
            if skip_default_check is None or not skip_default_check(item_value):
                return item_value

        # Check if item is part of a section with the attribute
        if isinstance(item, StatementItem):
            parent_section = self._find_parent_section_for_item(item)
            if parent_section and hasattr(parent_section, attribute_name):
                section_value = getattr(parent_section, attribute_name)
                if skip_default_check is None or not skip_default_check(section_value):
                    return section_value

        # Check statement-level attribute
        if hasattr(self.statement, attribute_name):
            statement_value = getattr(self.statement, attribute_name)
            if skip_default_check is None or not skip_default_check(statement_value):
                return statement_value

        # Check config if path provided
        if config_path:
            from fin_statement_model.config.helpers import cfg

            return cfg(config_path, default_value)

        # Return default value
        return default_value

    def _resolve_display_scale_factor(
        self, item: Union[StatementItem, Section]
    ) -> float:
        """Resolve the display scale factor for an item, considering hierarchy.

        Precedence: Item > Section > Statement > Default (from config)

        Args:
            item: The item or section to get the scale factor for

        Returns:
            The resolved scale factor
        """
        result = self._resolve_hierarchical_attribute(
            item=item,
            attribute_name="display_scale_factor",
            default_value=1.0,
            config_path="display.scale_factor",
            skip_default_check=lambda x: x == 1.0,
        )
        return float(result)

    def _resolve_units(self, item: Union[StatementItem, Section]) -> Optional[str]:
        """Resolve the unit description for an item, considering hierarchy.

        Precedence: Item > Section > Statement > None

        Args:
            item: The item or section to get the units for

        Returns:
            The resolved unit description or None
        """
        result = self._resolve_hierarchical_attribute(
            item=item,
            attribute_name="units",
            default_value=None,
            skip_default_check=lambda x: not x,  # Skip empty strings/None
        )
        return result if result is not None else None

    def _find_parent_section_for_item(
        self, target_item: StatementItem
    ) -> Optional[Section]:
        """Find the parent section that contains the given item.

        Args:
            target_item: The item to find the parent section for.

        Returns:
            The parent Section object, or None if not found.
        """

        def search_in_section(section: Section) -> Optional[Section]:
            # Check direct items
            for item in section.items:
                if item is target_item or (
                    hasattr(item, "id")
                    and hasattr(target_item, "id")
                    and item.id == target_item.id
                ):
                    return section
                # Check nested sections
                if isinstance(item, Section):
                    result = search_in_section(item)
                    if result:
                        return result

            # Check subtotal
            if hasattr(section, "subtotal") and section.subtotal is target_item:
                return section

            return None

        # Search through all top-level sections
        for section in self.statement.sections:
            result = search_in_section(section)
            if result:
                return result

        return None

    def _should_hide_item(
        self, item: Union[StatementItem, Section], values: dict[str, float]
    ) -> bool:
        """Check if an item should be hidden based on hide_if_all_zero setting.

        Args:
            item: The item to check
            values: Dictionary of period values for the item

        Returns:
            True if the item should be hidden
        """
        # Check if the item has hide_if_all_zero enabled
        hide_if_zero = getattr(item, "hide_if_all_zero", False)
        if not hide_if_zero:
            return False

        # Check if all values are zero or NaN
        return all(not (pd.notna(value) and value != 0) for value in values.values())

    def _apply_item_scaling(
        self, values: dict[str, float], scale_factor: float
    ) -> dict[str, float]:
        """Apply scaling to item values.

        Args:
            values: Dictionary of period values
            scale_factor: Factor to scale by

        Returns:
            Dictionary of scaled values
        """
        if scale_factor == 1.0:
            return values

        scaled_values = {}
        for period, value in values.items():
            if pd.notna(value):
                scaled_values[period] = value * scale_factor
            else:
                scaled_values[period] = value

        return scaled_values

    def _format_item_values(
        self,
        item: Union[StatementItem, Section],
        values: dict[str, float],
        period_columns: list[str],
    ) -> dict[str, str]:
        """Format values for an item using its specific display format if available.

        Args:
            item: The item to format values for
            values: Dictionary of period values
            period_columns: List of period column names

        Returns:
            Dictionary of formatted values
        """
        # Get item-specific display format
        item_format = getattr(item, "display_format", None)

        formatted_values = {}
        for period in period_columns:
            value = values.get(period, np.nan)

            if pd.notna(value):
                if item_format:
                    try:
                        formatted_values[period] = f"{value:{item_format}}"
                    except (ValueError, TypeError):
                        # Fall back to default if format is invalid
                        logger.warning(
                            f"Invalid display format '{item_format}' for item '{getattr(item, 'id', 'unknown')}', using default"
                        )
                        formatted_values[period] = str(
                            value
                        )  # Convert to string for consistency
                else:
                    formatted_values[period] = str(
                        value
                    )  # Convert to string for consistency
            else:
                formatted_values[period] = ""

        return formatted_values

    def _format_contra_value(
        self, value: float, display_style: str | None = None
    ) -> str:
        """Format a contra item value according to the specified display style.

        Args:
            value: The numeric value to format
            display_style: Style for contra display ("parentheses", "negative_sign", "brackets")

        Returns:
            Formatted string representation of the contra value
        """
        if pd.isna(value) or value == 0:
            return ""

        # For contra items, we typically want to show the absolute value with special formatting
        # regardless of the underlying sign, since sign_convention handles calculation logic
        from fin_statement_model.config.helpers import cfg

        style = display_style or self.default_formats.get(
            "contra_display_style", cfg("display.contra_display_style", "parentheses")
        )
        abs_value = abs(value)

        # Use dictionary for style formatting
        style_formats = {
            "parentheses": f"({abs_value:,.2f})",
            "negative_sign": f"-{abs_value:,.2f}",
            "brackets": f"[{abs_value:,.2f}]",
        }

        if style and isinstance(style, str) and style in style_formats:
            return style_formats[style]
        return f"({abs_value:,.2f})"  # Default fallback

    def _apply_contra_formatting(
        self,
        item: Union[StatementItem, Section],
        values: dict[str, float],
        period_columns: list[str],
        display_style: str | None = None,
    ) -> dict[str, str]:
        """Apply contra-specific formatting to item values.

        Args:
            item: The item to format
            values: Dictionary of period values
            period_columns: List of period column names
            display_style: Optional override for contra display style

        Returns:
            Dictionary of formatted contra values
        """
        contra_formatted = {}
        for period in period_columns:
            value = values.get(period, np.nan)
            contra_formatted[period] = self._format_contra_value(value, display_style)

        return contra_formatted

    def _prepare_formatting_context(self, **kwargs: Any) -> FormattingContext:
        """Prepare formatting context with config defaults.

        Args:
            **kwargs: All formatting parameters passed to generate_dataframe (overrides config)

        Returns:
            FormattingContext: Configured context object
        """
        from fin_statement_model import get_config

        config = get_config()

        # Create context with provided kwargs
        context = FormattingContext(
            should_apply_signs=kwargs.get(
                "should_apply_signs", config.display.apply_sign_conventions
            ),
            include_empty_items=kwargs.get(
                "include_empty_items", config.display.include_empty_items
            ),
            number_format=kwargs.get("number_format"),
            include_metadata_cols=kwargs.get(
                "include_metadata_cols", config.display.include_metadata_cols
            ),
            adjustment_filter=kwargs.get("adjustment_filter"),
            add_is_adjusted_column=kwargs.get(
                "add_is_adjusted_column", config.display.add_is_adjusted_column
            ),
            include_units_column=kwargs.get(
                "include_units_column", config.display.include_units_column
            ),
            include_css_classes=kwargs.get(
                "include_css_classes", config.display.include_css_classes
            ),
            include_notes_column=kwargs.get(
                "include_notes_column", config.display.include_notes_column
            ),
            apply_item_scaling=kwargs.get(
                "apply_item_scaling", config.display.apply_item_scaling
            ),
            apply_item_formatting=kwargs.get(
                "apply_item_formatting", config.display.apply_item_formatting
            ),
            respect_hide_flags=kwargs.get("respect_hide_flags"),
            contra_display_style=kwargs.get("contra_display_style"),
            apply_contra_formatting=kwargs.get(
                "apply_contra_formatting", config.display.apply_contra_formatting
            ),
            add_contra_indicator_column=kwargs.get(
                "add_contra_indicator_column",
                config.display.add_contra_indicator_column,
            ),
        )

        # Apply config defaults for None values
        if context.should_apply_signs is None:
            context.should_apply_signs = (
                True  # This is a calculation default, not display
            )
        if context.include_empty_items is None:
            context.include_empty_items = False  # Preserve historical default
        if context.respect_hide_flags is None:
            context.respect_hide_flags = config.display.hide_zero_rows
        if context.contra_display_style is None:
            context.contra_display_style = config.display.contra_display_style
        if context.number_format is None:
            context.number_format = config.display.default_number_format

        # Set default formats
        context.default_formats = self.default_formats

        return context

    def _fetch_statement_data(
        self, graph: Graph, context: FormattingContext
    ) -> tuple[dict[str, dict[str, float]], Any]:
        """Fetch data from graph using DataFetcher.

        Args:
            graph: The core.graph.Graph instance containing the data
            context: Formatting context with fetch parameters

        Returns:
            Tuple of (data dictionary, fetch errors)
        """
        data_fetcher = DataFetcher(self.statement, graph)
        fetch_result = data_fetcher.fetch_all_data(
            adjustment_filter=context.adjustment_filter,
            include_missing=context.include_empty_items,
        )

        # Log any warnings/errors
        if fetch_result.errors.has_warnings() or fetch_result.errors.has_errors():
            fetch_result.errors.log_all(
                prefix=f"Statement '{self.statement.id}' data fetch: "
            )

        # Update context with periods from graph
        context.all_periods = graph.periods

        return fetch_result.data, fetch_result.errors

    def _create_empty_dataframe(self, context: FormattingContext) -> pd.DataFrame:
        """Create an empty DataFrame with appropriate columns.

        Args:
            context: Formatting context with column configuration

        Returns:
            Empty DataFrame with proper column structure
        """
        base_cols = ["Line Item", "ID", *context.all_periods]
        if context.include_units_column:
            base_cols.append("units")
        return pd.DataFrame(columns=base_cols)

    def _build_row_data(
        self,
        graph: Graph,
        data: dict[str, dict[str, float]],
        context: FormattingContext,
    ) -> list[dict[str, Any]]:
        """Build row data recursively from statement structure.

        Args:
            graph: The core.graph.Graph instance
            data: Fetched data dictionary
            context: Formatting context

        Returns:
            List of row dictionaries
        """
        rows: list[dict[str, Any]] = []
        id_resolver = IDResolver(self.statement)

        # Process all sections
        self._process_items_recursive(
            items=list(self.statement.sections),
            depth=0,
            data=data,
            rows=rows,
            context=context,
            id_resolver=id_resolver,
            graph=graph,
        )

        # Filter hidden items if needed
        if context.respect_hide_flags:
            rows = [row for row in rows if row["ID"] not in context.items_to_hide]

        return rows

    def _process_items_recursive(
        self,
        items: list[Union[Section, StatementItem]],
        depth: int,
        data: dict[str, dict[str, float]],
        rows: list[dict[str, Any]],
        context: FormattingContext,
        id_resolver: IDResolver,
        graph: Graph,
    ) -> None:
        """Recursively process items and sections.

        Args:
            items: List of items or sections to process
            depth: Current indentation depth
            data: Fetched data dictionary
            rows: List to append row data to
            context: Formatting context
            id_resolver: ID resolver instance
            graph: Graph instance
        """
        for item in items:
            if isinstance(item, Section):
                self._process_section(
                    item, depth, data, rows, context, id_resolver, graph
                )
            elif isinstance(item, StatementItem):
                self._process_item(item, depth, data, rows, context, id_resolver, graph)

    def _process_section(
        self,
        section: Section,
        depth: int,
        data: dict[str, dict[str, float]],
        rows: list[dict[str, Any]],
        context: FormattingContext,
        id_resolver: IDResolver,
        graph: Graph,
    ) -> None:
        """Process a section and its items.

        Args:
            section: Section to process
            depth: Current indentation depth
            data: Fetched data dictionary
            rows: List to append row data to
            context: Formatting context
            id_resolver: ID resolver instance
            graph: Graph instance
        """
        # Process section items first to collect data for hide check
        self._process_items_recursive(
            section.items, depth + 1, data, rows, context, id_resolver, graph
        )

        # Process subtotal if it exists
        if hasattr(section, "subtotal") and section.subtotal:
            self._process_items_recursive(
                [section.subtotal], depth + 1, data, rows, context, id_resolver, graph
            )

        # Check if section should be hidden
        if context.respect_hide_flags and getattr(section, "hide_if_all_zero", False):
            # For sections, check if all contained items are hidden or zero
            section_has_data = False
            for section_item in section.items:
                node_id = id_resolver.resolve(section_item.id, graph)
                if node_id and node_id in data:
                    item_data = data[node_id]
                    if any(pd.notna(v) and v != 0 for v in item_data.values()):
                        section_has_data = True
                        break
            if not section_has_data:
                context.items_to_hide.add(section.id)

    def _process_item(
        self,
        item: StatementItem,
        depth: int,
        data: dict[str, dict[str, float]],
        rows: list[dict[str, Any]],
        context: FormattingContext,
        id_resolver: IDResolver,
        graph: Graph,
    ) -> None:
        """Process a single statement item.

        Args:
            item: Statement item to process
            depth: Current indentation depth
            data: Fetched data dictionary
            rows: List to append row data to
            context: Formatting context
            id_resolver: ID resolver instance
            graph: Graph instance
        """
        # Use ID resolver to get the correct node ID
        node_id = id_resolver.resolve(item.id, graph)
        if not node_id:
            return

        item_data = data.get(node_id, {})
        numeric_values: dict[str, float] = {
            p: item_data.get(p, np.nan) for p in context.all_periods
        }

        # Apply item-specific scaling if enabled
        if context.apply_item_scaling:
            numeric_values = self._apply_scaling(numeric_values, context, item)

        # Check if item should be hidden
        if context.respect_hide_flags and self._should_hide_item(item, numeric_values):
            context.items_to_hide.add(item.id)
            return

        # Start with numeric values and add formatted strings as needed
        row_values: dict[str, Union[float, str]] = dict(numeric_values)

        # Apply item-specific formatting if enabled (but only if not using global format)
        if context.apply_item_formatting and not context.number_format:
            formatted_values = self._format_item_values(
                item, numeric_values, context.all_periods
            )
            # Only apply if we got actual formatted strings
            if any(isinstance(v, str) for v in formatted_values.values()):
                for period in context.all_periods:
                    if period in formatted_values and isinstance(
                        formatted_values[period], str
                    ):
                        # Keep numeric value for calculations, store formatted for display
                        row_values[f"{period}_formatted"] = formatted_values[period]

        # Apply contra formatting if enabled and item is marked as contra
        if context.apply_contra_formatting and getattr(item, "is_contra", False):
            contra_formatted = self._apply_contra_formatting(
                item, numeric_values, context.all_periods, context.contra_display_style
            )
            # Store contra formatted values for later use
            for period in context.all_periods:
                if contra_formatted.get(period):
                    row_values[f"{period}_contra"] = contra_formatted[period]

        if context.include_empty_items or any(pd.notna(v) for v in row_values.values()):
            row = self._create_row_dict(item, node_id, row_values, depth, context)
            rows.append(row)

    def _create_row_dict(
        self,
        item: StatementItem,
        node_id: str,
        row_values: dict[str, Union[float, str]],
        depth: int,
        context: FormattingContext,
    ) -> dict[str, Any]:
        """Create a row dictionary for a statement item.

        Args:
            item: Statement item
            node_id: Resolved node ID
            row_values: Period values for the item
            depth: Indentation depth
            context: Formatting context

        Returns:
            Row dictionary
        """
        indent_char = context.default_formats["indent_character"]

        row = {
            "Line Item": indent_char * depth + item.name,
            "ID": item.id,
            **row_values,
            # Metadata
            "line_type": self._get_item_type(item),
            "node_id": node_id,
            "sign_convention": getattr(item, "sign_convention", 1),
            "is_subtotal": isinstance(item, SubtotalLineItem),
            "is_calculated": isinstance(item, CalculatedLineItem),
            "is_contra": getattr(item, "is_contra", False),
        }

        # Add enhanced metadata columns if requested
        if context.include_units_column:
            row["units"] = self._resolve_units(item)

        if context.include_css_classes:
            # Get item's CSS class and add contra class if applicable
            item_css_class = getattr(item, "css_class", None)
            if getattr(item, "is_contra", False):
                contra_css = context.default_formats.get(
                    "contra_css_class", "contra-item"
                )
                if item_css_class:
                    row["css_class"] = f"{item_css_class} {contra_css}"
                else:
                    row["css_class"] = contra_css
            else:
                row["css_class"] = item_css_class

        if context.include_notes_column:
            notes = getattr(item, "notes_references", [])
            row["notes"] = "; ".join(notes) if notes else ""

        return row

    def _apply_scaling(
        self, values: dict[str, float], context: FormattingContext, item: StatementItem
    ) -> dict[str, float]:
        """Apply item-specific scaling if enabled.

        Args:
            values: Dictionary of period values
            context: Formatting context
            item: Statement item

        Returns:
            Dictionary of scaled values
        """
        if not context.apply_item_scaling:
            return values

        scale_factor = self._resolve_display_scale_factor(item)
        return self._apply_item_scaling(values, scale_factor)

    def _organize_dataframe_columns(
        self, df: pd.DataFrame, context: FormattingContext
    ) -> pd.DataFrame:
        """Organize DataFrame columns in the correct order.

        Args:
            df: DataFrame to organize
            context: Formatting context

        Returns:
            DataFrame with organized columns
        """
        base_cols = ["Line Item", "ID"]
        metadata_cols = [
            "line_type",
            "node_id",
            "sign_convention",
            "is_subtotal",
            "is_calculated",
            "is_contra",
        ]

        # Enhanced metadata columns
        enhanced_cols = []
        if context.include_units_column:
            enhanced_cols.append("units")
        if context.include_css_classes:
            enhanced_cols.append("css_class")
        if context.include_notes_column:
            enhanced_cols.append("notes")
        if context.add_contra_indicator_column:
            enhanced_cols.append("is_contra")

        # Add adjustment columns if they exist
        adjusted_flag_cols = []
        if context.add_is_adjusted_column:
            adjusted_flag_cols = [
                f"{period}_is_adjusted" for period in context.all_periods
            ]

        final_cols = base_cols + context.all_periods
        if adjusted_flag_cols:
            final_cols += adjusted_flag_cols
        if enhanced_cols:
            final_cols += enhanced_cols
        if context.include_metadata_cols:
            # Add metadata cols (excluding adjustment flags if they are already added)
            final_cols += [
                m_col for m_col in metadata_cols if m_col not in adjusted_flag_cols
            ]

        # Ensure contra formatting columns are available temporarily (will be removed later)
        all_available_cols = final_cols.copy()
        if context.apply_contra_formatting:
            contra_formatted_cols = [
                f"{period}_contra" for period in context.all_periods
            ]
            all_available_cols += contra_formatted_cols

        for col in all_available_cols:
            if col not in df.columns:
                df[col] = (
                    np.nan
                    if col in context.all_periods
                    else ("" if col == "Line Item" else None)
                )

        return df[all_available_cols]

    def _add_adjustment_columns(
        self, df: pd.DataFrame, graph: Graph, context: FormattingContext
    ) -> pd.DataFrame:
        """Add adjustment status columns to the DataFrame.

        Args:
            df: DataFrame to add columns to
            graph: Graph instance
            context: Formatting context

        Returns:
            DataFrame with adjustment columns added
        """
        if not context.add_is_adjusted_column or not context.all_periods:
            return df

        # Get node IDs from the dataframe
        node_ids_to_check = []
        for _, row in df.iterrows():
            node_id = row.get("node_id")
            is_calc_or_subtotal = row.get("is_calculated", False) or row.get(
                "is_subtotal", False
            )
            if node_id and not is_calc_or_subtotal:
                node_ids_to_check.append(node_id)

        # Use DataFetcher to check adjustments
        if node_ids_to_check:
            data_fetcher = DataFetcher(self.statement, graph)
            adjustment_status = data_fetcher.check_adjustments(
                node_ids_to_check, context.all_periods, context.adjustment_filter
            )
        else:
            adjustment_status = {}

        # Build adjustment columns
        is_adjusted_data = []
        for _, row in df.iterrows():
            node_id = row.get("node_id")
            is_calc_or_subtotal = row.get("is_calculated", False) or row.get(
                "is_subtotal", False
            )

            if node_id and not is_calc_or_subtotal and node_id in adjustment_status:
                row_adj_flags = {
                    f"{period}_is_adjusted": adjustment_status[node_id].get(
                        period, False
                    )
                    for period in context.all_periods
                }
            else:
                # For calculated/subtotal items or missing nodes, flags are False
                row_adj_flags = {
                    f"{period}_is_adjusted": False for period in context.all_periods
                }
            is_adjusted_data.append(row_adj_flags)

        if is_adjusted_data:
            adj_df = pd.DataFrame(is_adjusted_data, index=df.index)
            df = pd.concat([df, adj_df], axis=1)

        return df

    def _apply_sign_conventions(
        self, df: pd.DataFrame, context: FormattingContext
    ) -> pd.DataFrame:
        """Apply sign conventions to the dataframe.

        Args:
            df: DataFrame to apply sign conventions to
            context: Formatting context

        Returns:
            DataFrame with sign conventions applied
        """
        if not context.should_apply_signs:
            return df

        return apply_sign_convention_func(df, context.all_periods)

    def _apply_contra_display_formatting(
        self, df: pd.DataFrame, context: FormattingContext
    ) -> pd.DataFrame:
        """Apply contra display formatting to the dataframe.

        Args:
            df: DataFrame to apply contra formatting to
            context: Formatting context

        Returns:
            DataFrame with contra formatting applied
        """
        if not context.apply_contra_formatting:
            return df

        for index, row in df.iterrows():
            if row.get("is_contra", False):
                for period in context.all_periods:
                    contra_col = f"{period}_contra"
                    if (
                        contra_col in row
                        and pd.notna(row[contra_col])
                        and row[contra_col]
                    ):
                        # Suppress dtype warnings since we're intentionally converting float to string
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore", FutureWarning)
                            df.at[index, period] = row[contra_col]

        return df

    def _apply_number_formatting(
        self, df: pd.DataFrame, context: FormattingContext
    ) -> pd.DataFrame:
        """Apply number formatting to the dataframe.

        Args:
            df: DataFrame to apply number formatting to
            context: Formatting context

        Returns:
            DataFrame with number formatting applied
        """
        if not context.apply_item_formatting or context.number_format:
            df = format_numbers(
                df,
                default_formats=context.default_formats,
                number_format=context.number_format,
                period_columns=context.all_periods,
            )
        return df

    def _cleanup_temporary_columns(
        self, df: pd.DataFrame, context: FormattingContext
    ) -> pd.DataFrame:
        """Clean up temporary columns from the dataframe.

        Args:
            df: DataFrame to clean up
            context: Formatting context

        Returns:
            DataFrame with temporary columns removed
        """
        # Remove the contra formatting columns from the final output
        if context.apply_contra_formatting:
            contra_cols_to_remove = [
                f"{period}_contra" for period in context.all_periods
            ]
            df = df.drop(
                columns=[col for col in contra_cols_to_remove if col in df.columns]
            )

        # Build final column list
        base_cols = ["Line Item", "ID"]
        metadata_cols = [
            "line_type",
            "node_id",
            "sign_convention",
            "is_subtotal",
            "is_calculated",
            "is_contra",
        ]

        enhanced_cols = []
        if context.include_units_column:
            enhanced_cols.append("units")
        if context.include_css_classes:
            enhanced_cols.append("css_class")
        if context.include_notes_column:
            enhanced_cols.append("notes")
        if context.add_contra_indicator_column:
            enhanced_cols.append("is_contra")

        adjusted_flag_cols = []
        if context.add_is_adjusted_column:
            adjusted_flag_cols = [
                f"{period}_is_adjusted" for period in context.all_periods
            ]

        final_cols = base_cols + context.all_periods
        if context.add_is_adjusted_column:
            final_cols += adjusted_flag_cols
        if enhanced_cols:
            final_cols += enhanced_cols
        if context.include_metadata_cols:
            # Add metadata cols (excluding adjustment flags if they are already added)
            final_cols += [
                m_col for m_col in metadata_cols if m_col not in adjusted_flag_cols
            ]

        # Select only the final columns for output
        return df[final_cols]

    def _apply_all_formatting(
        self, df: pd.DataFrame, context: FormattingContext
    ) -> pd.DataFrame:
        """Apply all formatting steps in the correct order.

        Args:
            df: DataFrame to format
            context: Formatting context

        Returns:
            Fully formatted DataFrame
        """
        # 1. Sign conventions
        df = self._apply_sign_conventions(df, context)

        # 2. Contra formatting (if applicable)
        df = self._apply_contra_display_formatting(df, context)

        # 3. Number formatting
        df = self._apply_number_formatting(df, context)

        # 4. Clean up temporary columns
        df = self._cleanup_temporary_columns(df, context)

        return df

    def generate_dataframe(
        self,
        graph: Graph,
        should_apply_signs: Optional[bool] = None,
        include_empty_items: Optional[bool] = None,
        number_format: Optional[str] = None,
        include_metadata_cols: Optional[bool] = None,
        # --- Adjustment Integration ---
        adjustment_filter: AdjustmentFilterInput = None,
        add_is_adjusted_column: Optional[bool] = None,
        # --- End Adjustment Integration ---
        # --- Enhanced Display Control ---
        include_units_column: Optional[bool] = None,
        include_css_classes: Optional[bool] = None,
        include_notes_column: Optional[bool] = None,
        apply_item_scaling: Optional[bool] = None,
        apply_item_formatting: Optional[bool] = None,
        respect_hide_flags: Optional[bool] = None,
        # --- Contra Item Support ---
        contra_display_style: Optional[str] = None,
        apply_contra_formatting: Optional[bool] = None,
        add_contra_indicator_column: Optional[bool] = None,
        # --- End Contra Item Support ---
        # --- End Enhanced Display Control ---
        **kwargs: Any,
    ) -> pd.DataFrame:
        """Generate a formatted DataFrame of the statement including subtotals.

        Queries the graph for data based on the statement structure,
        calculates subtotals, and formats the result with enhanced display control.

        Args:
            graph: The core.graph.Graph instance containing the data.
            should_apply_signs: Whether to apply sign conventions after calculation.
                              If None, uses config.display default.
            include_empty_items: Whether to include items with no data rows.
                               If None, uses !config.display.hide_zero_rows.
            number_format: Optional Python format string for numbers (e.g., ',.2f').
                          If None, uses config.display.default_number_format when formatting.
            include_metadata_cols: If True, includes hidden metadata columns
                                   (like sign_convention, node_id) in the output.
            adjustment_filter: Optional filter for applying adjustments during data fetch.
            add_is_adjusted_column: If True, adds a boolean column indicating if the
                                    value for a node/period was adjusted.
            include_units_column: If True, includes a column showing units for each item.
            include_css_classes: If True, includes CSS class information in metadata.
            include_notes_column: If True, includes a column with note references.
            apply_item_scaling: If True, applies item-specific scaling factors.
            apply_item_formatting: If True, applies item-specific number formats.
            respect_hide_flags: If True, respects hide_if_all_zero flags.
                              If None, uses config.display.hide_zero_rows.
            contra_display_style: Optional contra display style for items
                                If None, uses config.display.contra_display_style.
            apply_contra_formatting: If True, applies contra-specific formatting
            add_contra_indicator_column: If True, adds a column indicating contra items
            **kwargs: Additional keyword arguments (for future extensibility)

        Returns:
            pd.DataFrame: Formatted statement DataFrame with subtotals and enhanced display control.
        """
        # 1. Prepare context
        context = self._prepare_formatting_context(
            should_apply_signs=should_apply_signs,
            include_empty_items=include_empty_items,
            number_format=number_format,
            include_metadata_cols=include_metadata_cols,
            adjustment_filter=adjustment_filter,
            add_is_adjusted_column=add_is_adjusted_column,
            include_units_column=include_units_column,
            include_css_classes=include_css_classes,
            include_notes_column=include_notes_column,
            apply_item_scaling=apply_item_scaling,
            apply_item_formatting=apply_item_formatting,
            respect_hide_flags=respect_hide_flags,
            contra_display_style=contra_display_style,
            apply_contra_formatting=apply_contra_formatting,
            add_contra_indicator_column=add_contra_indicator_column,
            **kwargs,
        )

        # 2. Fetch data
        data, errors = self._fetch_statement_data(graph, context)

        # 3. Build rows
        rows = self._build_row_data(graph, data, context)

        # 4. Create DataFrame
        if not rows:
            return self._create_empty_dataframe(context)

        df = pd.DataFrame(rows)

        # 5. Apply adjustments first (before organizing columns)
        if context.add_is_adjusted_column:
            df = self._add_adjustment_columns(df, graph, context)

        # 6. Organize columns
        df = self._organize_dataframe_columns(df, context)

        # 7. Apply all formatting
        df = self._apply_all_formatting(df, context)

        return df

    def _get_item_type(self, item: StatementItem) -> str:
        """Get the type of a statement item.

        Args:
            item: Statement item to get type for

        Returns:
            str: Item type identifier
        """
        if isinstance(item, Section):
            return "section"
        elif isinstance(item, SubtotalLineItem):
            return "subtotal"
        elif isinstance(item, CalculatedLineItem):
            return "calculated"
        else:
            return "item"

    def format_html(
        self,
        graph: Graph,
        should_apply_signs: Optional[bool] = None,
        include_empty_items: Optional[bool] = None,
        css_styles: Optional[dict[str, str]] = None,
        use_item_css_classes: Optional[bool] = None,
        **kwargs: Any,
    ) -> str:
        """Format the statement data as HTML with enhanced styling support.

        Args:
            graph: The core.graph.Graph instance containing the data.
            should_apply_signs: Whether to apply sign conventions (override config).
            include_empty_items: Whether to include items with no data (override config).
            css_styles: Optional dict of CSS styles for the HTML.
            use_item_css_classes: Whether to use item-specific CSS classes (override config).
            **kwargs: Additional arguments passed to generate_dataframe.

        Returns:
            str: HTML string representing the statement with enhanced styling.
        """
        # Load display config defaults
        from fin_statement_model import get_config

        config = get_config()
        # Determine final values (kwargs override config)
        should_apply_signs = (
            should_apply_signs
            if should_apply_signs is not None
            else config.display.apply_sign_conventions
        )
        include_empty_items = (
            include_empty_items
            if include_empty_items is not None
            else config.display.include_empty_items
        )
        use_item_css_classes = (
            use_item_css_classes
            if use_item_css_classes is not None
            else config.display.include_css_classes
        )
        # Enable CSS classes if requested
        if use_item_css_classes:
            kwargs["include_css_classes"] = True

        df = self.generate_dataframe(
            graph=graph,
            should_apply_signs=should_apply_signs,
            include_empty_items=include_empty_items,
            # number_format is applied internally by generate_dataframe
            **kwargs,
        )

        html: str = df.to_html(
            index=False, classes="statement-table", table_id="financial-statement"
        )

        if css_styles or use_item_css_classes:
            style_str = "<style>\n"

            # Add default styles for statement tables
            style_str += """
            .statement-table { border-collapse: collapse; width: 100%; }
            .statement-table th, .statement-table td { padding: 8px; text-align: right; border: 1px solid #ddd; }
            .statement-table th { background-color: #f2f2f2; font-weight: bold; }
            .statement-table .Line.Item { text-align: left; }
            .contra-item { font-style: italic; color: #666; }
            """

            # Add custom styles
            if css_styles:
                for selector, style in css_styles.items():
                    style_str += f"{selector} {{ {style} }}\n"

            style_str += "</style>\n"
            html = style_str + html

        return html



================================================================================
File: fin_statement_model/statements/orchestration/__init__.py
================================================================================

"""High-level orchestration functions for statement processing.

This package provides the main public API for:
- Creating statement DataFrames from configurations
- Exporting statements to various formats
- Coordinating the overall workflow
"""

from .exporter import export_statements_to_excel, export_statements_to_json
from .orchestrator import create_statement_dataframe, populate_graph

__all__ = [
    # Main API
    "create_statement_dataframe",
    "export_statements_to_excel",
    "export_statements_to_json",
    # Internal helpers
    "populate_graph",
]



================================================================================
File: fin_statement_model/statements/orchestration/exporter.py
================================================================================

"""Statement export functionality.

This module handles exporting financial statements to various file formats
including Excel and JSON. It provides both internal helper functions and
public API functions for exporting statements.
"""

import logging
from pathlib import Path
from typing import Any, Callable, Optional

import pandas as pd

from fin_statement_model.core.errors import FinancialModelError
from fin_statement_model.core.graph import Graph
from fin_statement_model.io import write_statement_to_excel, write_statement_to_json
from fin_statement_model.io.exceptions import WriteError

logger = logging.getLogger(__name__)

__all__ = [
    "export_statements",
    "export_statements_to_excel",
    "export_statements_to_json",
]


def export_statements(
    graph: Graph,
    config_path_or_dir: str,
    output_dir: str,
    format_kwargs: dict[str, Any],
    writer_func: Callable[..., None],
    writer_kwargs: dict[str, Any],
    file_suffix: str,
) -> None:
    """Generate and export statement DataFrames using a specific writer function.

    Internal helper function that takes generated DataFrames (or generates them
    if needed via `create_statement_dataframe`) and uses the provided
    `writer_func` to save them to disk.

    Args:
        graph: The core.graph.Graph instance.
        config_path_or_dir: Path to config file or directory.
        output_dir: Directory where output files will be saved.
        format_kwargs: Optional arguments for `create_statement_dataframe`.
        writer_func: The function responsible for writing a DataFrame to a file
            (e.g., `write_statement_to_excel`).
        writer_kwargs: Optional arguments passed directly to the `writer_func`.
        file_suffix: The file extension to use for output files (e.g., ".xlsx").

    Raises:
        WriteError: If any errors occur during the file writing process.
        FinancialModelError: If errors occur during DataFrame generation.
        FileNotFoundError: If config path doesn't exist.
    """
    # Import here to avoid circular dependency
    from fin_statement_model.statements.orchestration.orchestrator import (
        create_statement_dataframe,
    )

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Load statement configuration(s) into memory if a path was provided
    def _parse_config_file(file_path: Path) -> dict[str, Any] | None:
        """Parse a YAML or JSON statement configuration file into a dictionary.

        Returns None if the file cannot be parsed or the root element is not a mapping.
        """
        # Security: Check file size to prevent memory exhaustion
        max_size = 10 * 1024 * 1024  # 10MB limit
        if file_path.stat().st_size > max_size:
            logger.warning("Config file '%s' exceeds size limit (%d bytes) - skipping", file_path, max_size)
            return None

        try:
            if file_path.suffix.lower() == ".json":
                import json
                data = json.loads(file_path.read_text(encoding="utf-8"))
            else:
                # Fallback to YAML for .yaml / .yml extensions (PyYAML is a dependency)
                import yaml
                data = yaml.safe_load(file_path.read_text(encoding="utf-8"))
            if not isinstance(data, dict):
                logger.warning(
                    "Statement config '%s' is not a mapping at the root level – skipping", file_path
                )
                return None
            return data  # type: ignore[return-value]
        except (json.JSONDecodeError, yaml.YAMLError) as exc:
            logger.warning("Failed to parse statement config file '%s': %s", file_path, exc)
            return None
        except Exception as exc:
            logger.exception(
                "Failed to parse statement config file '%s': %s", file_path, exc
            )
            return None

    # Determine raw_configs based on the provided argument type
    raw_configs: dict[str, dict[str, Any]]
    if isinstance(config_path_or_dir, (str, Path)):
        cfg_path = Path(config_path_or_dir)
        if not cfg_path.exists():
            raise FileNotFoundError(f"Config path '{cfg_path}' does not exist.")
        if cfg_path.is_dir():
            raw_configs = {}
            for item in cfg_path.iterdir():
                if item.is_file() and item.suffix.lower() in {".yaml", ".yml", ".json"}:
                    parsed_cfg = _parse_config_file(item)
                    if parsed_cfg:
                        stmt_id = str(parsed_cfg.get("id", item.stem))
                        raw_configs[stmt_id] = parsed_cfg
            if not raw_configs:
                raise FileNotFoundError(
                    f"No valid statement configuration files found in directory '{cfg_path}'."
                )
        else:
            parsed_cfg = _parse_config_file(cfg_path)
            if parsed_cfg is None:
                raise FileNotFoundError(
                    f"Failed to parse statement configuration file '{cfg_path}'."
                )
            stmt_id = str(parsed_cfg.get("id", cfg_path.stem))
            raw_configs = {stmt_id: parsed_cfg}
    elif isinstance(config_path_or_dir, dict):
        # Already provided a mapping of configs
        raw_configs = config_path_or_dir  # type: ignore[assignment]
    else:
        raise TypeError(
            "config_path_or_dir must be a path to a config file/directory or a mapping of configs."
        )

    try:
        dfs = create_statement_dataframe(graph, raw_configs, format_kwargs)
    except FinancialModelError:
        logger.exception("Failed to generate statement dataframes for export:")
        raise  # Re-raise critical errors from generation step

    if not dfs:
        logger.warning(
            f"No DataFrames generated, nothing to export to {file_suffix} files."
        )
        return

    # Standardize to dictionary format
    if isinstance(dfs, pd.DataFrame):
        # Derive a sensible statement ID when only a single DataFrame is returned
        if isinstance(config_path_or_dir, (str, Path)):
            # Use file/dir name if a path was provided
            path_obj = Path(config_path_or_dir)
            stmt_id = path_obj.stem if path_obj.is_file() else "statement"
        else:
            # Fallback to the first key in the raw_configs mapping
            stmt_id = next(iter(raw_configs.keys()))
        dfs_dict = {stmt_id: dfs}
    else:
        dfs_dict = dfs

    export_errors = []
    for stmt_id, df in dfs_dict.items():
        # Ensure stmt_id is filename-safe (basic replacement)
        safe_stmt_id = stmt_id.replace("/", "_").replace("\\", "_")
        file_path = output_path / f"{safe_stmt_id}{file_suffix}"
        try:
            writer_func(df, str(file_path), **writer_kwargs)
            logger.info(f"Successfully exported statement '{stmt_id}' to {file_path}")
        except WriteError as e:
            logger.exception(
                f"Failed to write {file_suffix} file for statement '{stmt_id}':"
            )
            export_errors.append((stmt_id, str(e)))
        except Exception as e:
            logger.exception(
                f"Unexpected error exporting statement '{stmt_id}' to {file_suffix}."
            )
            export_errors.append((stmt_id, f"Unexpected export error: {e!s}"))

    if export_errors:
        error_summary = "; ".join([f"{sid}: {err}" for sid, err in export_errors])
        raise WriteError(
            f"Encountered {len(export_errors)} errors during {file_suffix} export: {error_summary}"
        )


def export_statements_to_excel(
    graph: Graph,
    config_path_or_dir: str,
    output_dir: str,
    format_kwargs: Optional[dict[str, Any]] = None,
    writer_kwargs: Optional[dict[str, Any]] = None,
) -> None:
    """Generate statement DataFrames and export them to individual Excel files.

    Loads configurations, builds statements, populates the graph (if necessary),
    generates DataFrames for each statement, and saves each DataFrame to a
    separate `.xlsx` file in the specified `output_dir`.

    Args:
        graph: The core.graph.Graph instance containing necessary data.
        config_path_or_dir: Path to a single statement config file or a
            directory containing multiple config files.
        output_dir: The directory where the resulting Excel files will be saved.
            File names will be derived from the statement IDs (e.g.,
            'income_statement.xlsx').
        format_kwargs: Optional dictionary of arguments passed to
            `StatementFormatter.generate_dataframe` when creating the DataFrames.
        writer_kwargs: Optional dictionary of arguments passed to the underlying
            Excel writer (`write_statement_to_excel`), such as
            `sheet_name` or engine options.

    Raises:
        ConfigurationError: If loading or validating configurations fails.
        StatementError: If processing statements fails critically.
        FileNotFoundError: If `config_path_or_dir` is not found.
        WriteError: If writing any of the Excel files fails.
        FinancialModelError: Potentially other errors from graph operations.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> # Assume 'my_graph' is a pre-populated Graph instance
        >>> # Assume configs exist in './statement_configs/'
        >>> try:
        ...     export_statements_to_excel(
        ...         graph=my_graph,
        ...         config_path_or_dir='./statement_configs/',
        ...         output_dir='./output_excel/',
        ...         writer_kwargs={'freeze_panes': (1, 1)} # Freeze header row/col
        ...     )
        ...     # Use logger.info
        ...     logger.info("Statements exported to ./output_excel/")
        ... except (FileNotFoundError, ConfigurationError, StatementError, WriteError) as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Export failed: {e}")
    """
    export_statements(
        graph=graph,
        config_path_or_dir=config_path_or_dir,
        output_dir=output_dir,
        format_kwargs=format_kwargs or {},
        writer_func=write_statement_to_excel,
        writer_kwargs=writer_kwargs or {},
        file_suffix=".xlsx",
    )


def export_statements_to_json(
    graph: Graph,
    config_path_or_dir: str,
    output_dir: str,
    format_kwargs: Optional[dict[str, Any]] = None,
    writer_kwargs: Optional[dict[str, Any]] = None,
) -> None:
    """Generate statement DataFrames and export them to individual JSON files.

    Loads configurations, builds statements, populates the graph (if necessary),
    generates DataFrames for each statement, and saves each DataFrame to a
    separate `.json` file in the specified `output_dir`.

    Args:
        graph: The core.graph.Graph instance containing necessary data.
        config_path_or_dir: Path to a single statement config file or a
            directory containing multiple config files.
        output_dir: The directory where the resulting JSON files will be saved.
            File names will be derived from the statement IDs (e.g.,
            'balance_sheet.json').
        format_kwargs: Optional dictionary of arguments passed to
            `StatementFormatter.generate_dataframe` when creating the DataFrames.
        writer_kwargs: Optional dictionary of arguments passed to the underlying
            JSON writer (`write_statement_to_json`). Common options
            include `orient` (e.g., 'records', 'columns', 'split') and `indent`.
            Defaults to 'records' orient and indent=2 if not provided.

    Raises:
        ConfigurationError: If loading or validating configurations fails.
        StatementError: If processing statements fails critically.
        FileNotFoundError: If `config_path_or_dir` is not found.
        WriteError: If writing any of the JSON files fails.
        FinancialModelError: Potentially other errors from graph operations.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> # Assume 'my_graph' is a pre-populated Graph instance
        >>> # Assume configs exist in './statement_configs/'
        >>> try:
        ...     export_statements_to_json(
        ...         graph=my_graph,
        ...         config_path_or_dir='./statement_configs/',
        ...         output_dir='./output_json/',
        ...         writer_kwargs={'orient': 'split', 'indent': 4}
        ...     )
        ...     # Use logger.info
        ...     logger.info("Statements exported to ./output_json/")
        ... except (FileNotFoundError, ConfigurationError, StatementError, WriteError) as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Export failed: {e}")
    """
    final_writer_kwargs = writer_kwargs or {}
    # Set JSON specific defaults if not provided
    final_writer_kwargs.setdefault("orient", "records")
    final_writer_kwargs.setdefault("indent", 2)

    export_statements(
        graph=graph,
        config_path_or_dir=config_path_or_dir,
        output_dir=output_dir,
        format_kwargs=format_kwargs or {},
        writer_func=write_statement_to_json,
        writer_kwargs=final_writer_kwargs,
        file_suffix=".json",
    )



================================================================================
File: fin_statement_model/statements/orchestration/loader.py
================================================================================

"""Statement loading and building functionality.

This module handles validation, building, and registration of statement structures
from in-memory configuration dictionaries.
"""

import logging
from typing import Any, Optional
from pathlib import Path

from fin_statement_model.core.errors import ConfigurationError, StatementError
from fin_statement_model.statements.structure.builder import StatementStructureBuilder
from fin_statement_model.statements.configs.validator import StatementConfig
from fin_statement_model.statements.registry import StatementRegistry
from fin_statement_model.io.validation import UnifiedNodeValidator

logger = logging.getLogger(__name__)

__all__ = ["load_build_register_statements"]


def load_build_register_statements(
    raw_configs: dict[str, dict[str, Any]] | str | Path,
    registry: StatementRegistry,
    builder: Optional[StatementStructureBuilder] = None,
    enable_node_validation: bool = False,
    node_validation_strict: bool = False,
    node_validator: Optional[UnifiedNodeValidator] = None,
) -> list[str]:
    """Load, validate, build, and register statement structures from config dicts.

    Args:
        raw_configs: Mapping of statement IDs to configuration dicts, or a file path or directory path.
        registry: StatementRegistry instance to register loaded statements.
        builder: Optional StatementStructureBuilder instance.
        enable_node_validation: If True, validate node IDs during config and build.
        node_validation_strict: If True, treat validation failures as errors.
        node_validator: Optional pre-configured UnifiedNodeValidator.

    Returns:
        List of statement IDs successfully loaded and registered.

    Raises:
        ConfigurationError: If config validation fails.
        StatementError: If registration fails.
    """
    loaded_statement_ids: list[str] = []
    errors: list[tuple[str, str]] = []

    # NEW: Allow `raw_configs` to be a file path or directory path for backward compatibility
    #       Convert such inputs into the expected {statement_id: config_dict} mapping.
    if isinstance(raw_configs, (str, Path)):

        def _parse_cfg_file(path: Path) -> dict[str, Any] | None:
            """Parse a YAML or JSON configuration file into a dict mapping."""
            try:
                if path.suffix.lower() == ".json":
                    import json

                    data = json.loads(path.read_text(encoding="utf-8"))
                else:
                    import yaml

                    data = yaml.safe_load(path.read_text(encoding="utf-8"))
                if not isinstance(data, dict):
                    logger.warning(
                        "Skipping config '%s' – root element is not a mapping.", path
                    )
                    return None
                return data
            except Exception as exc:  # pragma: no cover
                logger.exception("Failed to parse statement config '%s': %s", path, exc)
                return None

        path_obj = Path(raw_configs)
        if not path_obj.exists():
            raise FileNotFoundError(
                f"Statement config path '{path_obj}' does not exist."
            )

        configs_dict: dict[str, dict[str, Any]] = {}
        if path_obj.is_dir():
            for file in path_obj.iterdir():
                if file.is_file() and file.suffix.lower() in {".yaml", ".yml", ".json"}:
                    parsed = _parse_cfg_file(file)
                    if parsed:
                        stmt_id = str(parsed.get("id", file.stem))
                        configs_dict[stmt_id] = parsed
        else:
            parsed = _parse_cfg_file(path_obj)
            if parsed:
                stmt_id = str(parsed.get("id", path_obj.stem))
                configs_dict[stmt_id] = parsed

        raw_configs = configs_dict  # type: ignore[assignment]

    if builder is None:
        builder = StatementStructureBuilder(
            enable_node_validation=enable_node_validation,
            node_validation_strict=node_validation_strict,
            node_validator=node_validator,
        )

    if not raw_configs:
        logger.warning("No statement configurations provided.")
        return []

    for stmt_id, raw_data in raw_configs.items():
        try:
            config = StatementConfig(
                config_data=raw_data,
                enable_node_validation=enable_node_validation,
                node_validation_strict=node_validation_strict,
                node_validator=node_validator,
            )
            validation_errors = config.validate_config()
            if validation_errors:
                raise ConfigurationError(
                    f"Invalid configuration for statement '{stmt_id}'",
                    errors=validation_errors,
                )
            statement = builder.build(config)
            registry.register(statement)
            loaded_statement_ids.append(statement.id)
        except (ConfigurationError, StatementError, ValueError) as e:
            logger.exception(f"Failed to process/register statement '{stmt_id}'.")
            errors.append((stmt_id, str(e)))
        except Exception as e:
            logger.exception(f"Unexpected error processing statement '{stmt_id}'.")
            errors.append((stmt_id, f"Unexpected error: {e!s}"))

    if errors:
        error_details = "; ".join(f"{sid}: {msg}" for sid, msg in errors)
        logger.warning(
            f"Encountered {len(errors)} errors during statement loading/building: {error_details}"
        )

    return loaded_statement_ids



================================================================================
File: fin_statement_model/statements/orchestration/orchestrator.py
================================================================================

"""Main orchestration for statement processing.

This module coordinates the workflow of building statements from in-memory
configurations, populating graphs, and generating DataFrames.
"""

import logging
from typing import Any, Optional

import pandas as pd

from fin_statement_model.core.errors import StatementError
from fin_statement_model.core.graph import Graph

from fin_statement_model.statements.structure.builder import StatementStructureBuilder
from fin_statement_model.statements.formatting.formatter import StatementFormatter
from fin_statement_model.statements.orchestration.loader import (
    load_build_register_statements,
)
from fin_statement_model.statements.population.populator import (
    populate_graph_from_statement,
)
from fin_statement_model.statements.registry import StatementRegistry

logger = logging.getLogger(__name__)

__all__ = ["create_statement_dataframe", "populate_graph"]


def populate_graph(registry: StatementRegistry, graph: Graph) -> list[tuple[str, str]]:
    """Populate the graph with nodes based on registered statements."""
    all_errors: list[tuple[str, str]] = []
    statements = registry.get_all_statements()
    if not statements:
        logger.warning("No statements registered to populate the graph.")
        return []

    for statement in statements:
        errors = populate_graph_from_statement(statement, graph)
        for item_id, msg in errors:
            all_errors.append((item_id, msg))

    if all_errors:
        logger.warning(f"Encountered {len(all_errors)} errors during graph population.")
    return all_errors


def create_statement_dataframe(
    graph: Graph,
    raw_configs: dict[str, dict[str, Any]],
    format_kwargs: Optional[dict[str, Any]] = None,
    enable_node_validation: Optional[bool] = None,
    node_validation_strict: Optional[bool] = None,
) -> dict[str, pd.DataFrame]:
    """Build statements from configurations, populate graph, and format DataFrames.

    Args:
        graph: Graph instance to populate.
        raw_configs: Mapping of statement IDs to configuration dicts.
        format_kwargs: Optional kwargs for formatter.
        enable_node_validation: If True, validate node IDs.
        node_validation_strict: If True, treat validation failures as errors.

    Returns:
        Mapping of statement IDs to pandas DataFrames.

    Raises:
        StatementError: If loading or formatting fails.
    """
    registry = StatementRegistry()
    enable_node_validation = (
        enable_node_validation if enable_node_validation is not None else False
    )
    node_validation_strict = (
        node_validation_strict if node_validation_strict is not None else False
    )
    builder = StatementStructureBuilder(
        enable_node_validation=enable_node_validation,
        node_validation_strict=node_validation_strict,
    )
    format_kwargs = format_kwargs or {}

    # Step 1: Load, build, register
    loaded_ids = load_build_register_statements(
        raw_configs,
        registry,
        builder,
        enable_node_validation=enable_node_validation,
        node_validation_strict=node_validation_strict,
    )
    if not loaded_ids:
        raise StatementError("No valid statements could be loaded.")

    # Step 2: Populate graph
    populate_graph(registry, graph)

    # Step 3: Format results
    results: dict[str, pd.DataFrame] = {}
    for stmt_id in loaded_ids:
        statement = registry.get(stmt_id)
        if statement is None:
            logger.error(f"Statement '{stmt_id}' not found in registry.")
            raise StatementError(f"Statement '{stmt_id}' not found in registry.")
        df = StatementFormatter(statement).generate_dataframe(graph, **format_kwargs)
        results[stmt_id] = df

    return results



================================================================================
File: fin_statement_model/statements/population/__init__.py
================================================================================

"""Graph population functionality for financial statements.

This package handles the conversion of statement structures into graph nodes:
- ID resolution between statement items and graph nodes
- Processing different item types (metrics, calculations, subtotals)
- Managing dependencies and retry logic
"""

from .id_resolver import IDResolver
from .item_processors import (
    CalculatedItemProcessor,
    ItemProcessor,
    ItemProcessorManager,
    MetricItemProcessor,
    ProcessorResult,
    SubtotalItemProcessor,
)
from .populator import populate_graph_from_statement

__all__ = [
    "CalculatedItemProcessor",
    # ID Resolution
    "IDResolver",
    # Item Processors
    "ItemProcessor",
    "ItemProcessorManager",
    "MetricItemProcessor",
    "ProcessorResult",
    "SubtotalItemProcessor",
    # Main Function
    "populate_graph_from_statement",
]



================================================================================
File: fin_statement_model/statements/population/id_resolver.py
================================================================================

"""ID resolution for statement items to graph nodes.

This module provides centralized logic for resolving statement item IDs to their
corresponding graph node IDs, handling the complexity of different item types
having different ID mapping rules, including standard node references.
"""

import logging
from typing import Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import standard_node_registry
from fin_statement_model.statements.structure import (
    StatementStructure,
    LineItem,
)

logger = logging.getLogger(__name__)

__all__ = ["IDResolver"]


class IDResolver:
    """Centralizes ID resolution from statement items to graph nodes.

    This class handles the complexity of mapping statement item IDs to graph
    node IDs, accounting for the fact that:
    - LineItems can have either a direct node_id property OR a standard_node_ref
      that gets resolved through the standard_node_registry
    - Other items (CalculatedLineItem, SubtotalLineItem, MetricLineItem) use
      their ID directly as the node ID
    - Some nodes may exist directly in the graph without being statement items

    The resolver caches mappings for performance and provides both single and
    batch resolution methods. Standard node references are resolved at cache
    build time for optimal performance.
    """

    def __init__(self, statement: StatementStructure):
        """Initialize the resolver with a statement structure.

        Args:
            statement: The statement structure containing items to resolve.
        """
        self.statement = statement
        self._item_to_node_cache: dict[str, str] = {}
        self._node_to_items_cache: dict[str, list[str]] = {}
        self._build_cache()

    def _build_cache(self) -> None:
        """Pre-build ID mappings for all items in the statement."""
        logger.debug(f"Building ID cache for statement '{self.statement.id}'")

        for item in self.statement.get_all_items():
            if isinstance(item, LineItem):
                # Get the resolved node ID (handles both direct node_id and standard_node_ref)
                resolved_node_id = item.get_resolved_node_id(standard_node_registry)
                if resolved_node_id:
                    # LineItems map their ID to their resolved node_id
                    self._item_to_node_cache[item.id] = resolved_node_id
                    self._node_to_items_cache.setdefault(resolved_node_id, []).append(
                        item.id
                    )

                    # Log if using standard node reference for debugging
                    if item.standard_node_ref:
                        logger.debug(
                            f"Resolved standard node reference '{item.standard_node_ref}' "
                            f"to '{resolved_node_id}' for item '{item.id}'"
                        )
                else:
                    logger.warning(
                        f"Could not resolve node reference for LineItem '{item.id}'. "
                        f"node_id: {item.node_id}, standard_node_ref: {item.standard_node_ref}"
                    )
            else:
                # Other items use their ID directly as the node ID
                self._item_to_node_cache[item.id] = item.id
                self._node_to_items_cache.setdefault(item.id, []).append(item.id)

        logger.debug(
            f"ID cache built: {len(self._item_to_node_cache)} item->node mappings, "
            f"{len(self._node_to_items_cache)} unique nodes"
        )

    def resolve(self, item_id: str, graph: Optional[Graph] = None) -> Optional[str]:
        """Resolve a statement item ID to its graph node ID.

        Resolution process:
        1. Check the pre-built cache for the item ID
        2. If not found and a graph is provided, check if the ID exists
           directly as a node in the graph
        3. Return None if not found anywhere

        Args:
            item_id: The statement item ID to resolve.
            graph: Optional graph to check for direct node existence.

        Returns:
            The resolved graph node ID if found, None otherwise.
        """
        # Rebuild cache if it's empty (e.g., after invalidation)
        if not self._item_to_node_cache:
            self._build_cache()

        # Check cache first
        if item_id in self._item_to_node_cache:
            return self._item_to_node_cache[item_id]

        # Check if it exists directly in graph
        if graph and graph.has_node(item_id):
            # Cache this discovery for future lookups
            self._item_to_node_cache[item_id] = item_id
            self._node_to_items_cache.setdefault(item_id, []).append(item_id)
            return item_id

        return None

    def resolve_multiple(
        self, item_ids: list[str], graph: Optional[Graph] = None
    ) -> dict[str, Optional[str]]:
        """Resolve multiple item IDs at once.

        Args:
            item_ids: List of statement item IDs to resolve.
            graph: Optional graph to check for direct node existence.

        Returns:
            Dictionary mapping each item ID to its resolved node ID (or None).
        """
        return {item_id: self.resolve(item_id, graph) for item_id in item_ids}

    def get_items_for_node(self, node_id: str) -> list[str]:
        """Get all statement item IDs that map to a given node ID.

        This reverse lookup can be useful for debugging and understanding
        which statement items contribute to a particular graph node.

        Args:
            node_id: The graph node ID to look up.

        Returns:
            List of statement item IDs that map to this node (may be empty).
        """
        # Rebuild cache if it's empty
        if not self._node_to_items_cache:
            self._build_cache()
        return self._node_to_items_cache.get(node_id, [])

    def get_all_mappings(self) -> dict[str, str]:
        """Get all item ID to node ID mappings.

        Returns:
            Dictionary of all cached mappings.
        """
        # Rebuild cache if it's empty
        if not self._item_to_node_cache:
            self._build_cache()
        return self._item_to_node_cache.copy()

    def invalidate_cache(self) -> None:
        """Clear the cache, forcing a rebuild on next resolution.

        This should be called if the statement structure changes after
        the resolver was created.
        """
        self._item_to_node_cache.clear()
        self._node_to_items_cache.clear()
        logger.debug(f"ID cache invalidated for statement '{self.statement.id}'")

    def refresh_cache(self) -> None:
        """Rebuild the cache from the current statement structure."""
        self.invalidate_cache()
        self._build_cache()



================================================================================
File: fin_statement_model/statements/population/item_processors.py
================================================================================

"""Item processors for converting statement items into graph nodes.

This module provides a processor hierarchy that handles the conversion of different
statement item types (MetricLineItem, CalculatedLineItem, SubtotalLineItem) into
graph nodes. Each processor encapsulates the logic for its specific item type,
reducing complexity and improving testability.
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import (
    NodeError,
    CircularDependencyError,
    CalculationError,
    ConfigurationError,
    MetricError,
)
from fin_statement_model.core.metrics import metric_registry
from fin_statement_model.statements.structure import (
    StatementStructure,
    StatementItem,
    MetricLineItem,
    CalculatedLineItem,
    SubtotalLineItem,
)
from fin_statement_model.statements.population.id_resolver import IDResolver
from fin_statement_model.statements.utilities.result_types import (
    Result,
    Success,
    Failure,
    ErrorDetail,
    ErrorSeverity,
)

logger = logging.getLogger(__name__)

__all__ = [
    "CalculatedItemProcessor",
    "ItemProcessor",
    "ItemProcessorManager",
    "MetricItemProcessor",
    "ProcessorResult",
    "SubtotalItemProcessor",
]


@dataclass
class ProcessorResult:
    """Result of processing a statement item.

    Attributes:
        success: Whether the processing was successful.
        node_added: Whether a new node was added to the graph.
        error_message: Error message if processing failed.
        missing_inputs: List of missing input details (item_id, resolved_node_id).
    """

    success: bool
    node_added: bool = False
    error_message: Optional[str] = None
    missing_inputs: Optional[list[tuple[str, Optional[str]]]] = None

    def to_result(self) -> Result[bool]:
        """Convert to the new Result type."""
        if self.success:
            return Success(value=self.node_added)

        errors = []
        if self.error_message:
            errors.append(
                ErrorDetail(
                    code="processing_error",
                    message=self.error_message,
                    severity=ErrorSeverity.ERROR,
                )
            )

        if self.missing_inputs:
            for item_id, node_id in self.missing_inputs:
                msg = (
                    f"Missing input: item '{item_id}' needs node '{node_id}'"
                    if node_id
                    else f"Missing input: item '{item_id}' not found/mappable"
                )
                errors.append(
                    ErrorDetail(
                        code="missing_input",
                        message=msg,
                        context=f"item_id={item_id}, node_id={node_id}",
                        severity=ErrorSeverity.ERROR,
                    )
                )

        return Failure(errors=errors)


class ItemProcessor(ABC):
    """Abstract base class for processing statement items into graph nodes.

    This base class provides common functionality for resolving input IDs
    and handling missing inputs across different item types.
    """

    def __init__(
        self, id_resolver: IDResolver, graph: Graph, statement: StatementStructure
    ):
        """Initialize the processor.

        Args:
            id_resolver: ID resolver for mapping statement IDs to graph node IDs.
            graph: The graph to add nodes to.
            statement: The statement structure being processed.
        """
        self.id_resolver = id_resolver
        self.graph = graph
        self.statement = statement

    @abstractmethod
    def can_process(self, item: StatementItem) -> bool:
        """Check if this processor can handle the given item type.

        Args:
            item: The statement item to check.

        Returns:
            True if this processor can handle the item type.
        """

    @abstractmethod
    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process the item and add it to the graph if needed.

        Args:
            item: The statement item to process.
            is_retry: Whether this is a retry attempt (affects error logging).

        Returns:
            ProcessorResult indicating success/failure and details.
        """

    def resolve_inputs(
        self, input_ids: list[str]
    ) -> tuple[list[str], list[tuple[str, Optional[str]]]]:
        """Resolve input IDs to graph node IDs.

        Args:
            input_ids: List of statement item IDs to resolve.

        Returns:
            Tuple of (resolved_node_ids, missing_details).
            missing_details contains tuples of (item_id, resolved_node_id_or_none).
        """
        resolved = []
        missing = []

        for input_id in input_ids:
            node_id = self.id_resolver.resolve(input_id, self.graph)
            if node_id and self.graph.has_node(node_id):
                resolved.append(node_id)
            else:
                missing.append((input_id, node_id))

        return resolved, missing

    def _handle_missing_inputs(
        self,
        item: StatementItem,
        missing: list[tuple[str, Optional[str]]],
        is_retry: bool,
    ) -> ProcessorResult:
        """Handle missing input nodes consistently across processors.

        Args:
            item: The item being processed.
            missing: List of missing input details.
            is_retry: Whether this is a retry attempt.

        Returns:
            ProcessorResult with appropriate error details.
        """
        missing_summary = [
            (
                f"item '{i_id}' needs node '{n_id}'"
                if n_id
                else f"item '{i_id}' not found/mappable"
            )
            for i_id, n_id in missing
        ]

        if is_retry:
            logger.error(
                f"Retry failed for {type(item).__name__} '{item.id}' in statement '{self.statement.id}': "
                f"missing required inputs: {'; '.join(missing_summary)}"
            )
            return ProcessorResult(
                success=False,
                error_message=f"Missing inputs on retry: {missing_summary}",
                missing_inputs=missing,
            )
        else:
            # Don't log on first attempt - allows dependency resolution
            return ProcessorResult(success=False, missing_inputs=missing)


class MetricItemProcessor(ItemProcessor):
    """Processor for MetricLineItem objects.

    Handles the creation of metric-based calculation nodes by:
    1. Looking up the metric in the registry
    2. Validating input mappings
    3. Resolving input IDs to graph nodes
    4. Adding the metric node to the graph
    """

    def can_process(self, item: StatementItem) -> bool:
        """Check if item is a MetricLineItem."""
        return isinstance(item, MetricLineItem)

    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process a MetricLineItem and add it to the graph."""
        # Early validation
        if not isinstance(item, MetricLineItem):
            return ProcessorResult(success=False, error_message="Invalid item type")

        # Check if node already exists
        if self.graph.has_node(item.id):
            return ProcessorResult(success=True, node_added=False)

        # Initialize result variables
        error_message = None
        node_added = False

        # Get metric from registry
        try:
            metric = metric_registry.get(item.metric_id)
        except MetricError as e:
            logger.exception(
                f"Cannot populate item '{item.id}': Metric '{item.metric_id}' not found in registry"
            )
            error_message = f"Metric '{item.metric_id}' not found: {e}"

        # Validate input mappings if no error yet
        if not error_message:
            error_message = self._validate_metric_inputs(metric, item)

        # Resolve metric inputs if no error yet
        if not error_message:
            resolved_map, missing = self._resolve_metric_inputs(metric, item)
            if missing:
                return self._handle_missing_inputs(item, missing, is_retry)

            # Add to graph
            try:
                self.graph.add_metric(
                    metric_name=item.metric_id,
                    node_name=item.id,
                    input_node_map=resolved_map,
                )
                node_added = True
            except Exception as e:
                logger.exception(f"Failed to add metric node '{item.id}'")
                error_message = f"Failed to add metric node: {e}"

        # Single exit point
        if error_message:
            return ProcessorResult(success=False, error_message=error_message)
        return ProcessorResult(success=True, node_added=node_added)

    def _validate_metric_inputs(
        self, metric: Any, item: MetricLineItem
    ) -> Optional[str]:
        """Validate that the item provides all required metric inputs."""
        provided_inputs = set(item.inputs.keys())
        required_inputs = set(metric.inputs)

        if provided_inputs != required_inputs:
            missing_req = required_inputs - provided_inputs
            extra_prov = provided_inputs - required_inputs
            error_msg = f"Input mapping mismatch for metric '{item.metric_id}' in item '{item.id}'."

            if missing_req:
                error_msg += f" Missing required metric inputs: {missing_req}."
            if extra_prov:
                error_msg += f" Unexpected inputs provided: {extra_prov}."

            logger.error(error_msg)
            return error_msg

        return None

    def _resolve_metric_inputs(
        self, metric: Any, item: MetricLineItem
    ) -> tuple[dict[str, str], list[tuple[str, Optional[str]]]]:
        """Resolve metric input mappings to graph node IDs."""
        resolved_map = {}
        missing = []

        for metric_input_name in metric.inputs:
            input_item_id = item.inputs[metric_input_name]
            node_id = self.id_resolver.resolve(input_item_id, self.graph)

            if node_id and self.graph.has_node(node_id):
                resolved_map[metric_input_name] = node_id
            else:
                missing.append((input_item_id, node_id))

        return resolved_map, missing


class CalculatedItemProcessor(ItemProcessor):
    """Processor for CalculatedLineItem objects.

    Handles the creation of calculation nodes with specific operations by:
    1. Resolving input IDs to graph nodes
    2. Getting sign conventions from input items
    3. Creating the calculation node with proper sign handling
    """

    def can_process(self, item: StatementItem) -> bool:
        """Check if item is a CalculatedLineItem."""
        return isinstance(item, CalculatedLineItem)

    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process a CalculatedLineItem and add it to the graph."""
        if not isinstance(item, CalculatedLineItem):
            return ProcessorResult(success=False, error_message="Invalid item type")

        # Check if node already exists
        if self.graph.has_node(item.id):
            return ProcessorResult(success=True, node_added=False)

        # Resolve inputs with sign conventions
        resolved_inputs, missing = self._resolve_inputs_with_sign_conventions(item)
        if missing:
            return self._handle_missing_inputs(item, missing, is_retry)

        # Add calculation node
        error_message = None
        try:
            self.graph.add_calculation(
                name=item.id,
                input_names=resolved_inputs,
                operation_type=item.calculation_type,
                **item.parameters,
            )
        except (
            NodeError,
            CircularDependencyError,
            CalculationError,
            ConfigurationError,
        ) as e:
            error_msg = f"Failed to add calculation node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = str(e)
        except Exception as e:
            error_msg = f"Unexpected error adding calculation node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = f"Unexpected error: {e}"

        if error_message:
            return ProcessorResult(success=False, error_message=error_message)
        return ProcessorResult(success=True, node_added=True)

    def _resolve_inputs_with_sign_conventions(
        self, item: CalculatedLineItem
    ) -> tuple[list[str], list[tuple[str, Optional[str]]]]:
        """Resolve input IDs to graph node IDs, applying sign conventions.

        Creates intermediate signed nodes for inputs that have sign_convention = -1.

        Args:
            item: The CalculatedLineItem being processed.

        Returns:
            Tuple of (resolved_node_ids, missing_details).
        """
        resolved = []
        missing = []

        for input_id in item.input_ids:
            # Get the input item to check its sign convention
            input_item = self.statement.find_item_by_id(input_id)
            if not input_item:
                # Try to resolve as a node ID
                node_id = self.id_resolver.resolve(input_id, self.graph)
                if node_id and self.graph.has_node(node_id):
                    resolved.append(node_id)
                else:
                    missing.append((input_id, node_id))
                continue

            # Resolve the node ID for this input item
            node_id = self.id_resolver.resolve(input_id, self.graph)
            if not node_id or not self.graph.has_node(node_id):
                missing.append((input_id, node_id))
                continue

            # Check if we need to apply sign convention
            sign_convention = getattr(input_item, "sign_convention", 1)
            if sign_convention == -1:
                # Create a signed version of this node using formula calculation
                signed_node_id = f"{node_id}_signed"
                if not self.graph.has_node(signed_node_id):
                    try:
                        # Create a formula node that multiplies by -1
                        self.graph.add_calculation(
                            name=signed_node_id,
                            input_names=[node_id],
                            operation_type="formula",
                            formula="-input_0",
                            formula_variable_names=["input_0"],
                        )
                        logger.debug(
                            f"Created signed node '{signed_node_id}' for input '{input_id}' with sign_convention=-1"
                        )
                    except Exception:
                        logger.exception(
                            f"Failed to create signed node for '{input_id}'"
                        )
                        missing.append((input_id, node_id))
                        continue
                resolved.append(signed_node_id)
            else:
                # Use the node directly (sign_convention = 1 or default)
                resolved.append(node_id)

        return resolved, missing


class SubtotalItemProcessor(ItemProcessor):
    """Processor for SubtotalLineItem objects.

    Handles the creation of subtotal (addition) nodes by:
    1. Resolving input IDs to graph nodes
    2. Adding an addition calculation node
    """

    def can_process(self, item: StatementItem) -> bool:
        """Check if item is a SubtotalLineItem."""
        return isinstance(item, SubtotalLineItem)

    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process a SubtotalLineItem and add it to the graph."""
        if not isinstance(item, SubtotalLineItem):
            return ProcessorResult(success=False, error_message="Invalid item type")

        # Check if node already exists
        if self.graph.has_node(item.id):
            return ProcessorResult(success=True, node_added=False)

        # Handle empty subtotals
        if not item.item_ids:
            logger.debug(f"Subtotal item '{item.id}' has no input items")
            return ProcessorResult(success=True, node_added=False)

        # Resolve inputs
        resolved, missing = self.resolve_inputs(item.item_ids)
        if missing:
            return self._handle_missing_inputs(item, missing, is_retry)

        # Add subtotal as addition calculation
        error_message = None
        try:
            self.graph.add_calculation(
                name=item.id, input_names=resolved, operation_type="addition"
            )
        except (
            NodeError,
            CircularDependencyError,
            CalculationError,
            ConfigurationError,
        ) as e:
            error_msg = f"Failed to add subtotal node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = str(e)
        except Exception as e:
            error_msg = f"Unexpected error adding subtotal node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = f"Unexpected error: {e}"

        if error_message:
            return ProcessorResult(success=False, error_message=error_message)
        return ProcessorResult(success=True, node_added=True)


class ItemProcessorManager:
    """Manages the collection of item processors.

    This class coordinates the processing of different statement item types
    by delegating to the appropriate processor based on the item type.
    """

    def __init__(
        self, id_resolver: IDResolver, graph: Graph, statement: StatementStructure
    ):
        """Initialize the processor manager with all available processors.

        Args:
            id_resolver: ID resolver for mapping statement IDs to graph node IDs.
            graph: The graph to add nodes to.
            statement: The statement structure being processed.
        """
        self.processors = [
            MetricItemProcessor(id_resolver, graph, statement),
            CalculatedItemProcessor(id_resolver, graph, statement),
            SubtotalItemProcessor(id_resolver, graph, statement),
        ]

    def process_item(
        self, item: StatementItem, is_retry: bool = False
    ) -> ProcessorResult:
        """Process a statement item using the appropriate processor.

        Args:
            item: The statement item to process.
            is_retry: Whether this is a retry attempt.

        Returns:
            ProcessorResult from the appropriate processor, or a success result
            if no processor handles the item type (e.g., for LineItem).
        """
        for processor in self.processors:
            if processor.can_process(item):
                return processor.process(item, is_retry)

        # No processor found - this is OK for non-calculation items like LineItem
        logger.debug(
            f"No processor for item type {type(item).__name__} with ID '{item.id}'. "
            "This is expected for non-calculation items."
        )
        return ProcessorResult(success=True, node_added=False)



================================================================================
File: fin_statement_model/statements/population/populator.py
================================================================================

"""Populates a `fin_statement_model.core.graph.Graph` with calculation nodes.

This module provides the function `populate_graph_from_statement`, which is
responsible for translating the calculation logic defined within a
`StatementStructure` (specifically `CalculatedLineItem`, `SubtotalLineItem`,
and `MetricLineItem`) into actual calculation nodes within a `Graph` instance.

Key Concepts:
- **ID Resolution**: Statement configurations use item IDs that must be resolved
  to graph node IDs. This is handled by the `IDResolver` class.
- **Dependency Ordering**: Items may depend on other items. The populator handles
  this by retrying failed items after successful ones, allowing dependencies to
  be satisfied.
- **Idempotency**: If a node already exists in the graph, it will be skipped.
"""

import logging

from fin_statement_model.core.graph import Graph
from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.statements.population.id_resolver import IDResolver
from fin_statement_model.statements.population.item_processors import (
    ItemProcessorManager,
)

logger = logging.getLogger(__name__)

__all__ = ["populate_graph_from_statement"]


def populate_graph_from_statement(
    statement: StatementStructure, graph: Graph
) -> list[tuple[str, str]]:
    """Add calculation nodes defined in a StatementStructure to a Graph.

    This function bridges the gap between static statement definitions and the
    dynamic calculation graph. It processes three types of items:

    1. **CalculatedLineItem**: Creates calculation nodes with specified operations
    2. **SubtotalLineItem**: Creates addition nodes that sum multiple items
    3. **MetricLineItem**: Creates metric-based calculation nodes

    ID Resolution Logic:
    - Input IDs in statement configurations are resolved to graph node IDs using
      the `IDResolver` class
    - This handles the mapping between statement item IDs and actual graph nodes
    - Resolution accounts for LineItem.node_id vs other items using their ID directly

    Dependency Handling:
    - Items may depend on other items that haven't been created yet
    - The function uses a retry mechanism: failed items are retried after
      successful ones, allowing dependencies to be resolved
    - Circular dependencies are detected and reported as errors

    Idempotency:
    - If a node already exists in the graph, it will be skipped
    - This allows the function to be called multiple times safely

    Args:
        statement: The `StatementStructure` object containing the definitions
            of calculated items, subtotals, and metrics.
        graph: The `core.graph.Graph` instance that will be populated with
            the calculation nodes.

    Returns:
        A list of tuples, where each tuple contains `(item_id, error_message)`
        for any items that could not be successfully added to the graph. An
        empty list indicates that all applicable items were added (or already
        existed) without critical errors.

    Raises:
        TypeError: If `statement` is not a `StatementStructure` or `graph` is
            not a `Graph` instance.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> from fin_statement_model.statements.structure import StatementStructure
        >>>
        >>> # Create graph with data nodes
        >>> graph = Graph()
        >>> graph.add_financial_statement_item('revenue_node', {'2023': 1000})
        >>> graph.add_financial_statement_item('cogs_node', {'2023': 600})
        >>>
        >>> # Create statement with calculations
        >>> statement = StatementStructure(id="IS", name="Income Statement")
        >>> # Add a LineItem that maps to 'revenue_node'
        >>> revenue_item = LineItem(id='revenue', name='Revenue', node_id='revenue_node')
        >>> # Add a CalculatedLineItem that references the LineItem
        >>> gross_profit = CalculatedLineItem(
        ...     id='gross_profit',
        ...     name='Gross Profit',
        ...     calculation_type='subtraction',
        ...     input_ids=['revenue', 'cogs']  # Uses LineItem IDs
        ... )
        >>>
        >>> errors = populate_graph_from_statement(statement, graph)
        >>> # The function will:
        >>> # 1. Resolve 'revenue' to 'revenue_node' via LineItem.node_id
        >>> # 2. Resolve 'cogs' to 'cogs_node' (if it exists in statement or graph)
        >>> # 3. Create a calculation node 'gross_profit' with the resolved inputs
    """
    # Validate inputs
    if not isinstance(statement, StatementStructure):
        raise TypeError("statement must be a StatementStructure instance")
    if not isinstance(graph, Graph):
        raise TypeError("graph must be a Graph instance")

    # Initialize components
    id_resolver = IDResolver(statement)
    processor_manager = ItemProcessorManager(id_resolver, graph, statement)

    # Get all items to process
    calculation_items = statement.get_calculation_items()
    metric_items = statement.get_metric_items()
    all_items_to_process = calculation_items + metric_items

    # Track results
    errors_encountered: list[tuple[str, str]] = []
    nodes_added_count = 0

    logger.info(
        f"Starting graph population for statement '{statement.id}'. "
        f"Processing {len(all_items_to_process)} calculation/metric items."
    )

    # Process items with retry mechanism
    items_to_process = list(all_items_to_process)
    processed_in_pass = -1  # Initialize to enter loop

    while items_to_process and processed_in_pass != 0:
        items_failed_this_pass = []
        processed_in_pass = 0

        logger.debug(f"Population loop: Processing {len(items_to_process)} items...")

        for item in items_to_process:
            # Determine if this is a retry (not the first overall pass)
            is_retry = len(items_to_process) < len(all_items_to_process)

            # Process the item
            result = processor_manager.process_item(item, is_retry)

            if result.success:
                processed_in_pass += 1
                if result.node_added:
                    nodes_added_count += 1
            else:
                items_failed_this_pass.append(item)
                # Only record errors on retry or for non-dependency errors
                if is_retry and result.error_message:
                    errors_encountered.append((item.id, result.error_message))

        # Prepare for next iteration
        items_to_process = items_failed_this_pass

        # Check for stalled progress
        if processed_in_pass == 0 and items_to_process:
            logger.warning(
                f"Population loop stalled. {len(items_to_process)} items could not be processed: "
                f"{[item.id for item in items_to_process]}"
            )
            # Add errors for items that couldn't be processed
            for item in items_to_process:
                if not any(err[0] == item.id for err in errors_encountered):
                    errors_encountered.append(
                        (
                            item.id,
                            "Failed to process due to unresolved dependencies or circular reference.",
                        )
                    )
            break

    # Log results
    if errors_encountered:
        logger.warning(
            f"Graph population for statement '{statement.id}' completed with "
            f"{len(errors_encountered)} persistent errors."
        )
    else:
        log_level = logging.INFO if nodes_added_count > 0 else logging.DEBUG
        logger.log(
            log_level,
            f"Graph population for statement '{statement.id}' completed. "
            f"Added {nodes_added_count} new nodes.",
        )

    return errors_encountered



================================================================================
File: fin_statement_model/statements/registry.py
================================================================================

"""Registry for managing loaded and validated financial statement structures.

This module provides the `StatementRegistry` class, which acts as a central
store for `StatementStructure` objects after they have been loaded from
configurations and built. It ensures uniqueness of statement IDs and provides
methods for retrieving registered statements.
"""

import logging
from typing import Optional

# Assuming StatementStructure is defined here or imported appropriately
# We might need to adjust this import based on the actual location
try:
    from .structure import StatementStructure
except ImportError:
    # Handle cases where structure might be in a different sub-package later if needed
    # For now, assume it's available via relative import
    from fin_statement_model.statements.structure import StatementStructure

from .errors import StatementError  # Assuming StatementError is in statements/errors.py

logger = logging.getLogger(__name__)

__all__ = ["StatementRegistry"]


class StatementRegistry:
    """Manages a collection of loaded financial statement structures.

    This registry holds instances of `StatementStructure`, keyed by their unique
    IDs. It prevents duplicate registrations and provides methods to access
    registered statements individually or collectively.

    Attributes:
        _statements: A dictionary mapping statement IDs (str) to their
                     corresponding `StatementStructure` objects.
    """

    def __init__(self) -> None:
        """Initialize an empty statement registry."""
        self._statements: dict[str, StatementStructure] = {}
        logger.debug("StatementRegistry initialized.")

    def register(self, statement: StatementStructure) -> None:
        """Register a statement structure with the registry.

        Ensures the provided object is a `StatementStructure` with a valid ID
        and that the ID is not already present in the registry.

        Args:
            statement: The `StatementStructure` instance to register.

        Raises:
            TypeError: If the `statement` argument is not an instance of
                `StatementStructure`.
            ValueError: If the `statement` has an invalid or empty ID.
            StatementError: If a statement with the same ID (`statement.id`) is
                already registered.
        """
        if not isinstance(statement, StatementStructure):
            raise TypeError("Only StatementStructure objects can be registered.")

        statement_id = statement.id
        if not statement_id:
            raise ValueError(
                "StatementStructure must have a valid non-empty id to be registered."
            )

        if statement_id in self._statements:
            # Policy: Raise error on conflict
            logger.error(
                f"Attempted to register duplicate statement ID: '{statement_id}'"
            )
            raise StatementError(
                message=f"Statement with ID '{statement_id}' is already registered.",
                # statement_id=statement_id # Add if StatementError accepts this arg
            )

        self._statements[statement_id] = statement
        logger.info(f"Registered statement '{statement.name}' with ID '{statement_id}'")

    def get(self, statement_id: str) -> Optional[StatementStructure]:
        """Get a registered statement by its ID.

        Returns:
            The `StatementStructure` instance associated with the given ID if
            it exists, otherwise returns `None`.

        Example:
            >>> registry = StatementRegistry()
            >>> # Assume 'income_statement' is a valid StatementStructure instance
            >>> # registry.register(income_statement)
            >>> retrieved_statement = registry.get("income_statement_id")
            >>> if retrieved_statement:
            ...     logger.info(f"Found: {retrieved_statement.name}")
            ... else:
            ...     logger.info("Statement not found.")
        """
        return self._statements.get(statement_id)

    def get_all_ids(self) -> list[str]:
        """Get the IDs of all registered statements.

        Returns:
            A list containing the unique IDs of all statements currently held
            in the registry.
        """
        return list(self._statements.keys())

    def get_all_statements(self) -> list[StatementStructure]:
        """Get all registered statement structure objects.

        Returns:
            A list containing all `StatementStructure` objects currently held
            in the registry.
        """
        return list(self._statements.values())

    def clear(self) -> None:
        """Remove all statement structures from the registry.

        Resets the registry to an empty state.
        """
        self._statements = {}
        logger.info("StatementRegistry cleared.")

    def __len__(self) -> int:
        """Return the number of registered statements."""
        return len(self._statements)

    def __contains__(self, statement_id: str) -> bool:
        """Check if a statement ID exists in the registry.

        Allows using the `in` operator with the registry.

        Args:
            statement_id: The statement ID to check for.

        Returns:
            `True` if a statement with the given ID is registered, `False` otherwise.

        Example:
            >>> registry = StatementRegistry()
            >>> # Assume 'income_statement' is registered with ID 'IS_2023'
            >>> # registry.register(income_statement)
            >>> print("IS_2023" in registry)  # Output: True
            >>> print("BS_2023" in registry)  # Output: False
        """
        return statement_id in self._statements



================================================================================
File: fin_statement_model/statements/structure/__init__.py
================================================================================

"""Statement structure package.

Re-export domain model classes from submodules.
"""

from .items import (
    StatementItem,
    StatementItemType,
    LineItem,
    CalculatedLineItem,
    MetricLineItem,
    SubtotalLineItem,
)
from .containers import Section, StatementStructure

__all__ = [
    "CalculatedLineItem",
    "LineItem",
    "MetricLineItem",
    "Section",
    "StatementItem",
    "StatementItemType",
    "StatementStructure",
    "SubtotalLineItem",
]



================================================================================
File: fin_statement_model/statements/structure/builder.py
================================================================================

"""Builds StatementStructure objects from validated StatementConfig models.

This module provides the `StatementStructureBuilder`, which translates the
deserialized and validated configuration (represented by `StatementConfig`
containing Pydantic models) into the hierarchical `StatementStructure` object
used internally for representing the layout and components of a financial
statement.
"""

import logging
from typing import Union, Optional, Any, cast

# Assuming config and structure modules are accessible
from fin_statement_model.statements.configs.validator import StatementConfig
from fin_statement_model.statements.configs.models import (
    SectionModel,
    BaseItemModel,
    LineItemModel,
    CalculatedItemModel,
    MetricItemModel,
    SubtotalModel,
    StatementModel,
    AdjustmentFilterSpec,
)
from fin_statement_model.statements.structure import (
    StatementStructure,
    Section,
    LineItem,
    MetricLineItem,
    CalculatedLineItem,
    SubtotalLineItem,
)
from fin_statement_model.statements.errors import ConfigurationError

# Import UnifiedNodeValidator for optional node validation during build
from fin_statement_model.io.validation import UnifiedNodeValidator
from fin_statement_model.config.helpers import cfg

# Import Result types for enhanced error handling
from fin_statement_model.statements.utilities.result_types import (
    ErrorCollector,
    ErrorSeverity,
)

# Import adjustment types for filter conversion
from fin_statement_model.core.adjustments.models import AdjustmentFilter, AdjustmentType

logger = logging.getLogger(__name__)

__all__ = ["StatementStructureBuilder"]


class StatementStructureBuilder:
    """Constructs a `StatementStructure` object from a validated configuration.

    Takes a `StatementConfig` instance (which should have successfully passed
    validation, populating its `.model` attribute) and recursively builds the
    corresponding `StatementStructure`, including its sections, line items,
    calculated items, subtotals, and nested sections.
    """

    def __init__(
        self,
        enable_node_validation: Optional[bool] = None,
        node_validation_strict: Optional[bool] = None,
        node_validator: Optional[UnifiedNodeValidator] = None,
    ) -> None:
        """Initialize the StatementStructureBuilder.

        Defaults for validation flags come from the global statements config, but can be overridden locally.
        Args:
            enable_node_validation: If True, validates node IDs during build.
            node_validation_strict: If True, treats validation failures as errors.
            node_validator: Optional pre-configured UnifiedNodeValidator instance.
        """
        # Pull defaults from global config if not provided
        if enable_node_validation is None:
            enable_node_validation = cfg("statements.enable_node_validation")
        if node_validation_strict is None:
            node_validation_strict = cfg("statements.node_validation_strict")
        self.enable_node_validation = enable_node_validation
        self.node_validation_strict = node_validation_strict

        # Initialize node_validator
        self.node_validator: Optional[UnifiedNodeValidator] = None
        if self.enable_node_validation:
            if node_validator is not None:
                self.node_validator = node_validator
            else:
                # Create default validator using strict flag
                self.node_validator = UnifiedNodeValidator(
                    strict_mode=self.node_validation_strict,
                    auto_standardize=True,
                    warn_on_non_standard=True,
                    enable_patterns=True,
                )
        else:
            self.node_validator = None

    def _convert_adjustment_filter(
        self, filter_input: Optional[Union[AdjustmentFilterSpec, list[str]]]
    ) -> Optional[Any]:
        """Convert configuration adjustment filter to core AdjustmentFilter or tag set.

        Args:
            filter_input: The filter specification from configuration.

        Returns:
            AdjustmentFilter instance, set of tags, or None.
        """
        # Use global default adjustment filter if none provided
        if filter_input is None:
            default_filter = cfg("statements.default_adjustment_filter")
            if default_filter is None:
                return None
            filter_input = default_filter
        # Simple list of tags - convert to set
        elif isinstance(filter_input, list):
            return set(filter_input)
        # Configuration spec to core filter conversion
        elif isinstance(filter_input, AdjustmentFilterSpec):
            # Convert AdjustmentFilterSpec to AdjustmentFilter
            kwargs: dict[str, Any] = {}

            # Convert list fields to sets
            if filter_input.include_scenarios:
                kwargs["include_scenarios"] = set(filter_input.include_scenarios)
            if filter_input.exclude_scenarios:
                kwargs["exclude_scenarios"] = set(filter_input.exclude_scenarios)
            if filter_input.include_tags:
                kwargs["include_tags"] = set(filter_input.include_tags)
            if filter_input.exclude_tags:
                kwargs["exclude_tags"] = set(filter_input.exclude_tags)
            if filter_input.require_all_tags:
                kwargs["require_all_tags"] = set(filter_input.require_all_tags)

            # Convert string types to AdjustmentType enums
            if filter_input.include_types:
                kwargs["include_types"] = cast(
                    set[AdjustmentType],
                    {
                        AdjustmentType(type_str)
                        for type_str in filter_input.include_types
                    },
                )
            if filter_input.exclude_types:
                kwargs["exclude_types"] = cast(
                    set[AdjustmentType],
                    {
                        AdjustmentType(type_str)
                        for type_str in filter_input.exclude_types
                    },
                )

            # Pass through period
            if filter_input.period:
                kwargs["period"] = filter_input.period

            # Create AdjustmentFilter from kwargs
            return AdjustmentFilter(**kwargs)

        # Unknown type - log warning and return None
        logger.warning(f"Unknown adjustment filter type: {type(filter_input)}")
        return None

    def build(self, config: StatementConfig) -> StatementStructure:
        """Build a `StatementStructure` from a validated `StatementConfig`.

        This is the main public method of the builder. It orchestrates the
        conversion process, calling internal helper methods to build sections
        and items.

        Args:
            config: A `StatementConfig` instance whose `.validate_config()`
                method has been successfully called, populating `config.model`.

        Returns:
            The fully constructed `StatementStructure` object, ready to be
            registered or used.

        Raises:
            ValueError: If the provided `config` object has not been validated
                (i.e., `config.model` is `None`).
            ConfigurationError: If an unexpected error occurs during the building
                process, potentially indicating an issue not caught by the
                initial Pydantic validation or an internal inconsistency.
        """
        if config.model is None:
            # Ensure validation has run successfully before building
            raise ValueError(
                "StatementConfig must be validated (config.model must be set) "
                "before building the structure."
            )

        # Build from the validated Pydantic model stored in config.model
        try:
            stmt_model = config.model  # Use validated model from config

            # Optional node validation during build
            if self.enable_node_validation and self.node_validator:
                error_collector = ErrorCollector()
                self._validate_structure_node_ids(stmt_model, error_collector)

                # Handle validation results
                if error_collector.has_errors() and self.node_validation_strict:
                    # Fail build on validation errors in strict mode
                    error_messages = [
                        str(error) for error in error_collector.get_errors()
                    ]
                    raise ConfigurationError(
                        message=f"Node validation failed for statement '{stmt_model.id}'",
                        errors=error_messages,
                    )
                elif error_collector.has_warnings() or error_collector.has_errors():
                    # Log warnings and non-strict errors
                    for warning in error_collector.get_warnings():
                        logger.warning(f"Build-time node validation: {warning}")
                    if not self.node_validation_strict:
                        for error in error_collector.get_errors():
                            logger.warning(f"Build-time node validation: {error}")

            statement = StatementStructure(
                id=stmt_model.id,
                name=stmt_model.name,
                description=cast(str, stmt_model.description),
                metadata=stmt_model.metadata,
                units=stmt_model.units,
                display_scale_factor=stmt_model.display_scale_factor,
            )
            for sec_model in stmt_model.sections:
                section = self._build_section_model(sec_model)
                statement.add_section(section)
            logger.info(
                f"Successfully built StatementStructure for ID '{statement.id}'"
            )
            return statement
        except Exception as e:
            # Catch potential errors during the building process itself
            logger.exception(
                f"Error building statement structure from validated model for ID '{config.model.id}'"
            )
            raise ConfigurationError(
                message=f"Failed to build statement structure from validated config: {e}",
                errors=[str(e)],
            ) from e

    def _validate_structure_node_ids(
        self, stmt_model: StatementModel, error_collector: ErrorCollector
    ) -> None:
        """Validate node IDs in the statement structure during build.

        This is a simpler validation focused on the final structure,
        complementing the config-level validation.

        Args:
            stmt_model: The StatementModel to validate.
            error_collector: ErrorCollector to accumulate validation issues.
        """
        logger.debug(f"Build-time node validation for statement '{stmt_model.id}'")

        # Validate key node references that will be used in the built structure
        collected_node_refs = set()

        # Collect all node references from the structure
        def collect_node_refs(items: list[Any]) -> None:
            for item in items:
                if isinstance(item, LineItemModel):
                    if item.node_id:
                        collected_node_refs.add(
                            (item.node_id, "line_item_node", f"item.{item.id}.node_id")
                        )
                    if item.standard_node_ref:
                        collected_node_refs.add(
                            (
                                item.standard_node_ref,
                                "standard_node",
                                f"item.{item.id}.standard_node_ref",
                            )
                        )
                elif isinstance(item, CalculatedItemModel):
                    collected_node_refs.add(
                        (item.id, "calculated_node", f"item.{item.id}.id")
                    )
                elif isinstance(item, MetricItemModel):
                    collected_node_refs.add(
                        (item.id, "metric_node", f"item.{item.id}.id")
                    )
                elif isinstance(item, SubtotalModel):
                    collected_node_refs.add(
                        (item.id, "subtotal_node", f"item.{item.id}.id")
                    )
                elif isinstance(item, SectionModel):
                    collect_node_refs(item.items)
                    collect_node_refs(item.subsections)
                    if item.subtotal:
                        collected_node_refs.add(
                            (
                                item.subtotal.id,
                                "subtotal_node",
                                f"section.{item.id}.subtotal.id",
                            )
                        )

        # Collect all node references
        for section in stmt_model.sections:
            collect_node_refs([section, *section.items, *section.subsections])

        # Validate collected references
        for node_id, node_type, context in collected_node_refs:
            self._validate_single_build_node_id(
                node_id, node_type, context, error_collector
            )

    def _validate_single_build_node_id(
        self,
        node_id: str,
        node_type: str,
        context: str,
        error_collector: ErrorCollector,
    ) -> None:
        """Validate a single node ID during build process.

        Args:
            node_id: The node ID to validate.
            node_type: Type description for error messages.
            context: Context string for error reporting.
            error_collector: ErrorCollector to accumulate validation issues.
        """
        if not self.node_validator:
            return

        try:
            validation_result = self.node_validator.validate(
                node_id,
                node_type=node_type,
                use_cache=True,
            )

            # Only report significant issues during build
            if not validation_result.is_valid:
                severity = (
                    ErrorSeverity.ERROR
                    if self.node_validation_strict
                    else ErrorSeverity.WARNING
                )
                message = f"Build validation: Invalid {node_type} '{node_id}': {validation_result.message}"

                if severity == ErrorSeverity.ERROR:
                    error_collector.add_error(
                        code="build_invalid_node_id",
                        message=message,
                        context=context,
                        source=node_id,
                    )
                else:
                    error_collector.add_warning(
                        code="build_invalid_node_id",
                        message=message,
                        context=context,
                        source=node_id,
                    )

        except Exception as e:
            logger.exception(
                f"Error during build-time validation of node ID '{node_id}'"
            )
            error_collector.add_warning(
                code="build_node_validation_error",
                message=f"Build validation error for {node_type} '{node_id}': {e}",
                context=context,
                source=node_id,
            )

    def _build_section_model(self, section_model: SectionModel) -> Section:
        """Build a `Section` object from a `SectionModel`.

        Recursively builds the items and subsections within this section.

        Args:
            section_model: The Pydantic model representing the section configuration.

        Returns:
            A `Section` instance corresponding to the model.
        """
        # Convert adjustment filter
        adjustment_filter = self._convert_adjustment_filter(
            section_model.default_adjustment_filter
        )

        section = Section(
            id=section_model.id,
            name=section_model.name,
            description=cast(str, section_model.description),
            metadata=section_model.metadata,
            default_adjustment_filter=adjustment_filter,
            display_format=section_model.display_format,
            hide_if_all_zero=section_model.hide_if_all_zero,
            css_class=section_model.css_class,
            notes_references=section_model.notes_references,
            units=section_model.units,
            display_scale_factor=section_model.display_scale_factor,
        )
        for item in section_model.items:
            section.add_item(self._build_item_model(item))
        for sub in section_model.subsections:
            # Recursively build subsections
            section.add_item(self._build_section_model(sub))
        if section_model.subtotal:
            section.subtotal = self._build_subtotal_model(section_model.subtotal)
        return section

    def _build_item_model(
        self, item_model: BaseItemModel
    ) -> Union[LineItem, CalculatedLineItem, MetricLineItem, SubtotalLineItem, Section]:
        """Build a statement item object from its corresponding Pydantic model.

        Dispatches the building process based on the specific type of the input
        model (`LineItemModel`, `CalculatedItemModel`, `MetricItemModel`,
        `SubtotalModel`, or `SectionModel` for nested sections).

        Args:
            item_model: The Pydantic model representing a line item, calculated
                item, metric item, subtotal, or nested section.

        Returns:
            The corresponding `StatementStructure` component (`LineItem`,
            `CalculatedLineItem`, `MetricLineItem`, `SubtotalLineItem`, or `Section`).

        Raises:
            ConfigurationError: If an unknown or unexpected model type is
                encountered.
        """
        # Convert adjustment filter for all item types
        adjustment_filter = self._convert_adjustment_filter(
            item_model.default_adjustment_filter
        )

        # Dispatch by model instance type
        if isinstance(item_model, SectionModel):
            # Handle nested sections directly
            return self._build_section_model(item_model)
        if isinstance(item_model, LineItemModel):
            return LineItem(
                id=item_model.id,
                name=item_model.name,
                node_id=item_model.node_id,
                standard_node_ref=item_model.standard_node_ref,
                description=cast(str, item_model.description),
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
                default_adjustment_filter=adjustment_filter,
                display_format=item_model.display_format,
                hide_if_all_zero=item_model.hide_if_all_zero,
                css_class=item_model.css_class,
                notes_references=item_model.notes_references,
                units=item_model.units,
                display_scale_factor=item_model.display_scale_factor,
                is_contra=item_model.is_contra,
            )
        if isinstance(item_model, CalculatedItemModel):
            # Pass the calculation model directly or its dict representation
            return CalculatedLineItem(
                id=item_model.id,
                name=item_model.name,
                # Pass the nested Pydantic model if structure expects dict
                calculation=item_model.calculation.model_dump(),
                description=cast(str, item_model.description),
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
                default_adjustment_filter=adjustment_filter,
                display_format=item_model.display_format,
                hide_if_all_zero=item_model.hide_if_all_zero,
                css_class=item_model.css_class,
                notes_references=item_model.notes_references,
                units=item_model.units,
                display_scale_factor=item_model.display_scale_factor,
                is_contra=item_model.is_contra,
            )
        if isinstance(item_model, MetricItemModel):
            return MetricLineItem(
                id=item_model.id,
                name=item_model.name,
                metric_id=item_model.metric_id,
                inputs=item_model.inputs,
                description=cast(str, item_model.description),
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
                default_adjustment_filter=adjustment_filter,
                display_format=item_model.display_format,
                hide_if_all_zero=item_model.hide_if_all_zero,
                css_class=item_model.css_class,
                notes_references=item_model.notes_references,
                units=item_model.units,
                display_scale_factor=item_model.display_scale_factor,
                is_contra=item_model.is_contra,
            )
        if isinstance(item_model, SubtotalModel):
            return self._build_subtotal_model(item_model)

        # Should be unreachable if Pydantic validation works
        logger.error(
            f"Encountered unknown item model type during build: {type(item_model).__name__}"
        )
        raise ConfigurationError(
            message=f"Unknown item model type: {type(item_model).__name__}",
            errors=[
                f"Item '{getattr(item_model, 'id', '<unknown>')}' has invalid model type."
            ],
        )

    def _build_subtotal_model(self, subtotal_model: SubtotalModel) -> SubtotalLineItem:
        """Build a `SubtotalLineItem` object from a `SubtotalModel`.

        Extracts the relevant item IDs to be summed, either from the explicit
        `items_to_sum` list or from the `calculation.inputs` if provided.

        Args:
            subtotal_model: The Pydantic model representing the subtotal configuration.

        Returns:
            A `SubtotalLineItem` instance.
        """
        # Convert adjustment filter
        adjustment_filter = self._convert_adjustment_filter(
            subtotal_model.default_adjustment_filter
        )

        # Consolidate logic for getting item IDs
        item_ids = (
            subtotal_model.calculation.inputs
            if subtotal_model.calculation and subtotal_model.calculation.inputs
            else subtotal_model.items_to_sum
        )
        if not item_ids:
            logger.warning(
                f"Subtotal '{subtotal_model.id}' has no items_to_sum or calculation inputs defined."
            )
            # Decide handling: error or allow empty subtotal?
            # Allowing for now, may need adjustment based on desired behavior.

        return SubtotalLineItem(
            id=subtotal_model.id,
            name=subtotal_model.name,
            item_ids=item_ids or [],  # Ensure it's a list
            description=cast(str, subtotal_model.description),
            sign_convention=subtotal_model.sign_convention,
            metadata=subtotal_model.metadata,
            default_adjustment_filter=adjustment_filter,
            display_format=subtotal_model.display_format,
            hide_if_all_zero=subtotal_model.hide_if_all_zero,
            css_class=subtotal_model.css_class,
            notes_references=subtotal_model.notes_references,
            units=subtotal_model.units,
            display_scale_factor=subtotal_model.display_scale_factor,
            is_contra=subtotal_model.is_contra,
        )



================================================================================
File: fin_statement_model/statements/structure/containers.py
================================================================================

"""Container classes for defining hierarchical financial statement structures.

This module provides Section and StatementStructure, which organize
LineItem and CalculatedLineItem objects into nested groups.
"""

from __future__ import annotations
from typing import Any, Optional, Union, Sequence

from fin_statement_model.core.errors import StatementError
from fin_statement_model.statements.structure.items import (
    StatementItem,
    StatementItemType,
    CalculatedLineItem,
    MetricLineItem,
    SubtotalLineItem,
)

"""Configuration helper will be imported inside methods to avoid circular imports."""

__all__ = ["Section", "StatementStructure"]


class Section:
    """Represents a section in a financial statement.

    Sections group related items and subsections into a hierarchical container
    with enhanced display control and units metadata.
    """

    def __init__(
        self,
        id: str,
        name: str,
        description: str = "",
        metadata: Optional[dict[str, Any]] = None,
        default_adjustment_filter: Optional[Any] = None,
        display_format: Optional[str] = None,
        hide_if_all_zero: bool = False,
        css_class: Optional[str] = None,
        notes_references: Optional[list[str]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
    ):
        """Initialize a section.

        Args:
            id: Unique identifier for the section.
            name: Display name for the section.
            description: Optional description text.
            metadata: Optional additional metadata.
            default_adjustment_filter: Optional default adjustment filter for this section.
            display_format: Optional specific number format string for section items.
            hide_if_all_zero: Whether to hide this section if all values are zero.
            css_class: Optional CSS class name for HTML/web outputs.
            notes_references: List of footnote/note IDs referenced by this section.
            units: Optional unit description for this section.
            display_scale_factor: Factor to scale values for display in this section.
                                If not provided, uses config default from display.scale_factor.

        Raises:
            StatementError: If id or name is invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(f"Invalid section ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(f"Invalid section name: {name} for ID: {id}")

        # Use config default if not provided (import only when needed)
        from fin_statement_model.config.helpers import cfg_or_param

        display_scale_factor = cfg_or_param(
            "display.scale_factor", display_scale_factor
        )

        if display_scale_factor is None or display_scale_factor <= 0:
            raise StatementError(
                f"display_scale_factor must be positive for section: {id}"
            )

        self._id = id
        self._name = name
        self._description = description
        self._metadata = metadata or {}
        self._default_adjustment_filter = default_adjustment_filter
        self._display_format = display_format
        self._hide_if_all_zero = hide_if_all_zero
        self._css_class = css_class
        self._notes_references = notes_references or []
        self._units = units
        self._display_scale_factor = display_scale_factor
        self._items: list[Union[Section, StatementItem]] = []
        # Optional subtotal for this section (may be set by builder)
        self.subtotal: Optional[StatementItem] = None

    @property
    def id(self) -> str:
        """Get the section identifier."""
        return self._id

    @property
    def name(self) -> str:
        """Get the section display name."""
        return self._name

    @property
    def description(self) -> str:
        """Get the section description."""
        return self._description

    @property
    def metadata(self) -> dict[str, Any]:
        """Get the section metadata."""
        return self._metadata

    @property
    def default_adjustment_filter(self) -> Optional[Any]:
        """Get the default adjustment filter for this section."""
        return self._default_adjustment_filter

    @property
    def display_format(self) -> Optional[str]:
        """Get the display format string for this section."""
        return self._display_format

    @property
    def hide_if_all_zero(self) -> bool:
        """Get whether to hide this section if all values are zero."""
        return self._hide_if_all_zero

    @property
    def css_class(self) -> Optional[str]:
        """Get the CSS class for this section."""
        return self._css_class

    @property
    def notes_references(self) -> list[str]:
        """Get the list of note references for this section."""
        return list(self._notes_references)

    @property
    def units(self) -> Optional[str]:
        """Get the unit description for this section."""
        return self._units

    @property
    def display_scale_factor(self) -> float:
        """Get the display scale factor for this section."""
        return self._display_scale_factor

    @property
    def items(self) -> list[Union["Section", StatementItem]]:
        """Get the child items and subsections."""
        return list(self._items)

    @property
    def item_type(self) -> StatementItemType:
        """Get the item type (SECTION)."""
        return StatementItemType.SECTION

    def add_item(self, item: Union["Section", StatementItem]) -> None:
        """Add a child item or subsection to this section.

        Args:
            item: The Section or StatementItem to add.

        Raises:
            StatementError: If an item with the same id already exists.
        """
        if any(existing.id == item.id for existing in self._items):
            raise StatementError(f"Duplicate item ID: {item.id} in section: {self.id}")
        self._items.append(item)

    def find_item_by_id(
        self, item_id: str
    ) -> Optional[Union["Section", StatementItem]]:
        """Recursively find an item by its identifier within this section.

        Args:
            item_id: Identifier of the item to search for.

        Returns:
            The found Section or StatementItem, or None if not found.
        """
        if self.id == item_id:
            return self
        for child in self._items:
            if child.id == item_id:
                return child
            if isinstance(child, Section):
                found = child.find_item_by_id(item_id)
                if found:
                    return found
        if hasattr(self, "subtotal") and self.subtotal and self.subtotal.id == item_id:
            return self.subtotal
        return None


class StatementStructure:
    """Top-level container for a financial statement structure.

    Manages a hierarchy of Section objects with statement-level display
    and units metadata.
    """

    def __init__(
        self,
        id: str,
        name: str,
        description: str = "",
        metadata: Optional[dict[str, Any]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
    ):
        """Initialize a statement structure.

        Args:
            id: Unique identifier for the statement.
            name: Display name for the statement.
            description: Optional description text.
            metadata: Optional additional metadata.
            units: Optional default unit description for the statement.
            display_scale_factor: Default scale factor for displaying values.
                                If not provided, uses config default from display.scale_factor.

        Raises:
            StatementError: If id or name is invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(f"Invalid statement ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(f"Invalid statement name: {name} for ID: {id}")

        # Use config default if not provided (import only when needed)
        from fin_statement_model.config.helpers import cfg_or_param

        display_scale_factor = cfg_or_param(
            "display.scale_factor", display_scale_factor
        )

        if display_scale_factor is None or display_scale_factor <= 0:
            raise StatementError(
                f"display_scale_factor must be positive for statement: {id}"
            )

        self._id = id
        self._name = name
        self._description = description
        self._metadata = metadata or {}
        self._units = units
        self._display_scale_factor = display_scale_factor
        self._sections: list[Section] = []

    @property
    def id(self) -> str:
        """Get the statement identifier."""
        return self._id

    @property
    def name(self) -> str:
        """Get the statement display name."""
        return self._name

    @property
    def description(self) -> str:
        """Get the statement description."""
        return self._description

    @property
    def metadata(self) -> dict[str, Any]:
        """Get the statement metadata."""
        return self._metadata

    @property
    def units(self) -> Optional[str]:
        """Get the default unit description for the statement."""
        return self._units

    @property
    def display_scale_factor(self) -> float:
        """Get the default display scale factor for the statement."""
        return self._display_scale_factor

    @property
    def sections(self) -> list[Section]:
        """Get the top-level sections."""
        return list(self._sections)

    @property
    def items(self) -> list[Section]:
        """Alias for sections to ease iteration."""
        return self.sections

    def add_section(self, section: Section) -> None:
        """Add a section to the statement.

        Args:
            section: Section to add.

        Raises:
            StatementError: If a section with the same id already exists.
        """
        if any(s.id == section.id for s in self._sections):
            raise StatementError(
                f"Duplicate section ID: {section.id} in statement: {self.id}"
            )
        self._sections.append(section)

    def find_item_by_id(self, item_id: str) -> Optional[Union[Section, StatementItem]]:
        """Find an item by its ID in the statement structure.

        Args:
            item_id: The ID of the item to find.

        Returns:
            Optional[Union[Section, StatementItem]]: The found item or None if not found.
        """
        for section in self._sections:
            item = section.find_item_by_id(item_id)
            if item:
                return item
        return None

    def get_calculation_items(
        self,
    ) -> list[Union[CalculatedLineItem, SubtotalLineItem]]:
        """Get all calculation items from the statement structure.

        Returns:
            List[Union[CalculatedLineItem, SubtotalLineItem]]: List of calculation items.
        """
        calculation_items = []

        def collect_calculation_items(
            items: Sequence[Union[Section, StatementItem]],
        ) -> None:
            for item in items:
                if isinstance(item, CalculatedLineItem | SubtotalLineItem):
                    calculation_items.append(item)
                elif isinstance(item, Section):
                    collect_calculation_items(item.items)
                    if hasattr(item, "subtotal") and item.subtotal:
                        if isinstance(item.subtotal, SubtotalLineItem):
                            calculation_items.append(item.subtotal)
                        else:
                            pass

        collect_calculation_items(self._sections)
        return calculation_items

    def get_metric_items(self) -> list[MetricLineItem]:
        """Get all metric items from the statement structure.

        Returns:
            List[MetricLineItem]: List of metric items.
        """
        metric_items = []

        def collect_metric_items(
            items: Sequence[Union[Section, StatementItem]],
        ) -> None:
            for item in items:
                if isinstance(item, MetricLineItem):
                    metric_items.append(item)
                elif isinstance(item, Section):
                    collect_metric_items(item.items)
                    # Subtotals are handled by get_calculation_items, not relevant here

        collect_metric_items(self._sections)
        return metric_items

    def get_all_items(self) -> list[StatementItem]:
        """Get all StatementItem instances recursively from the structure.

        Traverses all sections and nested sections, collecting only objects that
        are subclasses of StatementItem (e.g., LineItem, CalculatedLineItem),
        excluding Section objects themselves.

        Returns:
            List[StatementItem]: A flat list of all statement items found.
        """
        all_statement_items: list[StatementItem] = []

        def _collect_items_recursive(
            items_or_sections: Sequence[Union[Section, StatementItem]],
        ) -> None:
            for item in items_or_sections:
                if isinstance(item, Section):
                    _collect_items_recursive(item.items)
                    # Also collect the section's subtotal if it exists and is a StatementItem
                    if hasattr(item, "subtotal") and isinstance(
                        item.subtotal, StatementItem
                    ):
                        all_statement_items.append(item.subtotal)
                elif isinstance(item, StatementItem):
                    all_statement_items.append(item)

        _collect_items_recursive(self._sections)

        return all_statement_items



================================================================================
File: fin_statement_model/statements/structure/items.py
================================================================================

"""Statement structure items module defining line items, calculated items, and subtotals."""

from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Optional, cast

from fin_statement_model.core.errors import StatementError

__all__ = [
    "CalculatedLineItem",
    "LineItem",
    "MetricLineItem",
    "StatementItem",
    "StatementItemType",
    "SubtotalLineItem",
]


class StatementItemType(Enum):
    """Types of statement structure items.

    Attributes:
      SECTION: Section container
      LINE_ITEM: Basic financial line item
      SUBTOTAL: Subtotal of multiple items
      CALCULATED: Derived calculation item
      METRIC: Derived metric item from registry
    """

    SECTION = "section"
    LINE_ITEM = "line_item"
    SUBTOTAL = "subtotal"
    CALCULATED = "calculated"
    METRIC = "metric"


class StatementItem(ABC):
    """Abstract base class for all statement structure items.

    Defines a common interface: id, name, item_type, default_adjustment_filter,
    and enhanced display control and units metadata.
    """

    @property
    @abstractmethod
    def id(self) -> str:
        """Get the unique identifier of the item."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Get the display name of the item."""

    @property
    @abstractmethod
    def item_type(self) -> StatementItemType:
        """Get the type of this statement item."""

    @property
    @abstractmethod
    def default_adjustment_filter(self) -> Optional[Any]:
        """Get the default adjustment filter for this item."""

    @property
    @abstractmethod
    def display_format(self) -> Optional[str]:
        """Get the display format string for this item."""

    @property
    @abstractmethod
    def hide_if_all_zero(self) -> bool:
        """Get whether to hide this item if all values are zero."""

    @property
    @abstractmethod
    def css_class(self) -> Optional[str]:
        """Get the CSS class for this item."""

    @property
    @abstractmethod
    def notes_references(self) -> list[str]:
        """Get the list of note references for this item."""

    @property
    @abstractmethod
    def units(self) -> Optional[str]:
        """Get the unit description for this item."""

    @property
    @abstractmethod
    def display_scale_factor(self) -> float:
        """Get the display scale factor for this item."""

    @property
    @abstractmethod
    def is_contra(self) -> bool:
        """Get whether this is a contra item for special display formatting."""


class LineItem(StatementItem):
    """Represents a basic line item in a financial statement.

    Args:
      id: Unique ID for the line item
      name: Display name for the line item
      node_id: ID of the core graph node that holds values (optional if standard_node_ref provided)
      standard_node_ref: Reference to a standard node name from the registry (optional if node_id provided)
      description: Optional explanatory text
      sign_convention: 1 for normal values, -1 for inverted
      metadata: Optional additional attributes
      default_adjustment_filter: Optional default adjustment filter for this item
      display_format: Optional specific number format string
      hide_if_all_zero: Whether to hide this item if all values are zero
      css_class: Optional CSS class name for HTML/web outputs
      notes_references: List of footnote/note IDs referenced by this item
      units: Optional unit description
      display_scale_factor: Factor to scale values for display
      is_contra: Whether this is a contra item for special display formatting

    Raises:
      StatementError: If inputs are invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        node_id: Optional[str] = None,
        standard_node_ref: Optional[str] = None,
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
        default_adjustment_filter: Optional[Any] = None,
        display_format: Optional[str] = None,
        hide_if_all_zero: bool = False,
        css_class: Optional[str] = None,
        notes_references: Optional[list[str]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
        is_contra: bool = False,
    ):
        """Initialize a basic LineItem.

        Args:
            id: Unique ID for the line item.
            name: Display name for the line item.
            node_id: ID of the core graph node holding values (optional if standard_node_ref provided).
            standard_node_ref: Reference to a standard node name (optional if node_id provided).
            description: Optional explanatory text.
            sign_convention: Sign convention (1 for positive, -1 for negative).
            metadata: Optional additional attributes.
            default_adjustment_filter: Optional default adjustment filter for this item.
            display_format: Optional specific number format string (e.g., ",.2f").
            hide_if_all_zero: Whether to hide this item if all values are zero.
            css_class: Optional CSS class name for HTML/web outputs.
            notes_references: List of footnote/note IDs referenced by this item.
            units: Optional unit description (e.g., "USD Thousands").
            display_scale_factor: Factor to scale values for display (e.g., 0.001 for thousands).
                                If not provided, uses config default from display.scale_factor.
            is_contra: Whether this is a contra item for special display formatting.

        Raises:
            StatementError: If inputs are invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(f"Invalid line item ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(f"Invalid line item name: {name} for ID: {id}")

        # Validate that exactly one of node_id or standard_node_ref is provided
        if not node_id and not standard_node_ref:
            raise StatementError(
                f"Must provide either 'node_id' or 'standard_node_ref' for line item: {id}"
            )
        if node_id and standard_node_ref:
            raise StatementError(
                f"Cannot provide both 'node_id' and 'standard_node_ref' for line item: {id}"
            )

        if sign_convention not in (1, -1):
            raise StatementError(
                f"Invalid sign convention {sign_convention} for item: {id}"
            )

        # Use config default if not provided (import only when needed)
        from fin_statement_model.config.helpers import cfg_or_param

        display_scale_factor = cfg_or_param(
            "display.scale_factor", display_scale_factor
        )

        if display_scale_factor is None or display_scale_factor <= 0:
            raise StatementError(
                f"display_scale_factor must be positive for item: {id}"
            )

        self._id = id
        self._name = name
        self._node_id = node_id
        self._standard_node_ref = standard_node_ref
        self._description = description
        self._sign_convention = sign_convention
        self._metadata = metadata or {}
        self._default_adjustment_filter = default_adjustment_filter
        self._display_format = display_format
        self._hide_if_all_zero = hide_if_all_zero
        self._css_class = css_class
        self._notes_references = notes_references or []
        self._units = units
        self._display_scale_factor = display_scale_factor
        self._is_contra = is_contra

    @property
    def id(self) -> str:
        """Get the unique identifier of the line item."""
        return self._id

    @property
    def name(self) -> str:
        """Get the display name of the line item."""
        return self._name

    @property
    def node_id(self) -> Optional[str]:
        """Get the core graph node ID for this line item (if provided directly)."""
        return self._node_id

    @property
    def standard_node_ref(self) -> Optional[str]:
        """Get the standard node reference for this line item (if provided)."""
        return self._standard_node_ref

    @property
    def description(self) -> str:
        """Get the description for this line item."""
        return self._description

    @property
    def sign_convention(self) -> int:
        """Get the sign convention (1 or -1)."""
        return self._sign_convention

    @property
    def metadata(self) -> dict[str, Any]:
        """Get custom metadata associated with this item."""
        return self._metadata

    @property
    def default_adjustment_filter(self) -> Optional[Any]:
        """Get the default adjustment filter for this item."""
        return self._default_adjustment_filter

    @property
    def display_format(self) -> Optional[str]:
        """Get the display format string for this item."""
        return self._display_format

    @property
    def hide_if_all_zero(self) -> bool:
        """Get whether to hide this item if all values are zero."""
        return self._hide_if_all_zero

    @property
    def css_class(self) -> Optional[str]:
        """Get the CSS class for this item."""
        return self._css_class

    @property
    def notes_references(self) -> list[str]:
        """Get the list of note references for this item."""
        return list(self._notes_references)

    @property
    def units(self) -> Optional[str]:
        """Get the unit description for this item."""
        return self._units

    @property
    def display_scale_factor(self) -> float:
        """Get the display scale factor for this item."""
        return self._display_scale_factor

    @property
    def is_contra(self) -> bool:
        """Get whether this is a contra item for special display formatting."""
        return self._is_contra

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (LINE_ITEM)."""
        return StatementItemType.LINE_ITEM

    def get_resolved_node_id(
        self, standard_registry: Optional[Any] = None
    ) -> Optional[str]:
        """Get the resolved node ID, handling both direct node_id and standard_node_ref.

        Args:
            standard_registry: Optional standard node registry for resolving references.
                             If None, uses the global registry.

        Returns:
            The resolved node ID, or None if no registry is available for standard_node_ref.
        """
        if self._node_id:
            return self._node_id

        if self._standard_node_ref:
            if standard_registry is None:
                # Import here to avoid circular dependency
                from fin_statement_model.core.nodes import standard_node_registry

                standard_registry = standard_node_registry

            # Try to get the standard name (handles alternate names too)
            return cast(
                str,
                standard_registry.get_standard_name(self._standard_node_ref),
            )

        return None


class MetricLineItem(LineItem):
    """Represents a line item whose calculation is defined by a core metric.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      metric_id: ID of the metric in the core.metrics.registry
      inputs: Dict mapping metric input names to statement item IDs
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata
      default_adjustment_filter: Optional default adjustment filter for this item
      display_format: Optional specific number format string
      hide_if_all_zero: Whether to hide this item if all values are zero
      css_class: Optional CSS class name for HTML/web outputs
      notes_references: List of footnote/note IDs referenced by this item
      units: Optional unit description
      display_scale_factor: Factor to scale values for display
      is_contra: Whether this is a contra item for special display formatting

    Raises:
      StatementError: If metric_id or inputs are invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        metric_id: str,
        inputs: dict[str, str],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
        default_adjustment_filter: Optional[Any] = None,
        display_format: Optional[str] = None,
        hide_if_all_zero: bool = False,
        css_class: Optional[str] = None,
        notes_references: Optional[list[str]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
        is_contra: bool = False,
    ):
        """Initialize a MetricLineItem referencing a registered metric.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            metric_id: ID of the metric in the core.metrics.registry.
            inputs: Dict mapping metric input names to statement item IDs.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.
            default_adjustment_filter: Optional default adjustment filter for this item.
            display_format: Optional specific number format string.
            hide_if_all_zero: Whether to hide this item if all values are zero.
            css_class: Optional CSS class name for HTML/web outputs.
            notes_references: List of footnote/note IDs referenced by this item.
            units: Optional unit description.
            display_scale_factor: Factor to scale values for display.
                                If not provided, uses config default from display.scale_factor.
            is_contra: Whether this is a contra item for special display formatting.

        Raises:
            StatementError: If metric_id or inputs are invalid.
        """
        super().__init__(
            id=id,
            name=name,
            node_id=id,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
            default_adjustment_filter=default_adjustment_filter,
            display_format=display_format,
            hide_if_all_zero=hide_if_all_zero,
            css_class=css_class,
            notes_references=notes_references,
            units=units,
            display_scale_factor=display_scale_factor,
            is_contra=is_contra,
        )
        if not metric_id or not isinstance(metric_id, str):
            raise StatementError(f"Invalid metric_id '{metric_id}' for item: {id}")
        if not isinstance(inputs, dict) or not inputs:
            raise StatementError(
                f"Metric inputs must be a non-empty dictionary for item: {id}"
            )
        if not all(
            isinstance(k, str) and isinstance(v, str) for k, v in inputs.items()
        ):
            raise StatementError(
                f"Metric input keys and values must be strings for item: {id}"
            )

        self._metric_id = metric_id
        self._inputs = inputs

    @property
    def metric_id(self) -> str:
        """Get the ID of the metric referenced from the core registry."""
        return self._metric_id

    @property
    def inputs(self) -> dict[str, str]:
        """Get the mapping from metric input names to statement item IDs."""
        return self._inputs

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (METRIC)."""
        return StatementItemType.METRIC


class CalculatedLineItem(LineItem):
    """Represents a calculated line item whose values come from graph calculations.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      calculation: Dict with 'type', 'inputs', optional 'parameters'
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata
      default_adjustment_filter: Optional default adjustment filter for this item
      display_format: Optional specific number format string
      hide_if_all_zero: Whether to hide this item if all values are zero
      css_class: Optional CSS class name for HTML/web outputs
      notes_references: List of footnote/note IDs referenced by this item
      units: Optional unit description
      display_scale_factor: Factor to scale values for display
      is_contra: Whether this is a contra item for special display formatting

    Raises:
      StatementError: If calculation dictionary is invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        calculation: dict[str, Any],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
        default_adjustment_filter: Optional[Any] = None,
        display_format: Optional[str] = None,
        hide_if_all_zero: bool = False,
        css_class: Optional[str] = None,
        notes_references: Optional[list[str]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
        is_contra: bool = False,
    ):
        """Initialize a CalculatedLineItem based on calculation specification.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            calculation: Calculation spec dict with 'type', 'inputs', optional 'parameters'.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.
            default_adjustment_filter: Optional default adjustment filter for this item.
            display_format: Optional specific number format string.
            hide_if_all_zero: Whether to hide this item if all values are zero.
            css_class: Optional CSS class name for HTML/web outputs.
            notes_references: List of footnote/note IDs referenced by this item.
            units: Optional unit description.
            display_scale_factor: Factor to scale values for display.
                                If not provided, uses config default from display.scale_factor.
            is_contra: Whether this is a contra item for special display formatting.

        Raises:
            StatementError: If calculation dictionary is invalid.
        """
        super().__init__(
            id=id,
            name=name,
            node_id=id,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
            default_adjustment_filter=default_adjustment_filter,
            display_format=display_format,
            hide_if_all_zero=hide_if_all_zero,
            css_class=css_class,
            notes_references=notes_references,
            units=units,
            display_scale_factor=display_scale_factor,
            is_contra=is_contra,
        )
        if not isinstance(calculation, dict):
            raise StatementError(f"Invalid calculation spec for item: {id}")
        if "type" not in calculation:
            raise StatementError(f"Missing calculation type for item: {id}")
        inputs = calculation.get("inputs")
        if not isinstance(inputs, list) or not inputs:
            raise StatementError(
                f"Calculation inputs must be a non-empty list for item: {id}"
            )
        self._calculation = calculation

    @property
    def calculation_type(self) -> str:
        """Get the calculation operation type (e.g., 'addition')."""
        return cast(str, self._calculation["type"])

    @property
    def input_ids(self) -> list[str]:
        """Get the list of input item IDs for this calculation."""
        return cast(list[str], self._calculation["inputs"])

    @property
    def parameters(self) -> dict[str, Any]:
        """Get optional parameters for the calculation."""
        return cast(dict[str, Any], self._calculation.get("parameters", {}))

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (CALCULATED)."""
        return StatementItemType.CALCULATED


class SubtotalLineItem(CalculatedLineItem):
    """Represents a subtotal line item summing multiple other items.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      item_ids: List of IDs to sum
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata
      default_adjustment_filter: Optional default adjustment filter for this item
      display_format: Optional specific number format string
      hide_if_all_zero: Whether to hide this item if all values are zero
      css_class: Optional CSS class name for HTML/web outputs
      notes_references: List of footnote/note IDs referenced by this item
      units: Optional unit description
      display_scale_factor: Factor to scale values for display

    Raises:
      StatementError: If item_ids is empty or not a list
    """

    def __init__(
        self,
        id: str,
        name: str,
        item_ids: list[str],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
        default_adjustment_filter: Optional[Any] = None,
        display_format: Optional[str] = None,
        hide_if_all_zero: bool = False,
        css_class: Optional[str] = None,
        notes_references: Optional[list[str]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
        is_contra: bool = False,
    ):
        """Initialize a SubtotalLineItem summing multiple items.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            item_ids: List of IDs to sum.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.
            default_adjustment_filter: Optional default adjustment filter for this item.
            display_format: Optional specific number format string.
            hide_if_all_zero: Whether to hide this item if all values are zero.
            css_class: Optional CSS class name for HTML/web outputs.
            notes_references: List of footnote/note IDs referenced by this item.
            units: Optional unit description.
            display_scale_factor: Factor to scale values for display.
                                If not provided, uses config default from display.scale_factor.
            is_contra: Whether this is a contra item for special display formatting.

        Raises:
            StatementError: If item_ids is empty or not a list.
        """
        if not isinstance(item_ids, list) or not item_ids:
            raise StatementError(f"Invalid or empty item IDs for subtotal: {id}")
        calculation = {"type": "addition", "inputs": item_ids, "parameters": {}}
        super().__init__(
            id=id,
            name=name,
            calculation=calculation,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
            default_adjustment_filter=default_adjustment_filter,
            display_format=display_format,
            hide_if_all_zero=hide_if_all_zero,
            css_class=css_class,
            notes_references=notes_references,
            units=units,
            display_scale_factor=display_scale_factor,
            is_contra=is_contra,
        )
        self._item_ids = item_ids

    @property
    def item_ids(self) -> list[str]:
        """Get the IDs of items summed by this subtotal."""
        return self._item_ids

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (SUBTOTAL)."""
        return StatementItemType.SUBTOTAL



================================================================================
File: fin_statement_model/statements/utilities/__init__.py
================================================================================

"""Cross-cutting utilities for the statements package.

This package provides reusable components:
- Result types for functional error handling
- Retry mechanisms for transient failures
- Common error codes and handling patterns
"""

from .result_types import (
    ErrorCollector,
    ErrorDetail,
    ErrorSeverity,
    Failure,
    OperationResult,
    ProcessingResult,
    Result,
    Success,
    ValidationResult,
    combine_results,
)
from .retry_handler import (
    BackoffStrategy,
    ConstantBackoff,
    ExponentialBackoff,
    LinearBackoff,
    RetryConfig,
    RetryHandler,
    RetryResult,
    RetryStrategy,
    retry_on_specific_errors,
    retry_with_exponential_backoff,
)

__all__ = [
    "BackoffStrategy",
    "ConstantBackoff",
    "ErrorCollector",
    "ErrorDetail",
    "ErrorSeverity",
    "ExponentialBackoff",
    "Failure",
    "LinearBackoff",
    "OperationResult",
    "ProcessingResult",
    # Result Types
    "Result",
    "RetryConfig",
    # Retry Handler
    "RetryHandler",
    "RetryResult",
    "RetryStrategy",
    "Success",
    "ValidationResult",
    "combine_results",
    "retry_on_specific_errors",
    "retry_with_exponential_backoff",
]



================================================================================
File: fin_statement_model/statements/utilities/result_types.py
================================================================================

"""Common result types for standardized error handling in the statements module.

This module provides consistent result types and error collection utilities
to standardize error handling across the statements package. These types
enable functional error handling without exceptions for operations that
can fail in expected ways.
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Generic, Optional, TypeVar, Any, cast

logger = logging.getLogger(__name__)

__all__ = [
    "ErrorCollector",
    "ErrorDetail",
    "ErrorSeverity",
    "Failure",
    "OperationResult",
    "ProcessingResult",
    "Result",
    "Success",
    "ValidationResult",
]

T = TypeVar("T")


class ErrorSeverity(Enum):
    """Severity levels for errors."""

    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass(frozen=True)
class ErrorDetail:
    """Detailed information about an error.

    Attributes:
        code: Error code for programmatic handling
        message: Human-readable error message
        context: Optional context about where/what caused the error
        severity: Severity level of the error
        source: Optional source identifier (e.g., item ID, file path)
    """

    code: str
    message: str
    context: Optional[str] = None
    severity: ErrorSeverity = ErrorSeverity.ERROR
    source: Optional[str] = None

    def __str__(self) -> str:
        """Format error as string."""
        parts = [f"[{self.severity.value.upper()}]"]
        if self.source:
            parts.append(f"{self.source}:")
        parts.append(self.message)
        if self.context:
            parts.append(f"({self.context})")
        return " ".join(parts)


class Result(ABC, Generic[T]):
    """Abstract base class for operation results.

    Provides a functional approach to error handling, allowing
    operations to return either success or failure without exceptions.
    """

    @abstractmethod
    def is_success(self) -> bool:
        """Check if the result represents success."""

    @abstractmethod
    def is_failure(self) -> bool:
        """Check if the result represents failure."""

    @abstractmethod
    def get_value(self) -> Optional[T]:
        """Get the success value if available."""

    @abstractmethod
    def get_errors(self) -> list[ErrorDetail]:
        """Get error details if this is a failure."""

    def unwrap(self) -> T:
        """Get the value or raise an exception if failed.

        Raises:
            ValueError: If the result is a failure.
        """
        if self.is_failure():
            errors_str = "\n".join(str(e) for e in self.get_errors())
            raise ValueError(f"Cannot unwrap failed result:\n{errors_str}")
        return cast(T, self.get_value())

    def unwrap_or(self, default: T) -> T:
        """Get the value or return a default if failed."""
        return cast(T, self.get_value()) if self.is_success() else default


@dataclass(frozen=True)
class Success(Result[T]):
    """Represents a successful operation result."""

    value: T

    def is_success(self) -> bool:
        """Always returns True for Success."""
        return True

    def is_failure(self) -> bool:
        """Always returns False for Success."""
        return False

    def get_value(self) -> Optional[T]:
        """Return the success value."""
        return self.value

    def get_errors(self) -> list[ErrorDetail]:
        """Return empty list for Success."""
        return []


@dataclass(frozen=True)
class Failure(Result[T]):
    """Represents a failed operation result."""

    errors: list[ErrorDetail] = field(default_factory=list)

    def __post_init__(self) -> None:
        """Ensure at least one error is present."""
        if not self.errors:
            # Add a default error if none provided
            object.__setattr__(
                self,
                "errors",
                [
                    ErrorDetail(
                        code="unknown",
                        message="Operation failed with no specific error",
                    )
                ],
            )

    def is_success(self) -> bool:
        """Always returns False for Failure."""
        return False

    def is_failure(self) -> bool:
        """Always returns True for Failure."""
        return True

    def get_value(self) -> Optional[T]:
        """Always returns None for Failure."""
        return None

    def get_errors(self) -> list[ErrorDetail]:
        """Return the error details."""
        return self.errors

    @classmethod
    def from_exception(cls, exc: Exception, code: str = "exception") -> "Failure[T]":
        """Create a Failure from an exception."""
        return cls(
            errors=[
                ErrorDetail(
                    code=code,
                    message=str(exc),
                    context=type(exc).__name__,
                    severity=ErrorSeverity.ERROR,
                )
            ]
        )


class ErrorCollector:
    """Collects errors during multi-step operations.

    Useful for operations that should continue collecting errors
    rather than failing fast on the first error.
    """

    def __init__(self) -> None:
        """Initialize an empty error collector."""
        self._errors: list[ErrorDetail] = []
        self._warnings: list[ErrorDetail] = []

    def add_error(
        self,
        code: str,
        message: str,
        context: Optional[str] = None,
        source: Optional[str] = None,
    ) -> None:
        """Add an error to the collector."""
        self._errors.append(
            ErrorDetail(
                code=code,
                message=message,
                context=context,
                severity=ErrorSeverity.ERROR,
                source=source,
            )
        )

    def add_warning(
        self,
        code: str,
        message: str,
        context: Optional[str] = None,
        source: Optional[str] = None,
    ) -> None:
        """Add a warning to the collector."""
        self._warnings.append(
            ErrorDetail(
                code=code,
                message=message,
                context=context,
                severity=ErrorSeverity.WARNING,
                source=source,
            )
        )

    def add_from_result(
        self, result: Result[Any], source: Optional[str] = None
    ) -> None:
        """Add errors from a Result object."""
        if result.is_failure():
            for error in result.get_errors():
                # Override source if provided
                if source:
                    new_error = ErrorDetail(
                        code=error.code,
                        message=error.message,
                        context=error.context,
                        severity=error.severity,
                        source=source,
                    )
                    if new_error.severity == ErrorSeverity.WARNING:
                        self._warnings.append(new_error)
                    else:
                        self._errors.append(new_error)
                elif error.severity == ErrorSeverity.WARNING:
                    self._warnings.append(error)
                else:
                    self._errors.append(error)

    def has_errors(self) -> bool:
        """Check if any errors have been collected."""
        return len(self._errors) > 0

    def has_warnings(self) -> bool:
        """Check if any warnings have been collected."""
        return len(self._warnings) > 0

    def get_errors(self) -> list[ErrorDetail]:
        """Get all collected errors (not warnings)."""
        return list(self._errors)

    def get_warnings(self) -> list[ErrorDetail]:
        """Get all collected warnings."""
        return list(self._warnings)

    def get_all(self) -> list[ErrorDetail]:
        """Get all collected errors and warnings."""
        return self._errors + self._warnings

    def to_result(self, value: Optional[T] = None) -> Result[T]:
        """Convert collector state to a Result.

        If there are errors, returns Failure.
        Otherwise returns Success with the provided value.
        """
        if self.has_errors():
            return Failure(errors=self._errors)
        # Cast value to T for Success
        return Success(value=cast(T, value))

    def log_all(self, prefix: str = "") -> None:
        """Log all collected errors and warnings."""
        for warning in self._warnings:
            logger.warning(f"{prefix}{warning}")
        for error in self._errors:
            logger.error(f"{prefix}{error}")


# Type aliases for common result types
OperationResult = Result[
    bool
]  # For operations that succeed/fail without returning data
ValidationResult = Result[bool]  # For validation operations
ProcessingResult = Result[dict[str, Any]]  # For processing operations that return data


def combine_results(*results: Result[T]) -> Result[list[T]]:
    """Combine multiple results into a single result.

    If all results are successful, returns Success with list of values.
    If any result is a failure, returns Failure with all errors combined.
    """
    collector = ErrorCollector()
    values = []

    for result in results:
        if result.is_success():
            values.append(cast(T, result.get_value()))
        else:
            for error in result.get_errors():
                if error.severity == ErrorSeverity.WARNING:
                    collector.add_warning(
                        error.code, error.message, error.context, error.source
                    )
                else:
                    collector.add_error(
                        error.code, error.message, error.context, error.source
                    )

    if collector.has_errors():
        return Failure(errors=collector.get_all())
    return Success(value=values)



================================================================================
File: fin_statement_model/statements/utilities/retry_handler.py
================================================================================

"""Retry handler for managing transient failures in statement operations.

This module provides a flexible retry mechanism that can be used throughout
the statements package to handle transient failures gracefully. It supports
configurable retry strategies, backoff algorithms, and error classification.
"""

import logging
import random
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Generic, Optional, TypeVar, cast
from collections.abc import Callable

from fin_statement_model.statements.utilities.result_types import (
    Result,
    Failure,
    ErrorDetail,
    ErrorCollector,
)

logger = logging.getLogger(__name__)

__all__ = [
    "BackoffStrategy",
    "ConstantBackoff",
    "ExponentialBackoff",
    "LinearBackoff",
    "RetryConfig",
    "RetryHandler",
    "RetryResult",
    "RetryStrategy",
    "is_retryable_error",
    "retry_on_specific_errors",
    "retry_with_exponential_backoff",
]

T = TypeVar("T")


class RetryStrategy(Enum):
    """Strategy for determining when to retry."""

    IMMEDIATE = "immediate"  # Retry immediately on failure
    BACKOFF = "backoff"  # Use backoff strategy between retries
    CONDITIONAL = "conditional"  # Retry only for specific error types


@dataclass
class RetryConfig:
    """Configuration for retry behavior.

    Attributes:
        max_attempts: Maximum number of attempts (including initial).
                     If not provided, uses config default from api.api_retry_count
        strategy: Retry strategy to use
        backoff: Optional backoff strategy for delays
        retryable_errors: Set of error codes that are retryable
        log_retries: Whether to log retry attempts
        collect_all_errors: Whether to collect errors from all attempts
    """

    max_attempts: Optional[int] = None
    strategy: RetryStrategy = RetryStrategy.BACKOFF
    backoff: Optional["BackoffStrategy"] = None
    retryable_errors: Optional[set[str]] = None
    log_retries: bool = True
    collect_all_errors: bool = False

    def __post_init__(self) -> None:
        """Validate configuration and set defaults."""
        # Use config default if not provided
        if self.max_attempts is None:
            from fin_statement_model.config.helpers import cfg_or_param

            self.max_attempts = cfg_or_param("api.api_retry_count", None)

        if self.max_attempts < 1:
            raise ValueError("max_attempts must be at least 1")

        if self.strategy == RetryStrategy.BACKOFF and not self.backoff:
            # Default to exponential backoff
            self.backoff = ExponentialBackoff()

        if self.retryable_errors is None:
            # Default retryable errors
            self.retryable_errors = {
                "timeout",
                "connection_error",
                "rate_limit",
                "temporary_failure",
                "calculation_error",  # For graph calculations
                "node_not_ready",  # For dependency issues
            }


class BackoffStrategy(ABC):
    """Abstract base class for backoff strategies."""

    @abstractmethod
    def get_delay(self, attempt: int) -> float:
        """Get delay in seconds for the given attempt number.

        Args:
            attempt: The attempt number (1-based)

        Returns:
            Delay in seconds before the next retry
        """


class ExponentialBackoff(BackoffStrategy):
    """Exponential backoff strategy with optional jitter."""

    def __init__(
        self,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        multiplier: float = 2.0,
        jitter: bool = True,
    ):
        """Initialize exponential backoff.

        Args:
            base_delay: Initial delay in seconds
            max_delay: Maximum delay in seconds
            multiplier: Multiplier for each retry
            jitter: Whether to add random jitter
        """
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.multiplier = multiplier
        self.jitter = jitter

    def get_delay(self, attempt: int) -> float:
        """Calculate exponential backoff delay."""
        delay = min(
            self.base_delay * (self.multiplier ** (attempt - 1)), self.max_delay
        )

        if self.jitter:
            # Add up to 20% jitter
            jitter_amount = delay * 0.2 * random.random()  # noqa: S311
            delay += jitter_amount

        return delay


class LinearBackoff(BackoffStrategy):
    """Linear backoff strategy."""

    def __init__(self, base_delay: float = 1.0, increment: float = 1.0):
        """Initialize linear backoff.

        Args:
            base_delay: Initial delay in seconds
            increment: Increment for each retry
        """
        self.base_delay = base_delay
        self.increment = increment

    def get_delay(self, attempt: int) -> float:
        """Calculate linear backoff delay."""
        return self.base_delay + (attempt - 1) * self.increment


class ConstantBackoff(BackoffStrategy):
    """Constant delay backoff strategy."""

    def __init__(self, delay: float = 1.0):
        """Initialize constant backoff.

        Args:
            delay: Constant delay in seconds
        """
        self.delay = delay

    def get_delay(self, attempt: int) -> float:
        """Return constant delay."""
        return self.delay


@dataclass
class RetryResult(Generic[T]):
    """Result of a retry operation.

    Attributes:
        result: The final result (success or failure)
        attempts: Number of attempts made
        total_delay: Total delay time in seconds
        all_errors: All errors collected if configured
    """

    result: Result[T]
    attempts: int
    total_delay: float
    all_errors: Optional[list[ErrorDetail]] = None

    @property
    def success(self) -> bool:
        """Check if the operation eventually succeeded."""
        return self.result.is_success()

    def unwrap(self) -> T:
        """Get the value or raise an exception."""
        return self.result.unwrap()

    def unwrap_or(self, default: T) -> T:
        """Get the value or return default."""
        return self.result.unwrap_or(default)


def is_retryable_error(error: ErrorDetail, retryable_codes: set[str]) -> bool:
    """Check if an error is retryable based on its code.

    Args:
        error: The error to check
        retryable_codes: Set of error codes that are retryable

    Returns:
        True if the error should be retried
    """
    return error.code in retryable_codes


class RetryHandler:
    """Handles retry logic for operations that may fail transiently."""

    def __init__(self, config: Optional[RetryConfig] = None):
        """Initialize retry handler.

        Args:
            config: Retry configuration, uses defaults if not provided
        """
        self.config = config or RetryConfig()

    def retry(
        self,
        operation: Callable[[], Result[T]],
        operation_name: Optional[str] = None,
    ) -> RetryResult[T]:
        """Execute an operation with retry logic.

        Args:
            operation: Callable that returns a Result
            operation_name: Optional name for logging

        Returns:
            RetryResult containing the final result and metadata
        """
        error_collector = ErrorCollector() if self.config.collect_all_errors else None
        total_delay = 0.0
        # Ensure max_attempts is int
        max_attempts = cast(int, self.config.max_attempts)
        op_name = operation_name or operation.__name__

        for attempt in range(1, max_attempts + 1):
            if attempt > 1 and self.config.log_retries:
                logger.info(f"Retrying {op_name} (attempt {attempt}/{max_attempts})")

            # Execute the operation
            try:
                result = operation()
            except Exception as e:
                # Convert exception to Result
                result = Failure.from_exception(e)

            # Check if successful
            if result.is_success():
                return RetryResult(
                    result=result,
                    attempts=attempt,
                    total_delay=total_delay,
                    all_errors=error_collector.get_all() if error_collector else None,
                )

            # Handle failure
            errors = result.get_errors()

            # Collect errors if configured
            if error_collector:
                for error in errors:
                    error_collector.add_error(
                        code=error.code,
                        message=f"Attempt {attempt}: {error.message}",
                        context=error.context,
                        source=error.source,
                    )

            # Check if we should retry
            if attempt >= max_attempts:
                # No more retries
                if self.config.log_retries:
                    logger.warning(
                        f"{op_name} failed after {attempt} attempts. "
                        f"Final error: {errors[0].message if errors else 'Unknown'}"
                    )
                break

            # Check if errors are retryable
            if self.config.strategy == RetryStrategy.CONDITIONAL:
                # Safe cast retryable_errors to non-nullable set
                retryable_errors = cast(set[str], self.config.retryable_errors)
                retryable = any(
                    is_retryable_error(error, retryable_errors) for error in errors
                )
                if not retryable:
                    if self.config.log_retries:
                        logger.debug(
                            f"{op_name} failed with non-retryable error: "
                            f"{errors[0].code if errors else 'Unknown'}"
                        )
                    break

            # Calculate delay
            if self.config.strategy == RetryStrategy.BACKOFF and self.config.backoff:
                delay = self.config.backoff.get_delay(attempt)
                if self.config.log_retries:
                    logger.debug(f"Waiting {delay:.2f}s before retry")
                time.sleep(delay)
                total_delay += delay

        # Return final result
        return RetryResult(
            result=result,
            attempts=attempt,
            total_delay=total_delay,
            all_errors=error_collector.get_all() if error_collector else None,
        )

    def retry_async(
        self,
        operation: Callable[[], Result[T]],
        operation_name: Optional[str] = None,
    ) -> RetryResult[T]:
        """Execute an async operation with retry logic.

        Note: This is a placeholder for future async support.
        Currently just delegates to sync retry.

        Args:
            operation: Callable that returns a Result
            operation_name: Optional name for logging

        Returns:
            RetryResult containing the final result and metadata
        """
        # TODO: Implement proper async support when needed
        return self.retry(operation, operation_name)


# Convenience functions for common retry patterns


def retry_with_exponential_backoff(
    operation: Callable[[], Result[T]],
    max_attempts: Optional[int] = None,
    base_delay: float = 1.0,
    operation_name: Optional[str] = None,
) -> RetryResult[T]:
    """Retry an operation with exponential backoff.

    Args:
        operation: The operation to retry
        max_attempts: Maximum number of attempts. If not provided, uses config default from api.api_retry_count
        base_delay: Initial delay in seconds
        operation_name: Optional name for logging

    Returns:
        RetryResult with the final outcome
    """
    # Use config default if not provided
    from fin_statement_model.config.helpers import cfg_or_param

    max_attempts = cfg_or_param("api.api_retry_count", max_attempts)

    config = RetryConfig(
        max_attempts=max_attempts,
        strategy=RetryStrategy.BACKOFF,
        backoff=ExponentialBackoff(base_delay=base_delay),
    )
    handler = RetryHandler(config)
    return handler.retry(operation, operation_name)


def retry_on_specific_errors(
    operation: Callable[[], Result[T]],
    retryable_errors: set[str],
    max_attempts: Optional[int] = None,
    operation_name: Optional[str] = None,
) -> RetryResult[T]:
    """Retry an operation only for specific error codes.

    Args:
        operation: The operation to retry
        retryable_errors: Set of error codes that trigger retry
        max_attempts: Maximum number of attempts. If not provided, uses config default from api.api_retry_count
        operation_name: Optional name for logging

    Returns:
        RetryResult with the final outcome
    """
    # Use config default if not provided
    from fin_statement_model.config.helpers import cfg_or_param

    max_attempts = cfg_or_param("api.api_retry_count", max_attempts)

    config = RetryConfig(
        max_attempts=max_attempts,
        strategy=RetryStrategy.CONDITIONAL,
        retryable_errors=retryable_errors,
        backoff=ExponentialBackoff(),
    )
    handler = RetryHandler(config)
    return handler.retry(operation, operation_name)




================================================================================
File: fin_statement_model/__init__.py
================================================================================

"""Financial Statement Model library.

A comprehensive library for building and analyzing financial statement models
using a node-based graph structure.
"""

# Import key components at package level for easier access
from fin_statement_model.core.errors import FinancialModelError
from fin_statement_model.core.graph import Graph
from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.core.nodes import (
    CalculationNode,
    CustomGrowthForecastNode,
    CurveGrowthForecastNode,
    FinancialStatementItemNode,
    FixedGrowthForecastNode,
    ForecastNode,
    MultiPeriodStatNode,
    Node,
    StatisticalGrowthForecastNode,
    YoYGrowthNode,
)

# Import configuration management
from fin_statement_model.config import get_config, update_config

# ensure our library-wide logging policy is applied immediately
from . import logging_config  # noqa: F401

__version__ = "0.2.0"

__all__ = [
    "CalculationNode",
    "CurveGrowthForecastNode",
    "CustomGrowthForecastNode",
    "FinancialModelError",
    "FinancialStatementItemNode",
    "FixedGrowthForecastNode",
    "ForecastNode",
    "Graph",
    "MultiPeriodStatNode",
    "Node",
    "NodeFactory",
    "StatisticalGrowthForecastNode",
    "YoYGrowthNode",
    "__version__",
    "get_config",
    "update_config",
]

# Core API Exports (ensure essential classes/functions are accessible)
# Example:
# from .core.graph import Graph
# from .core.nodes import Node, FinancialStatementItemNode
# from .core.calculation_engine import CalculationEngine
# from .statements.manager import StatementManager

# Placeholder: Explicitly list key public API components later.
# For now, just rely on sub-package __init__ files if they exist.



================================================================================
File: fin_statement_model/config/__init__.py
================================================================================

"""Provide centralized configuration management for fin_statement_model.

This sub-package offers a single entry-point for accessing and mutating the
library's configuration during runtime.  It re-exports the most commonly used
helpers so that callers can interact with the configuration layer without
needing to know the underlying module structure.

Examples:
    >>> from fin_statement_model.config import get_config, update_config

    # Retrieve the current configuration
    >>> cfg = get_config()
    >>> cfg.logging.level
    'WARNING'

    # Apply an in-memory override (takes effect immediately)
    >>> update_config({
    ...     'forecasting': {
    ...         'default_method': 'historical_growth',
    ...         'default_periods': 5,
    ...     }
    ... })
    >>> get_config().forecasting.default_method
    'historical_growth'
"""

from .helpers import cfg, cfg_or_param, get_typed_config

# Importing helpers first ensures that "cfg" is available early, avoiding
# circular import issues when other modules import fin_statement_model.config
# during the initialization of sub-packages (e.g., io.formats.api.fmp).

from .models import Config
from .manager import get_config, update_config

__all__ = [
    "Config",
    "cfg",
    "cfg_or_param",
    "get_config",
    "get_typed_config",
    "update_config",
]



================================================================================
File: fin_statement_model/config/helpers.py
================================================================================

"""Utility functions for accessing configuration values.

This module provides functions to retrieve application configuration
settings from the central config object, with optional type checking and
default values. It also includes utilities for parsing environment variable
strings into native Python types.
"""

from __future__ import annotations
from typing import Any, Optional, TypeVar, overload
from collections.abc import Sequence
from fin_statement_model.core.errors import FinancialModelError


class ConfigurationAccessError(FinancialModelError):
    """Raised when there's an error accessing configuration values."""


T = TypeVar("T")


@overload
def cfg(path: str) -> Any: ...


@overload
def cfg(path: str, default: T) -> T: ...


@overload
def cfg(path: Sequence[str]) -> Any: ...


@overload
def cfg(path: Sequence[str], default: T) -> T: ...


def cfg(path: str | Sequence[str], default: Any = None) -> Any:
    """Get a configuration value by dotted path.

    Retrieves a configuration value from the global config object using a
    dotted path or sequence of path segments.

    Args:
        path: Dotted path string or sequence of keys to traverse the config.
        default: Default value to return if the key is not found.

    Returns:
        The configuration value or the default if provided.

    Raises:
        ConfigurationAccessError: If the path is empty or a key does not exist
            and no default is provided.

    Examples:
        >>> cfg("database.host")
        "localhost"

        >>> cfg(["database", "port"], default=5432)
        5432
    """
    from .manager import get_config

    # Convert string path to sequence
    if isinstance(path, str):
        if not path:
            raise ConfigurationAccessError("Configuration path cannot be empty")
        parts = path.split(".")
    else:
        parts = list(path)

    if not parts:
        raise ConfigurationAccessError("Configuration path cannot be empty")

    obj = get_config()
    for i, part in enumerate(parts):
        full_path = ".".join(parts[: i + 1])
        if not hasattr(obj, part):
            if default is not None:
                return default
            raise ConfigurationAccessError(
                f"Configuration key '{full_path}' does not exist"
            )
        obj = getattr(obj, part)
    return obj if obj is not None else default


def get_typed_config(
    path: str | Sequence[str], expected_type: type[T], default: Optional[T] = None
) -> T:
    """Get a configuration value with type checking.

    Retrieves a configuration value and verifies it is of the expected type.

    Args:
        path: Dotted path string or sequence of keys to traverse the config.
        expected_type: Type that the returned value must match.
        default: Default value to use if the key is not present.

    Returns:
        The configuration value of type `expected_type`.

    Raises:
        ConfigurationAccessError: If the key is None and no default is provided.
        TypeError: If the value is not an instance of `expected_type`.

    Examples:
        >>> get_typed_config("features.enable_feature_x", bool, default=False)
        True
    """
    value = cfg(path, default)
    if value is None and default is None:
        raise ConfigurationAccessError(
            f"Configuration key '{path}' is None and no default provided"
        )
    if not isinstance(value, expected_type):
        raise TypeError(
            f"Configuration key '{path}' has type {type(value).__name__}, "
            f"expected {expected_type.__name__}"
        )
    return value


def cfg_or_param(config_path: str, param_value: Any) -> Any:
    """Return the parameter value if provided, otherwise get it from config.

    Args:
        config_path: Dotted path string to the configuration key.
        param_value: The value explicitly provided by the user.

    Returns:
        The `param_value` if it is not None, otherwise the configuration value.

    Examples:
        >>> cfg_or_param("logging.level", "DEBUG")
        "DEBUG"

        >>> cfg_or_param("logging.level", None)
        "INFO"
    """
    return param_value if param_value is not None else cfg(config_path)


def parse_env_value(value: str) -> bool | int | float | str:
    """Parse an environment variable string into bool, int, float, or str.

    Attempts to convert the input string to a boolean if it matches
    "true"/"false", then to an integer if it represents a whole number,
    then to a float if it appears numeric with decimal or exponent. Otherwise,
    returns the original string.

    Args:
        value: The environment variable value as a string.

    Returns:
        The parsed native Python type: bool, int, float, or str.

    Examples:
        >>> parse_env_value("true")
        True

        >>> parse_env_value("42")
        42

        >>> parse_env_value("-3.14")
        -3.14

        >>> parse_env_value("foo")
        "foo"
    """
    val = value.strip()
    low = val.lower()
    # Boolean
    if low in ("true", "false"):
        return low == "true"
    # Integer (including negatives)
    if (val.startswith("-") and val[1:].isdigit()) or val.isdigit():
        try:
            return int(val)
        except ValueError:
            pass
    # Float
    try:
        float_val = float(val)
        if "." in val or "e" in low or "E" in value:
            return float_val
    except ValueError:
        pass
    # Fallback to string
    return val



================================================================================
File: fin_statement_model/config/manager.py
================================================================================

"""Configuration manager for fin_statement_model.

This module provides utilities to load and merge application configuration from:
    1. Default settings
    2. Project-level config file (.fsm_config.yaml)
    3. User-level config file (fsm_config.yaml)
    4. Environment variables (FSM_* prefix)
    5. Runtime overrides

Example:
    >>> from fin_statement_model.config.manager import ConfigManager
    >>> cm = ConfigManager()
    >>> cfg = cm.get()
    >>> cfg.logging.level
    'WARNING'
"""

from pathlib import Path
from typing import Any, Optional, cast
import logging
from threading import Lock

from .models import Config
from fin_statement_model.core.errors import FinancialModelError

logger = logging.getLogger(__name__)


# ----------------------------------------------------------------------


class ConfigurationError(FinancialModelError):
    """Exception raised for configuration-related errors."""


class ConfigManager:
    """Manage application configuration from multiple sources.

    Loads and merges configuration with the following precedence (highest to lowest):
        1. Runtime updates via `update()`
        2. Environment variables (FSM_* prefix)
        3. User config file (fsm_config.yaml or specified path)
        4. Project config file (.fsm_config.yaml in project root)
        5. Default configuration

    This class is thread-safe.

    Examples:
        >>> from fin_statement_model.config.manager import ConfigManager
        >>> config = ConfigManager()
        >>> config.get().logging.level
        'WARNING'
        >>> config.update({'logging': {'level': 'DEBUG'}})
        >>> config.get().logging.level
        'DEBUG'
    """

    # Environment variable prefix
    ENV_PREFIX = "FSM_"

    # Default config file names
    USER_CONFIG_FILE = "fsm_config.yaml"
    PROJECT_CONFIG_FILE = ".fsm_config.yaml"

    def __init__(self, config_file: Optional[Path] = None):
        """Initialize the configuration manager.

        Args:
            config_file: Optional path to a configuration file. If not provided,
                the manager will search for default config files in the
                current directory or home directory.
        """
        self._lock = Lock()
        self._config: Optional[Config] = None
        self._runtime_overrides: dict[str, Any] = {}
        self._config_file = config_file

    def get(self) -> Config:
        """Return the current merged configuration.

        Loads and merges configuration sources on first call or after an
        update/reset operation.

        Returns:
            A validated `Config` object representing the current configuration.

        Examples:
            >>> from fin_statement_model.config.manager import ConfigManager
            >>> cm = ConfigManager()
            >>> cfg = cm.get()
            >>> isinstance(cfg, Config)
            True
        """
        with self._lock:
            if self._config is None:
                self._load_config()
            assert self._config is not None
            return self._config

    def update(self, updates: dict[str, Any]) -> None:
        """Apply runtime overrides to the configuration.

        Merges the provided updates into existing runtime overrides and
        forces a reload on the next `get()` call.

        Args:
            updates: Nested dictionary of configuration keys and values to override.

        Examples:
            >>> cm = ConfigManager()
            >>> cm.update({'logging': {'level': 'DEBUG'}})
        """
        with self._lock:
            self._runtime_overrides = self._deep_merge(self._runtime_overrides, updates)
            self._config = None  # Force reload on next get()

    def _load_config(self) -> None:
        """Load and merge configuration from all supported sources.

        Reads default settings, then merges in project-level and user-level config
        files, environment variable overrides, and runtime overrides, in order.
        Finally, validates the result into a `Config` object and applies logging.
        """
        # Load environment variables from a .env file (if present) before any
        # configuration layers are processed. This allows users to keep secrets
        # like API keys in a `.env` file without explicitly exporting them in
        # the shell. Values already present in the process environment are NOT
        # overwritten.
        self._load_dotenv()

        # Start with defaults
        config_dict = Config(
            project_name="fin_statement_model",
            config_file_path=None,
            auto_save_config=False,
        ).to_dict()

        # Layer 1: Project config file
        project_config = self._find_project_config()
        if project_config:
            logger.debug(f"Loading project config from {project_config}")
            config_dict = self._deep_merge(config_dict, self._load_file(project_config))

        # Layer 2: User config file
        user_config = self._find_user_config()
        if user_config:
            logger.debug(f"Loading user config from {user_config}")
            config_dict = self._deep_merge(config_dict, self._load_file(user_config))

        # Layer 4: Runtime overrides
        if self._runtime_overrides:
            logger.debug("Applying runtime overrides")
            config_dict = self._deep_merge(config_dict, self._runtime_overrides)

        # Create and validate final config
        self._config = Config.from_dict(config_dict)

        # Apply logging configuration immediately
        self._apply_logging_config()

    def _find_project_config(self) -> Optional[Path]:
        """Locate the project-level config file (.fsm_config.yaml).

        Searches upward from the current working directory to the filesystem root.

        Returns:
            Path to the project config file if found, otherwise None.
        """
        # Look for .fsm_config.yaml in parent directories
        current = Path.cwd()
        while current != current.parent:
            config_path = current / self.PROJECT_CONFIG_FILE
            if config_path.exists():
                return config_path
            current = current.parent
        return None

    def _find_user_config(self) -> Optional[Path]:
        """Locate the user-level config file (fsm_config.yaml).

        Checks an explicitly provided path, then the current directory, then
        the home directory.

        Returns:
            Path to the user config file if found, otherwise None.
        """
        if self._config_file and self._config_file.exists():
            return self._config_file

        # Check current directory
        user_config = Path.cwd() / self.USER_CONFIG_FILE
        if user_config.exists():
            return user_config

        # Check home directory
        home_config = Path.home() / f".{self.USER_CONFIG_FILE}"
        if home_config.exists():
            return home_config

        return None

    def _load_file(self, path: Path) -> dict[str, Any]:
        """Load configuration values from a YAML or JSON file.

        Args:
            path: Path to the config file (.yaml, .yml, or .json).

        Returns:
            A dict of configuration values loaded from the file.

        Raises:
            ConfigurationError: If the file format is unsupported or loading fails.
        """
        try:
            if path.suffix in [".yaml", ".yml"]:
                import yaml

                result = yaml.safe_load(path.read_text()) or {}
                return cast(dict[str, Any], result)
            elif path.suffix == ".json":
                import json

                result = json.loads(path.read_text())
                return cast(dict[str, Any], result)
            else:
                raise ConfigurationError(
                    f"Unsupported config file format: {path.suffix}"
                )
        except Exception as e:
            raise ConfigurationError(f"Failed to load config from {path}: {e}") from e

    def _apply_logging_config(self) -> None:
        """Configure library logging based on current settings.

        Reads logging configuration from `self._config` and applies it via
        `logging_config.setup_logging()`.
        """
        if self._config:
            from fin_statement_model import logging_config

            logging_config.setup_logging(
                level=self._config.logging.level,
                format_string=self._config.logging.format,
                detailed=self._config.logging.detailed,
                log_file_path=(
                    str(self._config.logging.log_file_path)
                    if self._config.logging.log_file_path
                    else None
                ),
            )

    def _deep_merge(
        self, base: dict[str, Any], update: dict[str, Any]
    ) -> dict[str, Any]:
        """Recursively merge two dictionaries with `update` taking precedence."""
        result = base.copy()

        for key, value in update.items():
            if (
                key in result
                and isinstance(result[key], dict)
                and isinstance(value, dict)
            ):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value

        return result

    # ------------------------------------------------------------------
    # .env loading utilities

    def _load_dotenv(self) -> None:
        """Populate os.environ from the first `.env` file found upward.

        Searches upward from current directory to filesystem root, loading
        `key=value` pairs, skipping comments and blanks. Existing env vars
        are not overwritten. Also maps `FMP_API_KEY` to `FSM_API_FMP_API_KEY`
        for backward compatibility.
        """
        try:
            import os
            from pathlib import Path

            current = Path.cwd()
            while True:
                candidate = current / ".env"
                if candidate.exists() and candidate.is_file():
                    try:
                        for raw_line in candidate.read_text().splitlines():
                            line = raw_line.strip()
                            # Skip blanks and comments
                            if not line or line.startswith("#"):
                                continue
                            if "=" not in line:
                                continue
                            key, value = line.split("=", 1)
                            key = key.strip()
                            # Remove any surrounding quotes from the value
                            value = value.strip().strip("'\"")
                            if key and key not in os.environ:
                                os.environ[key] = value
                        logger.debug("Loaded environment variables from %s", candidate)

                        # Special fallback: if a generic FMP_API_KEY is defined, expose it
                        # via the namespaced FSM_API_FMP_API_KEY expected by the Config
                        # model. This provides compatibility with existing environment
                        # setups without forcing users to duplicate variables.
                        if (
                            "FMP_API_KEY" in os.environ
                            and "FSM_API_FMP_API_KEY" not in os.environ
                        ):
                            os.environ["FSM_API_FMP_API_KEY"] = os.environ[
                                "FMP_API_KEY"
                            ]
                            logger.debug(
                                "Mapped FMP_API_KEY → FSM_API_FMP_API_KEY for config integration"
                            )
                        break  # Stop searching after the first .env file
                    except Exception as err:  # noqa: BLE001  (broad but safe here)
                        logger.warning(
                            "Failed to load .env file %s: %s", candidate, err
                        )
                else:
                    # Ascend to parent directory, stop at filesystem root
                    if current.parent == current:
                        break
                    current = current.parent
        except Exception as err:  # noqa: BLE001
            # Never fail config loading due to .env issues
            logger.debug("_load_dotenv encountered an error: %s", err, exc_info=False)


# Global configuration instance
_config_manager = ConfigManager()


def get_config() -> Config:
    """Get the global configuration singleton.

    Returns:
        The `Config` object managed by the global ConfigManager.

    Examples:
        >>> from fin_statement_model.config.manager import get_config
        >>> cfg = get_config()
        >>> cfg.logging.level
        'WARNING'
    """
    return _config_manager.get()


def update_config(updates: dict[str, Any]) -> None:
    """Apply runtime overrides to the global configuration.

    Args:
        updates: Nested dict of configuration keys and values to override.
    """
    _config_manager.update(updates)


# Removed reset() method: use context manager for test isolation



================================================================================
File: fin_statement_model/config/models.py
================================================================================

"""Define configuration schemas for fin_statement_model.

This module provides Pydantic models to validate and type-check the
application's configuration settings, including logging, I/O, forecasting,
preprocessing, display, API, metrics, validation, and statement options.

Examples:
    >>> from fin_statement_model.config.models import Config
    >>> config = Config()
    >>> config.logging.level
    'WARNING'
"""

from typing import Optional, Literal, Any, Union
from pathlib import Path
from pydantic import BaseModel, Field, field_validator, ConfigDict
from fin_statement_model.statements.configs.models import AdjustmentFilterSpec
from fin_statement_model.preprocessing.config import (
    StatementFormattingConfig,
    TransformationType,
)


class LoggingConfig(BaseModel):
    """Settings for library logging.

    Attributes:
        level (Literal): Default logging level ('DEBUG', 'INFO', 'WARNING', 'ERROR', or 'CRITICAL').
        format (str): Log message format string.
        detailed (bool): Enable detailed logging with file and line numbers.
        log_file_path (Optional[Path]): Path for rotating log files; None disables file logging.

    Example:
        >>> LoggingConfig(level='DEBUG').level
        'DEBUG'
    """

    level: Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"] = Field(
        "WARNING", description="Default logging level for the library"
    )
    format: str = Field(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        description="Log message format string",
    )
    detailed: bool = Field(
        False, description="Enable detailed logging with file and line numbers"
    )
    log_file_path: Optional[Path] = Field(
        None,
        description=(
            "If provided, logs are written to this path (rotating handler). "
            "If None, file logging is disabled."
        ),
    )

    model_config = ConfigDict(extra="forbid")


class IOConfig(BaseModel):
    """Configuration for input/output operations.

    Attributes:
        default_excel_sheet (str): Default sheet name for Excel operations.
        default_csv_delimiter (str): Default delimiter for CSV files.
        auto_create_output_dirs (bool): Automatically create output directories.
        validate_on_read (bool): Validate data when reading.
        default_mapping_configs_dir (Optional[Path]): Directory for mapping configs.
        auto_standardize_columns (bool): Standardize column names on read.
        skip_invalid_rows (bool): Skip rows with invalid data.
        strict_validation (bool): Enforce strict data validation on read.
    """

    default_excel_sheet: str = Field(
        "Sheet1", description="Default sheet name for Excel operations"
    )
    default_csv_delimiter: str = Field(
        ",", description="Default delimiter for CSV files"
    )
    auto_create_output_dirs: bool = Field(
        True, description="Automatically create output directories if they don't exist"
    )
    validate_on_read: bool = Field(True, description="Validate data on read operations")
    default_mapping_configs_dir: Optional[Path] = Field(
        None, description="Directory containing custom mapping configuration files"
    )
    auto_standardize_columns: bool = Field(
        True, description="Automatically standardize column names when reading data"
    )
    skip_invalid_rows: bool = Field(
        False, description="Skip rows with invalid data instead of raising errors"
    )
    strict_validation: bool = Field(
        False, description="Use strict validation when reading data"
    )

    model_config = ConfigDict(extra="forbid")


class ForecastingConfig(BaseModel):
    """Settings for forecasting behavior.

    Attributes:
        default_method (Literal): Default forecasting method ('simple', 'historical_growth', 'curve',
            'statistical', or 'ml').
        default_periods (int): Default number of periods to forecast.
        default_growth_rate (float): Default growth rate for simple forecasting.
        min_historical_periods (int): Minimum historical periods required.
        allow_negative_forecasts (bool): Allow negative forecast values.
        add_missing_periods (bool): Add missing forecast periods.
        default_bad_forecast_value (float): Value for invalid forecasts.
        continue_on_error (bool): Continue forecasting other nodes if one fails.
        historical_growth_aggregation (Literal['mean', 'median']): Aggregation method.
        random_seed (Optional[int]): Seed for statistical forecasting.
        base_period_strategy (Literal): Strategy for selecting base period.

    Example:
        >>> ForecastingConfig(default_periods=10).default_periods
        10
    """

    default_method: Literal[
        "simple", "historical_growth", "curve", "statistical", "ml"
    ] = Field("simple", description="Default forecasting method")
    default_periods: int = Field(5, description="Default number of periods to forecast")
    default_growth_rate: float = Field(
        0.0, description="Default growth rate for simple forecasting"
    )
    min_historical_periods: int = Field(
        3, description="Minimum historical periods required for forecasting"
    )
    allow_negative_forecasts: bool = Field(
        True, description="Allow negative values in forecasts"
    )
    add_missing_periods: bool = Field(
        True, description="Whether to add missing forecast periods to the graph"
    )
    default_bad_forecast_value: float = Field(
        0.0, description="Default value to use for NaN, Inf, or error forecasts"
    )
    continue_on_error: bool = Field(
        True,
        description="Whether to continue forecasting other nodes if one node fails",
    )
    historical_growth_aggregation: Literal["mean", "median"] = Field(
        "mean",
        description="Aggregation method for historical growth rate: 'mean' or 'median'",
    )
    random_seed: Optional[int] = Field(
        None,
        description="Random seed for statistical forecasting to ensure reproducible results",
    )
    base_period_strategy: Literal[
        "preferred_then_most_recent", "most_recent", "last_historical"
    ] = Field(
        "preferred_then_most_recent",
        description=(
            "Strategy for selecting base period: 'preferred_then_most_recent' (default), "
            "'most_recent' (ignore preferred, pick most recent with data), or "
            "'last_historical' (always use last historical period)."
        ),
    )

    @field_validator("default_periods")
    def validate_periods(cls, v: int) -> int:
        """Validate that `default_periods` is positive.

        Args:
            v (int): Number of periods.

        Returns:
            The validated period count.

        Raises:
            ValueError: If `v` is not positive.

        Example:
            >>> ForecastingConfig.validate_periods(5)
            5
        """
        if v <= 0:
            raise ValueError("default_periods must be positive")
        return v

    model_config = ConfigDict(extra="forbid")


class PreprocessingConfig(BaseModel):
    """Settings for data preprocessing operations.

    Attributes:
        auto_clean_data (bool): Automatically clean data on import.
        fill_missing_with_zero (bool): Fill missing values with zero.
        remove_empty_periods (bool): Remove periods with no data.
        standardize_period_format (bool): Standardize period name formats.
        default_normalization_type (Optional[Literal[...] ): Default normalization method.
        default_transformation_type (TransformationType): Default time series transformation.
        default_time_series_periods (int): Number of periods for transformations.
        default_time_series_window_size (int): Window size for transformations.
        default_conversion_aggregation (str): Aggregation method for period conversion.
        statement_formatting (StatementFormattingConfig): Formatting settings.
    """

    auto_clean_data: bool = Field(
        True, description="Automatically clean data on import"
    )
    fill_missing_with_zero: bool = Field(
        False, description="Fill missing values with zero instead of None"
    )
    remove_empty_periods: bool = Field(
        True, description="Remove periods with all empty values"
    )
    standardize_period_format: bool = Field(
        True, description="Standardize period names to consistent format"
    )
    default_normalization_type: Optional[
        Literal["percent_of", "minmax", "standard", "scale_by"]
    ] = Field(None, description="Default normalization method")
    default_transformation_type: TransformationType = Field(
        TransformationType.GROWTH_RATE,
        description="Default time series transformation type",
    )
    default_time_series_periods: int = Field(
        1, description="Default number of periods for time series transformations"
    )
    default_time_series_window_size: int = Field(
        3, description="Default window size for time series transformations"
    )
    default_conversion_aggregation: str = Field(
        "sum", description="Default aggregation method for period conversion"
    )
    statement_formatting: StatementFormattingConfig = Field(
        default=StatementFormattingConfig.model_validate({}),
        description="Default statement formatting configuration for preprocessing",
    )

    model_config = ConfigDict(extra="forbid")


class DisplayConfig(BaseModel):
    """Settings for formatting and displaying statement outputs.

    Attributes:
        default_number_format (str): Format string for numbers.
        default_currency_format (str): Format string for currency.
        default_percentage_format (str): Format string for percentages.
        hide_zero_rows (bool): Hide rows with all zero values.
        contra_display_style (Literal['parentheses', 'brackets','negative']): Style for contra items.
        thousands_separator (str): Character for thousands separator.
        decimal_separator (str): Character for decimal separator.
        default_units (str): Default currency or unit label.
        scale_factor (float): Scale factor applied to values.
        indent_character (str): Characters used for indentation.
        subtotal_style (str): Style keyword for subtotal rows.
        total_style (str): Style keyword for total rows.
        header_style (str): Style keyword for headers.
        contra_css_class (str): CSS class for contra items.
        show_negative_sign (bool): Show minus sign instead of parentheses.
        apply_sign_conventions (bool): Apply default sign conventions.
        include_empty_items (bool): Include items with no data.
        include_metadata_cols (bool): Include metadata columns.
        add_is_adjusted_column (bool): Add 'is_adjusted' column.
        include_units_column (bool): Add units column.
        include_css_classes (bool): Add CSS class column.
        include_notes_column (bool): Add notes column.
        apply_item_scaling (bool): Apply item-specific scaling.
        apply_item_formatting (bool): Apply item-specific formatting.
        apply_contra_formatting (bool): Apply contra-specific formatting.
        add_contra_indicator_column (bool): Add contra indicator column.

    Example:
        >>> DisplayConfig(default_number_format='.1%').default_number_format
        '.1%'
    """

    default_number_format: str = Field(
        ",.2f", description="Default number format string"
    )
    default_currency_format: str = Field(
        ",.2f", description="Default currency format string"
    )
    default_percentage_format: str = Field(
        ".1%", description="Default percentage format string"
    )
    hide_zero_rows: bool = Field(
        False, description="Hide rows where all values are zero"
    )
    contra_display_style: Literal["parentheses", "brackets", "negative"] = Field(
        "parentheses", description="How to display contra items"
    )
    thousands_separator: str = Field(",", description="Thousands separator character")
    decimal_separator: str = Field(".", description="Decimal separator character")
    default_units: str = Field("USD", description="Default currency/units for display")
    scale_factor: float = Field(
        1.0, description="Default scale factor for display (e.g., 0.001 for thousands)"
    )

    # --- New advanced formatting options ---
    indent_character: str = Field(
        "  ", description="Indentation characters used for nested line items"
    )
    subtotal_style: str = Field(
        "bold", description="CSS/markup style keyword for subtotal rows"
    )
    total_style: str = Field(
        "bold", description="CSS/markup style keyword for total rows"
    )
    header_style: str = Field(
        "bold", description="CSS/markup style keyword for header cells"
    )
    contra_css_class: str = Field(
        "contra-item", description="Default CSS class name for contra items"
    )
    show_negative_sign: bool = Field(
        True,
        description="Whether to prefix negative numbers with a minus sign when not using parentheses",
    )
    # Statement formatting defaults
    apply_sign_conventions: bool = Field(
        True, description="Whether to apply sign conventions by default"
    )
    include_empty_items: bool = Field(
        False, description="Whether to include items with no data by default"
    )
    include_metadata_cols: bool = Field(
        False, description="Whether to include metadata columns by default"
    )
    add_is_adjusted_column: bool = Field(
        False, description="Whether to add an 'is_adjusted' column by default"
    )
    include_units_column: bool = Field(
        False, description="Whether to include units column by default"
    )
    include_css_classes: bool = Field(
        False, description="Whether to include CSS class column by default"
    )
    include_notes_column: bool = Field(
        False, description="Whether to include notes column by default"
    )
    apply_item_scaling: bool = Field(
        True, description="Whether to apply item-specific scaling by default"
    )
    apply_item_formatting: bool = Field(
        True, description="Whether to apply item-specific formatting by default"
    )
    apply_contra_formatting: bool = Field(
        True, description="Whether to apply contra-specific formatting by default"
    )
    add_contra_indicator_column: bool = Field(
        False, description="Whether to add a contra indicator column by default"
    )

    @field_validator("scale_factor")
    def validate_scale_factor(cls, v: float) -> float:
        """Ensure scale factor is positive."""
        if v <= 0:
            raise ValueError("scale_factor must be positive")
        return v

    model_config = ConfigDict(extra="forbid")


class APIConfig(BaseModel):
    """Settings for external API integrations.

    Attributes:
        fmp_api_key (Optional[str]): Financial Modeling Prep API key.
        fmp_base_url (str): Base URL for FMP API.
        api_timeout (int): HTTP request timeout in seconds.
        api_retry_count (int): Number of retries for failed requests.
        cache_api_responses (bool): Cache API responses to reduce calls.
        cache_ttl_hours (int): Time-to-live for cache entries in hours.

    Example:
        >>> APIConfig(api_timeout=60).api_timeout
        60
    """

    fmp_api_key: Optional[str] = Field(
        None, description="Financial Modeling Prep API key"
    )
    fmp_base_url: str = Field(
        "https://financialmodelingprep.com/api/v3", description="FMP API base URL"
    )
    api_timeout: int = Field(30, description="API request timeout in seconds")
    api_retry_count: int = Field(
        3, description="Number of retries for failed API requests"
    )
    cache_api_responses: bool = Field(
        True, description="Cache API responses to reduce API calls"
    )
    cache_ttl_hours: int = Field(24, description="Cache time-to-live in hours")

    @field_validator("api_timeout", "api_retry_count", "cache_ttl_hours")
    def validate_positive(cls, v: int) -> int:
        """Ensure values are positive."""
        if v <= 0:
            raise ValueError("Value must be positive")
        return v

    model_config = ConfigDict(extra="forbid")


class MetricsConfig(BaseModel):
    """Settings for metric registry behavior.

    Attributes:
        custom_metrics_dir (Optional[Path]): Directory for custom metric definitions.
        validate_metric_inputs (bool): Check that metric input nodes exist.
        auto_register_metrics (bool): Auto-register metrics from definition files.
    """

    custom_metrics_dir: Optional[Path] = Field(
        None, description="Directory containing custom metric definitions"
    )
    validate_metric_inputs: bool = Field(
        True, description="Validate metric inputs exist in graph"
    )
    auto_register_metrics: bool = Field(
        True, description="Automatically register metrics from definition files"
    )

    model_config = ConfigDict(extra="forbid")


class ValidationConfig(BaseModel):
    """Settings for data validation within the graph.

    Attributes:
        strict_mode (bool): Enable strict validation mode.
        auto_standardize_names (bool): Standardize node names to canonical form.
        warn_on_non_standard (bool): Warn on non-standard names.
        check_balance_sheet_balance (bool): Validate Assets = Liabilities + Equity.
        balance_tolerance (float): Tolerance for balance sheet validation.
        warn_on_negative_assets (bool): Warn on negative asset values.
        validate_sign_conventions (bool): Enforce expected sign conventions.
    """

    strict_mode: bool = Field(False, description="Enable strict validation mode")
    auto_standardize_names: bool = Field(
        True, description="Automatically standardize node names to canonical form"
    )
    warn_on_non_standard: bool = Field(
        True, description="Warn when using non-standard node names"
    )
    check_balance_sheet_balance: bool = Field(
        True, description="Validate that Assets = Liabilities + Equity"
    )
    balance_tolerance: float = Field(
        1.0, description="Maximum acceptable difference for balance sheet validation"
    )
    warn_on_negative_assets: bool = Field(
        True, description="Warn when asset values are negative"
    )
    validate_sign_conventions: bool = Field(
        True, description="Validate that items follow expected sign conventions"
    )

    @field_validator("balance_tolerance")
    def validate_tolerance(cls, v: float) -> float:
        """Ensure tolerance is non-negative."""
        if v < 0:
            raise ValueError("balance_tolerance must be non-negative")
        return v

    model_config = ConfigDict(extra="forbid")


class StatementsConfig(BaseModel):
    """Settings for building and formatting financial statements.

    Attributes:
        default_adjustment_filter (Optional[Union[AdjustmentFilterSpec,list[str]]]): Default filter spec or tag list.
        enable_node_validation (bool): Enable node ID validation during building.
        node_validation_strict (bool): Treat validation failures as errors.
    """

    default_adjustment_filter: Optional[Union[AdjustmentFilterSpec, list[str]]] = Field(
        None,
        description="Default adjustment filter spec or list of tags to apply when building statements",
    )
    enable_node_validation: bool = Field(
        False,
        description="Whether to enable node ID validation during statement building by default",
    )
    node_validation_strict: bool = Field(
        False,
        description="Whether to treat node validation failures as errors (strict) by default",
    )

    model_config = ConfigDict(extra="forbid")


class Config(BaseModel):
    """Primary configuration container aggregating all sub-configurations.

    Attributes:
        logging (LoggingConfig): Logging configuration.
        io (IOConfig): I/O configuration.
        forecasting (ForecastingConfig): Forecasting settings.
        preprocessing (PreprocessingConfig): Data preprocessing settings.
        display (DisplayConfig): Display and formatting settings.
        api (APIConfig): API integration settings.
        metrics (MetricsConfig): Metrics registry settings.
        validation (ValidationConfig): Data validation settings.
        statements (StatementsConfig): Statement building settings.
        project_name (str): Identifier for the project.
        config_file_path (Optional[Path]): Path to the loaded config file.
        auto_save_config (bool): Auto-save overrides to file.

    Example:
        >>> from fin_statement_model.config.models import Config
        >>> config = Config()
        >>> config.to_dict()
        {...}
    """

    # Sub-configurations
    logging: LoggingConfig = Field(
        default=LoggingConfig.model_validate({}), description="Logging configuration"
    )
    io: IOConfig = Field(
        default=IOConfig.model_validate({}), description="Input/Output configuration"
    )
    forecasting: ForecastingConfig = Field(
        default=ForecastingConfig.model_validate({}),
        description="Forecasting configuration",
    )
    preprocessing: PreprocessingConfig = Field(
        default=PreprocessingConfig.model_validate({}),
        description="Data preprocessing configuration",
    )
    display: DisplayConfig = Field(
        default=DisplayConfig.model_validate({}),
        description="Display and formatting configuration",
    )
    api: APIConfig = Field(
        default=APIConfig.model_validate({}),
        description="API and external service configuration",
    )
    metrics: MetricsConfig = Field(
        default=MetricsConfig.model_validate({}), description="Metrics configuration"
    )
    validation: ValidationConfig = Field(
        default=ValidationConfig.model_validate({}),
        description="Data validation configuration",
    )
    statements: StatementsConfig = Field(
        default=StatementsConfig.model_validate({}),
        description="Statement structure and formatting configuration",
    )

    # Global settings
    project_name: str = Field(
        "fin_statement_model", description="Project name for identification"
    )
    config_file_path: Optional[Path] = Field(
        None, description="Path to user configuration file"
    )
    auto_save_config: bool = Field(
        False, description="Automatically save configuration changes to file"
    )

    model_config = ConfigDict(extra="forbid", validate_assignment=True)

    def to_dict(self) -> dict[str, Any]:
        """Convert configuration to a JSON-serializable dict.

        Uses Pydantic's `model_dump` in JSON mode to convert complex types
        to primitive representations.

        Returns:
            A dict with no None values and JSON-compatible types.

        Example:
            >>> isinstance(Config().to_dict(), dict)
            True
        """
        return self.model_dump(exclude_none=True, mode="json")

    def to_yaml(self) -> str:
        """Serialize configuration to a YAML formatted string.

        Returns:
            YAML string representing the configuration.

        Example:
            >>> yaml_str = Config().to_yaml()
            >>> 'logging' in yaml_str
            True
        """
        import yaml

        return yaml.dump(self.to_dict(), default_flow_style=False, sort_keys=True)

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "Config":
        """Instantiate Config from a dictionary.

        Args:
            data: Dictionary with configuration values.

        Returns:
            A validated Config instance.

        Example:
            >>> data = {'project_name': 'test'}
            >>> isinstance(Config.from_dict(data), Config)
            True
        """
        return cls(**data)

    @classmethod
    def from_yaml(cls, yaml_str: str) -> "Config":
        """Instantiate Config from a YAML-formatted string.

        Args:
            yaml_str: YAML string containing configuration.

        Returns:
            A validated Config instance.

        Example:
            >>> yaml_str = Config().to_yaml()
            >>> isinstance(Config.from_yaml(yaml_str), Config)
            True
        """
        import yaml

        data = yaml.safe_load(yaml_str)
        return cls.from_dict(data)

    @classmethod
    def from_file(cls, path: Path) -> "Config":
        """Load and parse configuration from a file.

        Args:
            path: Path to a .yaml, .yml, or .json configuration file.

        Returns:
            A validated Config instance.

        Raises:
            ValueError: If the file suffix is not supported.

        Example:
            >>> from pathlib import Path
            >>> config = Config.from_file(Path('config.yaml'))
            >>> isinstance(config, Config)
            True
        """
        if path.suffix in [".yaml", ".yml"]:
            return cls.from_yaml(path.read_text())
        elif path.suffix == ".json":
            import json

            return cls.from_dict(json.loads(path.read_text()))
        else:
            raise ValueError(f"Unsupported config file format: {path.suffix}")



================================================================================
File: fin_statement_model/core/__init__.py
================================================================================

"""Foundation of *fin_statement_model* — graph engine, nodes, calculations & more.

The **core** package is intentionally self-contained (nothing here imports from
`statements/`, `io/`, or `extensions/`).  It provides the primitives that
higher-level layers build upon.

Sub-packages / key modules:

* `graph/` – directed-graph data structure (`Graph`, `GraphManipulator`, `GraphTraverser`).
* `nodes/` – raw data, calculation, statistical and forecast nodes plus helpers.
* `calculations/` – strategy objects implementing arithmetic and formula logic, with a global `Registry`.
* `metrics/` – YAML-driven metric definitions, registry and interpretation helpers.
* `adjustments/` – models and manager for discretionary adjustments & scenario analysis.
* `node_factory.py` – convenience factory for programmatic or YAML-based node creation.
* `errors.py` – unified exception hierarchy rooted at `FinancialModelError`.

Example:

```python
from fin_statement_model.core import Graph

# Build a simple two-period graph
g = Graph(periods=["2023", "2024"])
_ = g.add_financial_statement_item("Revenue", {"2023": 1_000, "2024": 1_200})
_ = g.add_financial_statement_item("COGS",    {"2023":   600, "2024":   720})

# Add a calculation node: gross profit = revenue − COGS
g.add_calculation(
    name="GrossProfit",
    input_names=["Revenue", "COGS"],
    operation_type="subtraction",
)

# Fetch a built-in metric (requires additional inputs to be present)
# g.add_metric("current_ratio")

print(g.calculate("GrossProfit", "2024"))  # -> 480.0
```

Refer to `core/README.md` for a deeper dive into each component.
"""

from .node_factory import NodeFactory
from .graph import Graph
from .nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    YoYGrowthNode,
    MultiPeriodStatNode,
    FormulaCalculationNode,
    CustomCalculationNode,
    TwoPeriodAverageNode,
)
from .calculations import (
    AdditionCalculation,
    SubtractionCalculation,
    MultiplicationCalculation,
    DivisionCalculation,
)
from .errors import (
    FinancialModelError,
    ConfigurationError,
    CalculationError,
    NodeError,
    GraphError,
    DataValidationError,
    CircularDependencyError,
    PeriodError,
    StatementError,
    StrategyError,
    TransformationError,
)

__all__ = [
    "AdditionCalculation",
    "CalculationError",
    "CalculationNode",
    "CircularDependencyError",
    "ConfigurationError",
    "CustomCalculationNode",
    "DataValidationError",
    "DivisionCalculation",
    "FinancialModelError",
    "FinancialStatementItemNode",
    "FormulaCalculationNode",
    "Graph",
    "GraphError",
    "MultiPeriodStatNode",
    "MultiplicationCalculation",
    "Node",
    "NodeError",
    "NodeFactory",
    "PeriodError",
    "StatementError",
    "StrategyError",
    "SubtractionCalculation",
    "TransformationError",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]



================================================================================
File: fin_statement_model/core/adjustments/__init__.py
================================================================================

"""Initialize core adjustments subpackage.

This module exposes core adjustment models, filters, and related utilities.
"""

from .models import (
    Adjustment,
    AdjustmentFilter,
    AdjustmentFilterInput,
    AdjustmentType,
    DEFAULT_SCENARIO,
)

__all__ = [
    "DEFAULT_SCENARIO",
    "Adjustment",
    "AdjustmentFilter",
    "AdjustmentFilterInput",
    "AdjustmentType",
]



================================================================================
File: fin_statement_model/core/adjustments/analytics.py
================================================================================

"""Analytics functions for summarizing and analyzing adjustments."""

import logging
from typing import Optional, Union
from collections.abc import Callable

import pandas as pd

from fin_statement_model.core.adjustments.manager import AdjustmentManager
from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentFilter,
    AdjustmentTag,
)
from fin_statement_model.core.adjustments.helpers import tag_matches

logger = logging.getLogger(__name__)


def _filter_adjustments_static(
    all_adjustments: list[Adjustment],
    filter_input: Optional[
        Union[AdjustmentFilter, set[AdjustmentTag], Callable[[Adjustment], bool]]
    ],
) -> list[Adjustment]:
    """Apply filtering to adjustments based on filter_input, excluding period context.

    This helper function centralizes the common filtering logic used by both
    summary() and list_by_tag() methods. It applies filters based on the static
    properties of adjustments (tags, scenario, type, etc.) but ignores any
    period-based filtering since that requires runtime context.

    Args:
        all_adjustments: List of all adjustments to filter.
        filter_input: Filter criteria (AdjustmentFilter, set of tags, callable, or None).

    Returns:
        Filtered list of adjustments matching the filter criteria.
    """
    if filter_input is None:
        # No filter means include all adjustments
        logger.debug("No filter applied.")
        return all_adjustments

    elif isinstance(filter_input, AdjustmentFilter):
        # Apply filter, ignoring its period attribute
        temp_filter = filter_input.model_copy(update={"period": None})
        filtered = [adj for adj in all_adjustments if temp_filter.matches(adj)]
        logger.debug(
            f"Applied AdjustmentFilter (ignoring period). Filter: {temp_filter}"
        )
        return filtered

    elif isinstance(filter_input, set):
        # Shorthand for include_tags
        filtered = [
            adj for adj in all_adjustments if tag_matches(adj.tags, filter_input)
        ]
        logger.debug(f"Applied tag filter. Tags: {filter_input}")
        return filtered

    elif callable(filter_input):
        filtered = [adj for adj in all_adjustments if filter_input(adj)]
        logger.debug("Applied callable filter.")
        return filtered

    else:
        # Should not happen due to type hint, but defensive
        logger.warning(
            f"Invalid filter_input type: {type(filter_input)}. No filtering applied."
        )
        return all_adjustments


def summary(
    manager: AdjustmentManager,
    filter_input: Optional[
        Union[AdjustmentFilter, set[AdjustmentTag], Callable[[Adjustment], bool]]
    ] = None,
    group_by: list[str] = ["period", "node_name"],
) -> pd.DataFrame:
    """Generate a summary DataFrame of adjustments, optionally filtered and grouped.

    Calculates count, sum of values, and mean of absolute values for adjustments.

    Args:
        manager: The AdjustmentManager instance containing the adjustments.
        filter_input: Optional filter criteria (AdjustmentFilter, set of tags, callable, or None)
                      to apply before summarizing.
        group_by: List of Adjustment attributes to group the summary by.
                  Defaults to ["period", "node_name"]. Valid fields include
                  'period', 'node_name', 'scenario', 'type', 'user'.

    Returns:
        A pandas DataFrame with the summary statistics (count, sum, mean_abs_value)
        indexed by the specified group_by columns.

    Examples:
        >>> manager = AdjustmentManager()
        >>> df = summary(manager)
        >>> df.index.names == ["period", "node_name"]
        True
    """
    logger.debug(f"Generating adjustment summary, grouping by: {group_by}")

    # Get all adjustments first
    # TODO: Optimization - If filtering is very restrictive, could filter first.
    # However, filtering requires period context which isn't directly available here.
    # Get all adjustments and filter based on the filter_input's static properties.
    # The period-based filtering (effective window) cannot be applied generically here.
    all_adjustments = manager.get_all_adjustments()

    # Apply filtering using the helper function
    filtered_adjustments = _filter_adjustments_static(all_adjustments, filter_input)

    if not filtered_adjustments:
        logger.info("No adjustments found matching the filter criteria for summary.")
        # Return empty DataFrame with expected columns
        cols = [*group_by, "count", "sum_value", "mean_abs_value"]
        return pd.DataFrame(columns=cols).set_index(group_by)

    # Convert to DataFrame for easier aggregation
    adj_data = [
        adj.model_dump(
            include=set([*group_by, "value"])
        )  # Include value for aggregation
        for adj in filtered_adjustments
    ]
    df = pd.DataFrame(adj_data)

    # Ensure correct types for grouping columns if needed (e.g., type as string)
    if "type" in group_by:
        df["type"] = df["type"].astype(str)

    # Add absolute value for mean calculation
    df["abs_value"] = df["value"].abs()

    # Perform aggregation
    summary_df = df.groupby(group_by).agg(
        count=("value", "size"),
        sum_value=("value", "sum"),
        mean_abs_value=("abs_value", "mean"),
    )

    logger.info(f"Generated adjustment summary with {len(summary_df)} groups.")
    return summary_df


def list_by_tag(
    manager: AdjustmentManager,
    tag_prefix: str,
    filter_input: Optional[
        Union[AdjustmentFilter, set[AdjustmentTag], Callable[[Adjustment], bool]]
    ] = None,
) -> list[Adjustment]:
    """List all adjustments matching a tag prefix, optionally applying further filters.

    Args:
        manager: The AdjustmentManager instance containing the adjustments.
        tag_prefix: The tag prefix string to match (e.g., "NonRecurring").
        filter_input: Optional additional filter criteria (AdjustmentFilter, set of tags,
                      callable, or None).

    Returns:
        A list of Adjustment objects that have at least one tag starting with
        the tag_prefix and also match the optional filter_input.

    Examples:
        >>> manager = AdjustmentManager()
        >>> adjustments = list_by_tag(manager, "NonRecurring")
        >>> all("NonRecurring" in tag for adj in adjustments for tag in adj.tags)
        True
    """
    logger.debug(f"Listing adjustments by tag prefix: '{tag_prefix}'")

    # Get all adjustments and apply filters using the helper function
    all_adjustments = manager.get_all_adjustments()
    filtered_adjustments = _filter_adjustments_static(all_adjustments, filter_input)

    # Apply the primary tag prefix filter
    prefix_set = {tag_prefix}
    final_list = [
        adj for adj in filtered_adjustments if tag_matches(adj.tags, prefix_set)
    ]

    logger.info(
        f"Found {len(final_list)} adjustments matching prefix '{tag_prefix}' and other filters."
    )
    # Sort by priority/timestamp for consistent output
    return sorted(final_list, key=lambda x: (x.priority, x.timestamp))



================================================================================
File: fin_statement_model/core/adjustments/helpers.py
================================================================================

"""Helper functions for the adjustments module."""


def tag_matches(target_tags: set[str], prefixes: set[str]) -> bool:
    """Check if any target tag starts with any of the given prefixes.

    Allows for hierarchical matching: a prefix "A/B" matches tag "A/B/C".
    A simple prefix "A" matches tag "A/B".

    Args:
        target_tags: The set of tags on an adjustment.
        prefixes: The set of prefixes to check against (e.g., from a filter).

    Returns:
        True if at least one tag in target_tags starts with at least one
        prefix in prefixes, False otherwise.

    Examples:
        >>> tag_matches({'A/B/C', 'X'}, {'A/B'})
        True
        >>> tag_matches({'A/B/C'}, {'D'})
        False
    """
    if not prefixes:  # Optimization: if no prefixes specified, it can't match
        return False
    if not target_tags:  # Optimization: if no tags exist, it can't match
        return False

    # Check if any combination of tag and prefix matches
    return any(t.startswith(p) for t in target_tags for p in prefixes)



================================================================================
File: fin_statement_model/core/adjustments/manager.py
================================================================================

"""Manages the storage, retrieval, and application of adjustments."""

from __future__ import annotations

import logging
import inspect
from collections import defaultdict
from typing import Optional
from uuid import UUID

from .models import (
    Adjustment,
    AdjustmentFilter,
    AdjustmentFilterInput,
    AdjustmentType,
    DEFAULT_SCENARIO,
)


logger = logging.getLogger(__name__)


class AdjustmentManager:
    """Handle storage, retrieval, and application of adjustments.

    This class provides methods to add, remove, filter, and apply adjustments
    to base values.

    Methods:
        add_adjustment: Add an adjustment, replacing any existing one with the same ID.
        remove_adjustment: Remove an adjustment by ID.
        apply_adjustments: Apply a series of adjustments to a base value.
        get_adjustments: Retrieve adjustments for a node and period.
        get_filtered_adjustments: Retrieve adjustments matching filter criteria.
        get_all_adjustments: List all adjustments.
        clear_all: Remove all adjustments.
        load_adjustments: Load a new list of adjustments.
    """

    def __init__(self) -> None:
        """Initialize the adjustment manager with empty storage.

        This sets up internal indices for storing and retrieving adjustments.
        """
        # Primary index: (scenario, node_name, period) -> list[Adjustment]
        self._by_location: dict[tuple[str, str, str], list[Adjustment]] = defaultdict(
            list
        )
        # Secondary index for quick lookup and removal by ID
        self._by_id: dict[UUID, Adjustment] = {}

    def add_adjustment(self, adj: Adjustment) -> None:
        """Add an adjustment to the manager.

        If an adjustment with the same ID already exists, it is replaced.

        Args:
            adj: The Adjustment object to add.

        Returns:
            None
        """
        # If an adjustment with the same ID already exists, remove it first
        if adj.id in self._by_id:
            self.remove_adjustment(adj.id)

        self._by_id[adj.id] = adj
        key = (adj.scenario, adj.node_name, adj.period)
        self._by_location[key].append(adj)
        # Keep the list sorted by priority, then timestamp for consistent application order
        self._by_location[key].sort(key=lambda x: (x.priority, x.timestamp))

    def remove_adjustment(self, adj_id: UUID) -> bool:
        """Remove an adjustment by its ID.

        Args:
            adj_id: The UUID of the adjustment to remove.

        Returns:
            True if the adjustment was found and removed, False otherwise.
        """
        if adj_id not in self._by_id:
            return False

        adj_to_remove = self._by_id.pop(adj_id)
        key = (adj_to_remove.scenario, adj_to_remove.node_name, adj_to_remove.period)

        if key in self._by_location:
            # Filter out the specific adjustment object instance
            self._by_location[key] = [
                a for a in self._by_location[key] if a.id != adj_id
            ]
            # If the list becomes empty, remove the key
            if not self._by_location[key]:
                del self._by_location[key]
        return True

    def _apply_one(self, base_value: float, adj: Adjustment) -> float:
        """Apply a single adjustment based on its type and scale.

        Args:
            base_value: The original numeric value.
            adj: The Adjustment object to apply.

        Returns:
            The adjusted value as a float.
        """
        if adj.type == AdjustmentType.ADDITIVE:
            # Ensuring result is float
            return float(base_value + adj.value * adj.scale)
        elif adj.type == AdjustmentType.MULTIPLICATIVE:
            # Ensure base_value is not zero to avoid issues with 0**(negative scale)
            # If base is 0, multiplicative adjustment usually results in 0 unless value is 0.
            # We also need to handle potential complex numbers if base is negative and scale is fractional.
            # For simplicity, let's assume standard financial contexts where this is less common
            # or handle it by convention (e.g., multiplicative doesn't apply to zero/negative base).
            # Let's default to returning 0 if base is 0 for multiplicative.
            if base_value == 0:
                return 0.0
            # Consider adding checks or specific handling for negative base + fractional scale if needed.
            # Cast to float after exponentiation and multiplication
            return float(base_value * (adj.value**adj.scale))
        elif adj.type == AdjustmentType.REPLACEMENT:
            # Scale is ignored for replacement type as per spec
            # Cast to float to satisfy return type
            return float(adj.value)
        else:
            # Should not happen with Enum, but defensively return base value
            return base_value  # pragma: no cover

    def apply_adjustments(
        self, base_value: float, adjustments: list[Adjustment]
    ) -> tuple[float, bool]:
        """Apply a list of adjustments sequentially to a base value.

        Adjustments are applied in order of priority (lower first), then timestamp.

        Args:
            base_value: The starting value before adjustments.
            adjustments: A list of Adjustment objects to apply.

        Returns:
            A tuple of (final adjusted value, boolean indicating if any adjustment was applied).
        """
        if not adjustments:
            return base_value, False

        current_value = base_value
        applied_flag = False

        # Sort by priority (ascending), then timestamp (ascending) as per spec
        # Note: add_adjustment already sorts the list in _by_location, but
        # this ensures correctness if an unsorted list is passed directly.
        sorted_adjustments = sorted(
            adjustments, key=lambda x: (x.priority, x.timestamp)
        )

        for adj in sorted_adjustments:
            current_value = self._apply_one(current_value, adj)
            applied_flag = True

        return current_value, applied_flag

    def get_adjustments(
        self, node_name: str, period: str, *, scenario: str = DEFAULT_SCENARIO
    ) -> list[Adjustment]:
        """Retrieve all adjustments for a specific node, period, and scenario.

        Args:
            node_name: The name of the target node.
            period: The period to retrieve adjustments for.
            scenario: The scenario name to filter adjustments by.

        Returns:
            A list of Adjustment objects for the specified node, period, and scenario.
        """
        key = (scenario, node_name, period)
        # Return a copy to prevent external modification of the internal list
        return list(self._by_location.get(key, []))

    def _normalize_filter(
        self, filter_input: AdjustmentFilterInput, period: Optional[str] = None
    ) -> AdjustmentFilter:
        """Convert flexible filter input into a baseline AdjustmentFilter instance.

        Args:
            filter_input: Criteria for selecting adjustments. Can be:
                - None: use default filter (only default scenario).
                - AdjustmentFilter: existing filter instance.
                - set of tags: shorthand for include_tags filter.
                - Callable[..., bool]: predicate accepting one or two args
                  (Adjustment[, period]).
            period: The period context for the filter (optional).

        Returns:
            A baseline AdjustmentFilter object for scenario and period checks.
        """
        if filter_input is None:
            # Default filter includes only the default scenario and sets the period context
            return AdjustmentFilter(include_scenarios={DEFAULT_SCENARIO}, period=period)
        elif isinstance(filter_input, AdjustmentFilter):
            # If period context wasn't set on the filter, set it now
            if filter_input.period is None:
                return filter_input.model_copy(update={"period": period})
            return filter_input
        elif isinstance(filter_input, set):
            # Shorthand for include_tags filter
            # Assume shorthand applies only to DEFAULT_SCENARIO unless specified otherwise?
            # Let's keep it simple: Shorthand applies to DEFAULT_SCENARIO only.
            return AdjustmentFilter(
                include_tags=filter_input,
                include_scenarios={DEFAULT_SCENARIO},
                period=period,
            )
        elif callable(filter_input):
            # For callable predicates we still construct a baseline AdjustmentFilter so
            # that core scenario / period checks remain in place. The callable itself
            # will be evaluated later in `get_filtered_adjustments`. We purposefully do
            # not restrict `include_scenarios` here – the caller can implement any
            # scenario logic inside the predicate if desired.
            return AdjustmentFilter(period=period)
        else:
            raise TypeError(f"Invalid filter_input type: {type(filter_input)}")

    def get_filtered_adjustments(
        self, node_name: str, period: str, filter_input: AdjustmentFilterInput = None
    ) -> list[Adjustment]:
        """Retrieve adjustments for a node and period that match given filter criteria.

        Args:
            node_name: The target node name.
            period: The period to retrieve adjustments for.
            filter_input: Criteria for selecting adjustments. Can be:
                - None: applies default filter (default scenario, all adjustments).
                - AdjustmentFilter: filter by scenarios, tags, types, and period window.
                - set of tags: shorthand for include_tags filter.
                - Callable[[Adjustment], bool] or Callable[[Adjustment, str], bool]:
                    predicate to select adjustments. Two-arg predicates receive
                    the current period as the second argument.

        Returns:
            A list of matching Adjustment objects sorted by priority and timestamp.
        """
        normalized_filter = self._normalize_filter(filter_input, period)

        candidate_adjustments: list[Adjustment] = []

        # Determine which scenarios to check based on the filter
        scenarios_to_check: set[str]
        if normalized_filter.include_scenarios is not None:
            scenarios_to_check = (
                normalized_filter.include_scenarios.copy()
            )  # Work on a copy
            if normalized_filter.exclude_scenarios is not None:
                scenarios_to_check -= normalized_filter.exclude_scenarios
        elif normalized_filter.exclude_scenarios is not None:
            # Get all scenarios currently known to the manager
            all_known_scenarios = {adj.scenario for adj in self._by_id.values()}
            scenarios_to_check = (
                all_known_scenarios - normalized_filter.exclude_scenarios
            )
        else:
            # No include/exclude specified: check all scenarios relevant for this node/period
            # This requires checking keys in _by_location
            scenarios_to_check = {
                key[0]
                for key in self._by_location
                if key[1] == node_name and key[2] == period
            }
            # If no specific adjustments exist for this node/period, we might check default?
            # Let's assume we only check scenarios that *have* adjustments for this location.
            if not scenarios_to_check:
                # Maybe return empty list early if no scenarios found for location?
                # Or should it behave differently? For now, proceed with empty set.
                pass

        # Gather candidates from relevant locations
        for scenario in scenarios_to_check:
            key = (scenario, node_name, period)
            candidate_adjustments.extend(self._by_location.get(key, []))

        # Apply the filter logic
        matching_adjustments: list[Adjustment] = []
        if callable(filter_input):
            # Determine how many positional arguments the predicate expects. If it
            # accepts two parameters we pass the current period as contextual
            # information. This allows users to write filters that combine
            # adjustment attributes with the calculation period in their logic.
            try:
                param_count = len(inspect.signature(filter_input).parameters)
            except (TypeError, ValueError):
                # Fallback in case the predicate is not introspectable (e.g., built-ins)
                param_count = 1

            if param_count == 1:
                matching_adjustments = [
                    adj
                    for adj in candidate_adjustments
                    if filter_input(adj) and normalized_filter.matches(adj)
                ]
            elif param_count == 2:
                matching_adjustments = [
                    adj
                    for adj in candidate_adjustments
                    if filter_input(adj, period) and normalized_filter.matches(adj)
                ]
            else:
                raise TypeError(
                    "Callable adjustment filter must accept one or two positional arguments"
                )
        else:
            # Apply the normalized AdjustmentFilter's matches method
            matching_adjustments = [
                adj for adj in candidate_adjustments if normalized_filter.matches(adj)
            ]

        # Return sorted list (sorting might be redundant if fetched lists are pre-sorted
        # and filtering maintains order, but ensures correctness)
        return sorted(matching_adjustments, key=lambda x: (x.priority, x.timestamp))

    def get_all_adjustments(self) -> list[Adjustment]:
        """List all adjustments stored in the manager.

        Returns:
            A list of all Adjustment objects currently stored.
        """
        # Return a copy to prevent external modification
        return list(self._by_id.values())

    def clear_all(self) -> None:
        """Remove all adjustments from the manager.

        Clears all internal storage.

        Returns:
            None
        """
        self._by_location.clear()
        self._by_id.clear()

    def load_adjustments(self, adjustments: list[Adjustment]) -> None:
        """Clear existing adjustments and load a new list of adjustments.

        Args:
            adjustments: A list of Adjustment objects to load into the manager.

        Returns:
            None
        """
        self.clear_all()
        for adj in adjustments:
            self.add_adjustment(adj)



================================================================================
File: fin_statement_model/core/adjustments/models.py
================================================================================

"""Define core adjustment data models and related types.

This module provides the Adjustment and AdjustmentFilter Pydantic models,
as well as related constants and enums.
"""

from __future__ import annotations

import uuid
from datetime import datetime
from enum import Enum
from typing import Final, Optional
from collections.abc import Callable

from pydantic import BaseModel, ConfigDict, Field, field_validator
import logging

from .helpers import tag_matches

logger = logging.getLogger(__name__)

# --------------------------------------------------------------------
# Core Types and Constants
# --------------------------------------------------------------------


class AdjustmentType(Enum):
    """Defines how an adjustment modifies a base value."""

    ADDITIVE = "additive"  # base + (value * scale)
    MULTIPLICATIVE = "multiplicative"  # base * (value ** scale)
    REPLACEMENT = "replacement"  # use value (scale ignored)


AdjustmentTag = str  # Slash (/) separates hierarchy levels in tags
DEFAULT_SCENARIO: Final[str] = "default"

# --------------------------------------------------------------------
# Adjustment Model
# --------------------------------------------------------------------


class Adjustment(BaseModel):
    """Immutable record describing a discretionary adjustment to a node's value.

    Each adjustment can modify a base value by addition, multiplication, or replacement.

    Attributes:
        id: Unique identifier for the adjustment.
        node_name: Name of the target node.
        period: Primary period the adjustment applies to.
        start_period: Optional start of the effective period range.
        end_period: Optional end of the effective period range.
        value: Numeric value of the adjustment.
        type: AdjustmentType defining how the adjustment is applied.
        scale: Attenuation factor between 0.0 and 1.0.
        priority: Tie-breaker for multiple adjustments (lower first).
        tags: Set of tags for filtering and analysis.
        scenario: Scenario name grouping the adjustment.
        reason: Description of the adjustment.
        user: Identifier of the user who created the adjustment.
        timestamp: UTC timestamp when the adjustment was created.

    Examples:
        >>> adj = Adjustment(
        ...     node_name='Revenue',
        ...     period='2023-01',
        ...     value=100.0,
        ...     reason='Manual update'
        ... )
        >>> adj.type == AdjustmentType.ADDITIVE
        True
    """

    # Target
    id: uuid.UUID = Field(default_factory=uuid.uuid4)
    node_name: str
    period: str  # Primary target period
    start_period: Optional[str] = None  # Phase 2 - effective range start (inclusive)
    end_period: Optional[str] = None  # Phase 2 - effective range end (inclusive)

    # Behaviour
    value: float
    type: AdjustmentType = AdjustmentType.ADDITIVE
    scale: float = 1.0  # Phase 2 - 0.0 <= scale <= 1.0
    priority: int = 0  # Lower value means higher priority (applied first)

    # Classification
    tags: set[AdjustmentTag] = Field(default_factory=set)
    scenario: str = DEFAULT_SCENARIO  # Phase 2 - Scenario grouping

    # Metadata
    reason: str
    user: Optional[str] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)

    model_config = ConfigDict(frozen=True)

    @field_validator("scale")
    @classmethod
    def _scale_bounds(cls, v: float) -> float:
        """Validate that the scale factor is between 0.0 and 1.0."""
        if not 0.0 <= v <= 1.0:
            raise ValueError("Scale must be between 0.0 and 1.0 (inclusive)")
        return v


# --------------------------------------------------------------------
# Adjustment Filtering
# --------------------------------------------------------------------


class AdjustmentFilter(BaseModel):
    """Define criteria for selecting adjustments.

    Attributes:
        include_scenarios: Set of scenarios to include.
        exclude_scenarios: Set of scenarios to exclude.
        include_tags: Set of tag prefixes to include.
        exclude_tags: Set of tag prefixes to exclude.
        require_all_tags: Set of tags that must all be present.
        include_types: Set of AdjustmentType to include.
        exclude_types: Set of AdjustmentType to exclude.
        period: Optional period for effective window checks.

    Examples:
        >>> filt = AdjustmentFilter(include_tags={'NonRecurring'}, period='2023-01')
        >>> filt.matches(adj)
        True
    """

    # Scenario Filtering
    include_scenarios: Optional[set[str]] = None
    exclude_scenarios: Optional[set[str]] = None

    # Tag Filtering (supports hierarchical matching via helpers.tag_matches)
    # Need to import the helper function first.
    # Let's assume it will be imported at the top level of the module later.

    include_tags: Optional[set[AdjustmentTag]] = None
    exclude_tags: Optional[set[AdjustmentTag]] = None
    require_all_tags: Optional[set[AdjustmentTag]] = None  # Exact match required

    # Type Filtering
    include_types: Optional[set[AdjustmentType]] = None
    exclude_types: Optional[set[AdjustmentType]] = None

    # Context for Effective Window Checks (Phase 2)
    period: Optional[str] = None  # The current period being calculated/viewed

    def matches(self, adj: Adjustment) -> bool:
        """Check whether a given adjustment meets the filter criteria.

        Args:
            adj: The Adjustment instance to test.

        Returns:
            True if the adjustment matches all criteria, False otherwise.

        Examples:
            >>> filt = AdjustmentFilter(include_types={AdjustmentType.ADDITIVE})
            >>> filt.matches(adj)
            True
        """
        # Need to import the helper function here to avoid circular dependency issues at module level

        # Start assuming it matches, then progressively set to False if any check fails.
        is_match = True

        # --- Scenario Checks ---
        if (
            self.include_scenarios is not None
            and adj.scenario not in self.include_scenarios
        ) or (
            self.exclude_scenarios is not None
            and adj.scenario in self.exclude_scenarios
        ):
            is_match = False

        # --- Tag Checks ---
        # Only check if still potentially a match
        if is_match and (
            (
                self.include_tags is not None
                and not tag_matches(adj.tags, self.include_tags)
            )
            or (
                self.exclude_tags is not None
                and tag_matches(adj.tags, self.exclude_tags)
            )
            or (
                self.require_all_tags is not None
                and not self.require_all_tags.issubset(adj.tags)
            )
        ):
            is_match = False

        # --- Type Checks ---
        # Only check if still potentially a match
        if is_match and (
            (self.include_types is not None and adj.type not in self.include_types)
            or (self.exclude_types is not None and adj.type in self.exclude_types)
        ):
            is_match = False

        # --- Effective Window Check (Phase 2) ---
        # Assumes periods are sortable strings (e.g., 'YYYY-MM' or 'Q1-2023')
        if (
            is_match and self.period is not None
        ):  # Only check if still potentially a match
            logger.debug(
                f"Period check: FilterPeriod={self.period}, AdjStart={adj.start_period}, AdjEnd={adj.end_period}"
            )
            period_match = True  # Assume period is ok unless proven otherwise
            if adj.start_period is not None and self.period < adj.start_period:
                logger.debug("Period check failed: FilterPeriod < AdjStart")
                period_match = False
            # Use 'if period_match' to avoid unnecessary log if start check already failed
            if (
                period_match
                and adj.end_period is not None
                and self.period > adj.end_period
            ):
                logger.debug("Period check failed: FilterPeriod > AdjEnd")
                period_match = False

            if period_match:
                logger.debug("Period check passed.")
            else:
                is_match = False  # Period check failed
        # else: # Optional log if period check was skipped
        #     if self.period is not None:
        #         logger.debug("Period check skipped because is_match was already False")
        #     else:
        #         logger.debug("Period check skipped: Filter has no period context.")

        return is_match


# Accept callables that take one or two positional arguments. Using Callable[..., bool]
# allows flexible predicates while keeping type safety at a reasonable level.
AdjustmentFilterInput = Optional[
    AdjustmentFilter | set[AdjustmentTag] | Callable[..., bool]
]



================================================================================
File: fin_statement_model/core/calculations/__init__.py
================================================================================

"""Calculations module for the Financial Statement Model.

This module provides classes for implementing the Calculation Pattern for calculations
in the Financial Statement Model. It allows different calculation algorithms to be
defined, registered, and applied to financial data.
"""

from .calculation import (
    Calculation,
    AdditionCalculation,
    SubtractionCalculation,
    MultiplicationCalculation,
    DivisionCalculation,
    WeightedAverageCalculation,
    CustomFormulaCalculation,
    FormulaCalculation,
)
from .registry import Registry

# Register calculations
Registry.register(AdditionCalculation)
Registry.register(SubtractionCalculation)
Registry.register(MultiplicationCalculation)
Registry.register(DivisionCalculation)
Registry.register(WeightedAverageCalculation)
Registry.register(CustomFormulaCalculation)
Registry.register(FormulaCalculation)

__all__ = [
    "AdditionCalculation",
    "Calculation",
    "CustomFormulaCalculation",
    "DivisionCalculation",
    "FormulaCalculation",
    "MultiplicationCalculation",
    "Registry",
    "SubtractionCalculation",
    "WeightedAverageCalculation",
]



================================================================================
File: fin_statement_model/core/calculations/calculation.py
================================================================================

"""Calculation for the Financial Statement Model.

This module provides the Calculation Pattern implementation for calculations,
allowing different calculation types to be encapsulated in calculation classes.
"""

from abc import ABC, abstractmethod
import ast
import logging
import operator
from typing import Optional, ClassVar, Any, Type
from collections.abc import Callable

from fin_statement_model.core.nodes.base import Node  # Absolute
from fin_statement_model.core.errors import CalculationError, StrategyError

# Configure logging
logger = logging.getLogger(__name__)


class Calculation(ABC):
    """Abstract base class for all calculations.

    This class defines the interface that all concrete calculation classes must
    implement. It employs a calculation pattern, allowing the algorithm
    used by a CalculationNode to be selected at runtime.

    Each concrete calculation encapsulates a specific method for computing a
    financial value based on a list of input nodes and a given time period.
    """

    @abstractmethod
    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculate a value based on input nodes for a specific period.

        This abstract method must be implemented by all concrete calculation classes.
        It defines the core logic for the calculation.

        Args:
            inputs: A list of input Node objects whose values will be used in
                the calculation.
            period: The time period string (e.g., "2023Q1") for which the
                calculation should be performed.

        Returns:
            The calculated numerical value as a float.

        Raises:
            NotImplementedError: If the method is not implemented by a subclass.
            ValueError: If the inputs are invalid for the specific calculation
                (e.g., wrong number of inputs, incompatible types).
            ZeroDivisionError: If the calculation involves division and a divisor
                is zero.
            Exception: Other exceptions depending on the calculation logic.
        """
        # pragma: no cover

    @property
    def description(self) -> str:
        """Provides a human-readable description of the calculation.

        This is useful for documentation, debugging, and for user interfaces
        that need to explain how a value is derived.

        Returns:
            A string describing the calculation.
        """
        # Default implementation returns the class name. Subclasses should override
        # for more specific descriptions.
        class_name = self.__class__.__name__  # pragma: no cover
        return class_name


class AdditionCalculation(Calculation):
    """Implements an addition calculation, summing values from multiple input nodes.

    This calculation sums the values obtained from calling
    the `calculate` method on each of the provided input nodes for a given period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Sums the calculated values from all input nodes for the specified period.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023Q4") for the calculation.

        Returns:
            The total sum of the values calculated from the input nodes. Returns
            0.0 if the input list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = AdditionCalculation()
            >>> nodes = [MockNode(10), MockNode(20), MockNode(5)]
            >>> strategy.calculate(nodes, "2023")
            35.0
            >>> strategy.calculate([], "2023")
            0.0
        """
        logger.debug(f"Applying addition calculation for period {period}")
        # Using a generator expression for potentially better memory efficiency
        return sum(input_node.calculate(period) for input_node in inputs)

    @property
    def description(self) -> str:
        """Returns a description of the addition calculation."""
        return "Addition (sum of all inputs)"


class SubtractionCalculation(Calculation):
    """Implements a subtraction calculation: first input minus the sum of the rest.

    This calculation takes the calculated value of the first node in the input list
    and subtracts the sum of the calculated values of all subsequent nodes for
    a specific period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the difference: value of the first input minus the sum of others.

        Args:
            inputs: A list of Node objects. Must contain at least one node.
            period: The time period string (e.g., "2024Q1") for the calculation.

        Returns:
            The result of the subtraction. If only one input node is provided,
            its value is returned.

        Raises:
            CalculationError: If the `inputs` list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = SubtractionCalculation()
            >>> nodes = [MockNode(100), MockNode(20), MockNode(30)]
            >>> strategy.calculate(nodes, "2023")
            50.0
            >>> nodes_single = [MockNode(100)]
            >>> strategy.calculate(nodes_single, "2023")
            100.0
        """
        if not inputs:
            raise CalculationError(
                "Subtraction calculation requires at least one input node",
                details={"strategy": "SubtractionCalculation"},
            )

        logger.debug(f"Applying subtraction calculation for period {period}")
        # Calculate values first to avoid multiple calls if nodes are complex
        values = [node.calculate(period) for node in inputs]
        return values[0] - sum(values[1:])

    @property
    def description(self) -> str:
        """Returns a description of the subtraction calculation."""
        return "Subtraction (first input minus sum of subsequent inputs)"


class MultiplicationCalculation(Calculation):
    """Implements a multiplication calculation, calculating the product of input values.

    This calculation multiplies the calculated values of all provided input nodes
    for a given period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the product of the values from all input nodes.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023FY") for the calculation.

        Returns:
            The product of all input values. Returns 1.0 (multiplicative identity)
            if the input list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = MultiplicationCalculation()
            >>> nodes = [MockNode(2), MockNode(3), MockNode(4)]
            >>> strategy.calculate(nodes, "2023")
            24.0
            >>> strategy.calculate([], "2023")
            1.0
        """
        # Multiplication calculation should ideally return 1.0 for empty inputs.
        # Raising error if empty seems less conventional for multiplication.
        if not inputs:
            logger.warning(
                "Multiplication calculation called with empty inputs, returning 1.0"
            )
            return 1.0

        logger.debug(f"Applying multiplication calculation for period {period}")
        result = 1.0
        for input_node in inputs:
            result *= input_node.calculate(period)
        return result

    @property
    def description(self) -> str:
        """Returns a description of the multiplication calculation."""
        return "Multiplication (product of all inputs)"


class DivisionCalculation(Calculation):
    """Implements a division calculation: first input divided by the product of the rest.

    This calculation takes the calculated value of the first node (numerator) and
    divides it by the product of the calculated values of all subsequent nodes
    (denominator) for a specific period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the division: first input / (product of subsequent inputs).

        Args:
            inputs: A list of Node objects. Must contain at least two nodes.
            period: The time period string (e.g., "2024Q2") for the calculation.

        Returns:
            The result of the division.

        Raises:
            CalculationError: If the `inputs` list contains fewer than two nodes or if the denominator product is zero.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = DivisionCalculation()
            >>> nodes = [MockNode(100), MockNode(5), MockNode(2)]
            >>> strategy.calculate(nodes, "2023")
            10.0
            >>> nodes_zero_denom = [MockNode(100), MockNode(5), MockNode(0)]
            >>> try:
            ...     strategy.calculate(nodes_zero_denom, "2023")
            ... except CalculationError as e:
            ...     # Example: logging the error instead of printing
            ...     logger.error(e)
            Division by zero: Denominator product is zero
        """
        if len(inputs) < 2:
            raise CalculationError(
                "Division calculation requires at least two input nodes",
                details={"strategy": "DivisionCalculation", "input_count": len(inputs)},
            )

        logger.debug(f"Applying division calculation for period {period}")

        values = [node.calculate(period) for node in inputs]
        numerator = values[0]

        denominator = 1.0
        for val in values[1:]:
            denominator *= val

        if denominator == 0.0:
            raise CalculationError(
                "Division by zero: Denominator product is zero",
                period=period,
                details={"numerator": numerator, "denominator": denominator},
            )

        return numerator / denominator

    @property
    def description(self) -> str:
        """Returns a description of the division calculation."""
        return "Division (first input / product of subsequent inputs)"


class WeightedAverageCalculation(Calculation):
    """Calculates the weighted average of input node values.

    This calculation computes the average of the values from input nodes, where each
    node's contribution is weighted. If no weights are provided during
    initialization, it defaults to an equal weighting (simple average).
    """

    def __init__(self, weights: Optional[list[float]] = None):
        """Initializes the WeightedAverageCalculation.

        Args:
            weights: An optional list of floats representing the weight for each
                corresponding input node. The length of this list must match the
                number of input nodes provided to the `calculate` method. If None,
                equal weights are assumed.
        """
        # Validate weights if provided immediately? No, validation happens in calculate
        # as the number of inputs isn't known here.
        self.weights = weights
        logger.info(f"Initialized WeightedAverageCalculation with weights: {weights}")

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Computes the weighted average of the input node values for the period.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023H1") for the calculation.

        Returns:
            The calculated weighted average as a float.

        Raises:
            CalculationError: If the `inputs` list is empty or if the sum of weights is zero.
            StrategyError: If `weights` were provided and length does not match number of inputs.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> # Equal weights (simple average)
            >>> strategy_equal = WeightedAverageCalculation()
            >>> nodes = [MockNode(10), MockNode(20), MockNode(30)]
            >>> strategy_equal.calculate(nodes, "2023")
            20.0
            >>> # Custom weights
            >>> strategy_custom = WeightedAverageCalculation(weights=[0.5, 0.3, 0.2])
            >>> strategy_custom.calculate(nodes, "2023")
            17.0
            >>> # Mismatched weights
            >>> strategy_mismatch = WeightedAverageCalculation(weights=[0.5, 0.5])
            >>> try:
            ...     strategy_mismatch.calculate(nodes, "2023")
            ... except StrategyError as e:
            ...     # Example: logging the error instead of printing
            ...     logger.error(e)
            Number of weights (2) must match number of inputs (3)
        """
        if not inputs:
            raise CalculationError(
                "Weighted average calculation requires at least one input node",
                details={"strategy": "WeightedAverageCalculation"},
            )

        num_inputs = len(inputs)
        effective_weights: list[float]

        if self.weights is None:
            # Use equal weights if none provided
            if num_inputs == 0:  # Should be caught by the check above, but defensive
                return 0.0
            equal_weight = 1.0 / num_inputs
            effective_weights = [equal_weight] * num_inputs
            logger.debug("Using equal weights for weighted average.")
        elif len(self.weights) == num_inputs:
            effective_weights = self.weights
            logger.debug(f"Using provided weights: {effective_weights}")
        else:
            raise StrategyError(
                f"Number of weights ({len(self.weights)}) must match "
                f"number of inputs ({num_inputs})",
                strategy_type="WeightedAverageCalculation",
            )

        logger.debug(f"Applying weighted average calculation for period {period}")
        weighted_sum = 0.0
        total_weight = sum(effective_weights)
        input_values = [node.calculate(period) for node in inputs]

        if total_weight == 0.0:
            # Avoid division by zero. If weights are all zero, the concept is ill-defined.
            # Returning 0 might be a reasonable default, or raising an error.
            # Let's raise ValueError for clarity.
            raise CalculationError(
                "Total weight for weighted average cannot be zero.",
                period=period,
                details={"weights": effective_weights},
            )

        for value, weight in zip(input_values, effective_weights):
            weighted_sum += value * weight

        # If weights don't sum to 1, this isn't a standard weighted average.
        # Decide whether to normalize or return the weighted sum directly.
        # Normalize by total weight for a true weighted average.
        return weighted_sum / total_weight

    @property
    def description(self) -> str:
        """Returns a description of the weighted average calculation."""
        if self.weights:
            return f"Weighted Average (using provided weights: {self.weights})"
        else:
            return "Weighted Average (using equal weights)"


# Type alias for the custom formula function
FormulaFunc = Callable[[dict[str, float]], float]


class CustomFormulaCalculation(Calculation):
    """Executes a user-defined Python function to calculate a value.

    This calculation provides maximum flexibility by allowing any custom Python
    function to be used for calculation. The function receives a dictionary
    mapping input node names (or fallback names) to their calculated values
    for the period and should return a single float result.
    """

    def __init__(self, formula_function: FormulaFunc):
        """Initializes the CustomFormulaCalculation with a calculation function.

        Args:
            formula_function: A callable (function, lambda, etc.) that accepts
                a single argument: a dictionary mapping string keys (input node
                names or `input_<i>`) to their float values for the period.
                It must return a float.

        Raises:
            TypeError: If `formula_function` is not callable.
        """
        if not callable(formula_function):
            raise StrategyError(
                "formula_function must be callable",
                strategy_type="CustomFormulaCalculation",
            )
        self.formula_function = formula_function
        logger.info(
            f"Initialized CustomFormulaCalculation with function: {formula_function.__name__}"
        )

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Applies the custom formula function to the calculated input values.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2025M1") for the calculation.

        Returns:
            The float result returned by the `formula_function`.

        Raises:
            CalculationError: If the `formula_function` encounters an error during execution
                (e.g., incorrect input keys, calculation errors). Wraps the original exception.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, name, value): self.name = name; self._value = value
            ...     def calculate(self, period): return self._value
            >>> def my_formula(data):
            ...     # Example: Gross Profit Margin
            ...     return (data['revenue'] - data['cogs']) / data['revenue'] * 100
            >>> strategy = CustomFormulaCalculation(my_formula)
            >>> nodes = [MockNode('revenue', 1000), MockNode('cogs', 600)]
            >>> strategy.calculate(nodes, "2023")
            40.0
            >>> # Example with unnamed nodes
            >>> def simple_sum(data):
            ...     return data['input_0'] + data['input_1']
            >>> strategy_unnamed = CustomFormulaCalculation(simple_sum)
            >>> nodes_unnamed = [MockNode(None, 10), MockNode(None, 20)] # No names
            >>> strategy_unnamed.calculate(nodes_unnamed, "2023")
            30.0
        """
        # Prepare input values dictionary, using names if available
        input_values: dict[str, float] = {}
        for i, node in enumerate(inputs):
            # Prefer node.name if it exists and is a non-empty string
            key = getattr(node, "name", None)
            if not isinstance(key, str) or not key:
                key = f"input_{i}"
            input_values[key] = node.calculate(period)

        logger.debug(
            f"Applying custom formula calculation for period {period} with inputs: {input_values}"
        )
        try:
            # Execute the user-provided function
            result = self.formula_function(input_values)
            if not isinstance(result, int | float):
                logger.warning(
                    f"Custom formula function {self.formula_function.__name__} "
                    f"returned non-numeric type: {type(result)}. Attempting cast."
                )
                # Attempt conversion, but be aware this might fail or be lossy
                try:
                    return float(result)
                except (ValueError, TypeError) as cast_err:
                    raise CalculationError(
                        f"Custom formula {self.formula_function.__name__} result "
                        f"({result!r}) could not be cast to float.",
                        period=period,
                        details={
                            "result": result,
                            "result_type": type(result).__name__,
                        },
                    ) from cast_err
            return float(result)  # Ensure result is float
        except Exception as e:
            # Catch any exception from the custom function and wrap it
            logger.error(
                f"Error executing custom formula '{self.formula_function.__name__}': {e}",
                exc_info=True,
            )
            raise CalculationError(
                f"Error in custom formula '{self.formula_function.__name__}': {e}",
                period=period,
                details={"original_error": str(e)},
            ) from e

    @property
    def description(self) -> str:
        """Returns a description of the custom formula calculation."""
        func_name = getattr(self.formula_function, "__name__", "[anonymous function]")
        return f"Custom Formula (using function: {func_name})"


class FormulaCalculation(Calculation):
    """Evaluates a mathematical formula string as a calculation strategy.

    This calculation parses and evaluates simple mathematical expressions
    involving input nodes. Supports basic arithmetic operators (+, -, *, /)
    and unary negation.

    Attributes:
        formula: The mathematical expression string to evaluate.
        input_variable_names: List of variable names used in the formula,
            corresponding to the order of input nodes.
        _ast: The parsed Abstract Syntax Tree of the formula.
    """

    # Supported AST operators mapping to Python operator functions
    OPERATORS: ClassVar[dict[Type[Any], Callable[..., float]]] = {
        ast.Add: operator.add,
        ast.Sub: operator.sub,
        ast.Mult: operator.mul,
        ast.Div: operator.truediv,
        ast.USub: operator.neg,
    }

    def __init__(self, formula: str, input_variable_names: list[str]):
        """Initialize the FormulaCalculation.

        Args:
            formula: The mathematical formula string (e.g., "a + b / 2").
            input_variable_names: List of variable names used in the formula,
                in the same order as the input nodes that will be provided
                to the calculate method.

        Raises:
            ValueError: If the formula string has invalid syntax.
        """
        self.formula = formula
        self.input_variable_names = input_variable_names
        try:
            # Parse the formula string into an AST expression
            self._ast = ast.parse(formula, mode="eval").body
        except SyntaxError as e:
            raise StrategyError(
                f"Invalid formula syntax: {formula}",
                strategy_type="FormulaCalculation",
            ) from e
        logger.info(
            f"Initialized FormulaCalculation with formula: {formula} and variables: {input_variable_names}"
        )

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculate the value by evaluating the formula with input node values.

        Args:
            inputs: A list of Node objects, in the same order as input_variable_names.
            period: The time period string for the calculation.

        Returns:
            The result of the formula evaluation.

        Raises:
            StrategyError: If the number of inputs doesn't match number of variable names.
            CalculationError: If an error occurs during evaluation of the formula.
        """
        if len(inputs) != len(self.input_variable_names):
            raise StrategyError(
                f"Number of inputs ({len(inputs)}) must match number of variable names "
                f"({len(self.input_variable_names)})",
                strategy_type="FormulaCalculation",
            )

        # Create mapping of variable names to nodes
        variable_map = dict(zip(self.input_variable_names, inputs))

        logger.debug(f"Applying formula calculation for period {period}")
        try:
            return self._evaluate(self._ast, period, variable_map)
        except (ValueError, TypeError, KeyError, ZeroDivisionError) as e:
            raise CalculationError(
                f"Error evaluating formula: {self.formula}. Error: {e!s}",
                period=period,
                details={"formula": self.formula, "original_error": str(e)},
            ) from e

    def _evaluate(
        self, node: ast.AST, period: str, variable_map: dict[str, Node]
    ) -> float:
        """Recursively evaluate the parsed AST node for the formula.

        Args:
            node: The current AST node to evaluate.
            period: The time period context for the evaluation.
            variable_map: Mapping of variable names to Node objects.

        Returns:
            The result of evaluating the AST node.

        Raises:
            TypeError: If a non-numeric constant or input node value is encountered.
            ValueError: If an unknown variable or unsupported operator/syntax is found.
            ZeroDivisionError: If division by zero occurs.
        """
        # Numeric literal (Constant in Python 3.8+)
        if isinstance(node, ast.Constant):
            if isinstance(node.value, int | float):
                return float(node.value)
            else:
                raise CalculationError(
                    f"Unsupported constant type '{type(node.value).__name__}' in formula",
                    period=period,
                    details={"constant_type": type(node.value).__name__},
                )

        # Variable reference
        elif isinstance(node, ast.Name):
            var_name = node.id
            if var_name not in variable_map:
                raise CalculationError(
                    f"Unknown variable '{var_name}' in formula. Available: {list(variable_map.keys())}",
                    period=period,
                    details={
                        "unknown_var": var_name,
                        "available_vars": list(variable_map.keys()),
                    },
                )
            input_node = variable_map[var_name]
            # Recursively calculate the value of the input node
            value = input_node.calculate(period)
            if not isinstance(value, int | float):
                raise CalculationError(
                    f"Input node '{input_node.name}' (variable '{var_name}') did not return a numeric value for period '{period}'",
                    node_id=input_node.name,
                    period=period,
                    details={"value_type": type(value).__name__},
                )
            return float(value)

        # Binary operation (e.g., a + b)
        elif isinstance(node, ast.BinOp):
            left_val = self._evaluate(node.left, period, variable_map)
            right_val = self._evaluate(node.right, period, variable_map)
            op_type = type(node.op)  # type: Type[Any]
            if op_type not in self.OPERATORS:
                raise StrategyError(
                    f"Unsupported binary operator '{op_type.__name__}' in formula",
                    strategy_type="FormulaCalculation",
                )
            # Perform the operation
            return float(self.OPERATORS[op_type](left_val, right_val))

        # Unary operation (e.g., -a)
        elif isinstance(node, ast.UnaryOp):
            operand_val = self._evaluate(node.operand, period, variable_map)
            unary_op_type = type(node.op)
            if unary_op_type not in self.OPERATORS:
                raise StrategyError(
                    f"Unsupported unary operator '{unary_op_type.__name__}' in formula",
                    strategy_type="FormulaCalculation",
                )
            # Perform the operation
            return float(self.OPERATORS[unary_op_type](operand_val))

        # If the node type is unsupported
        else:
            raise StrategyError(
                f"Unsupported syntax node type '{type(node).__name__}' in formula: {ast.dump(node)}",
                strategy_type="FormulaCalculation",
            )

    @property
    def description(self) -> str:
        """Returns a description of the formula calculation."""
        return f"Formula: {self.formula}"



================================================================================
File: fin_statement_model/core/calculations/registry.py
================================================================================

"""Registry for calculation classes in the Financial Statement Model.

This module provides a central registry for discovering and accessing different
calculation classes. Calculations can be registered using their class
object and later retrieved by their class name.

Examples:
    >>> from fin_statement_model.core.calculations.registry import Registry
    >>> from fin_statement_model.core.calculations import AdditionCalculation
    >>> Registry.register(AdditionCalculation)
    >>> Registry.get("AdditionCalculation") is AdditionCalculation
    >>> list(Registry.list().keys())
    ['AdditionCalculation', 'SubtractionCalculation', 'MultiplicationCalculation', ...]
"""

# Use lowercase built-in types
from typing import ClassVar  # Keep Type for now
import logging

from .calculation import Calculation

# Configure logging
logger = logging.getLogger(__name__)


class Registry:
    """A central registry for managing and accessing calculation classes.

    This class uses class methods to provide a global registry. Calculations
    are stored in a dictionary mapping their class name (string) to the
    calculation class itself.

    Attributes:
        _strategies: A dictionary holding the registered calculation classes.
                     Keys are calculation class names (str), values are calculation
                     types (Type[Calculation]).
    """

    _strategies: ClassVar[dict[str, type[Calculation]]] = {}  # Use dict, type

    @classmethod
    def register(cls, calculation: type[Calculation]) -> None:
        """Register a calculation class with the registry.

        If a calculation with the same name is already registered, it will be
        overwritten.

        Args:
            calculation: The calculation class (Type[Calculation]) to register.
                         The class's __name__ attribute will be used as the key.
        """
        if not issubclass(calculation, Calculation):
            raise TypeError(
                f"Can only register subclasses of Calculation, not {calculation}"
            )
        cls._strategies[calculation.__name__] = calculation
        logger.debug(f"Registered calculation: {calculation.__name__}")

    @classmethod
    def get(cls, name: str) -> type[Calculation]:
        """Retrieve a calculation class from the registry by its name.

        Args:
            name: The string name of the calculation class to retrieve.

        Returns:
            The calculation class (Type[Calculation]) associated with the given name.

        Raises:
            KeyError: If no calculation with the specified name is found in the
                      registry.
        """
        # Debug print including id of the dictionary
        if name not in cls._strategies:
            logger.error(f"Attempted to access unregistered calculation: {name}")
            raise KeyError(f"Calculation '{name}' not found in registry.")
        return cls._strategies[name]

    @classmethod
    def list(cls) -> dict[str, type[Calculation]]:  # Use dict, type
        """List all registered calculation classes.

        Returns:
            A dictionary containing all registered calculation names (str) and their
            corresponding calculation classes (Type[Calculation]). Returns a copy
            to prevent modification of the internal registry.
        """
        return cls._strategies.copy()



================================================================================
File: fin_statement_model/core/errors.py
================================================================================

"""Custom exception classes for the financial statement model.

This module defines exception classes for specific error cases,
allowing for more precise error handling and better error messages.
"""

from typing import Optional, Any


class FinancialModelError(Exception):
    """Base exception for all financial statement model errors.

    All custom exceptions raised within the library should inherit from this class.

    Examples:
        >>> raise FinancialModelError("An error occurred.")
    """

    def __init__(self, message: str):
        """Initialize the FinancialModelError.

        Args:
            message: A human-readable description of the error.
        """
        self.message = message
        super().__init__(self.message)


class ConfigurationError(FinancialModelError):
    """Error raised for invalid configuration files or objects.

    This typically occurs when parsing or validating configuration data,
    such as YAML files defining metrics or statement structures.

    Examples:
        >>> raise ConfigurationError("Invalid syntax", config_path="config.yaml")
        >>> raise ConfigurationError(
        ...     "Missing required fields",
        ...     config_path="metrics.yaml",
        ...     errors=["Missing 'formula' for 'revenue'"]
        ... )
    """

    def __init__(
        self,
        message: str,
        config_path: Optional[str] = None,
        errors: Optional[list[Any]] = None,
    ):
        """Initialize the ConfigurationError.

        Args:
            message: The base error message.
            config_path: Optional path to the configuration file where the error occurred.
            errors: Optional list of specific validation errors found.
        """
        self.config_path = config_path
        self.errors = errors or []

        if config_path and self.errors:
            full_message = (
                f"{message} in {config_path}: {' ; '.join(str(e) for e in self.errors)}"
            )
        elif config_path:
            full_message = f"{message} in {config_path}"
        elif self.errors:
            full_message = f"{message}: {' ; '.join(str(e) for e in self.errors)}"
        else:
            full_message = message

        super().__init__(full_message)


class CalculationError(FinancialModelError):
    """Error raised during calculation operations.

    Indicates a problem while computing the value of a node, often due
    to issues with the calculation logic, input data, or strategy used.

    Examples:
        >>> raise CalculationError("Division by zero", node_id="profit_margin", period="2023-Q1")
        >>> raise CalculationError(
        ...     "Incompatible input types",
        ...     node_id="total_assets",
        ...     details={"input_a_type": "str", "input_b_type": "int"}
        ... )
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str] = None,
        period: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
    ):
        """Initialize the CalculationError.

        Args:
            message: The base error message.
            node_id: Optional ID of the node where the calculation failed.
            period: Optional period for which the calculation failed.
            details: Optional dictionary containing additional context about the error.
        """
        self.node_id = node_id
        self.period = period
        self.details = details or {}

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if period:
            context.append(f"period '{period}'")

        full_message = f"{message} for {' and '.join(context)}" if context else message

        # Append details to the message for better context
        if self.details:
            details_str = ", ".join(f'{k}="{v}"' for k, v in self.details.items())
            # Prioritize showing the original underlying error if captured
            original_error_str = self.details.get("original_error")
            if original_error_str:
                full_message = f"{full_message}: {original_error_str}"
            else:
                full_message = f"{full_message} (Details: {details_str})"

        super().__init__(full_message)


class NodeError(FinancialModelError):
    """Error raised for issues related to graph nodes.

    Covers issues like trying to access a non-existent node,
    invalid node configurations, or type mismatches related to nodes.

    Examples:
        >>> raise NodeError("Node not found", node_id="non_existent_node")
        >>> raise NodeError("Invalid node type for operation", node_id="revenue")
    """

    def __init__(self, message: str, node_id: Optional[str] = None):
        """Initialize the NodeError.

        Args:
            message: The base error message.
            node_id: Optional ID of the node related to the error.
        """
        self.node_id = node_id

        full_message = f"{message} for node '{node_id}'" if node_id else message

        super().__init__(full_message)


class MissingInputError(FinancialModelError):
    """Error raised when a required calculation input is missing.

    Occurs when a calculation node needs data from another node for a
    specific period, but that data is unavailable.

    Examples:
        >>> raise MissingInputError(
        ...     "Required input data unavailable",
        ...     node_id="cogs",
        ...     input_name="inventory",
        ...     period="2023-12-31"
        ... )
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str] = None,
        input_name: Optional[str] = None,
        period: Optional[str] = None,
    ):
        """Initialize the MissingInputError.

        Args:
            message: The base error message.
            node_id: Optional ID of the node requiring the input.
            input_name: Optional name or ID of the missing input node.
            period: Optional period for which the input was missing.
        """
        self.node_id = node_id
        self.input_name = input_name
        self.period = period

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if input_name:
            context.append(f"input '{input_name}'")
        if period:
            context.append(f"period '{period}'")

        full_message = f"{message} for {' in '.join(context)}" if context else message

        super().__init__(full_message)


class GraphError(FinancialModelError):
    """Error raised for invalid graph structure or operations.

    Covers issues like inconsistencies in the graph (e.g., orphaned nodes),
    problems during graph traversal, or invalid modifications to the graph.

    Examples:
        >>> raise GraphError("Orphaned node detected", nodes=["unconnected_node"])
        >>> raise GraphError("Failed to add edge due to type mismatch")
    """

    def __init__(self, message: str, nodes: Optional[list[str]] = None):
        """Initialize the GraphError.

        Args:
            message: The base error message.
            nodes: Optional list of node IDs involved in the graph error.
        """
        self.nodes = nodes or []

        full_message = (
            f"{message} involving nodes: {', '.join(nodes)}" if nodes else message
        )

        super().__init__(full_message)


class DataValidationError(FinancialModelError):
    """Error raised for data validation failures.

    Typically occurs during data import or preprocessing when data
    does not conform to expected formats, types, or constraints.

    Examples:
        >>> raise DataValidationError(
        ...     "Input data failed validation",
        ...     validation_errors=[
        ...         "Column 'Date' has invalid format",
        ...         "Value '-100' is not allowed for 'Revenue'"
        ...     ]
        ... )
    """

    def __init__(self, message: str, validation_errors: Optional[list[str]] = None):
        """Initialize the DataValidationError.

        Args:
            message: The base error message.
            validation_errors: Optional list of specific validation failures.
        """
        self.validation_errors = validation_errors or []

        if validation_errors:
            full_message = f"{message}: {'; '.join(validation_errors)}"
        else:
            full_message = message

        super().__init__(full_message)


class CircularDependencyError(FinancialModelError):
    """Error raised when a circular dependency is detected in calculations.

    Occurs if the calculation graph contains cycles, meaning a node
    directly or indirectly depends on itself.

    Examples:
        >>> raise CircularDependencyError(cycle=["node_a", "node_b", "node_c", "node_a"])
    """

    def __init__(
        self,
        message: str = "Circular dependency detected",
        cycle: Optional[list[str]] = None,
    ):
        """Initialize the CircularDependencyError.

        Args:
            message: The base error message.
            cycle: Optional list of node IDs forming the detected cycle.
        """
        self.cycle = cycle or []

        if cycle:
            cycle_str = " -> ".join(cycle)
            full_message = f"{message}: {cycle_str}"
        else:
            full_message = message

        super().__init__(full_message)


class PeriodError(FinancialModelError):
    """Error raised for invalid or missing periods.

    Covers issues like requesting data for a non-existent period or
    using invalid period formats.

    Examples:
        >>> raise PeriodError("Invalid period format", period="2023Q5")
        >>> raise PeriodError("Period not found", period="2024-01-01", available_periods=["2023-12-31"])
    """

    def __init__(
        self,
        message: str,
        period: Optional[str] = None,
        available_periods: Optional[list[str]] = None,
    ):
        """Initialize the PeriodError.

        Args:
            message: The base error message.
            period: Optional specific period involved in the error.
            available_periods: Optional list of valid periods.
        """
        self.period = period
        self.available_periods = available_periods or []

        if period and available_periods:
            full_message = f"{message} for period '{period}'. Available periods: {', '.join(available_periods)}"
        elif period:
            full_message = f"{message} for period '{period}'"
        else:
            full_message = message

        super().__init__(full_message)


class StatementError(FinancialModelError):
    """Error raised for issues related to financial statements.

    Used for errors specific to the structure, definition, or
    processing of financial statements (e.g., Balance Sheet, P&L).

    Examples:
        >>> raise StatementError("Balance sheet does not balance", statement_id="BS_2023")
        >>> raise StatementError("Required account missing from P&L", statement_id="PnL_Q1")
    """

    def __init__(self, message: str, statement_id: Optional[str] = None):
        """Initialize the StatementError.

        Args:
            message: The base error message.
            statement_id: Optional ID or name of the statement involved.
        """
        self.statement_id = statement_id

        full_message = (
            f"{message} for statement '{statement_id}'" if statement_id else message
        )

        super().__init__(full_message)


class StrategyError(FinancialModelError):
    """Error raised for issues related to calculation strategies.

    Indicates a problem with the configuration or execution of a
    specific calculation strategy (e.g., Summation, GrowthRate).

    Examples:
        >>> raise StrategyError("Invalid parameter for GrowthRate strategy", strategy_type="GrowthRate", node_id="revenue_forecast")
        >>> raise StrategyError("Strategy not applicable to node type", strategy_type="Summation", node_id="text_description")
    """

    def __init__(
        self,
        message: str,
        strategy_type: Optional[str] = None,
        node_id: Optional[str] = None,
    ):
        """Initialize the StrategyError.

        Args:
            message: The base error message.
            strategy_type: Optional name or type of the strategy involved.
            node_id: Optional ID of the node using the strategy.
        """
        self.strategy_type = strategy_type
        self.node_id = node_id

        context = []
        if strategy_type:
            context.append(f"strategy type '{strategy_type}'")
        if node_id:
            context.append(f"node '{node_id}'")

        full_message = f"{message} for {' in '.join(context)}" if context else message

        super().__init__(full_message)


class TransformationError(FinancialModelError):
    """Error raised during data transformation.

    Occurs during preprocessing steps when a specific transformation
    (e.g., normalization, scaling) fails.

    Examples:
        >>> raise TransformationError("Log transform requires positive values", transformer_type="LogTransformer")
        >>> raise TransformationError(
        ...     "Incompatible data type for scaling",
        ...     transformer_type="MinMaxScaler",
        ...     parameters={"feature_range": (0, 1)}
        ... )
    """

    def __init__(
        self,
        message: str,
        transformer_type: Optional[str] = None,
        parameters: Optional[dict[str, Any]] = None,
    ):
        """Initialize the TransformationError.

        Args:
            message: The base error message.
            transformer_type: Optional name or type of the transformer involved.
            parameters: Optional dictionary of parameters used by the transformer.
        """
        self.transformer_type = transformer_type
        self.parameters = parameters or {}

        if transformer_type:
            full_message = f"{message} in transformer '{transformer_type}'"
            if parameters:
                params_str = ", ".join(f"{k}={v}" for k, v in parameters.items())
                full_message = f"{full_message} with parameters: {params_str}"
        else:
            full_message = message

        super().__init__(full_message)


class MetricError(FinancialModelError):
    """Error raised for issues related to metric definitions or registry.

    Covers issues with loading, validating, or accessing financial metrics,
    whether defined in YAML or Python code.

    Examples:
        >>> raise MetricError("Metric definition not found", metric_name="unknown_ratio")
        >>> raise MetricError(
        ...     "Invalid formula syntax in metric definition",
        ...     metric_name="profitability_index",
        ...     details={"formula": "NPV / Initial Investment)"}  # Missing parenthesis
        ... )
    """

    def __init__(
        self,
        message: str,
        metric_name: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
    ):
        """Initialize the MetricError.

        Args:
            message: The base error message.
            metric_name: Optional name of the metric involved in the error.
            details: Optional dictionary containing additional context about the error.
        """
        self.metric_name = metric_name
        self.details = details or {}

        full_message = (
            f"{message} related to metric '{metric_name}'" if metric_name else message
        )

        super().__init__(full_message)



================================================================================
File: fin_statement_model/core/graph/__init__.py
================================================================================

"""Expose the public graph API for *fin_statement_model*.

This module re-exports the most frequently used classes for working with the
graph layer so end-users can write concise import statements without digging
into the sub-package layout.

Specifically, it makes the following symbols directly available:

* `Graph` – central orchestrator that builds and evaluates calculation graphs.
* `GraphManipulator` – helper for structural mutations (add/remove/replace
  nodes, set values, etc.).
* `GraphTraverser` – read-only utilities for traversal, validation, and cycle
  detection.

Examples:
    Basic usage::

        >>> from fin_statement_model.core.graph import Graph
        >>> g = Graph(periods=["2023"])
        >>> _ = g.add_financial_statement_item("Revenue", {"2023": 100.0})
        >>> g.calculate("Revenue", "2023")
        100.0

Keeping these high-level classes here provides a stable import path should the
internal file structure change in future versions.
"""

from fin_statement_model.core.graph.graph import Graph
from fin_statement_model.core.graph.manipulator import GraphManipulator
from fin_statement_model.core.graph.traverser import GraphTraverser

__all__ = ["Graph", "GraphManipulator", "GraphTraverser"]



================================================================================
File: fin_statement_model/core/graph/graph.py
================================================================================

"""Core graph implementation for `fin_statement_model`.

The `Graph` class orchestrates the construction and evaluation of directed
graphs representing financial statements. It provides methods for:

* Adding data (`FinancialStatementItemNode`) and calculation nodes
* Managing time periods and ensuring uniqueness/sorting
* Performing calculations, forecasting, and applying adjustments
* Inspecting and mutating graph structure via the `manipulator` and
  `traverser` sub-APIs

Example:
    >>> from fin_statement_model.core.graph.graph import Graph
    >>> g = Graph(periods=["2023"])
    >>> g.add_financial_statement_item("Revenue", {"2023": 100.0})
    >>> g.calculate("Revenue", "2023")
    100.0
"""

import logging
from typing import Any, Optional, overload, Literal
from collections.abc import Callable
from uuid import UUID

from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.core.nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    is_calculation_node,
)
from fin_statement_model.core.errors import (
    NodeError,
    ConfigurationError,
    CalculationError,
    CircularDependencyError,
)
from fin_statement_model.core.metrics import metric_registry
from fin_statement_model.core.calculations import Registry
from fin_statement_model.core.graph.manipulator import GraphManipulator
from fin_statement_model.core.graph.traverser import GraphTraverser
from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentType,
    AdjustmentTag,
    DEFAULT_SCENARIO,
    AdjustmentFilterInput,
)
from fin_statement_model.core.adjustments.manager import AdjustmentManager


# Configure logging
logger = logging.getLogger(__name__)

__all__ = ["Graph"]


class Graph:
    """Core directed-graph abstraction for financial statement modeling.

    The `Graph` class orchestrates construction, mutation, traversal,
    calculation, and forecasting of nodes representing financial statement
    items and metrics. It exposes high-level convenience methods for
    building and evaluating the model, while delegating structural
    mutations and read-only inspections to its sub-APIs.

    Attributes:
        _nodes: Mapping of node names (str) to Node instances registered in the graph.
        _periods: Sorted list of unique period identifiers (str) managed by the graph.
        _cache: Nested dict caching calculated float values per node per period.
        _node_factory: `NodeFactory` instance for creating new nodes.
        manipulator: `GraphManipulator` for structural mutations (add/remove/replace nodes, set values).
        traverser: `GraphTraverser` for read-only traversal, validation, and cycle detection.
        adjustment_manager: `AdjustmentManager` handling discretionary adjustments.
    """

    def __init__(self, periods: Optional[list[str]] = None):
        """Initialize a new `Graph` instance.

        Sets up core components: node registry, period list, calculation cache,
        node factory, and sub-API instances (`manipulator`, `traverser`,
        `adjustment_manager`).

        Args:
            periods: Optional list of period identifiers (str) to initialize.
                     Periods are automatically deduplicated and sorted.

        Raises:
            TypeError: If `periods` is not a list of strings.

        Examples:
            >>> from fin_statement_model.core.graph.graph import Graph
            >>> g = Graph()
            >>> g.periods
            []
            >>> g = Graph(periods=["2024", "2023"])
            >>> g.periods
            ["2023", "2024"]
            >>> Graph(periods="2023")  # raises TypeError
        """
        # No super().__init__() needed as mixins don't have __init__
        # and GraphCore is removed.

        self._nodes: dict[str, Node] = {}

        # Initialize core attributes for periods, cache, and node factory
        self._periods: list[str] = []
        self._cache: dict[str, dict[str, float]] = {}
        self._node_factory: NodeFactory = NodeFactory()

        # Handle initial periods directly
        if periods:
            if not isinstance(periods, list):
                raise TypeError("Initial periods must be a list")
            self.add_periods(periods)

        self.manipulator = GraphManipulator(self)
        self.traverser = GraphTraverser(self)

        # --- Adjustment Manager Integration ---
        self.adjustment_manager = AdjustmentManager()
        # --- End Adjustment Manager Integration ---

    @property
    def nodes(self) -> dict[str, Node]:
        """Provide access to the dictionary of all nodes in the graph.

        Returns:
            A dictionary where keys are node names (str) and values are
            `Node` objects. This dictionary represents the shared node registry.

        Examples:
            >>> graph = Graph()
            >>> item_node = graph.add_financial_statement_item("Revenue", {"2023": 100})
            >>> logger.info(list(graph.nodes.keys()))
            >>> logger.info(graph.nodes["Revenue"] == item_node)
        """
        return self._nodes

    @property
    def periods(self) -> list[str]:
        """Retrieve the list of time periods currently managed by the graph.

        Returns:
            A sorted list of unique time period strings managed by the graph.

        Examples:
            >>> graph = Graph(periods=["2024", "2023"])
            >>> logger.info(graph.periods)
            >>> graph.add_periods(["2025"])
            >>> logger.info(graph.periods)
        """
        return self._periods

    def add_periods(self, periods: list[str]) -> None:
        """Add new time periods to the graph.

        Update the internal period list, ensuring uniqueness and sorting.

        Args:
            periods: A list of strings representing the time periods to add.

        Raises:
            TypeError: If `periods` is not a list.
        """
        if not isinstance(periods, list):
            raise TypeError("Periods must be provided as a list.")
        # Ensure unique and sorted periods
        combined = set(self._periods).union(periods)
        self._periods = sorted(combined)
        logger.debug(f"Added periods {periods}; current periods: {self._periods}")

    def add_calculation(
        self,
        name: str,
        input_names: list[str],
        operation_type: str,
        formula_variable_names: Optional[list[str]] = None,
        **calculation_kwargs: Any,
    ) -> Node:
        """Add a new calculation node to the graph using the node factory.

        Resolve input node names to Node objects, create a CalculationNode,
        register it in the graph, and return it.

        Args:
            name: Unique name for the calculation node.
            input_names: List of node names to use as inputs.
            operation_type: Calculation type key (e.g., 'addition').
            formula_variable_names: Optional list of variable names used in the formula
                string, required if creating a FormulaCalculationNode via this method.
            **calculation_kwargs: Additional parameters for the calculation constructor.

        Returns:
            The created calculation node.

        Raises:
            NodeError: If any input node name does not exist.
            ValueError: If the name is invalid or creation fails.
            TypeError: If inputs are invalid.
            CircularDependencyError: If adding the node would create a cycle.
        """
        # Validate inputs
        if not isinstance(input_names, list):
            raise TypeError("input_names must be a list of node names.")

        # Resolve input node names to Node objects
        resolved_inputs = self._resolve_input_nodes(input_names)

        # Create the node via factory
        try:
            node = self._node_factory.create_calculation_node(
                name=name,
                inputs=resolved_inputs,
                calculation_type=operation_type,
                formula_variable_names=formula_variable_names,
                **calculation_kwargs,
            )
        except (ValueError, TypeError):
            logger.exception(
                f"Failed to create calculation node '{name}' with type '{operation_type}'"
            )
            raise

        # Add with validation (includes cycle detection)
        added_node = self._add_node_with_validation(node)

        logger.info(
            f"Added calculation node '{name}' of type '{operation_type}' with inputs {input_names}"
        )
        return added_node

    def add_metric(
        self,
        metric_name: str,
        node_name: Optional[str] = None,
        *,
        input_node_map: Optional[dict[str, str]] = None,
    ) -> Node:
        """Add a metric calculation node based on a metric definition.

        If `node_name` is None, uses `metric_name` as the node name.

        Uses the metric registry to load inputs and formula, creates a
        calculation node using the formula strategy, registers it, and stores metric
        metadata on the node itself.

        Args:
            metric_name: Key of the metric definition to add.
            node_name: Optional name for the metric node; defaults to metric_name.
            input_node_map: Optional dictionary mapping metric input variable names
                (from metric definition) to the actual node names present in the graph.
                If None, assumes graph node names match metric input variable names.

        Returns:
            The created calculation node.

        Raises:
            TypeError: If node_name is invalid.
            ValueError: If node_name already exists.
            ConfigurationError: If metric definition is missing or invalid.
            NodeError: If required input nodes (after mapping) are missing.
        """
        # Default node_name to metric_name if not provided
        if node_name is None:
            node_name = metric_name
        if not node_name or not isinstance(node_name, str):
            raise TypeError("Metric node name must be a non-empty string.")
        # Check for name conflict
        if node_name in self._nodes:
            raise ValueError(
                f"A node with name '{node_name}' already exists in the graph."
            )

        # Load metric definition (Pydantic model)
        try:
            metric_def = metric_registry.get(metric_name)
        except KeyError as e:
            raise ConfigurationError(
                f"Unknown metric definition: '{metric_name}'"
            ) from e

        # Extract required fields from definition
        required_inputs = metric_def.inputs
        formula = metric_def.formula
        description = metric_def.description

        # Build list of input node names and formula variable names
        input_node_names: list[str] = []
        formula_variable_names: list[str] = []
        missing = []

        for req_input_name in required_inputs:
            # Determine the actual graph node name to look for
            target_node_name = req_input_name  # Default case
            if input_node_map and req_input_name in input_node_map:
                target_node_name = input_node_map[req_input_name]
            elif input_node_map:
                # If map provided but doesn't contain the required input, it's an error in the map
                missing.append(f"{req_input_name} (mapping missing in input_node_map)")
                continue  # Skip trying to find the node

            # Check if the node exists in the graph
            if target_node_name not in self._nodes:
                missing.append(target_node_name)  # Report the name we looked for
            else:
                input_node_names.append(target_node_name)
                formula_variable_names.append(
                    req_input_name
                )  # Use the metric's variable name

        if missing:
            raise NodeError(
                f"Cannot create metric '{metric_name}': missing required nodes {missing}",
                node_id=node_name,
            )

        # Create calculation node using add_calculation
        try:
            new_node = self.add_calculation(
                name=node_name,
                input_names=input_node_names,
                operation_type="formula",
                formula_variable_names=formula_variable_names,
                formula=formula,
                metric_name=metric_name,  # Pass metric metadata
                metric_description=description,  # Pass metric description
            )
        except Exception as e:
            logger.exception(
                f"Failed to create calculation node for metric '{metric_name}' as node '{node_name}'"
            )
            # Re-raise as ConfigurationError or keep original, depending on desired error reporting
            raise ConfigurationError(
                f"Error creating node for metric '{metric_name}': {e}"
            ) from e

        logger.info(
            f"Added metric '{metric_name}' as calculation node '{node_name}' with inputs {input_node_names}"
        )
        return new_node

    def add_custom_calculation(
        self,
        name: str,
        calculation_func: Callable[..., float],
        inputs: Optional[list[str]] = None,
        description: str = "",
    ) -> Node:
        """Add a custom calculation node using a Python callable.

        Args:
            name: Unique name for the custom calculation node.
            calculation_func: A callable that accepts (period, **inputs) and returns float.
            inputs: Optional list of node names to use as inputs.
            description: Optional description of the calculation.

        Returns:
            The created custom calculation node.

        Raises:
            NodeError: If any specified input nodes are missing.
            TypeError: If calculation_func is not callable.
            CircularDependencyError: If adding the node would create a cycle.
        """
        # Validate callable
        if not callable(calculation_func):
            raise TypeError("calculation_func must be callable.")

        # Resolve inputs if provided
        resolved_inputs: list[Node] = []
        if inputs is not None:
            if not isinstance(inputs, list):
                raise TypeError("inputs must be a list of node names.")
            resolved_inputs = self._resolve_input_nodes(inputs)

        # Create custom node via factory
        try:
            custom_node = self._node_factory._create_custom_node_from_callable(
                name=name,
                inputs=resolved_inputs,
                formula=calculation_func,
                description=description,
            )
        except (ValueError, TypeError):
            logger.exception(f"Failed to create custom calculation node '{name}'")
            raise

        # Add with validation (includes cycle detection)
        added_node = self._add_node_with_validation(custom_node)

        logger.info(f"Added custom calculation node '{name}' with inputs {inputs}")
        return added_node

    def ensure_signed_nodes(
        self, base_node_ids: list[str], *, suffix: str = "_signed"
    ) -> list[str]:
        """Ensure signed calculation nodes (-1 * input) exist for each base node.

        Args:
            base_node_ids: List of existing node names to sign.
            suffix: Suffix to append for signed node names.

        Returns:
            List of names of newly created signed nodes.
        """
        created: list[str] = []
        for base_id in base_node_ids:
            signed_id = f"{base_id}{suffix}"
            # Skip if already present
            if signed_id in self._nodes:
                continue
            # Ensure base node exists
            if base_id not in self._nodes:
                from fin_statement_model.core.errors import NodeError

                raise NodeError(
                    f"Cannot create signed node for missing base node '{base_id}'",
                    node_id=base_id,
                )
            # Create formula node that multiplies by -1
            self.add_calculation(
                name=signed_id,
                input_names=[base_id],
                operation_type="formula",
                formula="-input_0",
                formula_variable_names=["input_0"],
            )
            created.append(signed_id)
        return created

    def change_calculation_method(
        self,
        node_name: str,
        new_method_key: str,
        **kwargs: dict[str, Any],
    ) -> None:
        """Change the calculation method for an existing calculation-based node.

        Args:
            node_name: Name of the existing calculation node.
            new_method_key: Key of the new calculation method to apply.
            **kwargs: Additional parameters required by the new calculation.

        Returns:
            None

        Raises:
            NodeError: If the target node does not exist or is not a CalculationNode.
            ValueError: If `new_method_key` is not a recognized calculation key.
            TypeError: If the new calculation cannot be instantiated with the provided arguments.

        Examples:
            >>> graph.change_calculation_method("GrossProfit", "addition")
        """
        node = self.manipulator.get_node(node_name)
        if node is None:
            raise NodeError("Node not found for calculation change", node_id=node_name)
        if not isinstance(node, CalculationNode):
            raise NodeError(
                f"Node '{node_name}' is not a CalculationNode", node_id=node_name
            )
        # Map method key to registry name
        if new_method_key not in self._node_factory._calculation_methods:
            raise ValueError(f"Calculation '{new_method_key}' is not recognized.")
        calculation_class_name = self._node_factory._calculation_methods[new_method_key]
        try:
            calculation_cls = Registry.get(calculation_class_name)
        except KeyError as e:
            raise ValueError(
                f"Calculation class '{calculation_class_name}' not found in registry."
            ) from e
        try:
            calculation_instance = calculation_cls(**kwargs)
        except TypeError as e:
            raise TypeError(
                f"Failed to instantiate calculation '{new_method_key}': {e}"
            )
        # Apply new calculation
        node.set_calculation(calculation_instance)
        # Clear cached calculations for this node
        if node_name in self._cache:
            del self._cache[node_name]
        logger.info(f"Changed calculation for node '{node_name}' to '{new_method_key}'")

    def get_metric(self, metric_id: str) -> Optional[Node]:
        """Return the metric node for a given metric ID, if present.

        Searches for a node with the given ID that was created as a metric
        (identified by having a `metric_name` attribute).

        Args:
            metric_id: Identifier of the metric node to retrieve.

        Returns:
            The Node corresponding to `metric_id` if it's a metric node, or None.

        Examples:
            >>> m = graph.get_metric("current_ratio")
            >>> if m:
            ...     logger.info(m.name)
        """
        node = self._nodes.get(metric_id)
        # Check if the node exists and has the metric_name attribute populated
        if node and getattr(node, "metric_name", None) == metric_id:
            return node
        return None

    def get_available_metrics(self) -> list[str]:
        """Return a sorted list of all metric node IDs currently in the graph.

        Identifies metric nodes by checking for the presence and non-None value
        of the `metric_name` attribute.

        Returns:
            A sorted list of metric node names.

        Examples:
            >>> graph.get_available_metrics()
            ['current_ratio', 'debt_equity_ratio']
        """
        # Iterate through all nodes and collect names of those that are metrics
        metric_node_names = [
            node.name
            for node in self._nodes.values()
            if getattr(node, "metric_name", None) is not None
        ]
        return sorted(metric_node_names)

    def get_metric_info(self, metric_id: str) -> dict[str, Any]:
        """Return detailed information for a specific metric node.

        Args:
            metric_id: Identifier of the metric node to inspect.

        Returns:
            A dict containing 'id', 'name', 'description', and 'inputs' for the metric.

        Raises:
            ValueError: If `metric_id` does not correspond to a metric node.

        Examples:
            >>> info = graph.get_metric_info("current_ratio")
            >>> logger.info(info['inputs'])
        """
        metric_node = self.get_metric(metric_id)
        if metric_node is None:
            if metric_id in self._nodes:
                raise ValueError(
                    f"Node '{metric_id}' exists but is not a metric (missing metric_name attribute)."
                )
            raise ValueError(f"Metric node '{metric_id}' not found in graph.")

        # Extract info directly from the FormulaCalculationNode
        try:
            # Use getattr for safety, retrieving stored metric metadata
            description = getattr(metric_node, "metric_description", "N/A")
            # metric_name stored on the node is the key from the registry
            registry_key = getattr(metric_node, "metric_name", metric_id)

            # We might want the display name from the original definition.
            # Fetch the definition again if needed for the display name.
            try:
                metric_def = metric_registry.get(registry_key)
                display_name = metric_def.name
            except Exception:
                logger.warning(
                    f"Could not reload metric definition for '{registry_key}' to get display name. Using node name '{metric_id}' instead."
                )
                display_name = metric_id  # Fallback to node name

            inputs = metric_node.get_dependencies()
        except Exception as e:
            # Catch potential attribute errors or other issues
            logger.error(
                f"Error retrieving info for metric node '{metric_id}': {e}",
                exc_info=True,
            )
            raise ValueError(
                f"Failed to retrieve metric info for '{metric_id}': {e}"
            ) from e

        return {
            "id": metric_id,
            "name": display_name,
            "description": description,
            "inputs": inputs,
        }

    @overload
    def get_adjusted_value(
        self,
        node_name: str,
        period: str,
        filter_input: "AdjustmentFilterInput" = None,
        *,
        return_flag: Literal[True],
    ) -> tuple[float, bool]: ...

    @overload
    def get_adjusted_value(
        self,
        node_name: str,
        period: str,
        filter_input: "AdjustmentFilterInput" = None,
        *,
        return_flag: Literal[False] = False,
    ) -> float: ...

    def get_adjusted_value(
        self,
        node_name: str,
        period: str,
        filter_input: "AdjustmentFilterInput" = None,
        *,
        return_flag: bool = False,
    ) -> float | tuple[float, bool]:
        """Calculates the value of a node for a period, applying selected adjustments.

        Fetches the base calculated value, retrieves adjustments matching the filter,
        applies them in order, and returns the result.

        Args:
            node_name: The name of the node to calculate.
            period: The time period identifier.
            filter_input: Criteria for selecting which adjustments to apply. Can be:
                - None: applies default filter (default scenario, all adjustments).
                - AdjustmentFilter: filter by scenarios, tags, types, and period window.
                - set of tags: shorthand for include_tags filter.
                - Callable[[Adjustment], bool] or Callable[[Adjustment, str], bool]:
                    predicate to select adjustments. Two-arg predicates receive
                    the current period as the second argument.
            return_flag: If True, return a tuple (adjusted_value, was_adjusted_flag);
                         if False (default), return only the adjusted_value.

        Returns:
            The adjusted float value, or a tuple (value, flag) if return_flag is True.

        Raises:
            NodeError: If the specified node does not exist.
            CalculationError: If an error occurs during the base calculation or adjustment application.
            TypeError: If filter_input is an invalid type.
        """
        # 1. Get the base value (result of underlying node calculation)
        try:
            base_value = self.calculate(node_name, period)
        except (NodeError, CalculationError, TypeError):
            # Propagate errors from base calculation
            logger.exception(
                f"Error getting base value for '{node_name}' in period '{period}'"
            )
            raise

        # 2. Get filtered adjustments from the manager
        try:
            adjustments_to_apply = self.adjustment_manager.get_filtered_adjustments(
                node_name=node_name, period=period, filter_input=filter_input
            )
        except TypeError:
            logger.exception("Invalid filter type provided for get_adjusted_value")
            raise

        # 3. Apply the adjustments
        adjusted_value, was_adjusted = self.adjustment_manager.apply_adjustments(
            base_value, adjustments_to_apply
        )

        # 4. Return result based on flag
        if return_flag:
            return adjusted_value, was_adjusted
        else:
            return adjusted_value

    def calculate(self, node_name: str, period: str) -> float:
        """Calculate and return the value of a specific node for a given period.

        This method uses internal caching to speed repeated calls, and wraps
        underlying errors in CalculationError for clarity.

        Args:
            node_name: Name of the node to calculate.
            period: Time period identifier for the calculation.

        Returns:
            The calculated float value for the node and period.

        Raises:
            NodeError: If the specified node does not exist.
            TypeError: If the node has no callable `calculate` method.
            CalculationError: If an error occurs during the node's calculation.

        Examples:
            >>> value = graph.calculate("Revenue", "2023")
        """
        # Return cached value if present
        if node_name in self._cache and period in self._cache[node_name]:
            logger.debug(f"Cache hit for node '{node_name}', period '{period}'")
            return self._cache[node_name][period]
        # Resolve node
        node = self.manipulator.get_node(node_name)
        if node is None:
            raise NodeError(f"Node '{node_name}' not found", node_id=node_name)
        # Validate calculate method
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise TypeError(f"Node '{node_name}' has no callable calculate method.")
        # Perform calculation with error handling
        try:
            value = node.calculate(period)
        except (
            NodeError,
            ConfigurationError,
            CalculationError,
            ValueError,
            KeyError,
            ZeroDivisionError,
        ) as e:
            logger.error(
                f"Error calculating node '{node_name}' for period '{period}': {e}",
                exc_info=True,
            )
            raise CalculationError(
                message=f"Failed to calculate node '{node_name}'",
                node_id=node_name,
                period=period,
                details={"original_error": str(e)},
            ) from e
        # Cache and return
        self._cache.setdefault(node_name, {})[period] = value
        logger.debug(f"Cached value for node '{node_name}', period '{period}': {value}")
        return value

    def recalculate_all(self, periods: Optional[list[str]] = None) -> None:
        """Recalculate all nodes for given periods, clearing all caches first.

        Args:
            periods: List of period strings, a single string, or None to use all periods.

        Returns:
            None

        Raises:
            TypeError: If `periods` is not a list, string, or None.

        Examples:
            >>> graph.recalculate_all(["2023", "2024"])
        """
        # Normalize periods input
        if periods is None:
            periods_to_use = self.periods
        elif isinstance(periods, str):
            periods_to_use = [periods]
        elif isinstance(periods, list):
            periods_to_use = periods
        else:
            raise TypeError(
                "Periods must be a list of strings, a single string, or None."
            )
        # Clear all caches (node-level and central) to force full recalculation
        self.clear_all_caches()
        if not periods_to_use:
            return
        # Recalculate each node for each period
        for node_name in list(self._nodes.keys()):
            for period in periods_to_use:
                try:
                    self.calculate(node_name, period)
                except Exception as e:
                    logger.warning(
                        f"Error recalculating node '{node_name}' for period '{period}': {e}"
                    )

    def clear_all_caches(self) -> None:
        """Clear all node-level and central calculation caches.

        Returns:
            None

        Examples:
            >>> graph.clear_all_caches()
        """
        logger.debug(f"Clearing node-level caches for {len(self.nodes)} nodes.")
        for node in self.nodes.values():
            if hasattr(node, "clear_cache"):
                try:
                    node.clear_cache()
                except Exception as e:
                    logger.warning(f"Failed to clear cache for node '{node.name}': {e}")
        # Clear central calculation cache
        self.clear_calculation_cache()
        logger.debug("Cleared central calculation cache.")

    def clear_calculation_cache(self) -> None:
        """Clear the graph's internal calculation cache.

        Returns:
            None

        Examples:
            >>> graph.clear_calculation_cache()
        """
        self._cache.clear()
        logger.debug("Cleared graph calculation cache.")

    def clear(self) -> None:
        """Reset the graph by clearing nodes, periods, adjustments, and caches."""
        self._nodes = {}
        self._periods = []
        self._cache = {}

        # --- Adjustment Manager Integration ---
        self.adjustment_manager.clear_all()
        # --- End Adjustment Manager Integration ---

        logger.info("Graph cleared: nodes, periods, adjustments, and caches reset.")

    def add_financial_statement_item(
        self, name: str, values: dict[str, float]
    ) -> FinancialStatementItemNode:
        """Add a basic financial statement item (data node) to the graph.

        Args:
            name: Unique name for the financial statement item node.
            values: Mapping of period strings to float values for this item.

        Returns:
            The newly created `FinancialStatementItemNode`.

        Raises:
            ValueError: If node name is invalid.
            TypeError: If `values` is not a dict or contains invalid types.

        Examples:
            >>> item_node = graph.add_financial_statement_item("SG&A", {"2023": 50.0})
            >>> item_node.calculate("2023")
            50.0
        """
        # Validate inputs
        if not isinstance(values, dict):
            raise TypeError("Values must be provided as a dict[str, float]")

        # Create a new financial statement item node
        new_node = self._node_factory.create_financial_statement_item(
            name=name, values=values.copy()
        )

        # Add with validation (no cycle detection needed for data nodes)
        # Cast to FinancialStatementItemNode for correct return type
        from typing import cast

        added_node = cast(
            FinancialStatementItemNode,
            self._add_node_with_validation(
                new_node,
                check_cycles=False,  # Data nodes don't have inputs, so no cycles possible
                validate_inputs=False,  # Data nodes don't have inputs to validate
            ),
        )

        logger.info(
            f"Added FinancialStatementItemNode '{name}' with periods {list(values.keys())}"
        )
        return added_node

    def update_financial_statement_item(
        self, name: str, values: dict[str, float], replace_existing: bool = False
    ) -> FinancialStatementItemNode:
        """Update values for an existing financial statement item node.

        Args:
            name: Name of the existing financial statement item node.
            values: Mapping of new period strings to float values.
            replace_existing: If True, replace existing values entirely; otherwise merge.

        Returns:
            The updated `FinancialStatementItemNode`.

        Raises:
            NodeError: If the node does not exist.
            TypeError: If the node is not a `FinancialStatementItemNode` or `values` is not a dict.

        Examples:
            >>> graph.update_financial_statement_item("SG&A", {"2024": 60.0})
        """
        node = self.manipulator.get_node(name)
        if node is None:
            raise NodeError("Node not found", node_id=name)
        if not isinstance(node, FinancialStatementItemNode):
            raise TypeError(f"Node '{name}' is not a FinancialStatementItemNode")
        if not isinstance(values, dict):
            raise TypeError("Values must be provided as a dict[str, float]")
        if replace_existing:
            node.values = values.copy()
        else:
            node.values.update(values)
        self.add_periods(list(values.keys()))
        logger.info(
            f"Updated FinancialStatementItemNode '{name}' with periods {list(values.keys())}; replace_existing={replace_existing}"
        )
        return node

    def get_financial_statement_items(self) -> list[Node]:
        """Retrieve all financial statement item nodes from the graph.

        Returns:
            A list of `FinancialStatementItemNode` objects currently in the graph.

        Examples:
            >>> items = graph.get_financial_statement_items()
        """
        from fin_statement_model.core.nodes import (
            FinancialStatementItemNode,
        )  # Keep import local as it's specific

        return [
            node
            for node in self.nodes.values()
            if isinstance(node, FinancialStatementItemNode)
        ]

    def __repr__(self) -> str:
        """Provide a concise, developer-friendly string representation of the graph.

        Summarize total nodes, FS items, calculations, dependencies, and periods.

        Returns:
            A string summarizing the graph's structure and contents.

        Examples:
            >>> logger.info(repr(graph))
        """
        from fin_statement_model.core.nodes import (
            FinancialStatementItemNode,
        )  # Keep import local

        num_nodes = len(self.nodes)
        periods_str = ", ".join(map(repr, self.periods)) if self.periods else "None"

        fs_item_count = 0
        calc_node_count = 0
        other_node_count = 0
        dependencies_count = 0

        for node in self.nodes.values():
            if isinstance(node, FinancialStatementItemNode):
                fs_item_count += 1
            elif is_calculation_node(node):
                calc_node_count += 1
                # Prioritize get_dependencies if available, otherwise check inputs
                if hasattr(node, "get_dependencies"):
                    try:
                        dependencies_count += len(node.get_dependencies())
                    except Exception as e:
                        logger.warning(
                            f"Error calling get_dependencies for node '{node.name}': {e}"
                        )
                elif hasattr(node, "inputs"):
                    try:
                        if isinstance(node.inputs, list):
                            # Ensure inputs are nodes with names
                            dep_names = [
                                inp.name for inp in node.inputs if hasattr(inp, "name")
                            ]
                            dependencies_count += len(dep_names)
                        elif isinstance(node.inputs, dict):
                            # Assume keys are dependency names for dict inputs
                            dependencies_count += len(node.inputs)
                    except Exception as e:
                        logger.warning(
                            f"Error processing inputs for node '{node.name}': {e}"
                        )
            else:
                other_node_count += 1

        repr_parts = [
            f"Total Nodes: {num_nodes}",
            f"FS Items: {fs_item_count}",
            f"Calculations: {calc_node_count}",
        ]
        if other_node_count > 0:
            repr_parts.append(f"Other: {other_node_count}")
        repr_parts.append(f"Dependencies: {dependencies_count}")
        repr_parts.append(f"Periods: [{periods_str}]")

        return f"<{type(self).__name__}({', '.join(repr_parts)})>"

    def has_cycle(self, source_node: Node, target_node: Node) -> bool:
        """Check if a cycle exists from a source node to a target node.

        This method delegates to GraphTraverser to determine if `target_node` is
        reachable from `source_node` via successors, indicating that adding an edge
        from `target_node` to `source_node` would create a cycle.

        Args:
            source_node: The starting node for cycle detection.
            target_node: The node to detect return path to.

        Returns:
            True if a cycle exists, False otherwise.
        """
        if source_node.name not in self._nodes or target_node.name not in self._nodes:
            return False

        # Use GraphTraverser's reachability check
        return self.traverser._is_reachable(source_node.name, target_node.name)

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its name.

        Args:
            name: The unique name of the node to retrieve.

        Returns:
            The `Node` instance if found, else None.

        Examples:
            >>> node = graph.get_node("Revenue")
        """
        return self.manipulator.get_node(name)

    def _add_node_with_validation(
        self, node: Node, check_cycles: bool = True, validate_inputs: bool = True
    ) -> Node:
        """Internal method for adding nodes with common validation logic.

        Args:
            node: The Node instance to add
            check_cycles: Whether to perform cycle detection
            validate_inputs: Whether to validate input node references

        Returns:
            The added node

        Raises:
            ValueError: If node name is invalid
            NodeError: If input validation fails
            CircularDependencyError: If adding the node would create a cycle
        """
        # 1. Name validation
        if not node.name or not isinstance(node.name, str):
            raise ValueError("Node name must be a non-empty string")

        # 2. Check for existing node
        if node.name in self._nodes:
            logger.warning(f"Overwriting existing node '{node.name}'")

        # 3. Input validation (if applicable)
        if validate_inputs and hasattr(node, "inputs") and node.inputs:
            self._validate_node_inputs(node)

        # 4. Cycle detection (if applicable)
        if (
            check_cycles
            and hasattr(node, "inputs")
            and node.inputs
            and self.traverser.would_create_cycle(node)
        ):
            # Try to find the actual cycle path for better error message
            cycle_path = None
            for input_node in node.inputs:
                if hasattr(input_node, "name"):
                    path = self.traverser.find_cycle_path(input_node.name, node.name)
                    if path:
                        cycle_path = path
                        break

            raise CircularDependencyError(
                f"Adding node '{node.name}' would create a cycle",
                cycle=cycle_path or [node.name, "...", node.name],
            )

        # 5. Register node
        self._nodes[node.name] = node

        # 6. Update periods if applicable
        if hasattr(node, "values") and isinstance(node.values, dict):
            self.add_periods(list(node.values.keys()))

        logger.debug(f"Added node '{node.name}' to graph")
        return node

    def _validate_node_inputs(self, node: Node) -> None:
        """Validate that all input nodes exist in the graph.

        Args:
            node: The node whose inputs to validate

        Raises:
            NodeError: If any input node is missing
        """
        missing_inputs = []

        if hasattr(node, "inputs") and node.inputs:
            for input_node in node.inputs:
                if hasattr(input_node, "name"):
                    if input_node.name not in self._nodes:
                        missing_inputs.append(input_node.name)
                # Handle case where inputs might be strings instead of Node objects
                elif isinstance(input_node, str) and input_node not in self._nodes:
                    missing_inputs.append(input_node)

        if missing_inputs:
            raise NodeError(
                f"Cannot add node '{node.name}': missing required input nodes {missing_inputs}",
                node_id=node.name,
            )

    def _resolve_input_nodes(self, input_names: list[str]) -> list[Node]:
        """Resolve input node names to Node objects.

        Args:
            input_names: List of node names to resolve

        Returns:
            List of resolved Node objects

        Raises:
            NodeError: If any input node name does not exist
        """
        resolved_inputs: list[Node] = []
        missing = []

        for name in input_names:
            node = self._nodes.get(name)
            if node is None:
                missing.append(name)
            else:
                resolved_inputs.append(node)

        if missing:
            raise NodeError(f"Cannot resolve input nodes: missing nodes {missing}")

        return resolved_inputs

    def add_node(self, node: Node) -> None:
        """Add a node to the graph.

        Args:
            node: A ``Node`` instance to add to the graph.

        Raises:
            TypeError: If the provided object is not a Node instance.

        Examples:
            >>> from fin_statement_model.core.nodes import FinancialStatementItemNode
            >>> node = FinancialStatementItemNode("Revenue", {"2023": 1000})
            >>> graph.add_node(node)
        """
        from fin_statement_model.core.nodes.base import Node as _NodeBase

        if not isinstance(node, _NodeBase):
            raise TypeError(f"Expected Node instance, got {type(node).__name__}")

        return self.manipulator.add_node(node)

    def remove_node(self, node_name: str) -> None:
        """Remove a node from the graph by name, updating dependencies.

        Args:
            node_name: The name of the node to remove.

        Returns:
            None

        Examples:
            >>> graph.remove_node("OldItem")
        """
        return self.manipulator.remove_node(node_name)

    def replace_node(self, node_name: str, new_node: Node) -> None:
        """Replace an existing node with a new node instance.

        Args:
            node_name: Name of the node to replace.
            new_node: The new `Node` instance to substitute.

        Returns:
            None

        Examples:
            >>> graph.replace_node("Item", updated_node)
        """
        return self.manipulator.replace_node(node_name, new_node)

    def has_node(self, node_id: str) -> bool:
        """Check if a node with the given ID exists in the graph.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> graph.has_node("Revenue")
        """
        return self.manipulator.has_node(node_id)

    def set_value(self, node_id: str, period: str, value: float) -> None:
        """Set or update the value for a node in a specific period.

        Args:
            node_id: The name of the node.
            period: The period identifier to set the value for.
            value: The float value to assign.

        Returns:
            None

        Raises:
            ValueError: If the period is not recognized by the graph.
            NodeError: If the node does not exist.
            TypeError: If the node does not support setting a value.

        Examples:
            >>> graph.set_value("SG&A", "2024", 55.0)
        """
        return self.manipulator.set_value(node_id, period, value)

    def topological_sort(self) -> list[str]:
        """Perform a topological sort of all graph nodes.

        Returns:
            A list of node IDs in topological order.

        Raises:
            ValueError: If a cycle is detected in the graph.

        Examples:
            >>> order = graph.topological_sort()
        """
        return self.traverser.topological_sort()

    def get_calculation_nodes(self) -> list[str]:
        """Get all calculation node IDs in the graph.

        Returns:
            A list of node names that have associated calculations.

        Examples:
            >>> graph.get_calculation_nodes()
        """
        return self.traverser.get_calculation_nodes()

    def get_dependencies(self, node_id: str) -> list[str]:
        """Get the direct predecessor node IDs (dependencies) for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node depends on.

        Examples:
            >>> graph.get_dependencies("GrossProfit")
        """
        return self.traverser.get_dependencies(node_id)

    def get_dependency_graph(self) -> dict[str, list[str]]:
        """Get the full dependency graph mapping of node IDs to their inputs.

        Returns:
            A dict mapping each node ID to a list of its dependency node IDs.

        Examples:
            >>> graph.get_dependency_graph()
        """
        return self.traverser.get_dependency_graph()

    def detect_cycles(self) -> list[list[str]]:
        """Detect all cycles in the graph's dependency structure.

        Returns:
            A list of cycles, each represented as a list of node IDs.

        Examples:
            >>> graph.detect_cycles()
        """
        return self.traverser.detect_cycles()

    def validate(self) -> list[str]:
        """Validate the graph structure for errors such as cycles or missing nodes.

        Returns:
            A list of validation error messages, empty if valid.

        Examples:
            >>> graph.validate()
        """
        return self.traverser.validate()

    def breadth_first_search(
        self, start_node: str, direction: str = "successors"
    ) -> list[list[str]]:
        """Perform a breadth-first search (BFS) traversal of the graph.

        Args:
            start_node: The starting node ID for BFS.
            direction: Either 'successors' or 'predecessors' to traverse.

        Returns:
            A nested list of node IDs per BFS level.

        Raises:
            ValueError: If `direction` is not 'successors' or 'predecessors'.

        Examples:
            >>> graph.breadth_first_search("Revenue", "successors")
        """
        return self.traverser.breadth_first_search(start_node, direction)

    def get_direct_successors(self, node_id: str) -> list[str]:
        """Get immediate successor node IDs for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that directly follow the given node.

        Examples:
            >>> graph.get_direct_successors("Revenue")
        """
        return self.traverser.get_direct_successors(node_id)

    def get_direct_predecessors(self, node_id: str) -> list[str]:
        """Get immediate predecessor node IDs (inputs) for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node directly depends on.

        Examples:
            >>> graph.get_direct_predecessors("GrossProfit")
        """
        return self.traverser.get_direct_predecessors(node_id)

    def merge_from(self, other_graph: "Graph") -> None:
        """Merge nodes and periods from another Graph into this one.

        Adds periods from the other graph if they don't exist in this graph.
        Adds nodes from the other graph if they don't exist.
        If a node exists in both graphs, attempts to merge the 'values' dictionary
        from the other graph's node into this graph's node.

        Args:
            other_graph: The Graph instance to merge data from.

        Raises:
            TypeError: If other_graph is not a Graph instance.
        """
        if not isinstance(other_graph, Graph):
            raise TypeError("Can only merge from another Graph instance.")

        logger.info(f"Starting merge from graph {other_graph!r} into {self!r}")

        # 1. Update periods
        new_periods = [p for p in other_graph.periods if p not in self.periods]
        if new_periods:
            self.add_periods(new_periods)
            logger.debug(f"Merged periods: {new_periods}")

        # 2. Merge nodes
        nodes_added = 0
        nodes_updated = 0
        for node_name, other_node in other_graph.nodes.items():
            existing_node = self.get_node(node_name)
            if existing_node is not None:
                # Node exists, merge values if applicable
                if (
                    hasattr(existing_node, "values")
                    and hasattr(other_node, "values")
                    and isinstance(getattr(existing_node, "values", None), dict)
                    and isinstance(getattr(other_node, "values", None), dict)
                ):
                    try:
                        # Perform the update
                        existing_node.values.update(other_node.values)
                        nodes_updated += 1
                        logger.debug(f"Merged values into existing node '{node_name}'")
                        # No need to call self.add_node(existing_node) as it's already there
                    except AttributeError:
                        # Should not happen due to hasattr checks, but defensive
                        logger.warning(
                            f"Could not merge values for node '{node_name}' due to missing 'values' attribute despite hasattr check."
                        )
                    except Exception as e:
                        logger.warning(
                            f"Could not merge values for node '{node_name}': {e}"
                        )
                else:
                    # Nodes exist but cannot merge values (e.g., calculation nodes without stored values)
                    logger.debug(
                        f"Node '{node_name}' exists in both graphs, but values not merged (missing/incompatible 'values' attribute). Keeping target graph's node."
                    )
            else:
                # Node doesn't exist in target graph, add it
                try:
                    # Ensure we add a copy if nodes might be shared or mutable in complex ways,
                    # but for now, assume adding the instance is okay.
                    self.add_node(other_node)
                    nodes_added += 1
                except Exception:
                    logger.exception(
                        f"Failed to add new node '{node_name}' during merge:"
                    )

        logger.info(
            f"Merge complete. Nodes added: {nodes_added}, Nodes updated (values merged): {nodes_updated}"
        )

    # --- Adjustment Management API ---

    def add_adjustment(
        self,
        node_name: str,
        period: str,
        value: float,
        reason: str,
        adj_type: AdjustmentType = AdjustmentType.ADDITIVE,
        scale: float = 1.0,
        priority: int = 0,
        tags: Optional[set[AdjustmentTag]] = None,
        scenario: Optional[str] = None,
        user: Optional[str] = None,
        start_period: Optional[str] = None,  # Phase 2
        end_period: Optional[str] = None,  # Phase 2
        adj_id: Optional[UUID] = None,  # Allow specifying ID, e.g., for re-creation
    ) -> UUID:
        """Adds a discretionary adjustment to a specific node and period.

        Creates an Adjustment object and delegates storage to the AdjustmentManager.

        Args:
            node_name: The name of the target node.
            period: The primary period the adjustment applies to.
            value: The numeric value of the adjustment.
            reason: Text description of why the adjustment was made.
            adj_type: How the adjustment combines with the base value.
            scale: Attenuation factor for the adjustment (0.0 to 1.0, Phase 2).
            priority: Tie-breaker for applying multiple adjustments (lower number applied first).
            tags: Set of descriptive tags for filtering and analysis.
            scenario: The named scenario this adjustment belongs to. Defaults to DEFAULT_SCENARIO if None.
            user: Identifier for the user who created the adjustment.
            start_period: The first period the adjustment is effective (inclusive, Phase 2).
            end_period: The last period the adjustment is effective (inclusive, Phase 2).
            adj_id: Optional specific UUID to use for the adjustment.

        Returns:
            The UUID of the created or updated adjustment.

        Raises:
            NodeError: If the target node_name does not exist in the graph.
            ValidationError: If adjustment parameters are invalid (e.g., scale out of bounds).
        """
        if not self.has_node(node_name):
            raise NodeError(
                f"Cannot add adjustment: Node '{node_name}' not found.",
                node_id=node_name,
            )

        # Need Pydantic's ValidationError and uuid4
        from pydantic import ValidationError
        from uuid import uuid4

        # Need Adjustment model details
        from fin_statement_model.core.adjustments.models import Adjustment

        # Assign default scenario if None was passed
        actual_scenario = scenario if scenario is not None else DEFAULT_SCENARIO

        # Create the adjustment object - Pydantic handles validation (e.g., scale)
        try:
            adj = Adjustment(
                id=adj_id or uuid4(),  # Generate new ID if not provided
                node_name=node_name,
                period=period,
                start_period=start_period,
                end_period=end_period,
                value=value,
                type=adj_type,
                scale=scale,
                priority=priority,
                tags=tags or set(),
                scenario=actual_scenario,  # Use the actual scenario
                reason=reason,
                user=user,
                # timestamp is added automatically by the model
            )
        except ValidationError:
            logger.exception(f"Failed to create adjustment for node '{node_name}'")
            raise  # Re-raise Pydantic's validation error

        self.adjustment_manager.add_adjustment(adj)
        logger.info(
            f"Added adjustment {adj.id} for node '{node_name}', period '{period}', scenario '{scenario}'."
        )
        return adj.id

    def remove_adjustment(self, adj_id: UUID) -> bool:
        """Removes an adjustment by its unique ID.

        Args:
            adj_id: The UUID of the adjustment to remove.

        Returns:
            True if an adjustment was found and removed, False otherwise.
        """
        removed = self.adjustment_manager.remove_adjustment(adj_id)
        if removed:
            logger.info(f"Removed adjustment {adj_id}.")
        else:
            logger.warning(f"Attempted to remove non-existent adjustment {adj_id}.")
        return removed

    def get_adjustments(
        self, node_name: str, period: str, *, scenario: Optional[str] = None
    ) -> list[Adjustment]:
        """Retrieves all adjustments for a specific node, period, and scenario.

        Args:
            node_name: The name of the target node.
            period: The target period.
            scenario: The scenario to retrieve adjustments for. Defaults to DEFAULT_SCENARIO if None.

        Returns:
            A list of Adjustment objects matching the criteria, sorted by application order.
        """
        if not self.has_node(node_name):
            # Or return empty list? Returning empty seems safer.
            logger.warning(f"Node '{node_name}' not found when getting adjustments.")
            return []
        # Assign default scenario if None was passed
        actual_scenario = scenario if scenario is not None else DEFAULT_SCENARIO
        return self.adjustment_manager.get_adjustments(
            node_name, period, scenario=actual_scenario
        )

    def list_all_adjustments(self) -> list[Adjustment]:
        """Returns a list of all adjustments currently managed by the graph.

        Returns:
            A list containing all Adjustment objects across all nodes, periods, and scenarios.
        """
        return self.adjustment_manager.get_all_adjustments()

    def was_adjusted(
        self, node_name: str, period: str, filter_input: "AdjustmentFilterInput" = None
    ) -> bool:
        """Checks if a node's value for a given period was affected by any selected adjustments.

        Args:
            node_name: The name of the node to check.
            period: The time period identifier.
            filter_input: Criteria for selecting which adjustments to consider (same as get_adjusted_value).

        Returns:
            True if any adjustment matching the filter was applied to the base value, False otherwise.

        Raises:
            NodeError: If the specified node does not exist.
            CalculationError: If an error occurs during the underlying calculation.
            TypeError: If filter_input is an invalid type.
        """
        try:
            _, was_adjusted_flag = self.get_adjusted_value(
                node_name, period, filter_input, return_flag=True
            )
            return was_adjusted_flag
        except (NodeError, CalculationError, TypeError):
            # Propagate errors consistently
            logger.exception(f"Error checking if node '{node_name}' was adjusted")
            raise

    # --- End Adjustment Management API ---



================================================================================
File: fin_statement_model/core/graph/manipulator.py
================================================================================

"""Utilities for mutating a :class:`~fin_statement_model.core.graph.graph.Graph` instance.

The *Graph* class exposes a :pyattr:`graph.manipulator <fin_statement_model.core.graph.graph.Graph.manipulator>`
attribute that is an instance of :class:`GraphManipulator`.  The manipulator
groups together all *write* operations on the graph – adding, removing or
replacing nodes, updating values, and clearing caches – so that the rest of the
code base can rely on a single consistency layer.

Key responsibilities
===================
1. Ensure new nodes are properly registered on the graph.
2. Keep calculation-node input references up-to-date after structural changes.
3. Invalidate per-node and global caches whenever something that could affect
   results is modified.

Although you *can* instantiate `GraphManipulator` directly, in normal usage you
retrieve it from an existing graph:

Examples
~~~~~~~~
>>> from fin_statement_model.core.graph import Graph
>>> g = Graph(periods=["2023"])
>>> _ = g.add_financial_statement_item("Revenue", {"2023": 100.0})
>>> g.calculate("Revenue", "2023")
100.0

Now update the value via the manipulator and observe caches being cleared:

>>> g.manipulator.set_value("Revenue", "2023", 110.0)
>>> g.calculate("Revenue", "2023")
110.0

The manipulator is *internal API*; it is documented here solely to aid
contributors.  End-users should prefer the higher-level convenience methods on
`Graph` itself whenever possible.
"""

import logging
from typing import Optional, Any, cast
from fin_statement_model.core.errors import NodeError
from fin_statement_model.core.nodes import Node, CalculationNode

logger = logging.getLogger(__name__)


class GraphManipulator:
    """Encapsulate node-level mutation helpers for Graph.

    Attributes:
        graph: The Graph instance this manipulator operates on.
    """

    def __init__(self, graph: Any) -> None:
        """Initialize the GraphManipulator with a Graph reference.

        Args:
            graph: The Graph instance to manipulate.
        """
        self.graph = graph

    def add_node(self, node: Node) -> None:
        """Add a node to the graph, replacing any existing node with the same name.

        Args:
            node: The Node instance to add.

        Returns:
            None

        Raises:
            TypeError: If the provided object is not a Node instance.

        Examples:
            >>> manipulator.add_node(node)
        """
        if not isinstance(node, Node):
            raise TypeError(f"Object {node} is not a valid Node instance.")
        if self.has_node(node.name):
            self.remove_node(node.name)
        self.graph._nodes[node.name] = node

    def _update_calculation_nodes(self) -> None:
        """Refresh input references for all calculation nodes after structure changes.

        This method re-resolves `input_names` to current Node objects and clears
        individual node caches.

        Returns:
            None
        """
        for nd in self.graph._nodes.values():
            if (
                isinstance(nd, CalculationNode)
                and hasattr(nd, "input_names")
                and nd.input_names
            ):
                try:
                    resolved_inputs: list[Node] = []
                    for name in nd.input_names:
                        input_node = self.get_node(name)
                        if input_node is None:
                            raise NodeError(
                                f"Input node '{name}' not found for calculation node '{nd.name}'"
                            )
                        resolved_inputs.append(input_node)
                    nd.inputs = resolved_inputs
                    if hasattr(nd, "clear_cache"):
                        nd.clear_cache()
                except NodeError:
                    logger.exception(f"Error updating inputs for node '{nd.name}'")
                except AttributeError:
                    logger.warning(
                        f"Node '{nd.name}' has input_names but no 'inputs' attribute to update."
                    )

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its unique name.

        Args:
            name: The unique node name to retrieve.

        Returns:
            The Node instance if found, else None.

        Examples:
            >>> manipulator.get_node("Revenue")
        """
        return cast(Optional[Node], self.graph._nodes.get(name))

    def replace_node(self, node_name: str, new_node: Node) -> None:
        """Replace an existing node with a new one, ensuring consistency.

        Args:
            node_name: Name of the node to replace.
            new_node: The new Node instance; its name must match `node_name`.

        Returns:
            None

        Raises:
            NodeError: If `node_name` does not exist.
            ValueError: If `new_node.name` does not match `node_name`.

        Examples:
            >>> manipulator.replace_node("Revenue", updated_node)
        """
        if not self.has_node(node_name):
            raise NodeError(f"Node '{node_name}' not found, cannot replace.")
        if node_name != new_node.name:
            raise ValueError(
                "New node name must match the name of the node being replaced."
            )
        self.remove_node(node_name)
        self.add_node(new_node)

    def has_node(self, node_id: str) -> bool:
        """Check if a node with the given ID exists.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> manipulator.has_node("Revenue")
        """
        return node_id in self.graph._nodes

    def remove_node(self, node_name: str) -> None:
        """Remove a node from the graph and update calculation nodes.

        Args:
            node_name: The name of the node to remove.

        Returns:
            None

        Examples:
            >>> manipulator.remove_node("OldItem")
        """
        if not self.has_node(node_name):
            return
        self.graph._nodes.pop(node_name, None)
        self._update_calculation_nodes()

    def set_value(self, node_id: str, period: str, value: float) -> None:
        """Set the value for a specific node and period, clearing all caches.

        Args:
            node_id: The name of the node.
            period: The time period identifier.
            value: The numeric value to assign.

        Returns:
            None

        Raises:
            ValueError: If `period` is not recognized by the graph.
            NodeError: If the node does not exist.
            TypeError: If the node does not support setting a value.

        Examples:
            >>> manipulator.set_value("Revenue", "2023", 1100.0)
        """
        if period not in self.graph._periods:
            raise ValueError(f"Period '{period}' not in graph periods")
        nd = self.get_node(node_id)
        if not nd:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if not hasattr(nd, "set_value"):
            raise TypeError(
                f"Node '{node_id}' of type {type(nd).__name__} does not support set_value."
            )
        nd.set_value(period, value)
        self.graph.clear_all_caches()

    def clear_all_caches(self) -> None:
        """Clear caches associated with individual nodes in the graph.

        Returns:
            None

        Examples:
            >>> manipulator.clear_all_caches()
        """
        for nd in self.graph._nodes.values():
            if hasattr(nd, "clear_cache"):
                nd.clear_cache()



================================================================================
File: fin_statement_model/core/graph/traverser.py
================================================================================

"""Read-only traversal helpers for :class:`~fin_statement_model.core.graph.graph.Graph`.

`GraphTraverser` complements :class:`~fin_statement_model.core.graph.manipulator.GraphManipulator` by
offering a *read-only* view on a graph's structure.  It collects utility
methods required by higher-level APIs such as:

* dependency inspection (predecessors/successors)
* topological sorting
* cycle detection and validation
* breadth-first searches for visualisation or debugging

Unlike the manipulator, the traverser **never mutates** the graph; this makes it
safe to call from anywhere, including within calculation routines.

Examples
~~~~~~~~
>>> from fin_statement_model.core.graph import Graph
>>> g = Graph(periods=["2023", "2024"])
>>> _ = g.add_financial_statement_item("Revenue", {"2023": 100, "2024": 110})
>>> _ = g.add_financial_statement_item("COGS", {"2023": 60, "2024": 70})
>>> _ = g.add_calculation("GrossProfit", ["Revenue", "COGS"], "addition", formula="input_0 - input_1", formula_variable_names=["input_0", "input_1"])
>>> g.traverser.get_dependencies("GrossProfit")
['Revenue', 'COGS']

You can also validate the full graph:

>>> g.traverser.validate()
[]

As with the manipulator, end-users reach the traverser via
``graph.traverser`` rather than instantiating it directly.
"""

import logging
from typing import Optional, Any, TYPE_CHECKING, cast
from collections import deque

from fin_statement_model.core.errors import NodeError
from fin_statement_model.core.nodes import Node, is_calculation_node

if TYPE_CHECKING:
    from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class GraphTraverser:
    """Encapsulate traversal and validation helpers for Graph.

    Attributes:
        graph: The Graph instance this traverser operates on.
    """

    def __init__(self, graph: Any) -> None:
        """Initialize the GraphTraverser with a Graph reference.

        Args:
            graph: The Graph instance to traverse.
        """
        self.graph = graph

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its name.

        Args:
            name: The unique name of the node to retrieve.

        Returns:
            The Node instance if found, else None.

        Examples:
            >>> traverser.get_node("Revenue")
        """
        return cast(Optional[Node], self.graph.manipulator.get_node(name))

    def has_node(self, node_id: str) -> bool:
        """Check if a node exists in the graph.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> traverser.has_node("Revenue")
        """
        return cast(bool, self.graph.manipulator.has_node(node_id))

    @property
    def nodes(self) -> dict[str, Node]:
        """Access the full node registry dictionary.

        Returns:
            A dict mapping node names to Node instances.

        Examples:
            >>> list(traverser.nodes.keys())
        """
        return cast(dict[str, Node], self.graph.nodes)

    def get_direct_successors(self, node_id: str) -> list[str]:
        """Get immediate successor node IDs for a given node.

        Args:
            node_id: The name of the node whose successors to retrieve.

        Returns:
            A list of node IDs that directly follow the given node.

        Examples:
            >>> traverser.get_direct_successors("Revenue")
        """
        successors: list[str] = []
        for other_id, node in self.nodes.items():
            if hasattr(node, "inputs"):
                input_nodes: list[Node] = []
                if isinstance(node.inputs, list):
                    input_nodes = node.inputs
                elif isinstance(node.inputs, dict):
                    input_nodes = list(node.inputs.values())

                if any(
                    inp.name == node_id for inp in input_nodes if hasattr(inp, "name")
                ):
                    successors.append(other_id)
        return successors

    def get_direct_predecessors(self, node_id: str) -> list[str]:
        """Get immediate predecessor node IDs (dependencies) for a given node.

        Args:
            node_id: The name of the node whose dependencies to retrieve.

        Returns:
            A list of node IDs that the given node depends on.

        Raises:
            NodeError: If the node does not exist.

        Examples:
            >>> traverser.get_direct_predecessors("GrossProfit")
        """
        node = self.get_node(node_id)
        if not node:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if hasattr(node, "inputs"):
            return [inp.name for inp in node.inputs]
        return []

    def topological_sort(self) -> list[str]:
        """Perform a topological sort of nodes based on dependencies.

        Returns:
            A list of node IDs in topological order.

        Raises:
            ValueError: If a cycle is detected in the graph.

        Examples:
            >>> traverser.topological_sort()
        """
        in_degree: dict[str, int] = {n: 0 for n in self.nodes}
        adjacency: dict[str, list[str]] = {n: [] for n in self.nodes}
        for name, node in self.nodes.items():
            if hasattr(node, "inputs"):
                for inp in node.inputs:
                    adjacency[inp.name].append(name)
                    in_degree[name] += 1
        queue: list[str] = [n for n, d in in_degree.items() if d == 0]
        topo_order: list[str] = []
        while queue:
            current = queue.pop()
            topo_order.append(current)
            for nbr in adjacency[current]:
                in_degree[nbr] -= 1
                if in_degree[nbr] == 0:
                    queue.append(nbr)
        if len(topo_order) != len(self.nodes):
            raise ValueError(
                "Cycle detected in graph, can't do a valid topological sort."
            )
        return topo_order

    def get_calculation_nodes(self) -> list[str]:
        """Identify all nodes in the graph that represent calculations.

        Returns:
            A list of node IDs for nodes with calculations.

        Examples:
            >>> traverser.get_calculation_nodes()
        """
        return [
            node_id for node_id, node in self.nodes.items() if is_calculation_node(node)
        ]

    def get_dependencies(self, node_id: str) -> list[str]:
        """Retrieve the direct dependencies (inputs) of a specific node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node depends on.

        Raises:
            NodeError: If the node does not exist.

        Examples:
            >>> traverser.get_dependencies("GrossProfit")
        """
        node = self.get_node(node_id)
        if not node:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if hasattr(node, "inputs"):
            return [inp.name for inp in node.inputs]
        return []

    def get_dependency_graph(self) -> dict[str, list[str]]:
        """Construct a representation of the full dependency graph.

        Returns:
            A dict mapping each node ID to its list of dependency node IDs.

        Examples:
            >>> traverser.get_dependency_graph()
        """
        dependencies: dict[str, list[str]] = {}
        for node_id, node in self.nodes.items():
            try:
                if hasattr(node, "inputs"):
                    dependencies[node_id] = [inp.name for inp in node.inputs]
                else:
                    dependencies[node_id] = []
            except NodeError:
                dependencies[node_id] = []
        return dependencies

    def detect_cycles(self) -> list[list[str]]:
        """Detect all cycles present in the graph's dependency structure.

        Returns:
            A list of cycles, each cycle is a list of node IDs forming the cycle.

        Examples:
            >>> traverser.detect_cycles()
        """
        dependency_graph = self.get_dependency_graph()
        visited: set[str] = set()
        rec_stack: set[str] = set()
        cycles: list[list[str]] = []

        def dfs_detect_cycles(n_id: str, path: Optional[list[str]] = None) -> None:
            if path is None:
                path = []
            if n_id in rec_stack:
                cycle_start = path.index(n_id)
                cycle = path[cycle_start:] + [n_id]
                if cycle not in cycles:
                    cycles.append(cycle)
                return
            if n_id in visited:
                return
            visited.add(n_id)
            rec_stack.add(n_id)
            path.append(n_id)
            for dep in dependency_graph.get(n_id, []):
                dfs_detect_cycles(dep, path[:])
            rec_stack.remove(n_id)

        for node_id in self.nodes:
            if node_id not in visited:
                dfs_detect_cycles(node_id)
        return cycles

    def validate(self) -> list[str]:
        """Perform validation checks on the graph structure.

        Returns:
            A list of validation error messages; empty list if graph is valid.

        Examples:
            >>> traverser.validate()
        """
        errors: list[str] = [
            f"Circular dependency detected: {' -> '.join(cycle)}"
            for cycle in self.detect_cycles()
        ]
        errors.extend(
            f"Node '{node_id}' depends on non-existent node '{inp.name}'"
            for node_id, node in self.nodes.items()
            if hasattr(node, "inputs")
            for inp in node.inputs
            if not self.has_node(inp.name)
        )
        return errors

    def breadth_first_search(
        self, start_node: str, direction: str = "successors"
    ) -> list[list[str]]:
        """Perform a breadth-first search (BFS) traversal of the graph.

        Args:
            start_node: The starting node ID for the traversal.
            direction: The traversal direction, either 'successors' or 'predecessors'.

        Returns:
            A list of levels, each level is a list of node IDs visited at that depth.

        Raises:
            ValueError: If `direction` is not 'successors' or 'predecessors'.

        Examples:
            >>> traverser.breadth_first_search("Revenue", "successors")
        """
        if direction not in ["successors", "predecessors"]:
            raise ValueError("Invalid direction. Use 'successors' or 'predecessors'.")

        visited = set()
        queue = deque([start_node])
        visited.add(start_node)
        traversal_order = []

        while queue:
            level_size = len(queue)
            current_level = []

            for _ in range(level_size):
                n_id = queue.popleft()
                current_level.append(n_id)

                if direction == "successors":
                    for successor in self.get_direct_successors(n_id):
                        if successor not in visited:
                            visited.add(successor)
                            queue.append(successor)
                elif direction == "predecessors":
                    for predecessor in self.get_direct_predecessors(n_id):
                        if predecessor not in visited:
                            visited.add(predecessor)
                            queue.append(predecessor)

            traversal_order.append(current_level)

        return traversal_order

    def would_create_cycle(self, new_node: "Node") -> bool:
        """Check if adding a node would create a cycle.

        Args:
            new_node: The node to be added (must have 'inputs' attribute)

        Returns:
            True if adding the node would create a cycle
        """
        if not hasattr(new_node, "inputs") or not new_node.inputs:
            return False

        # For each input, check if new_node is reachable from it
        for input_node in new_node.inputs:
            if hasattr(input_node, "name") and self._is_reachable(
                input_node.name, new_node.name
            ):
                return True
        return False

    def _is_reachable(self, from_node: str, to_node: str) -> bool:
        """Check if to_node is reachable from from_node.

        Args:
            from_node: Starting node name
            to_node: Target node name

        Returns:
            True if to_node is reachable from from_node via successors
        """
        # If from_node doesn't exist, no reachability
        if from_node not in self.graph._nodes:
            return False

        # If to_node doesn't exist yet, check temporary reachability
        if to_node not in self.graph._nodes:
            return False

        try:
            bfs_levels = self.breadth_first_search(
                start_node=from_node, direction="successors"
            )
            reachable_nodes = {n for level in bfs_levels for n in level}
            return to_node in reachable_nodes
        except (ValueError, KeyError):
            # Handle cases where BFS fails (e.g., invalid node)
            return False

    def find_cycle_path(self, from_node: str, to_node: str) -> Optional[list[str]]:
        """Find the actual cycle path if one exists.

        Args:
            from_node: Starting node name
            to_node: Target node name that would complete the cycle

        Returns:
            List of node names forming the cycle path, or None if no cycle
        """
        if not self._is_reachable(from_node, to_node):
            return None

        # Use DFS to find the actual path
        visited = set()
        path: list[str] = []

        def dfs_find_path(current: str, target: str) -> bool:
            if current == target and len(path) > 0:
                return True
            if current in visited:
                return False

            visited.add(current)
            path.append(current)

            # Get successors of current node
            for successor in self.get_direct_successors(current):
                if dfs_find_path(successor, target):
                    return True

            path.pop()
            return False

        if dfs_find_path(from_node, to_node):
            return [*path, to_node]  # Complete the cycle
        return None



================================================================================
File: fin_statement_model/core/metrics/__init__.py
================================================================================

"""Provide a comprehensive system for defining, calculating, and interpreting financial metrics.

This module defines:
- MetricDefinition: Pydantic model for metric definitions
- MetricRegistry: Registry for loading and managing metrics
- MetricInterpreter: System for interpreting metric values with ratings
- calculate_metric: Helper to calculate a metric by name
- interpret_metric: Convenience function to interpret a metric value
- Built-in metrics: 75+ professional financial metrics organized by category

The metrics are organized into logical categories:
- Liquidity: Current ratio, quick ratio, working capital analysis
- Leverage: Debt ratios, coverage ratios, capital structure
- Profitability: Margins, returns on assets/equity/capital
- Efficiency: Asset turnover, working capital efficiency
- Valuation: Price multiples, enterprise value ratios
- Cash Flow: Cash generation, cash returns, quality metrics
- Growth: Revenue, earnings, asset growth rates
- Credit Risk: Altman Z-scores, warning flags
- Advanced: DuPont analysis, specialized ratios
"""

import logging
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from fin_statement_model.core.nodes.base import Node

from .models import MetricDefinition, MetricInterpretation
from .registry import MetricRegistry, metric_registry
from .interpretation import MetricInterpreter, MetricRating, interpret_metric

logger = logging.getLogger(__name__)

# Load metrics from the organized structure by default
try:
    # Load from organized structure
    organized_path = Path(__file__).parent / "metric_defn"
    if organized_path.exists():
        logger.info("Loading metrics from organized structure")
        from .metric_defn import load_organized_metrics

        organized_count = load_organized_metrics()
        logger.info(
            f"Successfully loaded {organized_count} metrics from organized structure"
        )
    else:
        logger.warning("Organized metric structure not found - no metrics loaded")

except Exception:
    logger.exception("Failed to load built-in metrics from organized structure")


def calculate_metric(
    metric_name: str,
    data_nodes: dict[str, "Node"],
    period: str,
    node_name: str | None = None,
) -> float:
    """Calculate a metric value using the metric registry and data nodes.

    This helper function simplifies the common pattern of:
    1. Getting a metric definition from the registry
    2. Creating a FormulaCalculationNode with the appropriate inputs
    3. Calculating the result for a specific period

    Args:
        metric_name: Name of the metric in the registry (e.g., "debt_yield")
        data_nodes: Dictionary mapping node names to Node instances
        period: Time period for calculation (e.g., "2023")
        node_name: Optional name for the calculation node (defaults to metric_name)

    Returns:
        The calculated metric value as a float

    Raises:
        KeyError: If the metric is not found in the registry
        ValueError: If required input nodes are missing from data_nodes
        CalculationError: If the calculation fails

    Examples:
        >>> data_nodes = {
        ...     "net_operating_income": FinancialStatementItemNode("noi", {"2023": 1000000}),
        ...     "total_debt": FinancialStatementItemNode("debt", {"2023": 10000000})
        ... }
        >>> debt_yield = calculate_metric("debt_yield", data_nodes, "2023")
        >>> print(f"Debt Yield: {debt_yield:.1f}%")
        Debt Yield: 10.0%
    """
    # Import here to avoid circular imports
    from fin_statement_model.core.nodes.calculation_nodes import FormulaCalculationNode

    # Get metric definition from registry
    try:
        metric_def = metric_registry.get(metric_name)
    except KeyError:
        available_metrics = metric_registry.list_metrics()
        raise KeyError(
            f"Metric '{metric_name}' not found in registry. "
            f"Available metrics: {available_metrics[:10]}..."  # Show first 10
        )

    # Build input mapping for the formula
    inputs = {}
    missing_inputs = []

    for input_name in metric_def.inputs:
        if input_name in data_nodes:
            inputs[input_name] = data_nodes[input_name]
        else:
            missing_inputs.append(input_name)

    if missing_inputs:
        available_nodes = list(data_nodes.keys())
        raise ValueError(
            f"Missing required input nodes for metric '{metric_name}': {missing_inputs}. "
            f"Available nodes: {available_nodes}"
        )

    # Create calculation node
    calc_node_name = node_name or f"{metric_name}_calc"
    calc_node = FormulaCalculationNode(
        calc_node_name,
        inputs=inputs,
        formula=metric_def.formula,
        metric_name=metric_name,
        metric_description=metric_def.description,
    )

    # Calculate and return result
    return calc_node.calculate(period)


__all__ = [
    "MetricDefinition",
    "MetricInterpretation",
    "MetricInterpreter",
    "MetricRating",
    "MetricRegistry",
    "calculate_metric",
    "interpret_metric",
    "metric_registry",
]



================================================================================
File: fin_statement_model/core/metrics/interpretation.py
================================================================================

"""Provide utilities for interpreting metric values based on defined guidelines."""

from enum import Enum
from typing import Any
from fin_statement_model.core.metrics.models import (
    MetricDefinition,
)


class MetricRating(Enum):
    """Rating levels for metric values."""

    EXCELLENT = "excellent"
    GOOD = "good"
    ADEQUATE = "adequate"
    WARNING = "warning"
    POOR = "poor"
    UNKNOWN = "unknown"


class MetricInterpreter:
    """Interpret metric values based on defined guidelines.

    Example:
        >>> from fin_statement_model.core.metrics import metric_registry, MetricInterpreter
        >>> metric_def = metric_registry.get("current_ratio")
        >>> interpreter = MetricInterpreter(metric_def)
        >>> interpreter.rate_value(1.8)
        <MetricRating.GOOD: 'good'>
    """

    def __init__(self, metric_definition: MetricDefinition):
        """Initialize with a metric definition.

        Args:
            metric_definition: The metric definition containing interpretation guidelines.
        """
        self.metric_definition = metric_definition
        self.interpretation = metric_definition.interpretation

    def rate_value(self, value: float) -> MetricRating:
        """Rate a metric value based on interpretation guidelines.

        Args:
            value: The metric value to rate.

        Returns:
            MetricRating indicating the quality of the value.

        Example:
            >>> interpreter.rate_value(1.8)
            <MetricRating.GOOD: 'good'>
        """
        if not self.interpretation:
            return MetricRating.UNKNOWN

        # Check for excellent rating
        if (
            self.interpretation.excellent_above is not None
            and value >= self.interpretation.excellent_above
        ):
            return MetricRating.EXCELLENT

        # Check for poor rating
        if (
            self.interpretation.poor_below is not None
            and value < self.interpretation.poor_below
        ):
            return MetricRating.POOR

        # Check for warning conditions
        warning_conditions = []
        if (
            self.interpretation.warning_below is not None
            and value < self.interpretation.warning_below
        ):
            warning_conditions.append("below_threshold")

        if (
            self.interpretation.warning_above is not None
            and value > self.interpretation.warning_above
        ):
            warning_conditions.append("above_threshold")

        if warning_conditions:
            return MetricRating.WARNING

        # Check if in good range
        if (
            self.interpretation.good_range is not None
            and len(self.interpretation.good_range) == 2
        ):
            min_good, max_good = self.interpretation.good_range
            if min_good <= value <= max_good:
                return MetricRating.GOOD

        # Default to adequate if no specific conditions met
        return MetricRating.ADEQUATE

    def get_interpretation_message(self, value: float) -> str:
        """Get a human-readable interpretation message for a metric value.

        Args:
            value: The metric value to interpret.

        Returns:
            A descriptive message about the metric value.

        Example:
            >>> interpreter.get_interpretation_message(1.8)
            'Good performance: 1.80'
        """
        rating = self.rate_value(value)

        # Base message based on rating
        rating_messages = {
            MetricRating.EXCELLENT: f"Excellent performance: {value:.2f}",
            MetricRating.GOOD: f"Good performance: {value:.2f}",
            MetricRating.ADEQUATE: f"Adequate performance: {value:.2f}",
            MetricRating.WARNING: f"Warning level: {value:.2f}",
            MetricRating.POOR: f"Poor performance: {value:.2f}",
            MetricRating.UNKNOWN: f"Value: {value:.2f} (no interpretation guidelines available)",
        }

        return rating_messages[rating]

    def get_detailed_analysis(self, value: float) -> dict[str, Any]:
        """Get a detailed analysis of a metric value.

        Args:
            value: The metric value to analyze.

        Returns:
            Dictionary containing detailed analysis information.

        Example:
            >>> analysis = interpreter.get_detailed_analysis(1.8)
            >>> analysis['rating']
            'good'
        """
        rating = self.rate_value(value)

        analysis: dict[str, Any] = {
            "value": value,
            "rating": rating.value,
            "metric_name": self.metric_definition.name,
            "units": self.metric_definition.units,
            "category": self.metric_definition.category,
            "interpretation_message": self.get_interpretation_message(value),
        }

        # Add interpretation details if available
        if self.interpretation:
            analysis["guidelines"] = {
                "good_range": self.interpretation.good_range,
                "warning_below": self.interpretation.warning_below,
                "warning_above": self.interpretation.warning_above,
                "excellent_above": self.interpretation.excellent_above,
                "poor_below": self.interpretation.poor_below,
            }

            if self.interpretation.notes:
                analysis["notes"] = self.interpretation.notes

        # Add related metrics for context
        if self.metric_definition.related_metrics:
            analysis["related_metrics"] = self.metric_definition.related_metrics

        return analysis


def interpret_metric(
    metric_definition: MetricDefinition, value: float
) -> dict[str, Any]:
    """Interpret a metric value using the MetricInterpreter.

    Args:
        metric_definition: The metric definition.
        value: The value to interpret.

    Returns:
        Detailed interpretation analysis.

    Example:
        >>> from fin_statement_model.core.metrics import metric_registry, interpret_metric
        >>> metric_def = metric_registry.get("current_ratio")
        >>> result = interpret_metric(metric_def, 1.8)
        >>> result['rating']
        'good'
    """
    interpreter = MetricInterpreter(metric_definition)
    return interpreter.get_detailed_analysis(value)



================================================================================
File: fin_statement_model/core/metrics/metric_defn/__init__.py
================================================================================

"""Organized Built-in Metrics Package.

This package contains financial metrics organized by analytical category for easier
maintenance and understanding. All metrics are automatically loaded into the
metric_registry when this package is imported.
"""

import logging
from pathlib import Path
from typing import Optional

from fin_statement_model.core.metrics.registry import metric_registry

logger = logging.getLogger(__name__)


def load_organized_metrics(base_path: Optional[Path] = None) -> int:
    """Load all metrics from the organized structure.

    Args:
        base_path: Base path to the metric_defn directory. If None, uses default.

    Returns:
        Total number of metrics loaded.
    """
    if base_path is None:
        base_path = Path(__file__).parent

    total_loaded = 0

    # Define the organized metric files to load
    metric_files = [
        # Liquidity metrics
        "liquidity/ratios.yaml",
        "liquidity/working_capital.yaml",
        # Leverage metrics
        "leverage/debt_ratios.yaml",
        "leverage/net_leverage.yaml",
        # Coverage metrics
        "coverage/interest_coverage.yaml",
        "coverage/debt_service.yaml",
        # Profitability metrics
        "profitability/margins.yaml",
        "profitability/returns.yaml",
        # Efficiency metrics
        "efficiency/asset_turnover.yaml",
        "efficiency/component_turnover.yaml",
        # Valuation metrics
        "valuation/price_multiples.yaml",
        "valuation/enterprise_multiples.yaml",
        "valuation/yields.yaml",
        # Cash flow metrics
        "cash_flow/generation.yaml",
        "cash_flow/returns.yaml",
        # Growth metrics
        "growth/growth_rates.yaml",
        # Per share metrics
        "per_share/per_share_metrics.yaml",
        # Credit risk metrics
        "credit_risk/altman_scores.yaml",
        "credit_risk/warning_flags.yaml",
        # Advanced metrics
        "advanced/dupont_analysis.yaml",
        # Special calculated items
        "special/gross_profit.yaml",
        "special/net_income.yaml",
        "special/retained_earnings.yaml",
        # Real estate metrics
        "real_estate/operational_metrics.yaml",
        "real_estate/valuation_metrics.yaml",
        "real_estate/per_share_metrics.yaml",
        "real_estate/debt_metrics.yaml",
        # Banking metrics
        "banking/asset_quality.yaml",
        "banking/capital_adequacy.yaml",
        "banking/profitability.yaml",
        "banking/liquidity.yaml",
    ]

    # Collect unique parent directories from the metric files
    unique_directories = set()
    for file_path in metric_files:
        full_path = base_path / file_path
        if full_path.exists():
            unique_directories.add(full_path.parent)
        else:
            logger.warning(f"Organized metric file not found: {full_path}")

    # Load metrics from each unique directory
    for directory in unique_directories:
        try:
            # load_metrics_from_directory returns the count of metrics loaded
            metrics_count = metric_registry.load_metrics_from_directory(directory)
            total_loaded += metrics_count
            logger.debug(f"Loaded {metrics_count} metrics from {directory}")
        except Exception:
            logger.exception(f"Failed to load metrics from directory {directory}")

    logger.info(f"Loaded {total_loaded} total metrics from organized structure")
    return total_loaded


# Auto-load on import
try:
    load_organized_metrics()
except Exception as e:
    logger.warning(f"Failed to auto-load organized metrics: {e}")



================================================================================
File: fin_statement_model/core/metrics/metric_defn/advanced/__init__.py
================================================================================

"""Advanced Metrics."""

# Advanced metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/banking/__init__.py
================================================================================

"""Banking Metrics."""

# Banking metrics organized by subcategory:
# - asset_quality.yaml: NPL ratios, charge-offs, provision coverage
# - capital_adequacy.yaml: Tier 1/2 capital ratios, leverage ratios
# - profitability.yaml: Net interest margin, efficiency ratio, ROA/ROE
# - liquidity.yaml: LCR, NSFR, deposit composition
# - credit_risk.yaml: Credit loss rates, risk-weighted asset ratios
# - regulatory.yaml: Regulatory compliance and stress test metrics



================================================================================
File: fin_statement_model/core/metrics/metric_defn/cash_flow/__init__.py
================================================================================

"""Cash_Flow Metrics."""

# Cash_Flow metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/coverage/__init__.py
================================================================================

"""Coverage Metrics."""

# Coverage metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/credit_risk/__init__.py
================================================================================

"""Credit_Risk Metrics."""

# Credit_Risk metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/efficiency/__init__.py
================================================================================

"""Efficiency Metrics."""

# Efficiency metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/growth/__init__.py
================================================================================

"""Growth Metrics."""

# Growth metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/leverage/__init__.py
================================================================================

"""Leverage Metrics."""

# Leverage metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/liquidity/__init__.py
================================================================================

"""Liquidity Metrics."""

# Liquidity metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/per_share/__init__.py
================================================================================

"""Per_Share Metrics."""

# Per_Share metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/profitability/__init__.py
================================================================================

"""Profitability Metrics."""

# Profitability metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/metric_defn/real_estate/__init__.py
================================================================================

"""Real Estate Metrics."""

# Real estate metrics organized by subcategory:
# - operational_metrics.yaml: NOI, FFO, AFFO, occupancy
# - valuation_metrics.yaml: Cap rates, price per SF, REIT multiples
# - per_share_metrics.yaml: FFO/share, AFFO/share, NAV/share
# - debt_metrics.yaml: LTV, DSCR, debt yield, interest coverage



================================================================================
File: fin_statement_model/core/metrics/metric_defn/valuation/__init__.py
================================================================================

"""Valuation Metrics."""

# Valuation metrics organized by subcategory



================================================================================
File: fin_statement_model/core/metrics/models.py
================================================================================

"""Provide models for metric definitions loaded from YAML files.

This module defines Pydantic models for metric definitions and their interpretation guidelines.
"""

from typing import Optional, Any
from pydantic import BaseModel, Field, model_validator
from pydantic import ConfigDict


class MetricInterpretation(BaseModel):
    """Provide interpretation guidelines for a metric.

    Attributes:
        good_range: Range of values considered good [min, max].
        warning_below: Value below which a warning should be issued.
        warning_above: Value above which a warning should be issued.
        excellent_above: Value above which the metric is considered excellent.
        poor_below: Value below which the metric is considered poor.
        notes: Additional interpretation notes and context.
    """

    good_range: Optional[list[float]] = Field(
        None, description="Range of values considered good [min, max]"
    )
    warning_below: Optional[float] = Field(
        None, description="Value below which a warning should be issued"
    )
    warning_above: Optional[float] = Field(
        None, description="Value above which a warning should be issued"
    )
    excellent_above: Optional[float] = Field(
        None, description="Value above which the metric is considered excellent"
    )
    poor_below: Optional[float] = Field(
        None, description="Value below which the metric is considered poor"
    )
    notes: Optional[str] = Field(
        None, description="Additional interpretation notes and context"
    )


class MetricDefinition(BaseModel):
    """Define schema for a single metric definition loaded from a YAML file.

    Attributes:
        name: The name of the metric.
        description: The description of the metric.
        inputs: List of input identifiers for the metric.
        formula: The formula used to calculate the metric.
        tags: List of tags classifying the metric.
        units: Unit of the metric (e.g., percentage, ratio).
        category: Category of the metric.
        interpretation: Guidelines for interpreting metric values.
        related_metrics: Names of related metrics to consider together.
    """

    name: str = Field(..., min_length=1, description="The name of the metric")
    description: str = Field(
        ..., min_length=1, max_length=500, description="The description of the metric"
    )
    inputs: list[str] = Field(..., min_length=1, description="The inputs of the metric")
    formula: str = Field(..., min_length=1, description="The formula of the metric")
    tags: list[str] = Field(default_factory=list, description="The tags of the metric")
    units: Optional[str] = Field(None, description="The units of the metric")
    category: Optional[str] = Field(
        None, description="Category of the metric (e.g., liquidity, profitability)"
    )
    interpretation: Optional[MetricInterpretation] = Field(
        None, description="Guidelines for interpreting the metric values"
    )
    related_metrics: Optional[list[str]] = Field(
        None, description="Names of related metrics that should be considered together"
    )

    model_config = ConfigDict(extra="forbid", frozen=False)

    @model_validator(mode="before")
    def _strip_whitespace(cls, values: dict[str, Any]) -> dict[str, Any]:
        # tiny quality-of-life clean-up
        for k, v in values.items():
            if isinstance(v, str):
                values[k] = v.strip()
        return values



================================================================================
File: fin_statement_model/core/metrics/registry.py
================================================================================

"""Provide a registry for loading and accessing metric definitions from YAML files.

This module defines:
- MetricRegistry: Load, validate, and retrieve metric definitions.
- metric_registry: Singleton instance of MetricRegistry.
"""

import logging
from pathlib import Path
from typing import ClassVar, Union

# Use a try-except block for the YAML import
try:
    import yaml

    HAS_YAML = True
except ImportError:
    HAS_YAML = False

from pydantic import ValidationError

from fin_statement_model.core.metrics.models import MetricDefinition

logger = logging.getLogger(__name__)


class MetricRegistry:
    """Provide methods to load, discover, and retrieve metric definitions from YAML files.

    Example:
        >>> from fin_statement_model.core.metrics.registry import MetricRegistry
        >>> registry = MetricRegistry()
        >>> registry.list_metrics()
        []
    """

    _REQUIRED_FIELDS: ClassVar[list[str]] = ["inputs", "formula", "description", "name"]

    def __init__(self) -> None:
        """Initialize the MetricRegistry with an empty metrics store.

        Examples:
            >>> registry = MetricRegistry()
            >>> len(registry)
            0
        """
        self._metrics: dict[str, MetricDefinition] = {}
        logger.info("MetricRegistry initialized.")

    def load_metrics_from_directory(self, directory_path: Union[str, Path]) -> int:
        """Load all metric definitions from a directory.

        This method searches for '*.yaml' files, validates their content,
        and stores them in the registry. Each YAML file can contain either:
        - A single metric definition (a YAML dictionary at the root)
        - A list of metric definitions (a YAML list of dictionaries at the root)

        Args:
            directory_path: Path to the directory containing metric YAML files.

        Returns:
            The number of metrics successfully loaded.

        Raises:
            ImportError: If PyYAML is not installed.
            FileNotFoundError: If the directory_path does not exist.

        Examples:
            >>> registry = MetricRegistry()
            >>> count = registry.load_metrics_from_directory("./metrics")
            >>> print(f"Loaded {count} metrics.")
        """
        if not HAS_YAML:
            logger.error(
                "PyYAML is required to load metrics from YAML files. Please install it."
            )
            raise ImportError("PyYAML is required to load metrics from YAML files.")

        dir_path = Path(directory_path)
        if not dir_path.is_dir():
            logger.error(f"Metric directory not found: {dir_path}")
            raise FileNotFoundError(f"Metric directory not found: {dir_path}")

        logger.info(f"Loading metrics from directory: {dir_path}")
        loaded_count = 0

        for filepath in dir_path.glob("*.yaml"):
            logger.debug(f"Processing file: {filepath}")
            try:
                with open(filepath, encoding="utf-8") as f:
                    content = f.read()

                # Use standard YAML parsing
                try:
                    data = yaml.safe_load(content)
                except yaml.YAMLError as e:
                    logger.warning(f"Failed to parse YAML file {filepath}: {e}")
                    continue

                if not data:
                    logger.debug(f"Empty or null content in {filepath}, skipping")
                    continue

                # Handle both single metric and list of metrics
                metrics_to_process = []

                if isinstance(data, dict):
                    # Single metric definition
                    metrics_to_process = [data]
                elif isinstance(data, list):
                    # List of metric definitions
                    metrics_to_process = data
                else:
                    logger.warning(
                        f"Invalid YAML structure in {filepath}: "
                        f"expected dict or list, got {type(data).__name__}"
                    )
                    continue

                # Process each metric definition
                for i, metric_data in enumerate(metrics_to_process):
                    if not isinstance(metric_data, dict):
                        logger.warning(
                            f"Invalid metric definition at index {i} in {filepath}: "
                            f"expected dict, got {type(metric_data).__name__}"
                        )
                        continue

                    try:
                        # Validate and register the metric
                        model = MetricDefinition.model_validate(metric_data)
                        self.register_definition(model)
                        loaded_count += 1
                        logger.debug(
                            f"Successfully loaded metric '{model.name}' from {filepath}"
                        )

                    except ValidationError as ve:
                        logger.warning(
                            f"Invalid metric definition at index {i} in {filepath}: {ve}"
                        )
                        continue

            except Exception:
                logger.exception(f"Failed to process file {filepath}")
                continue

        logger.info(f"Successfully loaded {loaded_count} metrics from {dir_path}.")
        return loaded_count

    def get(self, metric_id: str) -> MetricDefinition:
        """Retrieve a loaded metric definition by its ID.

        Args:
            metric_id: Identifier of the metric (filename stem).

        Returns:
            A MetricDefinition object containing the metric definition.

        Raises:
            KeyError: If the metric_id is not found in the registry.

        Examples:
            >>> definition = registry.get("gross_profit")
            >>> print(definition["formula"])
        """
        try:
            return self._metrics[metric_id]
        except KeyError:
            logger.warning(f"Metric ID '{metric_id}' not found in registry.")
            raise KeyError(
                f"Metric ID '{metric_id}' not found. Available: {self.list_metrics()}"
            )

    def list_metrics(self) -> list[str]:
        """Get a sorted list of all loaded metric IDs.

        Returns:
            A sorted list of available metric IDs.

        Examples:
            >>> registry.list_metrics()
            ['current_ratio', 'debt_equity_ratio']
        """
        return sorted(self._metrics.keys())

    def __len__(self) -> int:
        """Return the number of loaded metrics.

        Returns:
            The count of metrics loaded into the registry.

        Examples:
            >>> len(registry)
            5
        """
        return len(self._metrics)

    def __contains__(self, metric_id: str) -> bool:
        """Check if a metric ID exists in the registry.

        Args:
            metric_id: The metric identifier to check.

        Returns:
            True if the metric is present, False otherwise.

        Examples:
            >>> 'current_ratio' in registry
        """
        return metric_id in self._metrics

    def register_definition(self, definition: MetricDefinition) -> None:
        """Register a single metric definition.

        Args:
            definition: The metric definition to register.

        Example:
            >>> from fin_statement_model.core.metrics.registry import MetricRegistry
            >>> from fin_statement_model.core.metrics.models import MetricDefinition
            >>> registry = MetricRegistry()
            >>> model = MetricDefinition(name='test', description='desc', inputs=['a'], formula='a', tags=[])
            >>> registry.register_definition(model)
            >>> 'test' in registry
        """
        metric_id = definition.name.lower().replace(" ", "_").replace("-", "_")
        if metric_id in self._metrics:
            logger.debug(f"Overwriting existing metric definition for '{metric_id}'")
        self._metrics[metric_id] = definition
        logger.debug(f"Registered metric definition: {metric_id}")


# Create the singleton instance (without auto-loading to prevent duplicates)
metric_registry = MetricRegistry()



================================================================================
File: fin_statement_model/core/node_factory.py
================================================================================

"""Factory helpers for creating nodes in the financial statement model.

This module centralizes node-creation logic to ensure consistent initialization
for all node types (financial statement items, calculations, forecasts, stats).
"""

import logging
from typing import Any, Union, Optional, ClassVar, Callable, cast

# Force import of strategies package to ensure registration happens

from .nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    CustomCalculationNode,
)
from .nodes.calculation_nodes import FormulaCalculationNode
from .nodes.stats_nodes import (
    YoYGrowthNode,
    MultiPeriodStatNode,
    TwoPeriodAverageNode,
)
from .nodes.forecast_nodes import (
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
    CustomGrowthForecastNode,
)

# Force import of calculations package to ensure registration happens
from fin_statement_model.core.calculations import Registry, Calculation
from fin_statement_model.core.errors import ConfigurationError

# Configure logging
logger = logging.getLogger(__name__)


class NodeFactory:
    """Factory for creating nodes in the financial statement model.

    The class exposes convenience helpers that hide the underlying registry
    and deserialization logic, so client code can create nodes declaratively
    (e.g., via YAML configs or simple Python calls) without importing every
    concrete node class.

    Attributes:
        _calculation_methods: Mapping of calculation type keys (e.g., ``"addition"``)
            to calculation class names registered in :class:`fin_statement_model.core.calculations.Registry`.
        _node_type_registry: Mapping of node-type strings to concrete :class:`Node`
            subclasses used when deserializing from dictionaries.
    """

    # Mapping of calculation type strings to Calculation class names (keys in the Registry)
    _calculation_methods: ClassVar[dict[str, str]] = {
        "addition": "AdditionCalculation",
        "subtraction": "SubtractionCalculation",
        "formula": "FormulaCalculation",
        "division": "DivisionCalculation",
        "weighted_average": "WeightedAverageCalculation",
        "custom_formula": "CustomFormulaCalculation",
    }

    # Mapping from node type names to Node classes for deserialization
    _node_type_registry: ClassVar[dict[str, type[Node]]] = {
        "financial_statement_item": FinancialStatementItemNode,
        "calculation": CalculationNode,
        "formula_calculation": FormulaCalculationNode,
        "custom_calculation": CustomCalculationNode,
        "forecast": ForecastNode,
        # Specific forecast types
        "fixed_growth_forecast": FixedGrowthForecastNode,
        "curve_growth_forecast": CurveGrowthForecastNode,
        "statistical_growth_forecast": StatisticalGrowthForecastNode,
        "average_value_forecast": AverageValueForecastNode,
        "average_historical_growth_forecast": AverageHistoricalGrowthForecastNode,
        "custom_growth_forecast": CustomGrowthForecastNode,
        # Stats node types
        "yoy_growth": YoYGrowthNode,
        "multi_period_stat": MultiPeriodStatNode,
        "two_period_average": TwoPeriodAverageNode,
    }

    # Mapping from forecast type strings to specific forecast node classes
    _forecast_type_registry: ClassVar[dict[str, type[ForecastNode]]] = {
        "simple": FixedGrowthForecastNode,
        "curve": CurveGrowthForecastNode,
        "statistical": StatisticalGrowthForecastNode,
        "average": AverageValueForecastNode,
        "historical_growth": AverageHistoricalGrowthForecastNode,
        "custom": CustomGrowthForecastNode,
    }

    @classmethod
    def create_financial_statement_item(
        cls, name: str, values: dict[str, float]
    ) -> FinancialStatementItemNode:
        """Create a FinancialStatementItemNode representing a base financial item.

        This node holds historical or projected values for a specific
        line item (e.g., Revenue, COGS) over different periods.

        Args:
            name: Identifier for the node (e.g., "Revenue").
            values: Mapping of period identifiers to numerical values.

        Returns:
            A FinancialStatementItemNode initialized with the provided values.

        Raises:
            ValueError: If the provided name is empty or not a string.

        Examples:
            >>> revenue_node = NodeFactory.create_financial_statement_item(
            ...     name="Revenue",
            ...     values={"2023": 1000.0, "2024": 1100.0}
            ... )
            >>> revenue_node.calculate("2023")
            1000.0
        """
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        logger.debug(f"Creating financial statement item node: {name}")
        return FinancialStatementItemNode(name, values)

    @classmethod
    def create_calculation_node(
        cls,
        name: str,
        inputs: list[Node],
        calculation_type: str,
        formula_variable_names: Optional[list[str]] = None,
        **calculation_kwargs: Any,
    ) -> CalculationNode:
        """Create a CalculationNode using a pre-defined calculation.

        This method resolves a calculation class from a calculation_type key,
        instantiates it with optional parameters, and wraps it in
        a CalculationNode.

        Args:
            name: Identifier for the calculation node instance.
            inputs: List of Node instances serving as inputs to the calculation.
            calculation_type: Key for the desired calculation in the registry.
            formula_variable_names: Optional list of variable names used in the formula
                string. Required & used only if creating a FormulaCalculationNode
                via the 'custom_formula' type with a 'formula' kwarg.
            **calculation_kwargs: Additional parameters for the calculation constructor.

        Returns:
            A CalculationNode configured with the selected calculation.

        Raises:
            ValueError: If name is invalid, inputs list is empty, or the
                calculation_type is unrecognized.
            TypeError: If the calculation cannot be instantiated with given kwargs.

        Examples:
            >>> gross_profit = NodeFactory.create_calculation_node(
            ...     name="GrossProfit",
            ...     inputs=[revenue, cogs],
            ...     calculation_type="subtraction"
            ... )
        """
        # Validate inputs
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        if not inputs:
            raise ValueError("Calculation node must have at least one input")

        # Check if the calculation type maps to a known calculation name
        if calculation_type not in cls._calculation_methods:
            valid_types = list(cls._calculation_methods.keys())
            raise ValueError(
                f"Invalid calculation type: '{calculation_type}'. Valid types are: {valid_types}"
            )

        # Get the calculation name
        calculation_name = cls._calculation_methods[calculation_type]

        # For other types, resolve the Calculation class from the registry
        try:
            calculation_cls: type[Calculation] = Registry.get(calculation_name)
        except KeyError:
            # This should ideally not happen if _calculation_methods is synced with registry
            raise ValueError(
                f"Calculation class '{calculation_name}' (for type '{calculation_type}') not found in Registry."
            ) from None  # Prevent chaining the KeyError

        # Instantiate the calculation, passing any extra kwargs
        try:
            # Extract any metadata that should be stored on the node, not passed to calculation
            node_kwargs = {}
            if "metric_name" in calculation_kwargs:
                node_kwargs["metric_name"] = calculation_kwargs.pop("metric_name")
            if "metric_description" in calculation_kwargs:
                node_kwargs["metric_description"] = calculation_kwargs.pop(
                    "metric_description"
                )

            # Special handling for FormulaCalculation which needs input_variable_names
            if calculation_type == "formula":
                if formula_variable_names and len(formula_variable_names) == len(
                    inputs
                ):
                    calculation_kwargs["input_variable_names"] = formula_variable_names
                elif not formula_variable_names:
                    # Generate default names like var_0, var_1, ...
                    calculation_kwargs["input_variable_names"] = [
                        f"var_{i}" for i in range(len(inputs))
                    ]
                    logger.warning(
                        f"No formula_variable_names provided for formula node '{name}'. Using defaults: {calculation_kwargs['input_variable_names']}"
                    )
                else:
                    # Mismatch between provided names and number of inputs
                    raise ConfigurationError(
                        f"Mismatch between formula_variable_names ({len(formula_variable_names)}) and number of inputs ({len(inputs)}) for node '{name}'"
                    )

            calculation_instance = calculation_cls(**calculation_kwargs)
        except TypeError as e:
            logger.exception(
                f"Failed to instantiate calculation '{calculation_name}' with kwargs {calculation_kwargs}"
            )
            raise TypeError(
                f"Could not instantiate calculation '{calculation_name}' for node '{name}'. "
                f"Check required arguments for {calculation_cls.__name__}. Provided kwargs: {calculation_kwargs}"
            ) from e

        # Create and return a CalculationNode with the instantiated calculation
        logger.debug(
            f"Creating calculation node '{name}' with '{calculation_name}' calculation."
        )

        return CalculationNode(name, inputs, calculation_instance, **node_kwargs)

    @classmethod
    def create_forecast_node(
        cls,
        name: str,
        base_node: Node,
        base_period: str,
        forecast_periods: list[str],
        forecast_type: str,
        growth_params: Union[float, list[float], Callable[[], float]],
    ) -> Node:
        """Create a forecast node of the specified type using core forecast classes.

        Args:
            name: Custom name for the forecast node.
            base_node: The Node instance to base projections on.
            base_period: Period identifier providing the base value.
            forecast_periods: List of periods for which to forecast.
            forecast_type: Forecast method ('simple', 'curve', 'statistical',
                'average', 'historical_growth').
            growth_params: Parameters controlling forecast behavior (float,
                list of floats, or callable). Ignored for 'average' and 'historical_growth'.

        Returns:
            A Node instance implementing the chosen forecast.

        Raises:
            ValueError: If an unsupported forecast_type is provided.

        Examples:
            >>> forecast = NodeFactory.create_forecast_node(
            ...     name="RevForecast",
            ...     base_node=revenue,
            ...     base_period="2023",
            ...     forecast_periods=["2024", "2025"],
            ...     forecast_type="simple",
            ...     growth_params=0.05
            ... )
        """
        # Prepare placeholder to unify forecast node type
        node: ForecastNode
        # Instantiate the appropriate forecast node with proper type checking
        if forecast_type == "simple":
            if not isinstance(growth_params, (int, float)):
                raise TypeError("growth_params must be a float for 'simple' forecast")
            node = FixedGrowthForecastNode(
                base_node, base_period, forecast_periods, float(growth_params)
            )
        elif forecast_type == "curve":
            if not isinstance(growth_params, list):
                raise TypeError(
                    "growth_params must be a list of floats for 'curve' forecast"
                )
            rates: list[float] = [float(r) for r in growth_params]
            node = CurveGrowthForecastNode(
                base_node, base_period, forecast_periods, rates
            )
        elif forecast_type == "statistical":
            if not callable(growth_params):
                raise TypeError(
                    "growth_params must be a callable returning float for 'statistical' forecast"
                )
            node = StatisticalGrowthForecastNode(
                base_node, base_period, forecast_periods, growth_params
            )
        elif forecast_type == "average":
            node = AverageValueForecastNode(base_node, base_period, forecast_periods)
        elif forecast_type == "historical_growth":
            node = AverageHistoricalGrowthForecastNode(
                base_node, base_period, forecast_periods
            )
        else:
            raise ValueError(f"Invalid forecast type: {forecast_type}")

        # Override forecast node's name to match factory 'name' argument
        node.name = name
        logger.debug(
            f"Forecast node created with custom name: {name} (original: {base_node.name})"
        )
        return node

    @classmethod
    def create_from_dict(  # noqa: PLR0911
        cls, data: dict[str, Any], context: Optional[dict[str, Node]] = None
    ) -> Node:
        """Create a node from its dictionary representation.

        This method provides a unified interface for deserializing nodes from
        their dictionary representations, handling dependency resolution and
        type-specific deserialization logic.

        Args:
            data: Serialized node data containing at minimum a 'type' field.
            context: Optional dictionary of existing nodes for resolving dependencies.
                Required for nodes that have dependencies (calculation, forecast nodes).

        Returns:
            Reconstructed node instance.

        Raises:
            ValueError: If the data is invalid, missing required fields, or contains
                an unknown node type.
            ConfigurationError: If dependencies cannot be resolved or node creation fails.

        Examples:
            >>> # Simple node without dependencies
            >>> data = {
            ...     'type': 'financial_statement_item',
            ...     'name': 'Revenue',
            ...     'values': {'2023': 1000.0}
            ... }
            >>> node = NodeFactory.create_from_dict(data)

            >>> # Node with dependencies
            >>> calc_data = {
            ...     'type': 'calculation',
            ...     'name': 'GrossProfit',
            ...     'inputs': ['Revenue', 'COGS'],
            ...     'calculation_type': 'subtraction'
            ... }
            >>> context = {'Revenue': revenue_node, 'COGS': cogs_node}
            >>> calc_node = NodeFactory.create_from_dict(calc_data, context)
        """
        if not isinstance(data, dict):
            raise TypeError("Node data must be a dictionary")

        node_type = data.get("type")
        if not node_type:
            raise ValueError("Missing 'type' field in node data")

        logger.debug(f"Creating node of type '{node_type}' from dictionary")

        # Handle nodes without dependencies first
        if node_type == "financial_statement_item":
            return FinancialStatementItemNode.from_dict(data)

        # Handle nodes that require context for dependency resolution
        if context is None:
            context = {}

        # For calculation nodes, use the appropriate from_dict_with_context method
        if node_type == "calculation":
            return cast(Node, CalculationNode.from_dict_with_context(data, context))
        elif node_type == "formula_calculation":
            return cast(
                Node, FormulaCalculationNode.from_dict_with_context(data, context)
            )
        elif node_type == "custom_calculation":
            raise ConfigurationError(
                "CustomCalculationNode cannot be deserialized because it contains "
                "non-serializable Python functions. Manual reconstruction required."
            )

        # Handle stats nodes
        elif node_type == "yoy_growth":
            return YoYGrowthNode.from_dict_with_context(data, context)
        elif node_type == "multi_period_stat":
            return MultiPeriodStatNode.from_dict_with_context(data, context)
        elif node_type == "two_period_average":
            return TwoPeriodAverageNode.from_dict_with_context(data, context)

        # Handle forecast nodes
        elif node_type == "forecast":
            # Determine the specific forecast type from the data
            forecast_type = data.get("forecast_type")
            if not forecast_type:
                raise ValueError("Missing 'forecast_type' field in forecast node data")

            # Get the appropriate forecast node class
            forecast_class = cls._forecast_type_registry.get(forecast_type)
            if not forecast_class:
                valid_types = list(cls._forecast_type_registry.keys())
                raise ValueError(
                    f"Unknown forecast type '{forecast_type}'. Valid types: {valid_types}"
                )

            # Handle non-serializable forecast types
            if forecast_type in ["statistical", "custom"]:
                raise ConfigurationError(
                    f"Forecast type '{forecast_type}' cannot be deserialized because it contains "
                    "non-serializable functions. Manual reconstruction required."
                )

            # Use the specific forecast class's from_dict_with_context method
            return forecast_class.from_dict_with_context(data, context)

        # Handle specific forecast node types (for backward compatibility)
        elif node_type in cls._node_type_registry:
            node_class = cls._node_type_registry[node_type]
            if hasattr(node_class, "from_dict_with_context"):
                return cast(Node, node_class.from_dict_with_context(data, context))
            else:
                return cast(Node, node_class.from_dict(data))  # type: ignore[attr-defined]

        else:
            valid_types = list(cls._node_type_registry.keys())
            raise ValueError(
                f"Unknown node type: '{node_type}'. Valid types: {valid_types}"
            )

    @classmethod
    def _create_custom_node_from_callable(
        cls,
        name: str,
        inputs: list[Node],
        formula: Callable[..., Any],
        description: Optional[str] = None,
    ) -> CustomCalculationNode:
        """Create a :class:`CustomCalculationNode` from an arbitrary Python callable.

        This helper is useful for ad-hoc or complex calculations that are not
        (yet) formalized as reusable strategies. The supplied ``formula`` is
        invoked with the *values* of each input node during evaluation.

        Args:
            name: Identifier for the custom calculation node.
            inputs: List of nodes supplying arguments to ``formula``.
            formula: Callable performing the calculation.
            description: Human-readable description of the calculation logic.

        Returns:
            The newly created :class:`CustomCalculationNode` instance.

        Raises:
            ValueError: If *name* is empty.
            TypeError: If *formula* is not callable or *inputs* contain non-Node objects.

        Examples:
            Defining a tax-calculation node::

                def tax_logic(revenue, expenses, tax_rate):
                    profit = revenue - expenses
                    return max(profit, 0) * tax_rate

                tax_node = NodeFactory._create_custom_node_from_callable(
                    name="IncomeTax",
                    inputs=[revenue, expenses, tax_rate_node],
                    formula=tax_logic,
                )

            Using a lambda for a quick ratio::

                quick_ratio = NodeFactory._create_custom_node_from_callable(
                    name="QuickRatioCustom",
                    inputs=[cash, receivables, current_liabilities],
                    formula=lambda cash, rec, liab: (cash + rec) / liab if liab else 0,
                )
        """
        # Validate inputs
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        if not inputs:
            # Allowing no inputs might be valid for some custom functions (e.g., constants)
            # Reconsider if this check is always needed here.
            logger.warning(f"Creating CustomCalculationNode '{name}' with no inputs.")
            # raise ValueError("Custom node must have at least one input")

        if not callable(formula):
            raise TypeError("Formula must be a callable function")
        if not all(isinstance(i, Node) for i in inputs):
            raise TypeError("All items in inputs must be Node instances.")

        # Use the imported CustomCalculationNode
        logger.debug(f"Creating CustomCalculationNode: {name} using provided callable.")
        return CustomCalculationNode(
            name, inputs, formula_func=formula, description=description
        )



================================================================================
File: fin_statement_model/core/nodes/__init__.py
================================================================================

"""Provide core node implementations for the financial statement model.

This package exports the `Node` base class and specialized node types for building
and evaluating financial statement graphs:

Data Nodes:
    - FinancialStatementItemNode: Store raw financial data for specific periods.

Calculation Nodes:
    - CalculationNode: Delegate value computation to a calculation object.
    - FormulaCalculationNode: Evaluate expression-based formulas.
    - CustomCalculationNode: Compute values using custom Python functions.

Statistical Nodes:
    - YoYGrowthNode: Compute year-over-year percentage growth.
    - MultiPeriodStatNode: Compute statistical measures (mean, stdev) over multiple periods.
    - TwoPeriodAverageNode: Compute the average between two periods.

Forecast Nodes:
    - ForecastNode: Base class for forecasting future values.
    - FixedGrowthForecastNode: Apply a constant growth rate.
    - CurveGrowthForecastNode: Apply period-specific growth rates.
    - StatisticalGrowthForecastNode: Draw growth from a distribution.
    - CustomGrowthForecastNode: Compute growth via a custom function.
    - AverageValueForecastNode: Project the historical average forward.
    - AverageHistoricalGrowthForecastNode: Apply average historical growth rate.

Also provides `standard_node_registry` and `is_calculation_node` helper.
"""

import logging

# Import all node classes using actual file names
from .base import Node
from .item_node import FinancialStatementItemNode
from .calculation_nodes import (
    CalculationNode,
    FormulaCalculationNode,
    CustomCalculationNode,
)
from .stats_nodes import (
    YoYGrowthNode,
    MultiPeriodStatNode,
    TwoPeriodAverageNode,
)
from .forecast_nodes import (
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    CustomGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
)

# Import standard registry
from .standard_registry import standard_node_registry

logger = logging.getLogger(__name__)

# Initialize standard nodes from the organized definition directory
try:
    count = standard_node_registry.initialize_default_nodes()
    if count == 0:
        logger.warning(
            "No standard nodes were loaded. The registry is empty. "
            "This may cause issues with metrics and node validation."
        )
except Exception:
    logger.exception("Failed to initialize standard nodes")


def is_calculation_node(node: Node) -> bool:
    """Determine if a node performs a calculated value.

    A node is considered a calculation node if it computes values rather than
    storing raw data. Calculation node types include:
    - CalculationNode
    - FormulaCalculationNode
    - CustomCalculationNode
    - ForecastNode and its subclasses
    - YoYGrowthNode
    - MultiPeriodStatNode
    - TwoPeriodAverageNode

    Args:
        node (Node): Node instance to check.

    Returns:
        bool: True if `node` performs a calculation; False otherwise.

    Examples:
        >>> from fin_statement_model.core.nodes import is_calculation_node, FinancialStatementItemNode, CalculationNode
        >>> data_node = FinancialStatementItemNode('rev', {'2023': 100})
        >>> is_calculation_node(data_node)
        False
        >>> calc_node = CalculationNode('sum', inputs=[data_node], calculation=...)
        >>> is_calculation_node(calc_node)
        True
    """
    return isinstance(
        node,
        (
            CalculationNode,
            ForecastNode,
            CustomCalculationNode,
            YoYGrowthNode,
            MultiPeriodStatNode,
            TwoPeriodAverageNode,
        ),
    )


__all__ = [
    "AverageHistoricalGrowthForecastNode",
    "AverageValueForecastNode",
    "CalculationNode",
    "CurveGrowthForecastNode",
    "CustomCalculationNode",
    "CustomGrowthForecastNode",
    "FinancialStatementItemNode",
    "FixedGrowthForecastNode",
    "ForecastNode",
    "FormulaCalculationNode",
    "MultiPeriodStatNode",
    "Node",
    "StatisticalGrowthForecastNode",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
    "standard_node_registry",
    "is_calculation_node",
]



================================================================================
File: fin_statement_model/core/nodes/base.py
================================================================================

"""Define the abstract base class for all nodes in the graph.

This module provides the Node base class with interfaces for calculation,
attribute access, and optional caching behavior.
"""

from abc import ABC, abstractmethod
from typing import Any, TYPE_CHECKING

if TYPE_CHECKING:
    pass


class Node(ABC):
    """Abstract base class for nodes in the financial statement model.

    Provides the interface for calculating values, caching, serialization,
    and dependency inspection.

    Attributes:
        name (str): Unique identifier for the node instance.
    """

    name: str
    values: dict[str, Any]

    def __init__(self, name: str):
        """Initialize the Node instance with a unique name.

        Args:
            name: Unique identifier for the node. Must be a non-empty string.

        Raises:
            ValueError: If `name` is empty or not a string.

        Examples:
            >>> class Dummy(Node):
            ...     def calculate(self, period): return 0.0
            >>> dn = Dummy("Revenue")
            >>> dn.name
            'Revenue'
        """
        # Check if name is a non-empty string
        if not isinstance(name, str) or not name:
            raise ValueError("Node name must be a non-empty string.")
        # Check for invalid characters (including newline, tab)
        if "\n" in name or "\t" in name:
            raise ValueError(
                f"Invalid node name: '{name}'. Contains invalid characters."
            )
        # Check for leading/trailing whitespace
        if name != name.strip():
            raise ValueError(
                f"Invalid node name: '{name}'. Cannot have leading/trailing whitespace."
            )
        self.name = name

    @abstractmethod
    def calculate(self, period: str) -> float:
        """Calculate the node's value for a given period.

        Subclasses must override this method to implement specific calculation logic.

        Args:
            period (str): Identifier for the time period.

        Returns:
            float: Calculated value for the period.
        """

    def clear_cache(self) -> None:
        """Clear cached calculation results for this node.

        Subclasses with caching should override this method to clear their internal cache.

        Returns:
            None

        Examples:
            >>> node.clear_cache()
        """
        # Default: no cache to clear

    def has_attribute(self, attr_name: str) -> bool:
        """Check if the node has a specific attribute.

        Args:
            attr_name: The name of the attribute to check.

        Returns:
            True if the attribute exists, otherwise False.

        Examples:
            >>> node.has_attribute("name")
            True
        """
        return hasattr(self, attr_name)

    def get_attribute(self, attribute_name: str) -> object:
        """Get a named attribute from the node.

        Args:
            attribute_name: The name of the attribute to retrieve.

        Returns:
            The value of the specified attribute.

        Raises:
            AttributeError: If the attribute does not exist.

        Examples:
            >>> node.get_attribute("name")
            'Revenue'
        """
        try:
            return getattr(self, attribute_name)
        except AttributeError:
            raise AttributeError(
                f"Node '{self.name}' has no attribute '{attribute_name}'"
            )

    def set_value(self, period: str, value: float) -> None:
        """Set a value for a specific period on data-bearing nodes.

        Override in subclasses to support mutating stored data.

        Args:
            period (str): Period identifier.
            value (float): Numerical value to store.

        Raises:
            NotImplementedError: Always in base class.
        """
        raise NotImplementedError(f"Node '{self.name}' does not support set_value")

    @abstractmethod
    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        This method should return a dictionary containing all information
        necessary to reconstruct the node, including:
        - node type
        - name
        - any configuration parameters
        - values (for data nodes)
        - input references (for calculation nodes)

        Returns:
            Dictionary representation of the node.

        Examples:
            >>> node_dict = node.to_dict()
            >>> node_dict['type']
            'financial_statement_item'
        """

    def get_dependencies(self) -> list[str]:
        """Get the names of nodes this node depends on.

        Default implementation returns empty list. Override in nodes that have dependencies.

        Returns:
            List of node names this node depends on.
        """
        return []



================================================================================
File: fin_statement_model/core/nodes/calculation_nodes.py
================================================================================

"""Provide node implementations for performing calculations in the financial statement model.

This module defines the different types of calculation nodes available in the system:
- FormulaCalculationNode: Evaluates a formula expression string (e.g., "a + b / 2")
- CalculationNode: Uses a calculation object for calculation logic
- CustomCalculationNode: Calculates using a Python callable/function
"""

from typing import Optional, Any, Callable

from fin_statement_model.core.calculations.calculation import (
    Calculation,
    FormulaCalculation,
)
from fin_statement_model.core.errors import (
    CalculationError,
)
from fin_statement_model.core.nodes.base import Node


# === CalculationNode ===


class CalculationNode(Node):
    """Delegate calculation logic to a calculation object.

    Use a calculation object to encapsulate the algorithm for computing node values.

    Attributes:
        name (str): Identifier for this node.
        inputs (List[Node]): A list of input nodes required by the calculation.
        calculation (Any): An object possessing a `calculate(inputs: List[Node], period: str) -> float` method.
        _values (Dict[str, float]): Internal cache for calculated results.

    Examples:
        >>> from fin_statement_model.core.nodes import FinancialStatementItemNode
        >>> class SumCalculation:
        ...     def calculate(self, inputs, period):
        ...         return sum(node.calculate(period) for node in inputs)
        >>> node_a = FinancialStatementItemNode("a", {"2023": 10})
        >>> node_b = FinancialStatementItemNode("b", {"2023": 20})
        >>> sum_node = CalculationNode("sum_ab", inputs=[node_a, node_b], calculation=SumCalculation())
        >>> sum_node.calculate("2023")
        30.0
    """

    def __init__(
        self, name: str, inputs: list[Node], calculation: Calculation, **kwargs: Any
    ):
        """Initialize the CalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (List[Node]): List of input nodes needed by the calculation.
            calculation (Any): The calculation object implementing the calculation.
                Must have a `calculate` method.
            **kwargs: Additional attributes to store on the node (e.g., metric_name, metric_description).

        Raises:
            TypeError: If `inputs` is not a list of Nodes, or if `calculation`
                does not have a callable `calculate` method.
        """
        super().__init__(name)
        if not isinstance(inputs, list) or not all(isinstance(n, Node) for n in inputs):
            raise TypeError("CalculationNode inputs must be a list of Node instances.")
        if not hasattr(calculation, "calculate") or not callable(
            getattr(calculation, "calculate")
        ):
            raise TypeError(
                "Calculation object must have a callable 'calculate' method."
            )

        self.inputs = inputs
        self.calculation = calculation
        self._values: dict[str, float] = {}  # Cache for calculated values

        # Store any additional attributes passed via kwargs
        for key, value in kwargs.items():
            setattr(self, key, value)

    def calculate(self, period: str) -> float:
        """Calculate the node's value for a given period.

        Check the cache; on a miss, delegate to `calculation.calculate` and cache the result.

        Args:
            period (str): Identifier for the time period.

        Returns:
            float: Calculated value for the period.

        Raises:
            CalculationError: If calculation fails or returns a non-numeric value.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Delegate to the calculation object's calculate method
            result = self.calculation.calculate(self.inputs, period)
            if not isinstance(result, int | float):
                raise TypeError(
                    f"Calculation for node '{self.name}' did not return a numeric value (got {type(self.calculation).__name__})."
                )
            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the calculation
            raise CalculationError(
                message=f"Error during calculation for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={
                    "calculation": type(self.calculation).__name__,
                    "error": str(e),
                },
            ) from e

    def set_calculation(self, calculation: Calculation) -> None:
        """Change the calculation object for the node.

        Args:
            calculation (Any): The new calculation object. Must have a callable
                `calculate` method.

        Raises:
            TypeError: If the new calculation is invalid.
        """
        if not hasattr(calculation, "calculate") or not callable(
            getattr(calculation, "calculate")
        ):
            raise TypeError(
                "New calculation object must have a callable 'calculate' method."
            )
        self.calculation = calculation
        self.clear_cache()  # Clear cache as logic has changed

    def clear_cache(self) -> None:
        """Clear the internal cache of calculated values.

        Returns:
            None
        """
        self._values.clear()

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used by the calculation.

        Returns:
            A list of input node names.
        """
        return [node.name for node in self.inputs]

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's type, name, inputs, and calculation info.

        Note:
            This method requires access to NodeFactory's calculation registry
            to properly serialize the calculation type. Some calculation types
            with non-serializable parameters may include warnings.
        """
        # Import here to avoid circular imports
        from fin_statement_model.core.node_factory import NodeFactory

        node_dict: dict[str, Any] = {
            "type": "calculation",
            "name": self.name,
            "inputs": self.get_dependencies(),
        }

        # Add calculation type information
        calc_class_name = type(self.calculation).__name__
        node_dict["calculation_type_class"] = calc_class_name

        # Find the calculation type key from NodeFactory registry
        inv_map = {v: k for k, v in NodeFactory._calculation_methods.items()}
        type_key = inv_map.get(calc_class_name)
        if type_key:
            node_dict["calculation_type"] = type_key

            # Extract calculation-specific arguments
            calculation_args = {}

            # Handle specific calculation types
            if type_key == "weighted_average" and hasattr(self.calculation, "weights"):
                calculation_args["weights"] = self.calculation.weights
            elif type_key == "formula" and hasattr(self.calculation, "formula"):
                calculation_args["formula"] = self.calculation.formula
                if hasattr(self.calculation, "input_variable_names"):
                    node_dict["formula_variable_names"] = (
                        self.calculation.input_variable_names
                    )
            elif type_key == "custom_formula":
                node_dict["serialization_warning"] = (
                    "CustomFormulaCalculation uses a Python function which cannot be serialized. "
                    "Manual reconstruction required."
                )

            if calculation_args:
                node_dict["calculation_args"] = calculation_args

        # Add any additional attributes (like metric info)
        if hasattr(self, "metric_name") and self.metric_name:
            node_dict["metric_name"] = self.metric_name
        if hasattr(self, "metric_description") and self.metric_description:
            node_dict["metric_description"] = self.metric_description

        return node_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "CalculationNode":
        """Create a CalculationNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new CalculationNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        # Import here to avoid circular imports
        from fin_statement_model.core.node_factory import NodeFactory

        if data.get("type") != "calculation":
            raise ValueError(f"Invalid type for CalculationNode: {data.get('type')}")

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in CalculationNode data")

        input_names = data.get("inputs", [])
        if not isinstance(input_names, list):
            raise TypeError("'inputs' field must be a list")

        # Resolve input nodes from context
        input_nodes = []
        for input_name in input_names:
            if input_name not in context:
                raise ValueError(f"Input node '{input_name}' not found in context")
            input_nodes.append(context[input_name])

        calculation_type = data.get("calculation_type")
        if not calculation_type:
            raise ValueError("Missing 'calculation_type' field in CalculationNode data")

        # Get calculation arguments
        calculation_args = data.get("calculation_args", {})

        # Handle formula variable names for formula calculations
        formula_variable_names = data.get("formula_variable_names")

        # Extract metric information
        metric_name = data.get("metric_name")
        metric_description = data.get("metric_description")

        # Create the node using NodeFactory
        return NodeFactory.create_calculation_node(
            name=name,
            inputs=input_nodes,
            calculation_type=calculation_type,
            formula_variable_names=formula_variable_names,
            metric_name=metric_name,
            metric_description=metric_description,
            **calculation_args,
        )


# === FormulaCalculationNode ===


class FormulaCalculationNode(CalculationNode):
    """Calculate values based on a formula string.

    Use a formula expression and mapped input nodes to evaluate a calculation.

    Attributes:
        inputs_dict (dict[str, Node]): Mapping of variable names to input nodes.
        formula (str): Mathematical expression to evaluate.
        metric_name (Optional[str]): Metric identifier from the registry, if any.
        metric_description (Optional[str]): Description from the metric definition, if any.

    Examples:
        >>> from fin_statement_model.core.nodes import FinancialStatementItemNode
        >>> revenue = FinancialStatementItemNode("revenue", {"2023": 100})
        >>> cogs = FinancialStatementItemNode("cogs", {"2023": 60})
        >>> formula_node = FormulaCalculationNode(
        ...     "gross_profit", inputs={"rev": revenue, "cost": cogs}, formula="rev - cost"
        ... )
        >>> formula_node.calculate("2023")
        40.0
    """

    def __init__(
        self,
        name: str,
        inputs: dict[str, Node],
        formula: str,
        metric_name: Optional[str] = None,
        metric_description: Optional[str] = None,
    ):
        """Create a FormulaCalculationNode.

        Args:
            name (str): Unique identifier for the node.
            inputs (dict[str, Node]): Mapping of variable names to input nodes.
            formula (str): Mathematical formula string to evaluate.
            metric_name (Optional[str]): Original metric key from registry.
            metric_description (Optional[str]): Description from the metric definition.

        Raises:
            ValueError: If `formula` syntax is invalid.
            TypeError: If any entry in `inputs` is not a Node.
        """
        if not isinstance(inputs, dict) or not all(
            isinstance(n, Node) for n in inputs.values()
        ):
            raise TypeError(
                "FormulaCalculationNode inputs must be a dict of Node instances."
            )

        # Store the formula and metric attributes
        self.formula = formula
        self.metric_name = metric_name
        self.metric_description = metric_description

        # Extract variable names and input nodes in consistent order
        input_variable_names = list(inputs.keys())
        input_nodes = list(inputs.values())

        # Create FormulaCalculation strategy
        formula_calculation = FormulaCalculation(formula, input_variable_names)

        # Initialize parent CalculationNode with the strategy
        super().__init__(name, input_nodes, formula_calculation)

        # Store the inputs dict for compatibility (separate from parent's inputs list)
        self.inputs_dict = inputs

    def get_dependencies(self) -> list[str]:
        """Get names of nodes used in the formula.

        Returns:
            list[str]: Names of input nodes.
        """
        return [node.name for node in self.inputs_dict.values()]

    def to_dict(self) -> dict[str, Any]:
        """Serialize this node to a dictionary.

        Returns:
            dict[str, Any]: Serialized node data.
        """
        return {
            "type": "formula_calculation",
            "name": self.name,
            "inputs": self.get_dependencies(),
            "formula_variable_names": list(self.inputs_dict.keys()),
            "formula": self.formula,
            "calculation_type": "formula",
            "metric_name": self.metric_name,
            "metric_description": self.metric_description,
        }

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "FormulaCalculationNode":
        """Create a FormulaCalculationNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new FormulaCalculationNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        if data.get("type") != "formula_calculation":
            raise ValueError(
                f"Invalid type for FormulaCalculationNode: {data.get('type')}"
            )

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in FormulaCalculationNode data")

        formula = data.get("formula")
        if not formula:
            raise ValueError("Missing 'formula' field in FormulaCalculationNode data")

        input_names = data.get("inputs", [])
        formula_variable_names = data.get("formula_variable_names", [])

        if len(input_names) != len(formula_variable_names):
            raise ValueError(
                "Mismatch between inputs and formula_variable_names in FormulaCalculationNode data"
            )

        # Resolve input nodes from context and create inputs dict
        inputs_dict = {}
        for var_name, input_name in zip(formula_variable_names, input_names):
            if input_name not in context:
                raise ValueError(f"Input node '{input_name}' not found in context")
            inputs_dict[var_name] = context[input_name]

        # Extract metric information
        metric_name = data.get("metric_name")
        metric_description = data.get("metric_description")

        return FormulaCalculationNode(
            name=name,
            inputs=inputs_dict,
            formula=formula,
            metric_name=metric_name,
            metric_description=metric_description,
        )


# === CustomCalculationNode ===


class CustomCalculationNode(Node):
    """Calculate values using a custom Python function.

    Use a provided callable to compute node values from input nodes.

    Attributes:
        inputs (list[Node]): Nodes supplying inputs to the function.
        formula_func (Callable[..., float]): Function to compute values.
        description (Optional[str]): Description of the calculation.
        _values (dict[str, float]): Cache of computed results.

    Examples:
        >>> from fin_statement_model.core.nodes import FinancialStatementItemNode
        >>> def add(a, b): return a + b
        >>> a = FinancialStatementItemNode("A", {"2023": 10})
        >>> b = FinancialStatementItemNode("B", {"2023": 5})
        >>> node = CustomCalculationNode("add_node", inputs=[a, b], formula_func=add)
        >>> node.calculate("2023")
        15.0
    """

    def __init__(
        self,
        name: str,
        inputs: list[Node],
        formula_func: Callable[..., float],
        description: Optional[str] = None,
    ) -> None:
        """Create a CustomCalculationNode.

        Args:
            name (str): Unique identifier for the node.
            inputs (list[Node]): Nodes providing input values.
            formula_func (Callable[..., float]): Function to compute values.
            description (str, optional): Description of the calculation.

        Raises:
            TypeError: If `inputs` is not a list of Node or `formula_func` is not callable.
        """
        super().__init__(name)
        if not isinstance(inputs, list) or not all(isinstance(n, Node) for n in inputs):
            raise TypeError(
                "CustomCalculationNode inputs must be a list of Node instances"
            )
        if not callable(formula_func):
            raise TypeError(
                "CustomCalculationNode formula_func must be a callable function"
            )

        self.inputs = inputs
        self.formula_func = formula_func
        self.description = description
        self._values: dict[str, float] = {}  # Cache for calculated results

    def calculate(self, period: str) -> float:
        """Compute the node's value for a given period.

        Evaluate `formula_func` with inputs from `inputs` and cache the result.

        Args:
            period (str): The time period for which to perform the calculation.

        Returns:
            float: Computed value for the period.

        Raises:
            CalculationError: On errors retrieving inputs or computing the function.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Get input values
            input_values = []
            for node in self.inputs:
                value = node.calculate(period)
                if not isinstance(value, int | float):
                    raise TypeError(
                        f"Input node '{node.name}' did not return a numeric value for period '{period}'. Got {type(value).__name__}."
                    )
                input_values.append(value)

            # Calculate the value using the provided function
            result = self.formula_func(*input_values)
            if not isinstance(result, int | float):
                raise TypeError(
                    f"Formula did not return a numeric value. Got {type(result).__name__}."
                )

            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the function
            raise CalculationError(
                message=f"Error during custom calculation for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={"function": self.formula_func.__name__, "error": str(e)},
            ) from e

    def get_dependencies(self) -> list[str]:
        """Get names of nodes used by the function.

        Returns:
            list[str]: Names of input nodes.
        """
        return [node.name for node in self.inputs]

    def to_dict(self) -> dict[str, Any]:
        """Serialize this node to a dictionary.

        Returns:
            dict[str, Any]: Serialized node data with non-serializable function warning.
        """
        return {
            "type": "custom_calculation",
            "name": self.name,
            "inputs": self.get_dependencies(),
            "description": self.description,
            "serialization_warning": (
                "CustomCalculationNode uses a Python function which cannot be serialized. "
                "Manual reconstruction required."
            ),
        }

    # from_dict static method removed – CustomCalculationNode cannot be deserialized without source code; use NodeFactory or manual creation.



================================================================================
File: fin_statement_model/core/nodes/forecast_nodes.py
================================================================================

"""Provide forecast nodes to project future values from historical data.

This module defines the base `ForecastNode` class and its subclasses,
implementing various forecasting strategies (fixed, curve, statistical,
custom, average, and historical growth).
"""

import logging
from collections.abc import Callable
from typing import Optional, Any

# Use absolute imports
from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class ForecastNode(Node):
    """ForecastNode defines base behavior for projecting future values.

    ForecastNode uses a source node's historical data to generate projected values
    for specified future periods, caching results to avoid redundant computations.

    Attributes:
        input_node (Node): Node providing historical data.
        base_period (str): Last historical period used as forecast base.
        forecast_periods (list[str]): Future periods to project.
        values (dict[str, float]): Historical and forecasted values.
    """

    _cache: dict[str, float]

    def __init__(self, input_node: Node, base_period: str, forecast_periods: list[str]):
        """Initialize a ForecastNode.

        Args:
            input_node (Node): Source of historical data.
            base_period (str): Last historical period as forecast base.
            forecast_periods (list[str]): Future periods to generate forecasts for.
        """
        # Initialize with a default name based on input node, but allow it to be overridden
        super().__init__(input_node.name)
        self.input_node = input_node
        self.base_period = base_period
        self.forecast_periods = forecast_periods
        self._cache = {}

        # Copy historical values from input node
        if hasattr(input_node, "values"):
            self.values = input_node.values.copy()
        else:
            self.values = {}

    def calculate(self, period: str) -> float:
        """Calculate the node's value for a given period.

        Returns historical values for periods up to `base_period`; computes forecast for later periods.

        Args:
            period (str): Period identifier, historical or forecast.

        Returns:
            float: Value for the specified period.

        Raises:
            ValueError: If `period` is not a historical or forecast period.
        """
        if period not in self._cache:
            self._cache[period] = self._calculate_value(period)
        return self._cache[period]

    def clear_cache(self) -> None:
        """Clear cached forecast values.

        Use to force recomputation of all periods when input data changes.
        """
        self._cache.clear()

    def get_dependencies(self) -> list[str]:
        """Get names of nodes that this forecast depends on.

        Returns:
            list[str]: Single-element list of the input node's name.
        """
        return [self.input_node.name]

    def _calculate_value(self, period: str) -> float:
        """Compute the value for a given period without caching.

        Args:
            period (str): Period identifier to compute.

        Returns:
            float: Historical or forecasted value.

        Raises:
            ValueError: If `period` is not valid for this node.
        """
        # For historical periods, return the actual value
        if period <= self.base_period:
            # Return historical value, ensuring float type
            return float(self.values.get(period, 0.0))

        # For forecast periods, calculate using growth rate
        if period not in self.forecast_periods:
            raise ValueError(
                f"Period '{period}' not in forecast periods for {self.name}"
            )

        # Get the previous period's value
        prev_period = self._get_previous_period(period)
        prev_value = self.calculate(prev_period)

        # Get the growth rate for this period
        growth_factor = self._get_growth_factor_for_period(
            period, prev_period, prev_value
        )

        # Calculate the new value
        return prev_value * (1 + growth_factor)

    def _get_previous_period(self, current_period: str) -> str:
        all_periods = sorted([self.base_period, *self.forecast_periods])
        idx = all_periods.index(current_period)
        return all_periods[idx - 1]

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        raise NotImplementedError("Implement in subclass.")

    def to_dict(self) -> dict[str, Any]:
        """Serialize this node to a dictionary.

        Returns:
            dict[str, Any]: Serialized representation including base forecast parameters.

        Note:
            Subclasses should override to include specific forecast details.
        """
        return {
            "type": "forecast",
            "name": self.name,
            "base_node_name": self.input_node.name,
            "base_period": self.base_period,
            "forecast_periods": self.forecast_periods.copy(),
            "forecast_type": "base",  # Override in subclasses
        }

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "ForecastNode":
        """Recreate a ForecastNode from serialized data.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new ForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: This base method should be overridden by subclasses.
        """
        raise NotImplementedError("Subclasses must implement from_dict_with_context")


class FixedGrowthForecastNode(ForecastNode):
    """Forecast node that applies a single growth rate to every future period.

    Attributes:
        growth_rate (float): Constant growth factor expressed as a decimal (``0.05`` → 5 %).

    Examples:
        >>> from fin_statement_model.core.nodes import FinancialStatementItemNode
        >>> revenue = FinancialStatementItemNode("revenue", {"FY2022": 100})
        >>> forecast = FixedGrowthForecastNode(revenue, "FY2022", ["FY2023", "FY2024"], 0.05)
        >>> round(forecast.calculate("FY2024"), 2)
        110.25
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_rate: Optional[float] = None,
    ):
        """Create a FixedGrowthForecastNode.

        Args:
            input_node (Node): Source of historical data.
            base_period (str): Last historical period.
            forecast_periods (list[str]): Future periods to project.
            growth_rate (float | None): Constant growth rate (``0.05`` → 5 %).
                If ``None``, the default configured in ``cfg('forecasting.default_growth_rate')`` is used.
        """
        super().__init__(input_node, base_period, forecast_periods)

        # Use config default if not provided (import inside to avoid circular import)
        if growth_rate is None:
            from fin_statement_model.config.helpers import cfg

            growth_rate = cfg("forecasting.default_growth_rate")

        self.growth_rate = float(growth_rate)  # Ensure it's a float
        logger.debug(
            f"Created FixedGrowthForecastNode with growth rate: {self.growth_rate}"
        )

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        logger.debug(
            f"FixedGrowthForecastNode: Using growth rate {self.growth_rate} for period {period}"
        )
        return self.growth_rate

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "simple",
                "growth_params": self.growth_rate,
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "FixedGrowthForecastNode":
        """Create a FixedGrowthForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new FixedGrowthForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in FixedGrowthForecastNode data")

        base_node_name = data.get("base_node_name")
        if not base_node_name:
            raise ValueError(
                "Missing 'base_node_name' field in FixedGrowthForecastNode data"
            )

        if base_node_name not in context:
            raise ValueError(f"Base node '{base_node_name}' not found in context")

        base_node = context[base_node_name]
        base_period = data.get("base_period")
        forecast_periods = data.get("forecast_periods", [])
        growth_params = data.get("growth_params")

        if not base_period:
            raise ValueError(
                "Missing 'base_period' field in FixedGrowthForecastNode data"
            )

        node = FixedGrowthForecastNode(
            input_node=base_node,
            base_period=base_period,
            forecast_periods=forecast_periods,
            growth_rate=growth_params,
        )

        # Set the correct name from the serialized data
        node.name = name
        return node


class CurveGrowthForecastNode(ForecastNode):
    """Forecast node with period-specific growth rates.

    Apply a unique growth rate to each forecast period, allowing tapered or step-wise growth assumptions.

    Attributes:
        growth_rates (list[float]): Growth rate for each corresponding forecast period.

    Examples:
        >>> rates = [0.10, 0.08, 0.05]  # 10 %, 8 %, 5 %
        >>> forecast = CurveGrowthForecastNode(revenue, "FY2022", ["FY2023", "FY2024", "FY2025"], rates)
        >>> round(forecast.calculate("FY2025"), 2)
        123.48
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_rates: list[float],
    ):
        """Create a CurveGrowthForecastNode.

        Args:
            input_node (Node): Source of historical data.
            base_period (str): Last historical period.
            forecast_periods (list[str]): Future periods to project.
            growth_rates (list[float]): Growth rate for each forecast period; length must equal *forecast_periods*.
        """
        super().__init__(input_node, base_period, forecast_periods)
        if len(growth_rates) != len(forecast_periods):
            raise ValueError("Number of growth rates must match forecast periods.")
        self.growth_rates = [
            float(rate) for rate in growth_rates
        ]  # Ensure all are floats
        logger.debug(
            f"Created CurveGrowthForecastNode with growth rates: {self.growth_rates}"
        )
        logger.debug(f"  Base period: {base_period}")
        logger.debug(f"  Forecast periods: {forecast_periods}")
        logger.debug(f"  Base value: {input_node.calculate(base_period)}")

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Get the growth factor for a specific period."""
        idx = self.forecast_periods.index(period)
        growth_rate = self.growth_rates[idx]
        logger.debug(
            f"CurveGrowthForecastNode: Using growth rate {growth_rate} for period {period}"
        )
        logger.debug(f"  Previous period: {prev_period}")
        logger.debug(f"  Previous value: {prev_value}")
        return growth_rate

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "curve",
                "growth_params": self.growth_rates.copy(),
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "CurveGrowthForecastNode":
        """Create a CurveGrowthForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new CurveGrowthForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in CurveGrowthForecastNode data")

        base_node_name = data.get("base_node_name")
        if not base_node_name:
            raise ValueError(
                "Missing 'base_node_name' field in CurveGrowthForecastNode data"
            )

        if base_node_name not in context:
            raise ValueError(f"Base node '{base_node_name}' not found in context")

        base_node = context[base_node_name]
        base_period = data.get("base_period")
        forecast_periods = data.get("forecast_periods", [])
        growth_params = data.get("growth_params", [])

        if not base_period:
            raise ValueError(
                "Missing 'base_period' field in CurveGrowthForecastNode data"
            )

        if not isinstance(growth_params, list):
            raise TypeError(
                "'growth_params' must be a list for CurveGrowthForecastNode"
            )

        node = CurveGrowthForecastNode(
            input_node=base_node,
            base_period=base_period,
            forecast_periods=forecast_periods,
            growth_rates=growth_params,
        )

        # Set the correct name from the serialized data
        node.name = name
        return node


class StatisticalGrowthForecastNode(ForecastNode):
    """Forecast node whose growth rates are drawn from a random distribution.

    Use a zero-argument callable that samples from a statistical distribution to introduce stochasticity.

    Attributes:
        distribution_callable (Callable[[], float]): Function returning a pseudo-random growth rate.
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        distribution_callable: Callable[[], float],
    ):
        """Create a StatisticalGrowthForecastNode.

        Args:
            input_node (Node): Source of historical data.
            base_period (str): Last historical period.
            forecast_periods (list[str]): Future periods to project.
            distribution_callable (Callable[[], float]): Zero-argument function returning random growth rates.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.distribution_callable = distribution_callable

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.distribution_callable()

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.

        Note:
            The distribution_callable cannot be serialized, so a warning is included.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "statistical",
                "serialization_warning": (
                    "StatisticalGrowthForecastNode uses a distribution callable which cannot be serialized. "
                    "Manual reconstruction required."
                ),
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "StatisticalGrowthForecastNode":
        """Create a StatisticalGrowthForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new StatisticalGrowthForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: StatisticalGrowthForecastNode cannot be fully deserialized
                because the distribution_callable cannot be serialized.
        """
        raise NotImplementedError(
            "StatisticalGrowthForecastNode cannot be fully deserialized because the "
            "distribution_callable cannot be serialized. Manual reconstruction required."
        )


class CustomGrowthForecastNode(ForecastNode):
    """Forecast node that computes growth via a user-supplied function.

    The supplied ``growth_function`` receives ``period``, ``prev_period``, and ``prev_value`` and
    returns a growth factor for the period.
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_function: Callable[[str, str, float], float],
    ):
        """Create a CustomGrowthForecastNode.

        Args:
            input_node (Node): Source of historical data.
            base_period (str): Last historical period.
            forecast_periods (list[str]): Future periods to project.
            growth_function (Callable[[str, str, float], float]): Function returning growth factor.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.growth_function = growth_function

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.growth_function(period, prev_period, prev_value)

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.

        Note:
            The growth_function cannot be serialized, so a warning is included.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "custom",
                "serialization_warning": (
                    "CustomGrowthForecastNode uses a growth function which cannot be serialized. "
                    "Manual reconstruction required."
                ),
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "CustomGrowthForecastNode":
        """Create a CustomGrowthForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new CustomGrowthForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
            NotImplementedError: CustomGrowthForecastNode cannot be fully deserialized
                because the growth_function cannot be serialized.
        """
        raise NotImplementedError(
            "CustomGrowthForecastNode cannot be fully deserialized because the "
            "growth_function cannot be serialized. Manual reconstruction required."
        )


class AverageValueForecastNode(ForecastNode):
    """Forecast node that projects the historical average forward.

    The average of all historical periods up to *base_period* is used as the forecasted value
    for every future period.
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
    ):
        """Create an AverageValueForecastNode.

        Args:
            input_node (Node): Source of historical data.
            base_period (str): Last historical period.
            forecast_periods (list[str]): Future periods to project.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.average_value = self._calculate_average_value()
        logger.debug(
            f"Created AverageValueForecastNode with average value: {self.average_value}"
        )

    def _calculate_average_value(self) -> float:
        """Calculate the average historical value up to the base period.

        Returns:
            float: The average of historical values or 0.0 if none.
        """
        values = [
            value for period, value in self.values.items() if period <= self.base_period
        ]
        if not values:
            logger.warning(
                f"No historical values found for {self.name}, using 0.0 as average"
            )
            return 0.0
        # Compute average and ensure float type
        return float(sum(values)) / len(values)

    def _calculate_value(self, period: str) -> float:
        """Calculate the value for a specific period using the computed average value."""
        # For historical periods, return the actual value
        if period <= self.base_period:
            # Return historical value, ensuring float type
            return float(self.values.get(period, 0.0))

        # For forecast periods, return the constant average value
        if period not in self.forecast_periods:
            raise ValueError(
                f"Period '{period}' not in forecast periods for {self.name}"
            )

        return self.average_value

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Not used for average value forecasts."""
        return 0.0

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "average",
                "average_value": self.average_value,
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "AverageValueForecastNode":
        """Create an AverageValueForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new AverageValueForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in AverageValueForecastNode data")

        base_node_name = data.get("base_node_name")
        if not base_node_name:
            raise ValueError(
                "Missing 'base_node_name' field in AverageValueForecastNode data"
            )

        if base_node_name not in context:
            raise ValueError(f"Base node '{base_node_name}' not found in context")

        base_node = context[base_node_name]
        base_period = data.get("base_period")
        forecast_periods = data.get("forecast_periods", [])

        if not base_period:
            raise ValueError(
                "Missing 'base_period' field in AverageValueForecastNode data"
            )

        node = AverageValueForecastNode(
            input_node=base_node,
            base_period=base_period,
            forecast_periods=forecast_periods,
        )

        # Set the correct name from the serialized data
        node.name = name
        return node


class AverageHistoricalGrowthForecastNode(ForecastNode):
    """Forecast node that applies the average historical growth rate to all future periods."""

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
    ):
        """Create an AverageHistoricalGrowthForecastNode.

        Args:
            input_node (Node): Source of historical data.
            base_period (str): Last historical period.
            forecast_periods (list[str]): Future periods to project.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.avg_growth_rate = self._calculate_average_growth_rate()
        logger.debug(
            f"Created AverageHistoricalGrowthForecastNode with growth rate: {self.avg_growth_rate}"
        )

    def _calculate_average_growth_rate(self) -> float:
        """Calculate the average growth rate from historical values.

        Returns:
            float: The average growth rate or 0.0 if insufficient data.
        """
        # Get historical periods up to base_period, sorted
        historical_periods = sorted([p for p in self.values if p <= self.base_period])

        if len(historical_periods) < 2:
            logger.warning(
                f"Insufficient historical data for {self.name}, using 0.0 as growth rate"
            )
            return 0.0

        # Calculate growth rates between consecutive periods
        growth_rates = []
        for i in range(1, len(historical_periods)):
            prev_period = historical_periods[i - 1]
            curr_period = historical_periods[i]
            prev_value = self.values.get(prev_period, 0.0)
            curr_value = self.values.get(curr_period, 0.0)

            if prev_value != 0:
                growth_rate = (curr_value - prev_value) / prev_value
                growth_rates.append(growth_rate)

        if not growth_rates:
            logger.warning(
                f"No valid growth rates calculated for {self.name}, using 0.0"
            )
            return 0.0

        # Compute average growth rate and ensure float type
        return float(sum(growth_rates)) / len(growth_rates)

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.avg_growth_rate

    def to_dict(self) -> dict[str, Any]:
        """Serialize the node to a dictionary representation.

        Returns:
            Dictionary containing the node's forecast configuration.
        """
        base_dict = super().to_dict()
        base_dict.update(
            {
                "forecast_type": "historical_growth",
                "avg_growth_rate": self.avg_growth_rate,
            }
        )
        return base_dict

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "AverageHistoricalGrowthForecastNode":
        """Create an AverageHistoricalGrowthForecastNode from a dictionary with node context.

        Args:
            data: Dictionary containing the node's serialized data.
            context: Dictionary of existing nodes to resolve dependencies.

        Returns:
            A new AverageHistoricalGrowthForecastNode instance.

        Raises:
            ValueError: If the data is invalid or missing required fields.
        """
        name = data.get("name")
        if not name:
            raise ValueError(
                "Missing 'name' field in AverageHistoricalGrowthForecastNode data"
            )

        base_node_name = data.get("base_node_name")
        if not base_node_name:
            raise ValueError(
                "Missing 'base_node_name' field in AverageHistoricalGrowthForecastNode data"
            )

        if base_node_name not in context:
            raise ValueError(f"Base node '{base_node_name}' not found in context")

        base_node = context[base_node_name]
        base_period = data.get("base_period")
        forecast_periods = data.get("forecast_periods", [])

        if not base_period:
            raise ValueError(
                "Missing 'base_period' field in AverageHistoricalGrowthForecastNode data"
            )

        node = AverageHistoricalGrowthForecastNode(
            input_node=base_node,
            base_period=base_period,
            forecast_periods=forecast_periods,
        )

        # Set the correct name from the serialized data
        node.name = name
        return node



================================================================================
File: fin_statement_model/core/nodes/item_node.py
================================================================================

"""Define a node representing a basic financial statement item."""

import logging
from typing import Any

# Use absolute imports
from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class FinancialStatementItemNode(Node):
    """Store raw financial statement values for specific periods.

    Represents a leaf node containing actual reported financial data
    (e.g., revenue, COGS) across time periods.

    Attributes:
        name (str): Unique identifier for the financial item.
        values (dict[str, float]): Mapping from period identifiers to their values.

    Examples:
        >>> from fin_statement_model.core.nodes import FinancialStatementItemNode
        >>> data = {"2022": 1000.0, "2023": 1200.0}
        >>> node = FinancialStatementItemNode("revenue", data)
        >>> node.calculate("2023")
        1200.0
        >>> node.set_value("2024", 1500.0)
        >>> node.calculate("2024")
        1500.0
    """

    values: dict[str, float]

    def __init__(self, name: str, values: dict[str, float]):
        """Create a FinancialStatementItemNode.

        Args:
            name (str): Unique identifier for the financial item.
            values (dict[str, float]): Initial mapping of periods to values.

        Raises:
            ValueError: If `name` is empty, contains invalid characters, or has leading/trailing whitespace.
        """
        super().__init__(name)
        self.values = values

    def calculate(self, period: str) -> float:
        """Get the value for a specific period.

        Args:
            period (str): Period identifier to retrieve.

        Returns:
            float: Stored value for `period`, or 0.0 if not present.
        """
        return self.values.get(period, 0.0)

    def set_value(self, period: str, value: float) -> None:
        """Set the value for a specific period.

        Args:
            period (str): Period identifier.
            value (float): Numerical value to store.
        """
        self.values[period] = value

    def to_dict(self) -> dict[str, Any]:
        """Serialize this node to a dictionary.

        Returns:
            dict[str, Any]: Dictionary with keys 'type', 'name', and 'values'.

        Examples:
            >>> from fin_statement_model.core.nodes import FinancialStatementItemNode
            >>> node = FinancialStatementItemNode("Revenue", {"2023": 1000.0})
            >>> data = node.to_dict()
            >>> data['type']
            'financial_statement_item'
        """
        return {
            "type": "financial_statement_item",
            "name": self.name,
            "values": self.values.copy(),
        }

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "FinancialStatementItemNode":
        """Create a FinancialStatementItemNode from serialized data.

        Args:
            data (dict[str, Any]): Serialized node data; must contain keys 'type', 'name', and 'values'.

        Returns:
            FinancialStatementItemNode: Reconstructed node.

        Raises:
            ValueError: If 'type' is not 'financial_statement_item' or 'name' is missing.
            TypeError: If 'values' is not a dict.
        """
        if data.get("type") != "financial_statement_item":
            raise ValueError(
                f"Invalid type for FinancialStatementItemNode: {data.get('type')}"
            )

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in FinancialStatementItemNode data")

        values = data.get("values", {})
        if not isinstance(values, dict):
            raise TypeError("'values' field must be a dict[str, float]")

        return FinancialStatementItemNode(name, values)



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/__init__.py
================================================================================

"""Standard Node Definitions Package.

This package contains organized standard node definitions split into logical categories.
All definitions are automatically loaded into the standard_node_registry.
"""

import logging
from pathlib import Path
from typing import Optional

from fin_statement_model.core.nodes.standard_registry import standard_node_registry

logger = logging.getLogger(__name__)


def load_all_standard_nodes(base_path: Optional[Path] = None) -> int:
    """Load all standard node definitions from organized YAML files.

    Args:
        base_path: Base path to the standard_nodes_defn directory. If None, uses default.

    Returns:
        Total number of nodes loaded.
    """
    if base_path is None:
        base_path = Path(__file__).parent

    total_loaded = 0

    # Define the organized node files to load
    node_files = [
        # Balance sheet nodes
        "balance_sheet/assets.yaml",
        "balance_sheet/liabilities.yaml",
        "balance_sheet/equity.yaml",
        # Income statement nodes
        "income_statement/revenue_costs.yaml",
        "income_statement/operating.yaml",
        "income_statement/non_operating.yaml",
        "income_statement/shares.yaml",
        # Cash flow nodes
        "cash_flow/operating.yaml",
        "cash_flow/investing.yaml",
        "cash_flow/financing.yaml",
        # Calculated items
        "calculated/profitability.yaml",
        "calculated/liquidity.yaml",
        "calculated/leverage.yaml",
        "calculated/valuation.yaml",
        # Market data
        "market_data/market_data.yaml",
        # Real estate nodes
        "real_estate/property_operations.yaml",
        "real_estate/reit_specific.yaml",
        "real_estate/debt_financing.yaml",
        # Banking nodes
        "banking/assets.yaml",
        "banking/liabilities.yaml",
        "banking/income_statement.yaml",
        "banking/regulatory_capital.yaml",
        "banking/off_balance_sheet.yaml",
    ]

    for file_path in node_files:
        full_path = base_path / file_path
        if full_path.exists():
            try:
                count = standard_node_registry.load_from_yaml_file(full_path)
                total_loaded += count
                logger.debug(f"Loaded {count} nodes from {file_path}")
            except Exception:
                logger.exception(f"Failed to load {file_path}")
        else:
            logger.warning(f"Organized node file not found: {full_path}")

    logger.info(f"Loaded {total_loaded} total standard nodes from organized structure")
    return total_loaded


# Auto-load on import
try:
    load_all_standard_nodes()
except Exception as e:
    logger.warning(f"Failed to auto-load standard nodes: {e}")



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/balance_sheet/__init__.py
================================================================================

"""Balance Sheet Standard Node Definitions."""

# This package contains balance sheet node definitions split into:
# - assets.yaml: Current and non-current assets
# - liabilities.yaml: Current and non-current liabilities
# - equity.yaml: Equity components



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/banking/__init__.py
================================================================================

"""Banking Standard Node Definitions."""

# This package contains banking node definitions split into:
# - assets.yaml: Loans, securities, cash, and other banking assets
# - liabilities.yaml: Deposits, borrowings, and other banking liabilities
# - income_statement.yaml: Interest income/expense, fees, provisions
# - regulatory_capital.yaml: Tier 1/2 capital, risk-weighted assets
# - off_balance_sheet.yaml: Commitments, guarantees, derivatives



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/calculated/__init__.py
================================================================================

"""Calculated Standard Node Definitions."""

# This package contains calculated node definitions split into:
# - profitability.yaml: EBITDA, NOPAT, etc.
# - liquidity.yaml: Working capital measures
# - leverage.yaml: Net debt, leverage measures
# - valuation.yaml: Enterprise value, etc.



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/cash_flow/__init__.py
================================================================================

"""Cash Flow Statement Standard Node Definitions."""

# This package contains cash flow statement node definitions split into:
# - operating.yaml: Operating activities
# - investing.yaml: Investing activities
# - financing.yaml: Financing activities



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/income_statement/__init__.py
================================================================================

"""Income Statement Standard Node Definitions."""

# This package contains income statement node definitions split into:
# - revenue_costs.yaml: Revenue and direct costs
# - operating.yaml: Operating expenses and income
# - non_operating.yaml: Interest, other income, taxes
# - shares.yaml: Share-related items



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/market_data/__init__.py
================================================================================

"""Market Data Standard Node Definitions."""

# This package contains market data node definitions:
# - market_data.yaml: Market prices and per-share data



================================================================================
File: fin_statement_model/core/nodes/standard_nodes_defn/real_estate/__init__.py
================================================================================

"""Real Estate Standard Node Definitions."""

# This package contains real estate node definitions split into:
# - property_operations.yaml: Property income, expenses, and operational metrics
# - reit_specific.yaml: REIT-specific items like FFO, AFFO, and adjustments
# - property_metrics.yaml: Property-level metrics and measurements
# - debt_financing.yaml: Debt, financing, and loan-related nodes



================================================================================
File: fin_statement_model/core/nodes/standard_registry.py
================================================================================

"""Registry for standard node names.

This module provides a registry system for standardized node names,
ensuring consistency across financial models and enabling metrics to work properly.
"""

import logging
from pathlib import Path
from typing import Any, Optional
import yaml
from pydantic import BaseModel, ConfigDict

logger = logging.getLogger(__name__)


class StandardNodeDefinition(BaseModel):
    """Represent metadata for a standard node definition.

    Attributes:
        category (str): Main category (e.g., 'balance_sheet_assets').
        subcategory (str): Subcategory within the main category.
        description (str): Human-readable description of the node.
        alternate_names (list[str]): Alternate names mapping to this standard.
        sign_convention (str): Sign convention ('positive' or 'negative').
    """

    model_config = ConfigDict(str_strip_whitespace=True)

    category: str
    subcategory: str
    description: str
    alternate_names: list[str] = []
    sign_convention: str = "positive"  # 'positive' or 'negative'


class StandardNodeRegistry:
    """Manage loading and access to standard node definitions.

    Provide methods to load definitions, validate node names, and resolve alternate names.

    Attributes:
        _standard_nodes (dict[str, StandardNodeDefinition]): Map standard names to definitions.
        _alternate_to_standard (dict[str, str]): Map alternate names to standard names.
        _categories (set[str]): Registered categories.
        _initialized (bool): Whether default nodes have been initialized.
        _loaded_from (Optional[str]): Source path of loaded definitions.
    """

    def __init__(self) -> None:
        """Initialize an empty registry."""
        self._standard_nodes: dict[str, StandardNodeDefinition] = {}
        self._alternate_to_standard: dict[str, str] = {}
        self._categories: set[str] = set()
        self._initialized: bool = False  # Track initialization state
        self._loaded_from: Optional[str] = None  # Track source

    def _load_nodes_from_data(
        self,
        data: dict[str, Any],
        source_description: str,
        overwrite_existing: bool = False,
    ) -> int:
        """Load and process node definitions from parsed YAML data.

        Args:
            data (dict[str, Any]): Parsed YAML mapping node names to definitions.
            source_description (str): Description of the data source for logging.
            overwrite_existing (bool): Whether to replace existing definitions.

        Returns:
            int: Number of nodes successfully loaded.

        Raises:
            TypeError: If `data` is not a dict.
            ValueError: If definitions are invalid or duplicates are detected.
        """
        if not isinstance(data, dict):
            raise TypeError(
                f"Expected dict at root of {source_description}, got {type(data)}"
            )

        nodes_loaded = 0

        # Load each node definition
        for node_name, node_data in data.items():
            if not isinstance(node_data, dict):
                logger.warning(
                    f"Skipping invalid node definition '{node_name}' "
                    f"in {source_description}: not a dict"
                )
                continue

            try:
                definition = StandardNodeDefinition(**node_data)

                # Handle duplicate standard names
                if node_name in self._standard_nodes:
                    if not overwrite_existing:
                        raise ValueError(f"Duplicate standard node name: {node_name}")
                    else:
                        logger.debug(
                            f"Overwriting existing standard node: {node_name} "
                            f"from {source_description}"
                        )

                # Add to main registry
                self._standard_nodes[node_name] = definition
                self._categories.add(definition.category)

                # Map alternate names
                for alt_name in definition.alternate_names:
                    if alt_name in self._alternate_to_standard:
                        existing_standard = self._alternate_to_standard[alt_name]
                        if not overwrite_existing and existing_standard != node_name:
                            raise ValueError(
                                f"Alternate name '{alt_name}' already maps to "
                                f"'{existing_standard}', cannot also map to '{node_name}'"
                            )
                        elif overwrite_existing and existing_standard != node_name:
                            logger.debug(
                                f"Alternate name '{alt_name}' already maps to "
                                f"'{existing_standard}', now mapping to '{node_name}'"
                            )
                    self._alternate_to_standard[alt_name] = node_name

                nodes_loaded += 1

            except Exception as e:
                logger.exception(
                    f"Error loading node '{node_name}' from {source_description}"
                )
                raise ValueError(
                    f"Invalid node definition for '{node_name}': {e}"
                ) from e

        return nodes_loaded

    def get_standard_name(self, name: str) -> str:
        """Get the standard name for a given node name.

        If the name is already standard, returns it unchanged.
        If it's an alternate name, returns the corresponding standard name.
        If it's not recognized, returns the original name.

        Args:
            name: The node name to standardize.

        Returns:
            The standardized node name.
        """
        # Check if it's already a standard name
        if name in self._standard_nodes:
            return name

        # Check if it's an alternate name
        if name in self._alternate_to_standard:
            return self._alternate_to_standard[name]

        # Not recognized, return as-is
        return name

    def is_standard_name(self, name: str) -> bool:
        """Check if a name is a recognized standard node name.

        Args:
            name: The node name to check.

        Returns:
            True if the name is a standard node name, False otherwise.
        """
        return name in self._standard_nodes

    def is_alternate_name(self, name: str) -> bool:
        """Check if a name is a recognized alternate name.

        Args:
            name: The node name to check.

        Returns:
            True if the name is an alternate name, False otherwise.
        """
        return name in self._alternate_to_standard

    def is_recognized_name(self, name: str) -> bool:
        """Check if a name is either standard or alternate.

        Args:
            name: The node name to check.

        Returns:
            True if the name is recognized, False otherwise.
        """
        return self.is_standard_name(name) or self.is_alternate_name(name)

    def get_definition(self, name: str) -> Optional[StandardNodeDefinition]:
        """Get the definition for a node name.

        Works with both standard and alternate names.

        Args:
            name: The node name to look up.

        Returns:
            The node definition if found, None otherwise.
        """
        standard_name = self.get_standard_name(name)
        return self._standard_nodes.get(standard_name)

    def list_standard_names(self, category: Optional[str] = None) -> list[str]:
        """List all standard node names, optionally filtered by category.

        Args:
            category: Optional category to filter by.

        Returns:
            Sorted list of standard node names.
        """
        if category:
            names = [
                name
                for name, defn in self._standard_nodes.items()
                if defn.category == category
            ]
        else:
            names = list(self._standard_nodes.keys())

        return sorted(names)

    def list_categories(self) -> list[str]:
        """List all available categories.

        Returns:
            Sorted list of categories.
        """
        return sorted(self._categories)

    def validate_node_name(self, name: str, strict: bool = False) -> tuple[bool, str]:
        """Validate a node name against standards.

        Args:
            name: The node name to validate.
            strict: If True, only standard names are valid.
                   If False, alternate names are also valid.

        Returns:
            Tuple of (is_valid, message).
        """
        if strict:
            if self.is_standard_name(name):
                return True, f"'{name}' is a standard node name"
            elif self.is_alternate_name(name):
                standard = self.get_standard_name(name)
                return (
                    False,
                    f"'{name}' is an alternate name. Use standard name '{standard}'",
                )
            else:
                return False, f"'{name}' is not a recognized node name"
        elif self.is_recognized_name(name):
            if self.is_alternate_name(name):
                standard = self.get_standard_name(name)
                return True, f"'{name}' is valid (alternate for '{standard}')"
            else:
                return True, f"'{name}' is a standard node name"
        else:
            return False, f"'{name}' is not a recognized node name"

    def get_sign_convention(self, name: str) -> Optional[str]:
        """Get the sign convention for a node.

        Args:
            name: The node name (standard or alternate).

        Returns:
            The sign convention ('positive' or 'negative') if found, None otherwise.
        """
        definition = self.get_definition(name)
        return definition.sign_convention if definition else None

    def initialize_default_nodes(
        self,
        organized_path: Optional[Path] = None,
        force_reload: bool = False,
    ) -> int:
        """Load default standard nodes from organized directory.

        Args:
            organized_path (Path | None): Base path to `standard_nodes_defn`. Defaults to package directory.
            force_reload (bool): If True, reload even if already initialized.

        Returns:
            int: Count of nodes loaded.
        """
        if self._initialized and not force_reload:
            logger.debug(
                f"Standard nodes already initialized from {self._loaded_from}, skipping reload."
            )
            return len(self._standard_nodes)

        if organized_path is None:
            organized_path = Path(__file__).parent / "standard_nodes_defn"

        count = self._load_from_organized_structure(organized_path)
        self._initialized = True
        self._loaded_from = f"organized structure at {organized_path}"
        logger.info(
            f"Successfully loaded {count} standard nodes from organized structure"
        )
        return count

    def _load_from_organized_structure(self, base_path: Path) -> int:
        """Load standard nodes from an organized directory structure.

        Args:
            base_path (Path): Directory containing organized YAML node definition files.

        Returns:
            int: Number of nodes loaded.

        Raises:
            FileNotFoundError: If `base_path` does not exist.
            ImportError: If module loading fails.
            Exception: For other loading errors.
        """
        if not base_path.exists():
            raise FileNotFoundError(f"Organized structure path not found: {base_path}")

        # Check if it has the expected structure (contains __init__.py)
        init_file = base_path / "__init__.py"
        if not init_file.exists():
            raise ValueError(
                f"Invalid organized structure: missing __init__.py in {base_path}"
            )

        # Import and use the load_all_standard_nodes function
        import importlib.util

        spec = importlib.util.spec_from_file_location("standard_nodes_defn", init_file)
        if spec is None or spec.loader is None:
            raise ImportError(f"Failed to load module from {init_file}")

        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)

        # Clear existing data before loading
        self._standard_nodes.clear()
        self._alternate_to_standard.clear()
        self._categories.clear()

        # Call the loading function
        if hasattr(module, "load_all_standard_nodes"):
            loaded_count = module.load_all_standard_nodes(base_path)
            return int(loaded_count)
        else:
            raise AttributeError(
                f"Module at {init_file} missing load_all_standard_nodes function"
            )

    def is_initialized(self) -> bool:
        """Check if the registry has been initialized with default nodes."""
        return self._initialized

    def get_load_source(self) -> Optional[str]:
        """Get the source from which nodes were loaded."""
        return self._loaded_from

    def reload(self) -> int:
        """Force reload of standard nodes."""
        return self.initialize_default_nodes(force_reload=True)

    def __len__(self) -> int:
        """Return the number of standard nodes in the registry."""
        return len(self._standard_nodes)

    def load_from_yaml_file(self, yaml_path: Path) -> int:
        """Load standard node definitions from a YAML file.

        Does not clear existing definitions before loading.

        Args:
            yaml_path (Path): Path to the YAML file with node definitions.

        Returns:
            int: Number of nodes loaded from the file.

        Raises:
            FileNotFoundError: If `yaml_path` does not exist.
            ValueError: If YAML is invalid or structure is incorrect.
        """
        if not yaml_path.exists():
            raise FileNotFoundError(f"Standard nodes file not found: {yaml_path}")

        try:
            with open(yaml_path, encoding="utf-8") as f:
                data = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML in {yaml_path}: {e}") from e

        # Process the data without clearing existing
        nodes_loaded = self._load_nodes_from_data(
            data, str(yaml_path), overwrite_existing=True
        )
        if nodes_loaded:
            self._initialized = True
            self._loaded_from = str(yaml_path)
        logger.debug(f"Loaded {nodes_loaded} nodes from {yaml_path}")
        return nodes_loaded


# Global registry instance
standard_node_registry = StandardNodeRegistry()


# NOTE: Auto-loading is disabled to prevent conflicts with organized structure
# The nodes/__init__.py will handle loading from the appropriate source



================================================================================
File: fin_statement_model/core/nodes/stats_nodes.py
================================================================================

"""Provide statistical node implementations for time-series analyses.

This module defines nodes that perform statistical operations on node values across periods:
- YoYGrowthNode: Compute year-over-year percentage growth.
- MultiPeriodStatNode: Compute statistics (mean, stddev) over multiple periods.
- TwoPeriodAverageNode: Compute simple average over two periods.
"""

import logging
import math
import statistics

# Use lowercase built-in types for annotations
from typing import Optional, Union, Any
from typing import Callable

# Use absolute imports
from fin_statement_model.core.nodes.base import Node
from fin_statement_model.core.errors import CalculationError

# Added logger instance
logger = logging.getLogger(__name__)

Numeric = Union[int, float]
StatFunc = Callable[
    ..., Any
]  # Widen callable type to accept any callable returning Numeric


class YoYGrowthNode(Node):
    """Compute year-over-year percentage growth.

    Compare values of an input node for two periods and compute
    (current_value - prior_value) / prior_value.

    Attributes:
        input_node (Node): Node providing source values.
        prior_period (str): Identifier for the earlier period.
        current_period (str): Identifier for the later period.

    Examples:
        >>> from fin_statement_model.core.nodes import FinancialStatementItemNode, YoYGrowthNode
        >>> data = {"2022": 100.0, "2023": 120.0}
        >>> base = FinancialStatementItemNode("revenue", data)
        >>> yoy = YoYGrowthNode("rev_yoy", input_node=base, prior_period="2022", current_period="2023")
        >>> round(yoy.calculate(), 2)
        0.2
    """

    def __init__(
        self, name: str, input_node: Node, prior_period: str, current_period: str
    ):
        """Create a YoYGrowthNode.

        Args:
            name (str): Unique identifier for this node.
            input_node (Node): Node supplying values for comparison.
            prior_period (str): Identifier for the earlier period.
            current_period (str): Identifier for the later period.

        Raises:
            TypeError: If `input_node` is not a Node or periods are not strings.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError("YoYGrowthNode input_node must be a Node instance.")
        if not isinstance(prior_period, str) or not isinstance(current_period, str):
            raise TypeError(
                "YoYGrowthNode prior_period and current_period must be strings."
            )

        self.input_node = input_node
        self.prior_period = prior_period
        self.current_period = current_period

    def calculate(self, period: Optional[str] = None) -> float:
        """Compute the YoY growth rate.

        Ignore the `period` parameter; use configured periods.

        Args:
            period (str | None): Ignored.

        Returns:
            float: (current - prior) / prior, or NaN if prior is zero or non-finite.

        Raises:
            CalculationError: On errors retrieving or validating input values.
        """
        try:
            prior_value = self.input_node.calculate(self.prior_period)
            current_value = self.input_node.calculate(self.current_period)

            # Validate input types
            if not isinstance(prior_value, int | float):
                raise TypeError(
                    f"Prior period ('{self.prior_period}') value is non-numeric."
                )
            if not isinstance(current_value, int | float):
                raise TypeError(
                    f"Current period ('{self.current_period}') value is non-numeric."
                )

            # Handle division by zero or non-finite prior value
            if prior_value == 0 or not math.isfinite(prior_value):
                logger.warning(
                    f"YoYGrowthNode '{self.name}': Prior period '{self.prior_period}' value is zero or non-finite ({prior_value}). Returning NaN."
                )
                return float("nan")

            # Calculate growth
            growth = (float(current_value) - float(prior_value)) / float(prior_value)
            return growth

        except Exception as e:
            # Wrap any exception during calculation
            raise CalculationError(
                message=f"Failed to calculate YoY growth for node '{self.name}'",
                node_id=self.name,
                period=f"{self.prior_period}_to_{self.current_period}",  # Indicate period span
                details={
                    "input_node": self.input_node.name,
                    "prior_period": self.prior_period,
                    "current_period": self.current_period,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Get names of nodes this node depends on."""
        return [self.input_node.name]

    def to_dict(self) -> dict[str, Any]:
        """Serialize this node to a dictionary.

        Returns:
            dict[str, Any]: Serialized representation with type, name, and periods.
        """
        return {
            "type": "yoy_growth",
            "name": self.name,
            "input_node_name": self.input_node.name,
            "prior_period": self.prior_period,
            "current_period": self.current_period,
        }

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "YoYGrowthNode":
        """Recreate a YoYGrowthNode from serialized data.

        Args:
            data (dict[str, Any]): Serialized node data.
            context (dict[str, Node]): Existing nodes for dependencies.

        Returns:
            YoYGrowthNode: Reconstructed node.

        Raises:
            ValueError: If required fields are missing or invalid.
        """
        if data.get("type") != "yoy_growth":
            raise ValueError(f"Invalid type for YoYGrowthNode: {data.get('type')}")

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in YoYGrowthNode data")

        input_node_name = data.get("input_node_name")
        if not input_node_name:
            raise ValueError("Missing 'input_node_name' field in YoYGrowthNode data")

        if input_node_name not in context:
            raise ValueError(f"Input node '{input_node_name}' not found in context")

        input_node = context[input_node_name]
        prior_period = data.get("prior_period")
        current_period = data.get("current_period")

        if not prior_period:
            raise ValueError("Missing 'prior_period' field in YoYGrowthNode data")
        if not current_period:
            raise ValueError("Missing 'current_period' field in YoYGrowthNode data")

        return YoYGrowthNode(
            name=name,
            input_node=input_node,
            prior_period=prior_period,
            current_period=current_period,
        )


class MultiPeriodStatNode(Node):
    """Compute a statistical measure over multiple periods.

    Apply a statistical function (e.g., mean, stdev) to values from an input node across specified periods.

    Attributes:
        input_node (Node): Node providing source values.
        periods (list[str]): Period identifiers to include.
        stat_func (StatFunc): Function to apply to collected values.

    Examples:
        >>> from fin_statement_model.core.nodes import FinancialStatementItemNode, MultiPeriodStatNode
        >>> data = {"Q1": 10, "Q2": 12, "Q3": 11, "Q4": 13}
        >>> sales = FinancialStatementItemNode("sales", data)
        >>> avg = MultiPeriodStatNode("avg_sales", input_node=sales, periods=["Q1","Q2","Q3","Q4"], stat_func=statistics.mean)
        >>> avg.calculate()
        11.5
    """

    def __init__(
        self,
        name: str,
        input_node: Node,
        periods: list[str],
        stat_func: StatFunc = statistics.stdev,  # Default to standard deviation
    ):
        """Create a MultiPeriodStatNode.

        Args:
            name (str): Unique identifier for this node.
            input_node (Node): Node supplying values.
            periods (list[str]): Period identifiers to analyze.
            stat_func (StatFunc): Function applied to collected values. Defaults to statistics.stdev.

        Raises:
            ValueError: If `periods` is empty or not a list.
            TypeError: If `input_node` is not a Node or `stat_func` is not callable.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError("MultiPeriodStatNode input_node must be a Node instance.")
        if not isinstance(periods, list) or not periods:
            raise ValueError("MultiPeriodStatNode periods must be a non-empty list.")
        if not all(isinstance(p, str) for p in periods):
            raise TypeError("MultiPeriodStatNode periods must contain only strings.")
        if not callable(stat_func):
            raise TypeError(
                "MultiPeriodStatNode stat_func must be a callable function."
            )

        self.input_node = input_node
        self.periods = periods
        self.stat_func = stat_func

    def calculate(self, period: Optional[str] = None) -> float:
        """Compute the statistical measure across specified periods.

        Args:
            period (str | None): Ignored.

        Returns:
            float: Result of `stat_func` on collected values, or NaN if insufficient valid data.

        Raises:
            CalculationError: If input retrieval fails or unexpected errors occur.
        """
        values: list[Numeric] = []
        retrieval_errors = []
        try:
            for p in self.periods:
                try:
                    value = self.input_node.calculate(p)
                    if isinstance(value, int | float) and math.isfinite(value):
                        values.append(float(value))
                    else:
                        # Log non-numeric/non-finite values but continue if possible
                        logger.warning(
                            f"MultiPeriodStatNode '{self.name}': Input '{self.input_node.name}' gave non-numeric/non-finite value ({value}) for period '{p}'. Skipping."
                        )
                except Exception as node_err:
                    # Log error fetching data for a specific period but continue
                    logger.error(
                        f"MultiPeriodStatNode '{self.name}': Error getting value for period '{p}' from '{self.input_node.name}': {node_err}",
                        exc_info=True,
                    )
                    retrieval_errors.append(p)

            # If no valid numeric values were collected
            if not values:
                logger.warning(
                    f"MultiPeriodStatNode '{self.name}': No valid numeric data points found across periods {self.periods}. Returning NaN."
                )
                return float("nan")

            # Attempt the statistical calculation
            try:
                result = self.stat_func(values)
                # Ensure result is float, handle potential NaN from stat_func
                return float(result) if math.isfinite(result) else float("nan")
            except (statistics.StatisticsError, ValueError, TypeError) as stat_err:
                # Handle errors specific to statistical functions (e.g., stdev needs >= 2 points)
                logger.warning(
                    f"MultiPeriodStatNode '{self.name}': Stat function '{self.stat_func.__name__}' failed ({stat_err}). Values: {values}. Returning NaN."
                )
                return float("nan")

        except Exception as e:
            # Catch any other unexpected errors during the process
            raise CalculationError(
                message=f"Failed to calculate multi-period stat for node '{self.name}'",
                node_id=self.name,
                period="multi-period",  # Indicate calculation context
                details={
                    "input_node": self.input_node.name,
                    "periods": self.periods,
                    "stat_func": self.stat_func.__name__,
                    "collected_values_count": len(values),
                    "retrieval_errors_periods": retrieval_errors,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Get names of nodes this statistical node depends on."""
        return [self.input_node.name]

    def to_dict(self) -> dict[str, Any]:
        """Serialize this node to a dictionary.

        Returns:
            dict[str, Any]: Serialized data with function name and periods.

        Note:
            `stat_func` may not be fully serializable; manual reconstruction may be required.
        """
        return {
            "type": "multi_period_stat",
            "name": self.name,
            "input_node_name": self.input_node.name,
            "periods": self.periods.copy(),
            "stat_func_name": self.stat_func.__name__,
            "serialization_warning": (
                "MultiPeriodStatNode uses a statistical function which may not be fully serializable. "
                "Manual reconstruction may be required for custom functions."
            ),
        }

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "MultiPeriodStatNode":
        """Recreate a MultiPeriodStatNode from serialized data.

        Args:
            data (dict[str, Any]): Serialized node data.
            context (dict[str, Node]): Existing nodes for dependencies.

        Returns:
            MultiPeriodStatNode: Reconstructed node.

        Raises:
            ValueError: If required fields are missing or invalid.
        """
        if data.get("type") != "multi_period_stat":
            raise ValueError(
                f"Invalid type for MultiPeriodStatNode: {data.get('type')}"
            )

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in MultiPeriodStatNode data")

        input_node_name = data.get("input_node_name")
        if not input_node_name:
            raise ValueError(
                "Missing 'input_node_name' field in MultiPeriodStatNode data"
            )

        if input_node_name not in context:
            raise ValueError(f"Input node '{input_node_name}' not found in context")

        input_node = context[input_node_name]
        periods = data.get("periods", [])
        stat_func_name = data.get("stat_func_name", "stdev")

        if not periods:
            raise ValueError(
                "Missing or empty 'periods' field in MultiPeriodStatNode data"
            )

        # Map common statistical function names to their implementations
        stat_func_map: dict[str, StatFunc] = {
            "mean": statistics.mean,
            "stdev": statistics.stdev,
            "median": statistics.median,
            "variance": statistics.variance,
            "pstdev": statistics.pstdev,
            "pvariance": statistics.pvariance,
        }

        stat_func = stat_func_map.get(stat_func_name, statistics.stdev)
        if stat_func_name not in stat_func_map:
            logger.warning(
                f"Unknown stat_func_name '{stat_func_name}' for MultiPeriodStatNode '{name}'. "
                f"Using default statistics.stdev."
            )

        return MultiPeriodStatNode(
            name=name,
            input_node=input_node,
            periods=periods,
            stat_func=stat_func,
        )


class TwoPeriodAverageNode(Node):
    """Compute the average of an input node's values over two periods.

    Attributes:
        input_node (Node): Node supplying values.
        period1 (str): Identifier for the first period.
        period2 (str): Identifier for the second period.

    Examples:
        >>> from fin_statement_model.core.nodes import FinancialStatementItemNode, TwoPeriodAverageNode
        >>> data = {"Jan": 10.0, "Feb": 11.0}
        >>> price = FinancialStatementItemNode("price", data)
        >>> avg = TwoPeriodAverageNode("avg_price", input_node=price, period1="Jan", period2="Feb")
        >>> avg.calculate()
        10.5
    """

    def __init__(self, name: str, input_node: Node, period1: str, period2: str):
        """Create a TwoPeriodAverageNode.

        Args:
            name (str): Unique identifier for the node.
            input_node (Node): Node supplying values.
            period1 (str): Identifier for the first period.
            period2 (str): Identifier for the second period.

        Raises:
            TypeError: If `input_node` is not a Node or periods are not strings.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError(
                f"TwoPeriodAverageNode input_node must be a Node instance, got {type(input_node).__name__}"
            )
        if not isinstance(period1, str) or not isinstance(period2, str):
            raise TypeError("TwoPeriodAverageNode period1 and period2 must be strings.")

        self.input_node = input_node
        self.period1 = period1
        self.period2 = period2

    def calculate(self, period: Optional[str] = None) -> float:
        """Compute the average value for the two configured periods.

        Args:
            period (str | None): Ignored.

        Returns:
            float: (value1 + value2) / 2, or NaN if either value is non-numeric.

        Raises:
            CalculationError: On errors retrieving input node values.
        """
        try:
            val1 = self.input_node.calculate(self.period1)
            val2 = self.input_node.calculate(self.period2)

            # Ensure values are numeric and finite
            if not isinstance(val1, int | float) or not math.isfinite(val1):
                logger.warning(
                    f"TwoPeriodAverageNode '{self.name}': Value for period '{self.period1}' is non-numeric/non-finite ({val1}). Returning NaN."
                )
                return float("nan")
            if not isinstance(val2, int | float) or not math.isfinite(val2):
                logger.warning(
                    f"TwoPeriodAverageNode '{self.name}': Value for period '{self.period2}' is non-numeric/non-finite ({val2}). Returning NaN."
                )
                return float("nan")

            # Calculate the average
            return (float(val1) + float(val2)) / 2.0

        except Exception as e:
            # Wrap potential errors during input node calculation
            raise CalculationError(
                message=f"Failed to calculate two-period average for node '{self.name}'",
                node_id=self.name,
                period=f"{self.period1}_and_{self.period2}",  # Indicate context
                details={
                    "input_node": self.input_node.name,
                    "period1": self.period1,
                    "period2": self.period2,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Get names of nodes this average node depends on."""
        return [self.input_node.name]

    def to_dict(self) -> dict[str, Any]:
        """Serialize this node to a dictionary.

        Returns:
            dict[str, Any]: Serialized representation with type, name, and periods.
        """
        return {
            "type": "two_period_average",
            "name": self.name,
            "input_node_name": self.input_node.name,
            "period1": self.period1,
            "period2": self.period2,
        }

    @staticmethod
    def from_dict_with_context(
        data: dict[str, Any], context: dict[str, Node]
    ) -> "TwoPeriodAverageNode":
        """Recreate a TwoPeriodAverageNode from serialized data.

        Args:
            data (dict[str, Any]): Serialized node data.
            context (dict[str, Node]): Existing nodes for dependencies.

        Returns:
            TwoPeriodAverageNode: Reconstructed node.

        Raises:
            ValueError: If required fields are missing or invalid.
        """
        if data.get("type") != "two_period_average":
            raise ValueError(
                f"Invalid type for TwoPeriodAverageNode: {data.get('type')}"
            )

        name = data.get("name")
        if not name:
            raise ValueError("Missing 'name' field in TwoPeriodAverageNode data")

        input_node_name = data.get("input_node_name")
        if not input_node_name:
            raise ValueError(
                "Missing 'input_node_name' field in TwoPeriodAverageNode data"
            )

        if input_node_name not in context:
            raise ValueError(f"Input node '{input_node_name}' not found in context")

        input_node = context[input_node_name]
        period1 = data.get("period1")
        period2 = data.get("period2")

        if not period1:
            raise ValueError("Missing 'period1' field in TwoPeriodAverageNode data")
        if not period2:
            raise ValueError("Missing 'period2' field in TwoPeriodAverageNode data")

        return TwoPeriodAverageNode(
            name=name,
            input_node=input_node,
            period1=period1,
            period2=period2,
        )


__all__ = [
    "MultiPeriodStatNode",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]



================================================================================
File: fin_statement_model/extensions/__init__.py
================================================================================

"""Extensions package for fin_statement_model.

This package hosts optional in-repo extensions and third-party plugins discovered via entry-points under
`fin_statement_model.extensions`.
"""



================================================================================
File: fin_statement_model/extensions/llm/__init__.py
================================================================================

"""LLM extension subpackage for fin_statement_model.

Provides built-in OpenAI-based LLM client extension for generating and injecting content.
"""



================================================================================
File: fin_statement_model/extensions/llm/llm_client.py
================================================================================

"""LLM client module for OpenAI and backoff integration.

This module provides `LLMConfig` for client configuration and `LLMClient` for
asynchronous interactions with OpenAI's ChatCompletion API, including retry logic.
"""

import logging
import openai
from openai.api_resources.chat_completion import ChatCompletion
import backoff
from dataclasses import dataclass, field
from typing import Optional, Any, cast
from types import TracebackType

from fin_statement_model.config import cfg
from fin_statement_model.core.errors import FinancialModelError

logger = logging.getLogger(__name__)


@dataclass
class LLMConfig:
    """Configuration data for LLMClient.

    Attributes:
        api_key: API key for OpenAI authentication.
        model_name: Model to use (e.g., 'gpt-4o').
        temperature: Sampling temperature setting.
        max_tokens: Maximum tokens to generate.
        timeout: Request timeout in seconds.
        max_retries: Number of retries on failure.
    """

    api_key: str
    model_name: str = "gpt-4o"
    temperature: float = 0.7
    max_tokens: int = 1500
    timeout: int = field(default_factory=lambda: cfg("api.api_timeout", 30))
    max_retries: int = field(default_factory=lambda: cfg("api.api_retry_count", 3))
    # base_url is no longer needed as the openai library handles the endpoint configuration.


class LLMClientError(FinancialModelError):
    """Base exception for LLM client errors."""


class LLMTimeoutError(LLMClientError):
    """Exception for timeout errors."""


class LLMClient:
    """Asynchronous client for OpenAI ChatCompletion API with retry logic.

    Utilizes `LLMConfig` and supports retries on rate limits and timeouts.

    Methods:
        _make_api_call: Internal method for performing the API call with retry logic.
        get_completion: High-level method to obtain chat completions.
    """

    def __init__(self, config: Optional[LLMConfig] = None):
        """Initialize the async LLM client with configuration."""
        self.config = config or LLMConfig(api_key="")
        openai.api_key = self.config.api_key

    @backoff.on_exception(
        backoff.expo,
        (Exception, LLMTimeoutError),
        max_tries=3,
        giveup=lambda e: isinstance(e, LLMTimeoutError),
    )
    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=8)
    async def _make_api_call(self, messages: list[dict[str, str]]) -> dict[str, Any]:
        """Make the async API call to OpenAI with retry logic.

        Args:
            messages: List of message dicts for the ChatCompletion API

        Returns:
            Dict containing the API response

        Raises:
            LLMClientError: For any client-related errors, including timeout if applicable
        """
        try:
            logger.debug(
                f"Sending async request to OpenAI API with model {self.config.model_name}"
            )
            response = await ChatCompletion.acreate(
                model=self.config.model_name,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                timeout=self.config.timeout,
            )

            if not response.get("choices"):
                logger.error("No suggestions received from OpenAI API")
                raise LLMClientError("No suggestions received from API")

            return cast(dict[str, Any], response)
        except Exception as e:
            if "timeout" in str(e).lower():
                logger.exception("Async request timed out")
                raise LLMTimeoutError("Request timed out") from e
            logger.exception("OpenAI async API request failed")
            raise LLMClientError(f"API request failed: {e!s}") from e

    async def get_completion(self, messages: list[dict[str, str]]) -> str:
        """Get a completion result from the LLM using async API.

        Args:
            messages: List of message dictionaries for the ChatCompletion API

        Returns:
            str: The suggested completion from the LLM

        Raises:
            LLMClientError: For any client-related errors
        """
        try:
            logger.info("Requesting completion from OpenAI async API")
            response = await self._make_api_call(messages)
            completion = cast(str, response["choices"][0]["message"]["content"]).strip()
            logger.info("Successfully received completion")
            return completion
        except Exception as e:
            logger.exception("Error getting completion")
            raise LLMClientError(f"Failed to get completion: {e!s}") from e

    async def __aenter__(self) -> "LLMClient":
        """Enter the asynchronous context manager, returning the client."""
        return self

    async def __aexit__(
        self,
        exc_type: Optional[type[BaseException]],
        exc: Optional[BaseException],
        tb: Optional[TracebackType],
    ) -> None:
        """Exit the asynchronous context manager, performing cleanup."""
        # Ensure method has a body



================================================================================
File: fin_statement_model/forecasting/__init__.py
================================================================================

"""Forecasting sub-module for financial statement models.

This module provides comprehensive forecasting capabilities including:
- Multiple forecast methods (simple, curve, statistical, average, historical growth)
- Mutating and non-mutating forecast operations
- Extensible architecture for custom forecast methods
- Period management and validation utilities

Example:
    >>> from fin_statement_model.forecasting import StatementForecaster
    >>> forecaster = StatementForecaster(graph)
    >>>
    >>> # Mutating forecast - modifies the graph
    >>> forecaster.create_forecast(
    ...     forecast_periods=['2024', '2025'],
    ...     node_configs={
    ...         'revenue': {'method': 'simple', 'config': 0.05},
    ...         'costs': {'method': 'curve', 'config': [0.03, 0.04]}
    ...     }
    ... )
    >>>
    >>> # Non-mutating forecast - returns values without modifying graph
    >>> values = forecaster.forecast_value(
    ...     'revenue',
    ...     forecast_periods=['2024', '2025'],
    ...     forecast_config={'method': 'simple', 'config': 0.05}
    ... )
"""

# Main forecaster class
from .forecaster import StatementForecaster

# Forecast methods
from .methods import (
    ForecastMethod,
    BaseForecastMethod,
    SimpleForecastMethod,
    CurveForecastMethod,
    StatisticalForecastMethod,
    AverageForecastMethod,
    HistoricalGrowthForecastMethod,
)

# Registry and strategies
from .strategies import (
    ForecastMethodRegistry,
    forecast_registry,
    get_forecast_method,
    register_forecast_method,
)

# Utilities
from .period_manager import PeriodManager
from .validators import ForecastValidator

# Types
from .types import (
    ForecastMethodType,
    ForecastConfig,
    StatisticalConfig,
    ForecastResult,
)

# Error classes
from .errors import (
    ForecastingError,
    ForecastMethodError,
    ForecastConfigurationError,
    ForecastNodeError,
    ForecastResultError,
)

__all__ = [
    "AverageForecastMethod",
    "BaseForecastMethod",
    "CurveForecastMethod",
    "ForecastConfig",
    "ForecastConfigurationError",
    "ForecastMethod",
    "ForecastMethodError",
    "ForecastMethodRegistry",
    "ForecastMethodType",
    "ForecastNodeError",
    "ForecastResult",
    "ForecastResultError",
    "ForecastValidator",
    "ForecastingError",
    "HistoricalGrowthForecastMethod",
    "PeriodManager",
    "SimpleForecastMethod",
    "StatementForecaster",
    "StatisticalConfig",
    "StatisticalForecastMethod",
    "forecast_registry",
    "get_forecast_method",
    "register_forecast_method",
]



================================================================================
File: fin_statement_model/forecasting/errors.py
================================================================================

"""Custom Exception classes for the forecasting package.

These exceptions provide specific error information related to forecasting
operations, method configuration, and forecast result handling.
"""

from typing import Optional, Any
from fin_statement_model.core.errors import FinancialModelError

__all__ = [
    "ForecastConfigurationError",
    "ForecastMethodError",
    "ForecastNodeError",
    "ForecastResultError",
    "ForecastingError",
]


class ForecastingError(FinancialModelError):
    """Base exception for all forecasting-related errors."""


class ForecastMethodError(ForecastingError):
    """Exception raised for invalid or unsupported forecast methods.

    This includes unknown method names, invalid method parameters,
    or methods incompatible with the data type.
    """

    def __init__(
        self,
        message: str,
        method: Optional[str] = None,
        supported_methods: Optional[list[str]] = None,
        node_id: Optional[str] = None,
    ):
        """Initialize a ForecastMethodError.

        Args:
            message: The primary error message.
            method: Optional name of the invalid method.
            supported_methods: Optional list of supported methods.
            node_id: Optional ID of the node being forecasted.
        """
        self.method = method
        self.supported_methods = supported_methods or []
        self.node_id = node_id

        full_message = message
        if method:
            full_message = f"{message}: '{method}'"
        if node_id:
            full_message = f"{full_message} for node '{node_id}'"
        if supported_methods:
            full_message = (
                f"{full_message}. Supported methods: {', '.join(supported_methods)}"
            )

        super().__init__(full_message)


class ForecastConfigurationError(ForecastingError):
    """Exception raised for invalid forecast configuration.

    This includes missing required parameters, invalid parameter values,
    or incompatible configuration combinations.
    """

    def __init__(
        self,
        message: str,
        config: Optional[dict[str, Any]] = None,
        missing_params: Optional[list[str]] = None,
        invalid_params: Optional[dict[str, str]] = None,
    ):
        """Initialize a ForecastConfigurationError.

        Args:
            message: The primary error message.
            config: Optional configuration dictionary that caused the error.
            missing_params: Optional list of missing required parameters.
            invalid_params: Optional dict of parameter names to error descriptions.
        """
        self.config = config
        self.missing_params = missing_params or []
        self.invalid_params = invalid_params or {}

        details = []
        if missing_params:
            details.append(f"Missing parameters: {', '.join(missing_params)}")
        if invalid_params:
            param_errors = [f"{k}: {v}" for k, v in invalid_params.items()]
            details.append(f"Invalid parameters: {'; '.join(param_errors)}")

        full_message = message
        if details:
            full_message = f"{message} - {' | '.join(details)}"

        super().__init__(full_message)


class ForecastNodeError(ForecastingError):
    """Exception raised for node-related forecast errors.

    This includes nodes not found in the graph, nodes without historical data,
    or nodes that cannot be forecasted.
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str],
        available_nodes: Optional[list[str]] = None,
        reason: Optional[str] = None,
    ):
        """Initialize a ForecastNodeError.

        Args:
            message: The primary error message.
            node_id: The ID of the problematic node.
            available_nodes: Optional list of available node IDs.
            reason: Optional specific reason why the node cannot be forecasted.
        """
        self.node_id = node_id
        self.available_nodes = available_nodes or []
        self.reason = reason

        full_message = f"{message} for node '{node_id}'"
        if reason:
            full_message = f"{full_message}: {reason}"
        if available_nodes and len(available_nodes) < 10:  # Only show if list is small
            full_message = (
                f"{full_message}. Available nodes: {', '.join(available_nodes)}"
            )

        super().__init__(full_message)


class ForecastResultError(ForecastingError):
    """Exception raised for forecast result access or manipulation errors.

    This includes accessing results for non-existent periods, invalid result
    formats, or result validation failures.
    """

    def __init__(
        self,
        message: str,
        period: Optional[str] = None,
        available_periods: Optional[list[str]] = None,
        node_id: Optional[str] = None,
    ):
        """Initialize a ForecastResultError.

        Args:
            message: The primary error message.
            period: Optional period that caused the error.
            available_periods: Optional list of available periods.
            node_id: Optional ID of the node whose results are being accessed.
        """
        self.period = period
        self.available_periods = available_periods or []
        self.node_id = node_id

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if period:
            context.append(f"period '{period}'")

        full_message = message
        if context:
            full_message = f"{message} for {' and '.join(context)}"
        if available_periods and len(available_periods) < 10:
            full_message = (
                f"{full_message}. Available periods: {', '.join(available_periods)}"
            )

        super().__init__(full_message)



================================================================================
File: fin_statement_model/forecasting/forecaster.py
================================================================================

"""Forecasting operations dedicated to statement-level financial graphs.

This module provides the StatementForecaster class, which handles forecasting
operations for financial statement graphs. It offers both mutating operations
(that modify the graph) and non-mutating operations (that return forecast values
without changing the graph state).
"""

import logging
from typing import Any, Optional, cast
import numpy as np

# Core imports
from fin_statement_model.config import cfg
from fin_statement_model.core.nodes import Node
from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.forecasting.errors import (
    ForecastNodeError,
)

# Forecasting module imports
from .period_manager import PeriodManager
from .validators import ForecastValidator
from .strategies import get_forecast_method
from .types import ForecastConfig, ForecastResult
from .methods import BaseForecastMethod

logger = logging.getLogger(__name__)


class StatementForecaster:
    """Handles forecasting operations specifically for a FinancialStatementGraph.

    This class provides two main approaches to forecasting:

    1. **Mutating operations** (`create_forecast`): Modifies the graph by adding
       forecast periods and updating node values directly. This is useful when
       you want to extend the graph with forecast data for further analysis.

    2. **Non-mutating operations** (`forecast_value`): Returns forecast values
       without modifying the graph state. This is useful for what-if scenarios
       or when you need forecast values without altering the original data.

    The forecaster supports multiple forecasting methods:
    - simple: Simple growth rate
    - curve: Variable growth rates per period
    - statistical: Random sampling from distributions
    - average: Average of historical values
    - historical_growth: Based on historical growth patterns
    """

    def __init__(self, fsg: Any) -> None:
        """Initialize the forecaster.

        Args:
            fsg: The FinancialStatementGraph instance this forecaster will operate on.
        """
        self.fsg = fsg

    def create_forecast(
        self,
        forecast_periods: list[str],
        node_configs: Optional[dict[str, dict[str, Any]]] = None,
        historical_periods: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> None:
        """Create forecasts for financial statement items on the graph.

        **IMPORTANT**: This method MUTATES the graph by:
        - Adding new periods to the graph if they don't exist
        - Updating node values with forecast data
        - Clearing node caches after updates

        Use `forecast_value` instead if you need forecast values without
        modifying the graph.

        Args:
            forecast_periods: List of future periods to forecast.
            node_configs: Mapping of node names to their forecast configurations.
                Each config should contain:
                - 'method': Forecasting method ('simple', 'curve', 'statistical',
                           'average', 'historical_growth')
                - 'config': Method-specific parameters (growth rate, distribution, etc.)
            historical_periods: Optional list of historical periods to use as base.
                If not provided, will be inferred from the graph's existing periods.
            **kwargs: Additional arguments passed to the forecasting logic:
                add_missing_periods (bool): Whether to add missing forecast periods to the graph.

        Returns:
            None (modifies the graph in-place)

        Raises:
            ForecastNodeError: If no historical periods found, no forecast periods provided,
                              or invalid forecasting method/configuration.
        """
        logger.info(
            f"StatementForecaster: Creating forecast for periods {forecast_periods}"
        )
        try:
            # Use PeriodManager to infer historical periods
            historical_periods = PeriodManager.infer_historical_periods(
                self.fsg, forecast_periods, historical_periods
            )

            # Validate inputs using ForecastValidator
            ForecastValidator.validate_forecast_inputs(
                historical_periods, forecast_periods, node_configs
            )

            # Ensure forecast periods exist in the graph (override via add_missing_periods)
            add_missing = kwargs.get(
                "add_missing_periods", cfg("forecasting.add_missing_periods")
            )
            PeriodManager.ensure_periods_exist(
                self.fsg, forecast_periods, add_missing=add_missing
            )

            if node_configs is None:
                node_configs = {}

            for node_name, config in node_configs.items():
                node = self.fsg.get_node(node_name)
                if node is None:
                    raise ForecastNodeError(
                        f"Node {node_name} not found in graph",
                        node_id=node_name,
                        available_nodes=list(self.fsg.nodes.keys()),
                    )

                # Validate node can be forecasted
                forecast_config = ForecastValidator.validate_forecast_config(config)
                ForecastValidator.validate_node_for_forecast(
                    node, forecast_config.method
                )

                self._forecast_node(
                    node, historical_periods, forecast_periods, forecast_config
                )

            logger.info(
                f"Created forecast for {len(forecast_periods)} periods and {len(node_configs)} nodes"
            )
        except Exception as e:
            logger.error(f"Error creating forecast: {e}", exc_info=True)
            raise ForecastNodeError(
                f"Error creating forecast: {e}", node_id=None, reason=str(e)
            )

    def _forecast_node(
        self,
        node: Node,
        historical_periods: list[str],
        forecast_periods: list[str],
        forecast_config: ForecastConfig,
        **kwargs: Any,
    ) -> None:
        """Calculate forecast values and update the original node.

        **IMPORTANT**: This is an internal MUTATING method that:
        - Creates a temporary forecast node for calculations
        - Updates the original node's values dictionary with forecast results
        - Clears the original node's cache after updates

        This method should not be called directly. Use `create_forecast` for
        mutating operations or `forecast_value` for non-mutating operations.

        Args:
            node: The graph node to forecast. Must have a 'values' dictionary.
            historical_periods: List of historical periods for base values.
            forecast_periods: List of periods for which to calculate forecasts.
            forecast_config: Validated forecast configuration.
            **kwargs: Additional arguments passed to growth logic.

        Returns:
            None (modifies the node in-place)

        Raises:
            ForecastNodeError: If no historical periods provided or invalid method.
        """
        logger.debug(
            f"StatementForecaster: Forecasting node {node.name} using method {forecast_config.method}"
        )

        # Determine base period using PeriodManager
        base_period = PeriodManager.determine_base_period(node, historical_periods)

        # Get the forecast method from registry
        method = get_forecast_method(forecast_config.method)

        # Get normalized parameters for NodeFactory
        # All built-in methods extend BaseForecastMethod which has get_forecast_params
        base_method = cast(BaseForecastMethod, method)
        params = base_method.get_forecast_params(
            forecast_config.config, forecast_periods
        )

        # Create a temporary node to perform calculations
        tmp_node = NodeFactory.create_forecast_node(
            name=f"{node.name}_forecast_temp",
            base_node=node,
            base_period=base_period,
            forecast_periods=forecast_periods,
            forecast_type=params["forecast_type"],
            growth_params=params["growth_params"],
        )

        # Ensure the original node has a values dictionary
        if not hasattr(node, "values") or not isinstance(node.values, dict):
            logger.error(
                f"Cannot store forecast for node {node.name}: node does not have a 'values' dictionary."
            )
            return  # Cannot proceed

        # Calculate and update original node's values
        for period in forecast_periods:
            try:
                val = tmp_node.calculate(period)
                # Default fallback for bad forecasts, overrideable via bad_forecast_value kwarg
                bad_value = kwargs.get(
                    "bad_forecast_value", cfg("forecasting.default_bad_forecast_value")
                )
                if np.isnan(val) or np.isinf(val):
                    logger.warning(
                        f"Bad forecast {val} for {node.name}@{period}; defaulting to {bad_value}"
                    )
                    val = bad_value
                # Clamp negative values if disallowed
                allow_neg = kwargs.get(
                    "allow_negative_forecasts",
                    cfg("forecasting.allow_negative_forecasts"),
                )
                if not allow_neg and val < 0:
                    # Ensure bad_value is non-negative when clamping negative values
                    if bad_value < 0:
                        logger.warning(
                            f"bad_forecast_value ({bad_value}) is negative but allow_negative_forecasts is False. Using 0.0 instead."
                        )
                        bad_value = 0.0
                    logger.warning(
                        f"Negative forecast {val} for {node.name}@{period}; clamping to {bad_value}"
                    )
                    val = bad_value
                node.values[period] = float(val)  # Update the original node
            except Exception as e:
                logger.error(
                    f"Error forecasting {node.name}@{period}: {e}", exc_info=True
                )
                node.values[period] = bad_value  # Set default on error

        # Clear cache of the original node as its values have changed
        if hasattr(node, "clear_cache") and callable(node.clear_cache):
            node.clear_cache()
            logger.debug(f"Cleared cache for node {node.name} after forecast update.")

    def forecast_value(
        self,
        node_name: str,
        forecast_periods: list[str],
        base_period: Optional[str] = None,
        forecast_config: Optional[dict[str, Any]] = None,
        **kwargs: Any,
    ) -> dict[str, float]:
        """Forecast and return values for a node without mutating the graph.

        **IMPORTANT**: This is a NON-MUTATING method that:
        - Does NOT add periods to the graph
        - Does NOT modify any node values
        - Does NOT affect the graph state in any way
        - Returns forecast values as a separate dictionary

        This method is ideal for:
        - What-if analysis
        - Comparing different forecast scenarios
        - Getting forecast values without committing them to the graph
        - API responses where you don't want to modify server state

        Args:
            node_name: Name of the node to forecast.
            forecast_periods: List of future periods to forecast.
            base_period: Optional base period to use for forecasting.
                        If omitted, will be inferred from the node's historical data.
            forecast_config: Forecast configuration dict with:
                - 'method': Forecasting method ('simple', 'curve', 'statistical',
                           'average', 'historical_growth')
                - 'config': Method-specific parameters
                If not provided, uses global forecasting defaults.
            **kwargs: Additional arguments passed to the internal forecasting logic.
                bad_forecast_value (float): Default to use for NaN/Inf or errors (overrides config)
                allow_negative_forecasts (bool): Whether to allow negative forecast values (overrides config)

        Returns:
            A dictionary mapping forecast periods to their calculated values.
            Example: {'2024': 1050.0, '2025': 1102.5}

        Raises:
            ForecastNodeError: If node not found, no historical periods available,
                              or invalid forecast configuration.
        """
        # Optional override for bad forecast fallback
        bad_value = kwargs.get(
            "bad_forecast_value", cfg("forecasting.default_bad_forecast_value")
        )
        # Locate the node
        node = self.fsg.get_node(node_name)
        if node is None:
            raise ForecastNodeError(
                f"Node {node_name} not found in graph",
                node_id=node_name,
                available_nodes=list(self.fsg.nodes.keys()),
            )

        # Determine historical periods
        if base_period:
            historical_periods = [base_period]
        else:
            historical_periods = PeriodManager.infer_historical_periods(
                self.fsg, forecast_periods
            )

        # Validate inputs
        ForecastValidator.validate_forecast_inputs(historical_periods, forecast_periods)

        # Set default config if not provided, using global forecasting defaults
        if forecast_config is None:
            default_method = cfg("forecasting.default_method")
            # Use method-appropriate default config
            if default_method == "simple":
                forecast_config = {
                    "method": default_method,
                    "config": cfg("forecasting.default_growth_rate"),
                }
            else:
                # For other methods, use empty config and let the method handle defaults
                forecast_config = {"method": default_method, "config": {}}

        # Validate and create ForecastConfig
        validated_config = ForecastValidator.validate_forecast_config(forecast_config)
        ForecastValidator.validate_node_for_forecast(node, validated_config.method)

        # Determine base period
        calc_base_period = PeriodManager.determine_base_period(
            node, historical_periods, base_period
        )

        # Get the forecast method from registry
        method = get_forecast_method(validated_config.method)

        # Get normalized parameters for NodeFactory
        # All built-in methods extend BaseForecastMethod which has get_forecast_params
        base_method = cast(BaseForecastMethod, method)
        params = base_method.get_forecast_params(
            validated_config.config, forecast_periods
        )

        # Create a temporary forecast node (DO NOT add to graph)
        try:
            temp_forecast_node = NodeFactory.create_forecast_node(
                name=f"{node_name}_temp_forecast",
                base_node=node,
                base_period=calc_base_period,
                forecast_periods=forecast_periods,
                forecast_type=params["forecast_type"],
                growth_params=params["growth_params"],
            )
        except Exception as e:
            logger.error(
                f"Failed to create temporary forecast node for '{node_name}': {e}",
                exc_info=True,
            )
            raise ForecastNodeError(
                f"Could not create temporary forecast node: {e}",
                node_id=node_name,
                reason=str(e),
            )

        # Calculate results using the temporary node
        results: dict[str, float] = {}
        for period in forecast_periods:
            try:
                value = temp_forecast_node.calculate(period)
                # Handle potential NaN/Inf results from calculation
                results[period] = bad_value if not np.isfinite(value) else float(value)
                # Clamp negative values if disallowed
                allow_neg = kwargs.get(
                    "allow_negative_forecasts",
                    cfg("forecasting.allow_negative_forecasts"),
                )
                if not allow_neg and results[period] < 0:
                    logger.warning(
                        f"Negative forecast {results[period]} for {node_name}@{period}; clamping to {bad_value}"
                    )
                    results[period] = bad_value
            except Exception as e:
                logger.warning(
                    f"Error calculating temporary forecast for {node_name}@{period}: {e}. Returning {bad_value}"
                )
                results[period] = bad_value

        # Validate results before returning
        ForecastValidator.validate_forecast_result(results, forecast_periods, node_name)

        return results

    def forecast_multiple(
        self,
        node_names: list[str],
        forecast_periods: list[str],
        forecast_configs: Optional[dict[str, dict[str, Any]]] = None,
        base_period: Optional[str] = None,
        **kwargs: Any,
    ) -> dict[str, ForecastResult]:
        """Forecast multiple nodes without mutating the graph.

        This is a convenience method that forecasts multiple nodes at once
        and returns structured results.

        Args:
            node_names: List of node names to forecast.
            forecast_periods: List of future periods to forecast.
            forecast_configs: Optional mapping of node names to their forecast configs.
                             If not provided, uses simple method with 0% growth for all.
            base_period: Optional base period to use for all nodes.
            **kwargs: Additional arguments passed to forecast_value.

        Returns:
            Dictionary mapping node names to ForecastResult objects.

        Example:
            >>> results = forecaster.forecast_multiple(
            ...     ['revenue', 'costs'],
            ...     ['2024', '2025'],
            ...     {'revenue': {'method': 'simple', 'config': 0.05}}
            ... )
            >>> print(results['revenue'].get_value('2024'))
        """
        # Determine error propagation strategy (override via continue_on_error kwarg)
        continue_on_err = kwargs.get(
            "continue_on_error", cfg("forecasting.continue_on_error")
        )
        results: dict[str, ForecastResult] = {}
        configs = forecast_configs or {}

        for node_name in node_names:
            try:
                # Get config for this node or use default
                node_config = configs.get(node_name)

                # Forecast the node
                values = self.forecast_value(
                    node_name, forecast_periods, base_period, node_config, **kwargs
                )

                # Determine actual base period used
                node = self.fsg.get_node(node_name)
                historical_periods = PeriodManager.infer_historical_periods(
                    self.fsg, forecast_periods
                )
                actual_base_period = PeriodManager.determine_base_period(
                    node, historical_periods, base_period
                )

                # Create ForecastResult
                default_method = cfg("forecasting.default_method")
                config = ForecastValidator.validate_forecast_config(
                    node_config
                    or {
                        "method": default_method,
                        "config": cfg("forecasting.default_growth_rate"),
                    }
                )

                results[node_name] = ForecastResult(
                    node_name=node_name,
                    periods=forecast_periods,
                    values=values,
                    method=config.method,
                    base_period=actual_base_period,
                )
            except Exception as e:
                logger.exception(f"Error forecasting node {node_name}")
                if continue_on_err:
                    continue
                raise ForecastNodeError(
                    f"Error forecasting node {node_name}: {e}",
                    node_id=node_name,
                    reason=str(e),
                )

        return results

    def forecast_node(
        self,
        node_name: str,
        config: ForecastConfig,
        historical_periods: Optional[int] = None,
    ) -> ForecastResult:
        """Forecast values for a specific node.

        Args:
            node_name: Name of the node to forecast
            config: Forecast configuration
            historical_periods: Number of historical periods to use

        Returns:
            ForecastResult containing the forecasted values

        Raises:
            ForecastNodeError: If node not found in graph
        """
        if node_name not in self.fsg.nodes:
            raise ForecastNodeError(
                f"Node {node_name} not found in graph",
                node_id=node_name,
                available_nodes=list(self.fsg.nodes.keys()),
            )
        # TODO: implement detailed forecasting logic for a single node
        raise NotImplementedError("forecast_node is not implemented")

    def forecast_all(
        self,
        default_config: ForecastConfig,
        node_configs: Optional[dict[str, ForecastConfig]] = None,
    ) -> dict[str, ForecastResult]:
        """Forecast all forecastable nodes in the graph.

        Args:
            default_config: Default configuration for nodes without specific config
            node_configs: Optional node-specific configurations

        Returns:
            Dictionary mapping node names to forecast results

        Raises:
            ForecastNodeError: If node not found in graph
        """
        node_configs = node_configs or {}
        results = {}

        for node_name, node in self.fsg.nodes.items():
            if self._is_forecastable(node):
                config = node_configs.get(node_name, default_config)
                try:
                    results[node_name] = self.forecast_node(node_name, config)
                except Exception as e:
                    logger.warning(f"Failed to forecast {node_name}: {e}")
                    continue

        return results

    def _is_forecastable(self, node: Node) -> bool:
        """Determine if a node is forecastable (has a 'values' dictionary)."""
        return hasattr(node, "values") and isinstance(node.values, dict)

    def create_forecast_node(
        self,
        base_node_name: str,
        forecast_name: str,
        config: ForecastConfig,
    ) -> str:
        """Create a new forecast node based on an existing node.

        Args:
            base_node_name: Name of the node to base forecast on
            forecast_name: Name for the new forecast node
            config: Forecast configuration

        Returns:
            Name of the created forecast node

        Raises:
            ForecastNodeError: If base node not found or forecast node creation fails
        """
        if base_node_name not in self.fsg.nodes:
            raise ForecastNodeError(
                f"Node {base_node_name} not found in graph",
                node_id=base_node_name,
                available_nodes=list(self.fsg.nodes.keys()),
            )

        base_node = self.fsg.nodes[base_node_name]

        # Create forecast node
        try:
            forecast_node = NodeFactory.create_forecast_node(  # type: ignore[call-arg]
                name=forecast_name,
                base_node=base_node,
                forecast_config=config,
            )
            self.fsg.add_node(forecast_node)
            return forecast_name
        except Exception as e:
            raise ForecastNodeError(
                f"Could not create temporary forecast node: {e}",
                node_id=forecast_name,
                reason=str(e),
            ) from e



================================================================================
File: fin_statement_model/forecasting/methods/__init__.py
================================================================================

"""Forecast method implementations.

This module contains all the built-in forecast methods available in the library.
"""

from .base import ForecastMethod, BaseForecastMethod
from .simple import SimpleForecastMethod
from .curve import CurveForecastMethod
from .statistical import StatisticalForecastMethod
from .average import AverageForecastMethod
from .historical_growth import HistoricalGrowthForecastMethod

__all__ = [
    "AverageForecastMethod",
    "BaseForecastMethod",
    "CurveForecastMethod",
    "ForecastMethod",
    "HistoricalGrowthForecastMethod",
    "SimpleForecastMethod",
    "StatisticalForecastMethod",
]



================================================================================
File: fin_statement_model/forecasting/methods/average.py
================================================================================

"""Average forecasting method based on historical values.

This method forecasts future values as the average of historical values.
"""

import logging
from typing import Any, Optional
import numpy as np

from fin_statement_model.core.nodes import Node
from .base import BaseForecastMethod

logger = logging.getLogger(__name__)


class AverageForecastMethod(BaseForecastMethod):
    """Historical average forecasting.

    This method calculates forecast values as the average of historical
    values. Useful for stable metrics or when expecting mean reversion.

    Configuration:
        - Not required (pass 0 or None)
        - The method will automatically use all available historical data

    Example:
        >>> method = AverageForecastMethod()
        >>> params = method.get_forecast_params(None, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'average', 'growth_params': None}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "average"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "average"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for average method.

        Args:
            config: Not used for average method, can be None or 0.

        Note:
            Average method doesn't require configuration as it uses
            historical data automatically.
        """
        # Average method doesn't need specific configuration
        # Accept None, 0, or any placeholder value

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Not used for average method.
            forecast_periods: List of periods to forecast (not used).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
            For average method, growth_params is None.
        """
        return {
            "forecast_type": self.internal_type,
            "growth_params": None,  # Average method doesn't use growth params
        }

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for average calculation.

        Args:
            node: The node to extract historical data from.
            historical_periods: List of historical periods.

        Returns:
            List of valid historical values.

        Raises:
            ValueError: If no valid historical data is available.
        """
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise ValueError(
                f"Node {node.name} cannot be calculated for average method"
            )

        if not hasattr(node, "values") or not isinstance(node.values, dict):
            raise ValueError(
                f"Node {node.name} does not have values dictionary for average method"
            )

        # Extract historical values
        historical_values = []
        for period in historical_periods:
            if period in node.values:
                try:
                    value = node.calculate(period)
                    if (
                        value is not None
                        and not np.isnan(value)
                        and not np.isinf(value)
                    ):
                        historical_values.append(float(value))
                except Exception as e:
                    # Log the exception and skip this period
                    logger.debug(
                        f"Skipping period {period} for node {node.name} in average calculation: {e}"
                    )
                    continue

        if not historical_values:
            raise ValueError(
                f"No valid historical data available for node {node.name} to compute average"
            )

        return historical_values



================================================================================
File: fin_statement_model/forecasting/methods/base.py
================================================================================

"""Base protocol and abstract class for forecast methods.

This module defines the interface that all forecast methods must implement.
"""

from typing import Protocol, Any, Optional, runtime_checkable
from abc import ABC, abstractmethod

from fin_statement_model.core.nodes import Node


@runtime_checkable
class ForecastMethod(Protocol):
    """Protocol that all forecast methods must implement."""

    @property
    def name(self) -> str:
        """Return the method name."""
        ...

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for this method.

        Args:
            config: The method-specific configuration to validate.

        Raises:
            ValueError: If configuration is invalid.
        """
        ...

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: The method-specific configuration.
            forecast_periods: List of periods to forecast.

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
        """
        ...

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for methods that need it.

        Args:
            node: The node to extract historical data from.
            historical_periods: List of historical periods.

        Returns:
            List of historical values or None if not needed.
        """
        ...


class BaseForecastMethod(ABC):
    """Abstract base class for forecast methods.

    Provides common functionality and enforces the interface.
    """

    @property
    @abstractmethod
    def name(self) -> str:
        """Return the method name."""

    @property
    @abstractmethod
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""

    @abstractmethod
    def validate_config(self, config: Any) -> None:
        """Validate the configuration for this method."""

    @abstractmethod
    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory."""

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for methods that need it.

        Default implementation returns None (not needed).
        Override in methods that require historical data.
        """
        return None

    def get_forecast_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Get complete forecast parameters.

        This is a convenience method that validates and normalizes in one call.

        Args:
            config: The method-specific configuration.
            forecast_periods: List of periods to forecast.

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.

        Raises:
            ValueError: If configuration is invalid.
        """
        self.validate_config(config)
        return self.normalize_params(config, forecast_periods)



================================================================================
File: fin_statement_model/forecasting/methods/curve.py
================================================================================

"""Curve forecasting method with variable growth rates.

This method applies different growth rates for each forecast period.
"""

from typing import Any

from .base import BaseForecastMethod


class CurveForecastMethod(BaseForecastMethod):
    """Variable growth rate forecasting.

    This method applies different growth rates for each forecast period,
    allowing for non-linear growth patterns.

    Configuration:
        - List of numeric values: One growth rate per forecast period
        - Single numeric value: Will be expanded to match forecast periods

    Example:
        >>> method = CurveForecastMethod()
        >>> params = method.get_forecast_params([0.05, 0.04, 0.03], ['2024', '2025', '2026'])
        >>> # Returns: {'forecast_type': 'curve', 'growth_params': [0.05, 0.04, 0.03]}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "curve"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "curve"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for curve method.

        Args:
            config: Should be a numeric value or a list of numeric values.

        Raises:
            ValueError: If config is empty list.
            TypeError: If config is not numeric or list of numerics.
        """
        if isinstance(config, list):
            if not config:
                raise ValueError("Curve method: empty list provided")
            for i, value in enumerate(config):
                if not isinstance(value, int | float):
                    raise TypeError(
                        f"Curve method: non-numeric value at index {i}: {type(value)}"
                    )
        elif not isinstance(config, int | float):
            raise TypeError(
                f"Curve method requires numeric or list of numeric values, got {type(config)}"
            )

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Growth rates (single value or list).
            forecast_periods: List of periods to forecast.

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.

        Raises:
            ValueError: If list length doesn't match forecast periods.
        """
        if not isinstance(config, list):
            # Single value - expand to match forecast periods
            growth_rates = [float(config)] * len(forecast_periods)
        else:
            # List of values - must match forecast periods length
            if len(config) != len(forecast_periods):
                raise ValueError(
                    f"Curve method: growth rate list length ({len(config)}) "
                    f"must match forecast periods ({len(forecast_periods)})"
                )
            growth_rates = [float(x) for x in config]

        return {"forecast_type": self.internal_type, "growth_params": growth_rates}



================================================================================
File: fin_statement_model/forecasting/methods/historical_growth.py
================================================================================

"""Historical growth forecasting method based on past growth patterns.

This method calculates future values based on the average historical growth rate.
"""

import logging
from typing import Any, Optional
import numpy as np

from fin_statement_model.core.nodes import Node
from .base import BaseForecastMethod
from fin_statement_model.config.helpers import cfg

logger = logging.getLogger(__name__)


class HistoricalGrowthForecastMethod(BaseForecastMethod):
    """Historical growth pattern forecasting.

    This method calculates the average historical growth rate and applies
    it to forecast future values. It's useful when past growth patterns
    are expected to continue.

    Configuration:
        - Not required (pass 0 or None)
        - The method will automatically calculate growth from historical data

    Example:
        >>> method = HistoricalGrowthForecastMethod()
        >>> params = method.get_forecast_params(None, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'historical_growth', 'growth_params': None}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "historical_growth"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "historical_growth"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for historical growth method.

        Args:
            config: Not used for historical growth method, can be None or 0.

        Note:
            Historical growth method doesn't require configuration as it
            calculates growth from historical data automatically.
        """
        # Historical growth method doesn't need specific configuration
        # Accept None, 0, or any placeholder value

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Not used for historical growth method.
            forecast_periods: List of periods to forecast (not used).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
            For historical growth method, growth_params is None.
        """
        return {
            "forecast_type": self.internal_type,
            "growth_params": None,  # Historical growth method calculates internally
        }

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for growth calculation.

        Args:
            node: The node to extract historical data from.
            historical_periods: List of historical periods.

        Returns:
            List of valid historical values (at least 2 needed for growth).

        Raises:
            ValueError: If insufficient historical data is available.
        """
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise ValueError(
                f"Node {node.name} cannot be calculated for historical growth method"
            )

        if not hasattr(node, "values") or not isinstance(node.values, dict):
            raise ValueError(
                f"Node {node.name} does not have values dictionary for historical growth method"
            )

        # Extract historical values in chronological order
        historical_values = []
        for period in historical_periods:
            if period in node.values:
                try:
                    value = node.calculate(period)
                    if (
                        value is not None
                        and not np.isnan(value)
                        and not np.isinf(value)
                    ):
                        historical_values.append(float(value))
                except Exception as e:
                    # Log the exception and skip this period
                    logger.debug(
                        f"Skipping period {period} for node {node.name} in historical growth calculation: {e}"
                    )
                    continue

        min_periods = cfg("forecasting.min_historical_periods")
        if len(historical_values) < min_periods:
            raise ValueError(
                f"Need at least {min_periods} historical data points for node {node.name} "
                f"to compute growth rate, found {len(historical_values)}"
            )

        return historical_values

    def calculate_average_growth_rate(self, historical_values: list[float]) -> float:
        """Calculate the average growth rate from historical values.

        Args:
            historical_values: List of historical values in chronological order.

        Returns:
            Average growth rate.

        Note:
            This is a helper method that can be used by the forecast node
            implementation to calculate the growth rate.
        """
        # Calculate period-over-period growth rates
        if len(historical_values) < 2:
            return 0.0

        growth_rates: list[float] = []
        for i in range(1, len(historical_values)):
            previous_value = historical_values[i - 1]
            if previous_value != 0:
                growth_rates.append(
                    (historical_values[i] - previous_value) / previous_value
                )

        if not growth_rates:
            return 0.0

        # Determine aggregation method: 'mean' or 'median'
        agg_method = cfg("forecasting.historical_growth_aggregation")
        if agg_method == "median":
            try:
                return float(np.median(growth_rates))
            except (ValueError, TypeError) as e:
                logger.warning(
                    f"Failed to calculate median growth rate, falling back to mean: {e}"
                )
                return float(np.mean(growth_rates))
        # Default to mean
        return float(np.mean(growth_rates))



================================================================================
File: fin_statement_model/forecasting/methods/simple.py
================================================================================

"""Simple growth rate forecasting method.

This method applies a constant growth rate to forecast future values.
"""

from typing import Any

from .base import BaseForecastMethod


class SimpleForecastMethod(BaseForecastMethod):
    """Simple growth rate forecasting.

    This method applies a constant growth rate to the base value
    for all forecast periods.

    Configuration:
        - Single numeric value: The growth rate (e.g., 0.05 for 5% growth)
        - List with single value: Will use the first value

    Example:
        >>> method = SimpleForecastMethod()
        >>> params = method.get_forecast_params(0.05, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'simple', 'growth_params': 0.05}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "simple"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "simple"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for simple method.

        Args:
            config: Should be a numeric value or a list containing a numeric value.

        Raises:
            ValueError: If config is empty list.
            TypeError: If config is not numeric or a list with numeric value.
        """
        if isinstance(config, list):
            if not config:
                raise ValueError("Simple method: empty list provided")
            if not isinstance(config[0], int | float):
                raise TypeError(
                    f"Simple method requires numeric growth rate, got {type(config[0])}"
                )
        elif not isinstance(config, int | float):
            raise TypeError(
                f"Simple method requires numeric growth rate, got {type(config)}"
            )

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: The growth rate (numeric or list with numeric).
            forecast_periods: List of periods to forecast (not used for simple method).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
        """
        # Handle list input - take first value
        growth_rate = float(config[0]) if isinstance(config, list) else float(config)

        return {"forecast_type": self.internal_type, "growth_params": growth_rate}



================================================================================
File: fin_statement_model/forecasting/methods/statistical.py
================================================================================

"""Statistical forecasting method using random sampling.

This method generates forecast values by sampling from statistical distributions.
"""

from typing import Any
import numpy as np
from pydantic import ValidationError
from fin_statement_model.config.helpers import cfg

from .base import BaseForecastMethod
from fin_statement_model.forecasting.types import StatisticalConfig


class StatisticalForecastMethod(BaseForecastMethod):
    """Statistical forecasting using random distributions.

    This method generates forecast values by sampling from specified
    statistical distributions, useful for Monte Carlo simulations
    and uncertainty analysis.

    Configuration:
        Dict with:
        - 'distribution': 'normal' or 'uniform'
        - 'params': Distribution-specific parameters
            - For 'normal': {'mean': float, 'std': float}
            - For 'uniform': {'low': float, 'high': float}

    Example:
        >>> method = StatisticalForecastMethod()
        >>> config = {
        ...     'distribution': 'normal',
        ...     'params': {'mean': 0.05, 'std': 0.02}
        ... }
        >>> params = method.get_forecast_params(config, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'statistical', 'growth_params': <callable>}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "statistical"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "statistical"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for statistical method.

        Args:
            config: Should be a dict with 'distribution' and 'params' keys.

        Raises:
            TypeError: If config is invalid.
        """
        if not isinstance(config, dict):
            raise TypeError(
                f"Statistical method requires dict configuration, got {type(config)}"
            )

        if "distribution" not in config:
            raise ValueError("Statistical method requires 'distribution' key")

        if "params" not in config:
            raise ValueError("Statistical method requires 'params' key")

        # Validate using StatisticalConfig model (raises ValidationError or ForecastConfigurationError)
        try:
            StatisticalConfig(
                distribution=config["distribution"], params=config["params"]
            )
        except (ValueError, TypeError, ValidationError) as e:
            raise ValueError(f"Invalid statistical configuration: {e}") from e

    def normalize_params(
        self, config: Any, forecast_periods: list[str]
    ) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Statistical distribution configuration.
            forecast_periods: List of periods to forecast (not used).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
            The 'growth_params' value is a callable that generates random values.
        """
        # Create validated config
        stat_config = StatisticalConfig(
            distribution=config["distribution"], params=config["params"]
        )

        # Seed RNG if configured
        seed = cfg("forecasting.random_seed")
        if seed is not None:
            rng = np.random.RandomState(seed)
        else:
            rng = np.random.RandomState()

        # Create generator function based on distribution
        def generator() -> float:
            """Generate a random growth rate from the specified distribution."""
            if stat_config.distribution == "normal":
                return float(
                    rng.normal(stat_config.params["mean"], stat_config.params["std"])
                )
            elif stat_config.distribution == "uniform":
                return float(
                    rng.uniform(stat_config.params["low"], stat_config.params["high"])
                )
            else:
                # This shouldn't happen due to validation, but just in case
                raise ValueError(
                    f"Unsupported distribution: {stat_config.distribution}"
                )

        return {"forecast_type": self.internal_type, "growth_params": generator}



================================================================================
File: fin_statement_model/forecasting/period_manager.py
================================================================================

"""Period inference and management utilities for forecasting.

This module handles the logic for determining historical periods,
base periods, and managing period-related operations.
"""

import logging
from typing import Any, Optional

from fin_statement_model.core.nodes import Node
from fin_statement_model.config.helpers import cfg

logger = logging.getLogger(__name__)


class PeriodManager:
    """Handles period inference and management for forecasting.

    This class provides utilities for:
    - Inferring historical periods from graph state
    - Determining base periods for forecasting
    - Validating period sequences
    - Managing period transitions
    """

    @staticmethod
    def infer_historical_periods(  # type: ignore
        graph: Any,
        forecast_periods: list[str],
        provided_periods: Optional[list[str]] = None,
    ) -> list[str]:
        """Infer historical periods from graph state.

        Args:
            graph: The financial statement graph instance.
            forecast_periods: List of periods to forecast.
            provided_periods: Optional explicitly provided historical periods.

        Returns:
            List of historical periods.

        Raises:
            ValueError: If historical periods cannot be determined.
        """
        # If explicitly provided, use them
        if provided_periods is not None:
            logger.debug(
                f"Using explicitly provided historical periods: {provided_periods}"
            )
            return provided_periods

        # Infer from graph periods and forecast periods
        if not hasattr(graph, "periods") or not graph.periods:
            raise ValueError(
                "Cannot infer historical periods: graph has no periods attribute"
            )

        if not forecast_periods:
            raise ValueError(
                "Cannot infer historical periods: no forecast periods provided"
            )

        # Try to find where forecast periods start
        first_forecast = forecast_periods[0]
        try:
            idx = graph.periods.index(first_forecast)
            historical_periods = graph.periods[:idx]
            logger.debug(
                f"Inferred historical periods by splitting at {first_forecast}: "
                f"{historical_periods}"
            )
        except ValueError:
            # First forecast period not in graph periods
            # Assume all current periods are historical
            historical_periods = list(graph.periods)
            logger.warning(
                f"First forecast period {first_forecast} not found in graph periods. "
                f"Using all existing periods as historical: {historical_periods}"
            )

        if not historical_periods:
            raise ValueError(
                "No historical periods found. Ensure graph has periods before "
                "the first forecast period."
            )

        return historical_periods

    @staticmethod
    def determine_base_period(
        node: Node,
        historical_periods: list[str],
        preferred_period: Optional[str] = None,
    ) -> str:
        """Determine the base period for forecasting a node.

        Args:
            node: The node to forecast.
            historical_periods: List of available historical periods.
            preferred_period: Optional preferred base period.

        Returns:
            The base period to use for forecasting.

        Raises:
            ValueError: If no valid base period can be determined.
        """
        if not historical_periods:
            raise ValueError("No historical periods provided")

        # Determine strategy for selecting base period
        strategy = cfg("forecasting.base_period_strategy")

        # Validate strategy
        valid_strategies = {
            "preferred_then_most_recent",
            "most_recent",
            "last_historical",
        }
        if strategy not in valid_strategies:
            logger.warning(
                f"Unknown base period strategy '{strategy}', falling back to 'preferred_then_most_recent'"
            )
            strategy = "preferred_then_most_recent"

        # 1. preferred_then_most_recent: check preferred first
        if strategy == "preferred_then_most_recent" and preferred_period:
            if preferred_period in historical_periods and hasattr(node, "values"):
                values = getattr(node, "values", {})
                if isinstance(values, dict) and preferred_period in values:
                    return preferred_period

        # 2. most_recent: pick most recent available data
        if strategy in ("preferred_then_most_recent", "most_recent"):
            if hasattr(node, "values") and isinstance(
                getattr(node, "values", None), dict
            ):
                values_dict = node.values
                available_periods = [p for p in historical_periods if p in values_dict]
                if available_periods:
                    return available_periods[-1]

        # 3. last_historical: always use last in historical_periods
        if strategy == "last_historical":
            return historical_periods[-1]

        # Final fallback: use last historical period
        base_period = historical_periods[-1]
        logger.info(
            f"Using last historical period as base for {node.name}: {base_period} "
            "(node may lack values)"
        )
        return base_period

    @staticmethod
    def validate_period_sequence(periods: list[str]) -> None:
        """Validate that a period sequence is valid.

        Args:
            periods: List of periods to validate.

        Raises:
            ValueError: If the period sequence is invalid.
        """
        if not periods:
            raise ValueError("Period sequence cannot be empty")

        if len(periods) != len(set(periods)):
            duplicates = [p for p in periods if periods.count(p) > 1]
            raise ValueError(f"Period sequence contains duplicates: {set(duplicates)}")

    @staticmethod
    def get_period_index(period: str, periods: list[str]) -> int:
        """Get the index of a period in a period list.

        Args:
            period: The period to find.
            periods: List of periods.

        Returns:
            The index of the period.

        Raises:
            ValueError: If period not found in list.
        """
        try:
            return periods.index(period)
        except ValueError:
            raise ValueError(f"Period '{period}' not found in period list") from None

    @staticmethod
    def ensure_periods_exist(
        graph: Any, periods: list[str], add_missing: bool = True
    ) -> list[str]:
        """Ensure periods exist in the graph.

        Args:
            graph: The financial statement graph instance.
            periods: List of periods that should exist.
            add_missing: Whether to add missing periods to the graph.

        Returns:
            List of periods that were added (empty if none).

        Raises:
            ValueError: If add_missing is False and periods are missing.
        """
        if not hasattr(graph, "periods"):
            raise ValueError("Graph does not have a periods attribute")

        existing_periods = set(graph.periods)
        missing_periods = [p for p in periods if p not in existing_periods]

        if missing_periods:
            if add_missing:
                # Add missing periods to graph
                if hasattr(graph, "add_periods") and callable(graph.add_periods):
                    graph.add_periods(missing_periods)
                    logger.info(f"Added missing periods to graph: {missing_periods}")
                else:
                    raise ValueError(
                        f"Graph is missing periods {missing_periods} but has no add_periods method"
                    )
            else:
                raise ValueError(
                    f"The following periods do not exist in the graph: {missing_periods}"
                )

        return missing_periods



================================================================================
File: fin_statement_model/forecasting/strategies.py
================================================================================

"""Forecast method registry and selection strategies.

This module provides a registry for forecast methods and handles method
selection and configuration.
"""

import logging
from typing import Any

from .methods import (
    ForecastMethod,
    SimpleForecastMethod,
    CurveForecastMethod,
    StatisticalForecastMethod,
    AverageForecastMethod,
    HistoricalGrowthForecastMethod,
)

logger = logging.getLogger(__name__)


class ForecastMethodRegistry:
    """Registry for forecast methods.

    This class manages the available forecast methods and provides
    a centralized way to access and register them.

    Example:
        >>> registry = ForecastMethodRegistry()
        >>> method = registry.get_method('simple')
        >>> print(registry.list_methods())
        ['simple', 'curve', 'statistical', 'average', 'historical_growth']
    """

    def __init__(self) -> None:
        """Initialize the registry with built-in methods."""
        self._methods: dict[str, ForecastMethod] = {}
        self._register_builtin_methods()

    def _register_builtin_methods(self) -> None:
        """Register all built-in forecast methods."""
        builtin_methods = [
            SimpleForecastMethod(),
            CurveForecastMethod(),
            StatisticalForecastMethod(),
            AverageForecastMethod(),
            HistoricalGrowthForecastMethod(),
        ]

        for method in builtin_methods:
            self.register(method)
            logger.debug(f"Registered built-in forecast method: {method.name}")

    def register(self, method: ForecastMethod) -> None:
        """Register a new forecast method.

        Args:
            method: The forecast method to register.

        Raises:
            ValueError: If a method with the same name is already registered.
        """
        if method.name in self._methods:
            raise ValueError(f"Forecast method '{method.name}' is already registered")

        self._methods[method.name] = method
        logger.info(f"Registered forecast method: {method.name}")

    def unregister(self, name: str) -> None:
        """Unregister a forecast method.

        Args:
            name: The name of the method to unregister.

        Raises:
            KeyError: If the method is not registered.
        """
        if name not in self._methods:
            raise KeyError(f"Forecast method '{name}' is not registered")

        del self._methods[name]
        logger.info(f"Unregistered forecast method: {name}")

    def get_method(self, name: str) -> ForecastMethod:
        """Get a forecast method by name.

        Args:
            name: The name of the method to retrieve.

        Returns:
            The requested forecast method.

        Raises:
            ValueError: If the method is not registered.
        """
        if name not in self._methods:
            available = ", ".join(sorted(self._methods.keys()))
            raise ValueError(
                f"Unknown forecast method: '{name}'. Available methods: {available}"
            )

        return self._methods[name]

    def list_methods(self) -> list[str]:
        """List all available forecast methods.

        Returns:
            Sorted list of registered method names.
        """
        return sorted(self._methods.keys())

    def has_method(self, name: str) -> bool:
        """Check if a method is registered.

        Args:
            name: The name of the method to check.

        Returns:
            True if the method is registered, False otherwise.
        """
        return name in self._methods

    def get_method_info(self, name: str) -> dict[str, Any]:
        """Get information about a forecast method.

        Args:
            name: The name of the method.

        Returns:
            Dictionary with method information including docstring.

        Raises:
            ValueError: If the method is not registered.
        """
        method = self.get_method(name)
        return {
            "name": method.name,
            "class": method.__class__.__name__,
            "description": method.__class__.__doc__ or "No description available",
            "module": method.__class__.__module__,
        }


# Global registry instance
forecast_registry = ForecastMethodRegistry()


def get_forecast_method(name: str) -> ForecastMethod:
    """Get a forecast method from the global registry.

    This is a convenience function that uses the global registry.

    Args:
        name: The name of the method to retrieve.

    Returns:
        The requested forecast method.

    Raises:
        ValueError: If the method is not registered.
    """
    return forecast_registry.get_method(name)


def register_forecast_method(method: ForecastMethod) -> None:
    """Register a custom forecast method in the global registry.

    This is a convenience function that uses the global registry.

    Args:
        method: The forecast method to register.

    Raises:
        ValueError: If a method with the same name is already registered.
    """
    forecast_registry.register(method)



================================================================================
File: fin_statement_model/forecasting/types.py
================================================================================

"""Type definitions and data structures for forecasting module.

This module contains all the type aliases, enums, and data structures
used throughout the forecasting sub-module.
"""

from typing import Any, Union, Literal
from collections.abc import Callable
import numpy as np

from pydantic import BaseModel, ConfigDict, model_validator, ValidationError

from fin_statement_model.forecasting.errors import (
    ForecastMethodError,
    ForecastConfigurationError,
    ForecastResultError,
)

# Type aliases for clarity
Numeric = Union[int, float, np.number[Any]]
GrowthRate = Union[float, list[float], Callable[[], float]]
PeriodValue = dict[str, float]

# Forecast method types
ForecastMethodType = Literal[
    "simple",
    "curve",
    "statistical",
    "average",
    "historical_growth",
]


class StatisticalConfig(BaseModel):
    """Configuration for statistical forecasting method."""

    distribution: str
    params: dict[str, float]

    model_config = ConfigDict(extra="forbid")

    @model_validator(mode="after")  # type: ignore[arg-type]
    def _validate_distribution(cls, values: "StatisticalConfig") -> "StatisticalConfig":
        distribution = values.distribution
        params = values.params

        if distribution == "normal":
            required = {"mean", "std"}
            missing = required - params.keys()
            if missing:
                raise ForecastConfigurationError(
                    "Normal distribution requires 'mean' and 'std' parameters",
                    config=params,
                    missing_params=list(missing),
                )
        elif distribution == "uniform":
            required = {"low", "high"}
            missing = required - params.keys()
            if missing:
                raise ForecastConfigurationError(
                    "Uniform distribution requires 'low' and 'high' parameters",
                    config=params,
                    missing_params=list(missing),
                )
        else:
            raise ForecastConfigurationError(
                f"Unsupported distribution: {distribution}",
                config=params,
                invalid_params={"distribution": f"'{distribution}' is not supported"},
            )

        return values


class ForecastConfig(BaseModel):
    """Configuration for a forecast operation."""

    method: ForecastMethodType
    config: Any  # Method-specific configuration

    model_config = ConfigDict(extra="forbid")

    @model_validator(mode="after")  # type: ignore[arg-type]
    def _validate_config(cls, values: "ForecastConfig") -> "ForecastConfig":
        method = values.method
        cfg = values.config or {}

        valid_methods = {
            "simple",
            "curve",
            "statistical",
            "average",
            "historical_growth",
        }

        if method not in valid_methods:
            raise ForecastMethodError(
                "Invalid forecast method",
                method=method,
                supported_methods=list(valid_methods),
            )

        if method == "statistical":
            # Delegate validation to StatisticalConfig for detailed checks
            try:
                (
                    StatisticalConfig(**cfg)
                    if isinstance(cfg, dict)
                    else StatisticalConfig.model_validate(cfg)
                )
            except (ForecastConfigurationError, ValidationError) as exc:
                # Re-raise as ForecastConfigurationError for consistency
                raise ForecastConfigurationError(
                    "Invalid statistical configuration",
                    config=cfg,
                ) from exc

        return values


class ForecastResult(BaseModel):
    """Result of a forecast operation."""

    node_name: str
    periods: list[str]
    values: PeriodValue
    method: ForecastMethodType
    base_period: str

    model_config = ConfigDict(extra="forbid")

    def get_value(self, period: str) -> float:
        """Get the forecast value for a specific period."""
        if period not in self.values:
            raise ForecastResultError(
                f"Period {period} not found in forecast results",
                period=period,
                available_periods=list(self.values.keys()),
            )
        return self.values[period]



================================================================================
File: fin_statement_model/forecasting/validators.py
================================================================================

"""Input validation and error checking for forecasting operations.

This module provides validation utilities to ensure forecast inputs
are valid before processing.
"""

import logging
from typing import Any, Optional

from fin_statement_model.core.nodes import Node
from .types import ForecastMethodType, ForecastConfig

logger = logging.getLogger(__name__)


class ForecastValidator:
    """Validates inputs for forecasting operations.

    This class provides methods to validate various aspects of forecast
    inputs including periods, node configurations, and method parameters.
    """

    @staticmethod
    def validate_forecast_inputs(
        historical_periods: list[str],
        forecast_periods: list[str],
        node_configs: Optional[dict[str, dict[str, Any]]] = None,
    ) -> None:
        """Validate basic forecast inputs.

        Args:
            historical_periods: List of historical periods.
            forecast_periods: List of periods to forecast.
            node_configs: Optional node configuration mapping.

        Raises:
            ValueError: If inputs are logically invalid.
            TypeError: If inputs are of wrong type.
        """
        # Validate historical periods
        if not historical_periods:
            raise ValueError("No historical periods provided for forecasting")

        if not isinstance(historical_periods, list):
            raise TypeError(
                f"Historical periods must be a list, got {type(historical_periods)}"
            )

        # Validate forecast periods
        if not forecast_periods:
            raise ValueError("No forecast periods provided")

        if not isinstance(forecast_periods, list):
            raise TypeError(
                f"Forecast periods must be a list, got {type(forecast_periods)}"
            )

        # Check for overlapping periods
        historical_set = set(historical_periods)
        forecast_set = set(forecast_periods)
        overlap = historical_set & forecast_set
        if overlap:
            logger.warning(
                f"Forecast periods overlap with historical periods: {overlap}. "
                f"This may overwrite historical data."
            )

        # Validate node configs if provided
        if node_configs is not None:
            if not isinstance(node_configs, dict):
                raise TypeError(
                    f"Node configs must be a dict, got {type(node_configs)}"
                )

            for node_name, config in node_configs.items():
                ForecastValidator.validate_node_config(node_name, config)

    @staticmethod
    def validate_node_config(node_name: str, config: dict[str, Any]) -> None:
        """Validate configuration for a single node.

        Args:
            node_name: Name of the node being configured.
            config: Configuration dictionary for the node.

        Raises:
            ValueError: If configuration is logically invalid.
            TypeError: If configuration is of wrong type.
        """
        if not isinstance(config, dict):
            raise TypeError(
                f"Configuration for node '{node_name}' must be a dict, got {type(config)}"
            )

        # Validate method
        if "method" not in config:
            raise ValueError(
                f"Configuration for node '{node_name}' missing required 'method' key"
            )

        method = config["method"]
        valid_methods: list[ForecastMethodType] = [
            "simple",
            "curve",
            "statistical",
            "average",
            "historical_growth",
        ]
        if method not in valid_methods:
            raise ValueError(
                f"Invalid forecast method '{method}' for node '{node_name}'. "
                f"Valid methods: {valid_methods}"
            )

        # Validate config exists (can be None for some methods)
        if "config" not in config:
            raise ValueError(
                f"Configuration for node '{node_name}' missing required 'config' key"
            )

    @staticmethod
    def validate_node_for_forecast(node: Node, method: str) -> None:
        """Validate that a node can be forecasted with the given method.

        Args:
            node: The node to validate.
            method: The forecast method to use.

        Raises:
            ValueError: If node cannot be forecasted.
        """
        # Check if node has values dictionary
        if not hasattr(node, "values") or not isinstance(node.values, dict):
            raise ValueError(
                f"Node '{node.name}' cannot be forecasted: missing or invalid "
                f"'values' attribute. Only nodes with values dictionaries can "
                f"be forecasted."
            )

        # Check if node has calculate method for certain forecast types
        if method in ["average", "historical_growth"] and (
            not hasattr(node, "calculate") or not callable(node.calculate)
        ):
            raise ValueError(
                f"Node '{node.name}' cannot use '{method}' forecast method: "
                f"missing calculate() method"
            )

    @staticmethod
    def validate_forecast_config(config: dict[str, Any]) -> ForecastConfig:
        """Validate and convert a forecast configuration dictionary.

        Args:
            config: Raw configuration dictionary.

        Returns:
            Validated ForecastConfig instance.

        Raises:
            ValueError: If configuration is logically invalid.
            TypeError: If configuration is of wrong type.
        """
        if not isinstance(config, dict):
            raise TypeError(f"Forecast config must be a dict, got {type(config)}")

        if "method" not in config:
            raise ValueError("Forecast config missing required 'method' key")

        if "config" not in config:
            raise ValueError("Forecast config missing required 'config' key")

        # Create and validate using dataclass
        return ForecastConfig(method=config["method"], config=config["config"])

    @staticmethod
    def validate_base_period(
        base_period: str, available_periods: list[str], node_name: str
    ) -> None:
        """Validate that a base period is valid for forecasting.

        Args:
            base_period: The proposed base period.
            available_periods: List of available periods.
            node_name: Name of the node (for error messages).

        Raises:
            ValueError: If base period is invalid.
        """
        if not base_period:
            raise ValueError(f"No base period determined for node '{node_name}'")

        if base_period not in available_periods:
            raise ValueError(
                f"Base period '{base_period}' for node '{node_name}' not found in available periods"
            )

    @staticmethod
    def validate_forecast_result(
        result: dict[str, float], expected_periods: list[str], node_name: str
    ) -> None:
        """Validate forecast results.

        Args:
            result: Dictionary of period -> value mappings.
            expected_periods: List of expected forecast periods.
            node_name: Name of the node (for error messages).

        Raises:
            ValueError: If results are logically invalid or incomplete.
            TypeError: If results are of wrong type.
        """
        if not isinstance(result, dict):
            raise TypeError(
                f"Forecast result for node '{node_name}' must be a dict, got {type(result)}"
            )

        # Check all expected periods are present
        missing_periods = set(expected_periods) - set(result.keys())
        if missing_periods:
            raise ValueError(
                f"Forecast result for node '{node_name}' missing periods: {missing_periods}"
            )

        # Validate all values are numeric
        for period, value in result.items():
            if not isinstance(value, int | float):
                raise TypeError(
                    f"Forecast value for node '{node_name}' period '{period}' "
                    f"must be numeric, got {type(value)}"
                )



================================================================================
File: fin_statement_model/io/__init__.py
================================================================================

"""Input/Output components for the Financial Statement Model.

This package provides a unified interface for reading and writing financial model
data from/to various formats using a registry-based approach.
"""

import logging

from .core import (
    DataReader,
    DataWriter,
    get_reader,
    get_writer,
    list_readers,
    list_writers,
    read_data,
    write_data,
)
from .exceptions import IOError, ReadError, WriteError, FormatNotSupportedError
from . import formats  # noqa: F401
from . import specialized  # noqa: F401

# Import specialized functions for convenience
from .specialized import (
    import_from_cells,
    load_adjustments_from_excel,
    export_adjustments_to_excel,
    list_available_builtin_configs,
    write_statement_to_excel,
    write_statement_to_json,
)

# Configure logging for the io package
logger = logging.getLogger(__name__)

# --- Public API ---

__all__ = [
    # Base classes
    "DataReader",
    "DataWriter",
    # Exceptions
    "FormatNotSupportedError",
    "IOError",
    "ReadError",
    "WriteError",
    # Specialized functions
    "export_adjustments_to_excel",
    # Registry functions
    "get_reader",
    "get_writer",
    "import_from_cells",
    "list_available_builtin_configs",
    "list_readers",
    "list_writers",
    "load_adjustments_from_excel",
    # Facade functions
    "read_data",
    "write_data",
    "write_statement_to_excel",
    "write_statement_to_json",
]



================================================================================
File: fin_statement_model/io/config/models.py
================================================================================

"""Pydantic models for IO reader and writer configuration.

This module provides declarative schemas for validating configuration passed to IO readers.
"""

from __future__ import annotations

from typing import Optional, Literal, Any, Union
from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator

from fin_statement_model.config.helpers import cfg
from fin_statement_model.core.adjustments.models import (
    AdjustmentFilterInput,
)


# Define MappingConfig locally to avoid circular import
MappingConfig = Union[dict[str, str], dict[Optional[str], dict[str, str]]]


class BaseReaderConfig(BaseModel):
    """Base configuration for IO readers."""

    source: Any = Field(
        ..., description="URI or path to data source (file path, ticker, etc.)"
    )
    format_type: Literal["csv", "excel", "dataframe", "dict", "fmp"] = Field(
        ..., description="Type of reader (csv, excel, dataframe, dict, fmp)."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class CsvReaderConfig(BaseReaderConfig):
    """CSV reader options."""

    # Falls back to cfg("io.default_csv_delimiter") when not supplied
    delimiter: str = Field(
        default_factory=lambda: cfg("io.default_csv_delimiter"),
        description="Field delimiter for CSV files.",
    )
    header_row: int = Field(
        1, description="Row number containing column names (1-indexed)."
    )
    index_col: Optional[int] = Field(
        None, description="1-indexed column for row labels."
    )
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    # Runtime override options: these will override config defaults when provided at read-time
    statement_type: Optional[
        Literal["income_statement", "balance_sheet", "cash_flow"]
    ] = Field(
        None,
        description="Type of statement ('income_statement', 'balance_sheet', 'cash_flow') to select mapping scope.",
    )
    item_col: Optional[str] = Field(
        None, description="Name of the column containing item identifiers."
    )
    period_col: Optional[str] = Field(
        None, description="Name of the column containing period identifiers."
    )
    value_col: Optional[str] = Field(
        None, description="Name of the column containing numeric values."
    )
    pandas_read_csv_kwargs: dict[str, Any] = Field(
        default_factory=dict,
        description="Additional kwargs for pandas.read_csv, overriding config defaults.",
    )

    @model_validator(mode="after")  # type: ignore[arg-type]
    def check_header_row(cls, cfg: CsvReaderConfig) -> CsvReaderConfig:
        """Ensure header_row is at least 1."""
        if cfg.header_row < 1:
            raise ValueError("header_row must be >= 1")
        return cfg


class ExcelReaderConfig(BaseReaderConfig):
    """Excel reader options."""

    # Uses cfg("io.default_excel_sheet") unless caller overrides
    sheet_name: Optional[str] = Field(
        default_factory=lambda: cfg("io.default_excel_sheet"),
        description="Worksheet name or index.",
    )
    items_col: int = Field(1, description="1-indexed column where item names reside.")
    periods_row: int = Field(1, description="1-indexed row where periods reside.")
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    statement_type: Optional[
        Literal["income_statement", "balance_sheet", "cash_flow"]
    ] = Field(
        None,
        description="Type of statement ('income_statement', 'balance_sheet', 'cash_flow'). Used to select a mapping scope.",
    )
    header_row: Optional[int] = Field(
        None, description="1-indexed row for pandas header reading."
    )
    nrows: Optional[int] = Field(
        None, description="Number of rows to read from the sheet."
    )
    skiprows: Optional[int] = Field(
        None, description="Number of rows to skip at the beginning."
    )

    @model_validator(mode="after")  # type: ignore[arg-type]
    def check_indices(cls, cfg: ExcelReaderConfig) -> ExcelReaderConfig:
        """Ensure items_col and periods_row are at least 1."""
        if cfg.items_col < 1 or cfg.periods_row < 1:
            raise ValueError("items_col and periods_row must be >= 1")
        return cfg


class FmpReaderConfig(BaseReaderConfig):
    """Financial Modeling Prep API reader options."""

    statement_type: Literal["income_statement", "balance_sheet", "cash_flow"] = Field(
        ..., description="Type of financial statement to fetch."
    )
    period_type: Literal["FY", "QTR"] = Field(
        "FY", description="Period type: 'FY' or 'QTR'."
    )
    limit: int = Field(5, description="Number of periods to fetch.")
    # Caller value → env var → cfg("api.fmp_api_key")
    api_key: Optional[str] = Field(
        default=None,
        description="Financial Modeling Prep API key.",
    )
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    @field_validator("api_key", mode="before")
    def load_api_key_env(cls, value: Optional[str]) -> Optional[str]:
        """Cascade lookup: explicit param → env → global config."""
        if value:
            return value
        import os

        return os.getenv("FMP_API_KEY") or cfg("api.fmp_api_key", None)

    @model_validator(mode="after")  # type: ignore[arg-type]
    def check_api_key(cls, cfg: FmpReaderConfig) -> FmpReaderConfig:
        """Ensure an API key is provided."""
        if not cfg.api_key:
            raise ValueError("api_key is required (env var FMP_API_KEY or param)")
        return cfg


# --- New Reader Configs for DataFrame and Dict readers ---


class DataFrameReaderConfig(BaseReaderConfig):
    """Configuration for DataFrameReader.

    No additional reader-specific options are required at the moment because
    the reader consumes an in-memory :class:`pandas.DataFrame` supplied to
    :py:meth:`DataFrameReader.read`.  The `source` field therefore serves only
    to preserve a consistent registry-initialisation contract.
    """

    source: Any = Field(..., description="In-memory pandas DataFrame source")
    format_type: Literal["dataframe"] = "dataframe"

    # Runtime override for read-time periods selection
    periods: Optional[list[str]] = Field(
        None,
        description="Optional list of periods (columns) to include when reading a DataFrame.",
    )


class DictReaderConfig(BaseReaderConfig):
    """Configuration for DictReader.

    Mirrors :class:`DataFrameReaderConfig` - no custom options yet.  The
    placeholder keeps the IO registry symmetric and future-proof.
    """

    source: dict[str, dict[str, float]] = Field(
        ..., description="In-memory dictionary source"
    )
    format_type: Literal["dict"] = "dict"

    # Runtime override for read-time periods selection
    periods: Optional[list[str]] = Field(
        None, description="Optional list of periods to include when reading a dict."
    )


# --- Writer-side Pydantic configuration models ---
class BaseWriterConfig(BaseModel):
    """Base configuration for IO writers."""

    target: Optional[str] = Field(
        None,
        description="URI or path to data target (file path, in-memory target, etc.)",
    )
    format_type: Literal["excel", "dataframe", "dict", "markdown"] = Field(
        ..., description="Type of writer (excel, dataframe, dict, markdown)."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class ExcelWriterConfig(BaseWriterConfig):
    """Excel writer options."""

    # Default comes from cfg("io.default_excel_sheet")
    sheet_name: str = Field(
        default_factory=lambda: cfg("io.default_excel_sheet"),
        description="Name of the sheet to write to.",
    )
    recalculate: bool = Field(
        True, description="Whether to recalculate graph before export."
    )
    include_nodes: Optional[list[str]] = Field(
        None, description="Optional list of node names to include in export."
    )
    excel_writer_kwargs: dict[str, Any] = Field(
        default_factory=dict,
        description="Additional kwargs for pandas.DataFrame.to_excel.",
    )


class DataFrameWriterConfig(BaseWriterConfig):
    """DataFrame writer options."""

    target: Optional[str] = Field(
        None, description="Optional target path (ignored by DataFrameWriter)."
    )
    recalculate: bool = Field(
        True, description="Whether to recalculate graph before export."
    )
    include_nodes: Optional[list[str]] = Field(
        None, description="Optional list of node names to include in export."
    )


class DictWriterConfig(BaseWriterConfig):
    """Dict writer has no additional options."""

    target: Optional[str] = Field(
        None, description="Optional target (ignored by DictWriter)."
    )


class MarkdownWriterConfig(BaseWriterConfig):
    """Markdown writer options.

    The writer is configured via an in-memory mapping of statement IDs to
    configuration dictionaries.
    """

    # In-memory configs only.
    raw_configs: Optional[dict[str, dict[str, Any]]] = Field(
        None,
        description=(
            "Mapping of statement IDs to configuration dictionaries.  This allows "
            "fully in-memory operation without relying on the filesystem."
        ),
    )

    historical_periods: Optional[list[str]] = Field(
        None, description="List of historical period names."
    )
    forecast_periods: Optional[list[str]] = Field(
        None, description="List of forecast period names."
    )
    adjustment_filter: Optional[AdjustmentFilterInput] = Field(
        None, description="Adjustment filter to apply."
    )
    forecast_configs: Optional[dict[str, Any]] = Field(
        None,
        description="Dictionary mapping node IDs to forecast configurations for notes.",
    )
    indent_spaces: int = Field(4, description="Number of spaces per indentation level.")
    target: Optional[str] = Field(
        None, description="Optional target path (ignored by MarkdownWriter)."
    )

    # Allow extra write() kwargs like 'statement_structure' to pass through without error
    model_config = ConfigDict(extra="ignore", frozen=True)



================================================================================
File: fin_statement_model/io/core/__init__.py
================================================================================

"""Core IO components including base classes, registry, and utilities."""

from .base import DataReader, DataWriter
from .facade import read_data, write_data
from .mixins import (
    FileBasedReader,
    ConfigurationMixin,
    DataFrameBasedWriter,
    ValueExtractionMixin,
    ValidationResultCollector,
    handle_read_errors,
    handle_write_errors,
)
from .registry import (
    HandlerRegistry,
    get_reader,
    get_writer,
    list_readers,
    list_writers,
    register_reader,
    register_writer,
)

__all__ = [
    # Mixins and utilities
    "ConfigurationMixin",
    "DataFrameBasedWriter",
    # Base classes
    "DataReader",
    "DataWriter",
    "FileBasedReader",
    # Registry
    "HandlerRegistry",
    "ValidationResultCollector",
    "ValueExtractionMixin",
    "get_reader",
    "get_writer",
    "handle_read_errors",
    "handle_write_errors",
    "list_readers",
    "list_writers",
    # Facade functions
    "read_data",
    "register_reader",
    "register_writer",
    "write_data",
]



================================================================================
File: fin_statement_model/io/core/base.py
================================================================================

"""Base classes for data readers and writers."""

from abc import ABC, abstractmethod
from typing import Any

# Use absolute import based on project structure
from fin_statement_model.core.graph import Graph


class DataReader(ABC):
    """Abstract base class for all data readers.

    Defines the interface for classes that read data from various sources
    and typically populate or return a Graph object.
    """

    @abstractmethod
    def read(self, source: Any, **kwargs: dict[str, Any]) -> Graph:
        """Read data from the specified source and return a Graph.

        Args:
            source: The data source. Type depends on the reader implementation
                (e.g., file path `str`, ticker `str`, `pd.DataFrame`, `dict`).
            **kwargs: Additional format-specific options for reading.

        Returns:
            A Graph object populated with the data from the source.

        Raises:
            ReadError: If an error occurs during the reading process.
            NotImplementedError: If the method is not implemented by a subclass.
        """
        raise NotImplementedError


class DataWriter(ABC):
    """Abstract base class for all data writers.

    Defines the interface for classes that write graph data to various targets.
    """

    @abstractmethod
    def write(self, graph: Graph, target: Any, **kwargs: dict[str, Any]) -> object:
        """Write data from the Graph object to the specified target.

        Args:
            graph: The Graph object containing the data to write.
            target: The destination target. Type depends on the writer implementation
                (e.g., file path `str`, or ignored if the writer returns an object).
            **kwargs: Additional format-specific options for writing.

        Raises:
            WriteError: If an error occurs during the writing process.
            NotImplementedError: If the method is not implemented by a subclass.
        """
        raise NotImplementedError



================================================================================
File: fin_statement_model/io/core/facade.py
================================================================================

"""Facade functions for simplified IO operations.

This module provides the main public API for reading and writing data,
abstracting away the complexity of the registry system.
"""

import logging
import os
from typing import Union, Any

from fin_statement_model.core.graph import Graph
from .registry import get_reader, get_writer
from fin_statement_model.io.exceptions import (
    IOError,
    ReadError,
    WriteError,
    FormatNotSupportedError,
)

logger = logging.getLogger(__name__)


def read_data(
    format_type: str, source: Any, **kwargs: dict[str, Union[str, int, float, bool]]
) -> Graph:
    """Reads data from a source using the specified format.

    This function acts as a facade for the underlying reader implementations.
    It uses the `format_type` to look up the appropriate reader class in the registry.
    The `source` and `**kwargs` are combined and validated against the specific
    reader's Pydantic configuration model (e.g., `CsvReaderConfig`).

    The validated configuration is used to initialize the reader instance.
    The `source` (which might be the original object for dict/dataframe formats, or
    the validated string path/ticker otherwise) and the original `**kwargs` are then
    passed to the reader instance's `.read()` method, which handles format-specific
    read-time options.

    Args:
        format_type (str): The format identifier (e.g., 'excel', 'csv', 'fmp', 'dict').
        source (Any): The data source. Its type depends on `format_type`:
            - `str`: file path (for 'excel', 'csv'), ticker symbol (for 'fmp').
            - `pd.DataFrame`: for 'dataframe'.
            - `dict`: for 'dict'.
        **kwargs: Additional keyword arguments used for reader configuration (e.g.,
            `api_key`, `delimiter`, `sheet_name`, `mapping_config`) and potentially
            passed to the reader's `.read()` method (e.g., `periods`). Consult the
            specific reader's Pydantic config model and `.read()` docstring.

    Returns:
        Graph: A new Graph object populated with the read data.

    Raises:
        ReadError: If reading fails.
        FormatNotSupportedError: If the format_type is not registered.
        Exception: Other errors during reader initialization or reading.
    """
    logger.info(
        f"Attempting to read data using format '{format_type}' from source type '{type(source).__name__}'"
    )

    # Prepare kwargs for registry validation (includes source and format_type)
    config_kwargs = {**kwargs, "source": source, "format_type": format_type}
    # Keep separate kwargs for the read method itself (e.g., 'periods')
    # This assumes Pydantic configs *don't* capture read-time args.

    try:
        # Pass the config kwargs directly to get_reader
        reader = get_reader(**config_kwargs)

        # Determine the actual source object for the read method
        actual_source = (
            source if format_type in ("dict", "dataframe") else config_kwargs["source"]
        )

        # Pass the determined source and the original kwargs (excluding config keys potentially)
        # to the read method. Specific readers handle relevant kwargs.
        return reader.read(actual_source, **kwargs)
    except (IOError, FormatNotSupportedError):
        logger.exception("IO Error reading data")
        raise  # Re-raise specific IO errors
    except Exception as e:
        logger.exception(f"Unexpected error reading data with format '{format_type}'")
        # Wrap unexpected errors in ReadError for consistency?
        raise ReadError(
            "Unexpected error during read",
            source=str(source),
            reader_type=format_type,
            original_error=e,
        ) from e


def write_data(
    format_type: str,
    graph: Graph,
    target: Any,
    **kwargs: dict[str, Union[str, int, float, bool]],
) -> object:
    """Writes graph data to a target using the specified format.

    Similar to `read_data`, this acts as a facade for writer implementations.
    It uses `format_type` to find the writer class in the registry.
    The `target` and `**kwargs` are combined and validated against the specific
    writer's Pydantic configuration model (e.g., `ExcelWriterConfig`).

    The validated configuration initializes the writer instance.
    The original `graph`, `target`, and `**kwargs` are then passed to the writer
    instance's `.write()` method for format-specific write-time options.

    Args:
        format_type (str): The format identifier (e.g., 'excel', 'dataframe', 'dict').
        graph (Graph): The graph object containing data to write.
        target (Any): The destination target. Its type depends on `format_type`:
            - `str`: file path (usually required for file-based writers like 'excel').
            - Ignored: for writers that return objects (like 'dataframe', 'dict').
        **kwargs: Additional keyword arguments used for writer configuration (e.g.,
            `sheet_name`, `recalculate`) and potentially passed to the writer's
            `.write()` method. Consult the specific writer's Pydantic config model
            and `.write()` docstring.

    Returns:
        object: The result of the write operation. For writers like DataFrameWriter
                or DictWriter, this is the created object. For file writers, it's None.

    Raises:
        WriteError: If writing fails.
        FormatNotSupportedError: If the format_type is not registered.
        Exception: Other errors during writer initialization or writing.
    """
    logger.info(
        f"Attempting to write graph data using format '{format_type}' to target type '{type(target).__name__}'"
    )

    # Prepare kwargs for registry validation (includes target and format_type)
    config_kwargs = {**kwargs, "target": target, "format_type": format_type}

    # Pass the config kwargs directly to get_writer
    writer = get_writer(**config_kwargs)
    # Now call write with all writer-specific kwargs
    try:
        # Pass original graph, target, and non-config kwargs to write()
        result = writer.write(graph, target, **kwargs)

        # If the writer returns a string and target is a path, write it to the file.
        if isinstance(result, str) and isinstance(target, str):
            try:
                # Ensure target directory exists
                dir_path = os.path.dirname(target)
                if dir_path:
                    os.makedirs(dir_path, exist_ok=True)
                logger.debug(
                    f"Writing string result from writer '{type(writer).__name__}' to file: {target}"
                )
                with open(target, "w", encoding="utf-8") as f:
                    f.write(result)
                return None  # Consistent return for file writers
            except OSError as e:
                logger.exception(
                    f"Failed to write writer output to target file: {target}"
                )
                raise WriteError(
                    f"Failed to write writer output to file: {target}",
                    target=target,
                    writer_type=format_type,
                    original_error=e,
                ) from e
        else:
            # Otherwise, return the original result (e.g., DataFrame, dict)
            return result
    except (IOError, FormatNotSupportedError):
        logger.exception("IO Error writing data")
        raise  # Re-raise specific IO errors
    except Exception as e:
        logger.exception(f"Unexpected error writing data with format '{format_type}'")
        # Wrap unexpected errors
        raise WriteError(
            "Unexpected error during write",
            target=str(target),
            writer_type=format_type,
            original_error=e,
        ) from e


__all__ = ["read_data", "write_data"]



================================================================================
File: fin_statement_model/io/core/mixins.py
================================================================================

"""Reusable mixins and decorators for IO operations.

This module provides shared functionality for readers and writers including
error handling decorators and mixins for consistent behavior.
"""

import os
import functools
import logging
import importlib.resources
import yaml
from abc import abstractmethod
from typing import Any, TypeVar, Optional, ClassVar
from collections.abc import Callable

from fin_statement_model.core.graph import Graph
from .base import DataReader
from fin_statement_model.io.exceptions import ReadError, WriteError
from fin_statement_model.io.core.utils import normalize_mapping, MappingConfig

logger = logging.getLogger(__name__)

F = TypeVar("F", bound=Callable[..., Any])


# ===== Mapping Mixins =====


class MappingAwareMixin:
    """Mixin for readers that support name mapping functionality.

    Provides common methods for:
    - Loading default mappings from YAML files
    - Getting effective mappings based on context
    - Applying mappings to source names
    """

    # Class variable to store default mappings per reader type
    _default_mappings_cache: ClassVar[dict[str, MappingConfig]] = {}

    @classmethod
    def _get_default_mapping_path(cls) -> Optional[str]:
        """Get the path to default mapping YAML file for this reader.

        Subclasses should override this to specify their default mapping file.

        Returns:
            Path relative to fin_statement_model.io.config.mappings package,
            or None if no default mappings.
        """
        return None

    @classmethod
    def _load_default_mappings(cls) -> MappingConfig:
        """Load default mapping configurations from YAML file.

        Returns:
            Dictionary containing default mappings, empty if none found.
        """
        cache_key = cls.__name__

        # Return cached value if available
        if cache_key in cls._default_mappings_cache:
            return cls._default_mappings_cache[cache_key]

        mapping_path = cls._get_default_mapping_path()
        if not mapping_path:
            cls._default_mappings_cache[cache_key] = {}
            return {}

        try:
            yaml_content = (
                importlib.resources.files("fin_statement_model.io.config.mappings")
                .joinpath(mapping_path)
                .read_text(encoding="utf-8")
            )
            mappings = yaml.safe_load(yaml_content) or {}
            cls._default_mappings_cache[cache_key] = mappings
            logger.debug(f"Loaded default mappings for {cls.__name__}")
            return mappings
        except Exception:
            logger.exception(f"Failed to load default mappings for {cls.__name__}")
            cls._default_mappings_cache[cache_key] = {}
            return {}

    def _get_mapping(self, context_key: Optional[str] = None) -> dict[str, str]:
        """Get the effective mapping based on context and configuration.

        Args:
            context_key: Optional context (e.g., sheet name, statement type)
                        to select specific mapping scope.

        Returns:
            Flat dictionary mapping source names to canonical names.

        Raises:
            TypeError: If mapping configuration is invalid.
        """
        # Load defaults if not already loaded
        default_mappings = self._load_default_mappings()

        # Get user config
        user_config = self.get_config_value("mapping_config")

        # Start with defaults for the context
        mapping = normalize_mapping(default_mappings, context_key=context_key)

        # Overlay user mappings if provided
        if user_config:
            user_mapping = normalize_mapping(user_config, context_key=context_key)
            mapping.update(user_mapping)

        return mapping

    def _apply_mapping(self, source_name: str, mapping: dict[str, str]) -> str:
        """Apply mapping to convert source name to canonical name.

        Args:
            source_name: Original name from the data source
            mapping: Mapping dictionary

        Returns:
            Canonical name (mapped name or original if no mapping exists)
        """
        return mapping.get(source_name, source_name)

    def get_config_value(
        self,
        key: str,
        default: Any = None,
        required: bool = False,
        value_type: Optional[type] = None,
        validator: Optional[Callable[[Any], bool]] = None,
    ) -> Any:
        """Stub for configuration value retrieval; overridden by ConfigurationMixin."""
        ...


# ===== Validation Mixins =====


class ValidationMixin:
    """Mixin for readers that need comprehensive validation capabilities.

    Provides standardized validation methods for common data validation scenarios
    including data type validation, range validation, and custom validation rules.
    """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        """Initialize validation mixin."""
        super().__init__(*args, **kwargs)
        self._validation_context: dict[str, Any] = {}

    def set_validation_context(self, **context: Any) -> None:
        """Set validation context for error reporting.

        Args:
            **context: Key-value pairs to store as validation context.
        """
        self._validation_context.update(context)

    def get_validation_context(self) -> dict[str, Any]:
        """Get current validation context.

        Returns:
            Dictionary containing current validation context.
        """
        return self._validation_context.copy()

    def validate_required_columns(
        self,
        df: Any,  # pandas.DataFrame
        required_columns: list[str],
        source_identifier: str = "data source",
    ) -> None:
        """Validate that required columns exist in DataFrame.

        Args:
            df: DataFrame to validate.
            required_columns: List of required column names.
            source_identifier: Identifier for the data source (for error messages).

        Raises:
            ReadError: If required columns are missing.
        """
        if not hasattr(df, "columns"):
            raise ReadError(
                "Invalid data structure: expected DataFrame with columns attribute",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

        missing_columns = set(required_columns) - set(df.columns)
        if missing_columns:
            raise ReadError(
                f"Missing required columns in {source_identifier}: {missing_columns}",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

    def validate_column_bounds(
        self,
        df: Any,  # pandas.DataFrame
        column_index: int,
        source_identifier: str = "data source",
        context: str = "column",
    ) -> None:
        """Validate that column index is within DataFrame bounds.

        Args:
            df: DataFrame to validate.
            column_index: 0-based column index to validate.
            source_identifier: Identifier for the data source.
            context: Context description for error messages.

        Raises:
            ReadError: If column index is out of bounds.
        """
        if not hasattr(df, "columns"):
            raise ReadError(
                "Invalid data structure: expected DataFrame with columns attribute",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

        if column_index >= len(df.columns) or column_index < 0:
            raise ReadError(
                f"{context} index ({column_index + 1}) is out of bounds. "
                f"Found {len(df.columns)} columns.",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

    def validate_periods_exist(
        self,
        periods: list[str],
        source_identifier: str = "data source",
        min_periods: int = 1,
    ) -> None:
        """Validate that periods list is not empty and meets minimum requirements.

        Args:
            periods: List of period identifiers.
            source_identifier: Identifier for the data source.
            min_periods: Minimum number of periods required.

        Raises:
            ReadError: If periods validation fails.
        """
        if not periods:
            raise ReadError(
                f"No periods found in {source_identifier}",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

        if len(periods) < min_periods:
            raise ReadError(
                f"Insufficient periods in {source_identifier}. "
                f"Found {len(periods)}, minimum required: {min_periods}",
                source=source_identifier,
                reader_type=getattr(self, "__class__", {}).get("__name__", "Unknown"),
            )

    def validate_numeric_value(  # noqa: PLR0911
        self,
        value: Any,
        item_name: str,
        period: str,
        validator: Optional["ValidationResultCollector"] = None,
        allow_conversion: bool = True,
    ) -> tuple[bool, Optional[float]]:
        """Validate and optionally convert a value to numeric.

        Args:
            value: Value to validate.
            item_name: Name of the item (for error reporting).
            period: Period identifier (for error reporting).
            validator: Optional ValidationResultCollector to record errors.
            allow_conversion: Whether to attempt string-to-float conversion.

        Returns:
            Tuple of (is_valid, converted_value).
            If is_valid is False, converted_value will be None.
        """
        import pandas as pd

        # Skip NaN/None values
        if pd.isna(value) or value is None:
            return True, None

        # Already numeric
        if isinstance(value, int | float):
            if not pd.isfinite(value):
                error_msg = f"Non-finite numeric value '{value}' for period '{period}'"
                if validator:
                    validator.add_result(item_name, False, error_msg)
                return False, None
            return True, float(value)

        # Attempt conversion if allowed
        if allow_conversion:
            try:
                converted = float(value)
                if not pd.isfinite(converted):
                    error_msg = f"Converted to non-finite value '{converted}' for period '{period}'"
                    if validator:
                        validator.add_result(item_name, False, error_msg)
                    return False, None
                return True, converted
            except (ValueError, TypeError):
                error_msg = f"Non-numeric value '{value}' for period '{period}'"
                if validator:
                    validator.add_result(item_name, False, error_msg)
                return False, None

        # Not numeric and conversion not allowed
        error_msg = f"Non-numeric value '{value}' for period '{period}'"
        if validator:
            validator.add_result(item_name, False, error_msg)
        return False, None

    def validate_node_name(
        self,
        node_name: Any,
        source_name: str = "",
        allow_empty: bool = False,
    ) -> tuple[bool, Optional[str]]:
        """Validate and normalize a node name.

        Args:
            node_name: Raw node name to validate.
            source_name: Original source name (for error context).
            allow_empty: Whether to allow empty/None node names.

        Returns:
            Tuple of (is_valid, normalized_name).
            If is_valid is False, normalized_name will be None.
        """
        import pandas as pd

        if pd.isna(node_name) or node_name is None:
            return allow_empty, None

        if not node_name or (isinstance(node_name, str) and not node_name.strip()):
            return allow_empty, None

        # Normalize to string and strip whitespace
        normalized = str(node_name).strip()
        return True, normalized

    def create_validation_summary(
        self,
        validator: "ValidationResultCollector",
        source_identifier: str,
        operation: str = "processing",
    ) -> str:
        """Create a formatted validation summary message.

        Args:
            validator: ValidationResultCollector with results.
            source_identifier: Identifier for the data source.
            operation: Description of the operation being performed.

        Returns:
            Formatted summary message.
        """
        summary = validator.get_summary()

        if not validator.has_errors():
            return f"Successfully completed {operation} {source_identifier}"

        error_summary = (
            f"Validation errors occurred during {operation} {source_identifier}: "
        )
        error_details = "; ".join(summary["errors"][:5])  # Limit to first 5 errors

        if len(summary["errors"]) > 5:
            error_details += f" (and {len(summary['errors']) - 5} more errors)"

        return error_summary + error_details


# ===== Error Handling Decorators =====


def handle_read_errors(source_attr: str = "source") -> Callable[[F], F]:
    """Decorator to standardize error handling for readers.

    This decorator catches common exceptions during read operations and
    converts them to appropriate ReadError instances with consistent
    error messages and context.

    Args:
        source_attr: Name of the attribute containing the source identifier.
                    Defaults to "source".

    Returns:
        Decorated function that handles errors consistently.
    """

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(self: Any, source: Any, **kwargs: Any) -> Any:
            try:
                return func(self, source, **kwargs)
            except ReadError:
                raise  # Re-raise our own errors without modification
            except FileNotFoundError as e:
                raise ReadError(
                    f"File not found: {source}",
                    source=str(source),
                    reader_type=self.__class__.__name__,
                    original_error=e,
                )
            except ValueError as e:
                raise ReadError(
                    f"Invalid value encountered: {e}",
                    source=str(source),
                    reader_type=self.__class__.__name__,
                    original_error=e,
                )
            except Exception as e:
                logger.error(f"Failed to read from {source}: {e}", exc_info=True)
                raise ReadError(
                    f"Failed to process source: {e}",
                    source=str(source),
                    reader_type=self.__class__.__name__,
                    original_error=e,
                ) from e

        return wrapper  # type: ignore

    return decorator


def handle_write_errors(target_attr: str = "target") -> Callable[[F], F]:
    """Decorator to standardize error handling for writers.

    Similar to handle_read_errors but for write operations.

    Args:
        target_attr: Name of the attribute containing the target identifier.
                    Defaults to "target".

    Returns:
        Decorated function that handles errors consistently.
    """

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(self: Any, graph: Any, target: Any = None, **kwargs: Any) -> Any:
            try:
                return func(self, graph, target, **kwargs)
            except WriteError:
                raise  # Re-raise our own errors without modification
            except Exception as e:
                logger.error(f"Failed to write to {target}: {e}", exc_info=True)
                raise WriteError(
                    f"Failed to write data: {e}",
                    target=str(target) if target else "unknown",
                    writer_type=self.__class__.__name__,
                    original_error=e,
                ) from e

        return wrapper  # type: ignore

    return decorator


# ===== Reader Mixins =====


class FileBasedReader(DataReader):
    """Base class for file-based readers with common validation.

    This class provides common file validation methods and ensures
    consistent error handling for all file-based readers.

    Note: Subclasses should apply the @handle_read_errors() decorator
    to their read() method implementation for consistent error handling.
    """

    def validate_file_exists(self, path: str) -> None:
        """Validate that file exists.

        Args:
            path: Path to the file to validate.

        Raises:
            ReadError: If the file does not exist.
        """
        if not os.path.exists(path):
            raise ReadError(
                f"File not found: {path}",
                source=path,
                reader_type=self.__class__.__name__,
            )

    def validate_file_extension(
        self, path: str, valid_extensions: tuple[str, ...]
    ) -> None:
        """Validate file has correct extension.

        Args:
            path: Path to the file to validate.
            valid_extensions: Tuple of valid file extensions (e.g., ('.csv', '.txt')).

        Raises:
            ReadError: If the file extension is not valid.
        """
        if not path.lower().endswith(valid_extensions):
            raise ReadError(
                f"Invalid file extension. Expected one of {valid_extensions}, "
                f"got '{os.path.splitext(path)[1]}'",
                source=path,
                reader_type=self.__class__.__name__,
            )

    @abstractmethod
    def read(self, source: str, **kwargs: Any) -> Graph:
        """Read from file source.

        Subclasses must implement this method with their specific
        file reading logic. It's recommended to apply the @handle_read_errors()
        decorator to the implementation.

        Args:
            source: Path to the file to read.
            **kwargs: Additional reader-specific options.

        Returns:
            Graph populated with data from the file.
        """


class ConfigurationMixin:
    """Enhanced mixin for readers that use configuration objects.

    Provides comprehensive configuration management including:
    - Safe configuration value access with defaults
    - Configuration validation and type checking
    - Configuration inheritance and merging
    - Environment variable integration
    - Configuration context tracking
    """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        """Initialize configuration mixin."""
        super().__init__(*args, **kwargs)
        self._config_context: dict[str, Any] = {}
        self._config_overrides: dict[str, Any] = {}

    def set_config_context(self, **context: Any) -> None:
        """Set configuration context for enhanced error reporting.

        Args:
            **context: Key-value pairs to store as configuration context.
        """
        self._config_context.update(context)

    def get_config_context(self) -> dict[str, Any]:
        """Get current configuration context.

        Returns:
            Dictionary containing current configuration context.
        """
        return self._config_context.copy()

    def set_config_override(self, key: str, value: Any) -> None:
        """Set a configuration override for runtime customization.

        Args:
            key: Configuration key to override.
            value: Override value.
        """
        self._config_overrides[key] = value

    def clear_config_overrides(self) -> None:
        """Clear all configuration overrides."""
        self._config_overrides.clear()

    def get_config_value(
        self,
        key: str,
        default: Any = None,
        required: bool = False,
        value_type: Optional[type] = None,
        validator: Optional[Callable[[Any], bool]] = None,
    ) -> Any:
        """Get a configuration value with comprehensive validation and fallback.

        Args:
            key: Configuration key to retrieve.
            default: Default value if key is not found.
            required: Whether the configuration value is required.
            value_type: Expected type for the configuration value.
            validator: Optional validation function that takes the value and returns bool.

        Returns:
            Configuration value, override, or default.

        Raises:
            ReadError: If required value is missing or validation fails.
        """
        # Check for runtime overrides first
        if key in self._config_overrides:
            value = self._config_overrides[key]
        # Then check configuration object
        elif hasattr(self, "cfg") and self.cfg:
            value = getattr(self.cfg, key, default)
        else:
            value = default

        # Handle required values
        if required and value is None:
            raise ReadError(
                f"Required configuration value '{key}' is missing",
                reader_type=self.__class__.__name__,
            )

        # Type validation
        if (
            value is not None
            and value_type is not None
            and not isinstance(value, value_type)
        ):
            try:
                # Attempt type conversion
                value = value_type(value)
            except (ValueError, TypeError) as e:
                raise ReadError(
                    f"Configuration value '{key}' has invalid type. "
                    f"Expected {value_type.__name__}, got {type(value).__name__}",
                    reader_type=self.__class__.__name__,
                    original_error=e,
                )

        # Custom validation
        if value is not None and validator is not None:
            try:
                if not validator(value):
                    raise ReadError(
                        f"Configuration value '{key}' failed validation",
                        reader_type=self.__class__.__name__,
                    )
            except Exception as e:
                raise ReadError(
                    f"Configuration validation error for '{key}': {e}",
                    reader_type=self.__class__.__name__,
                    original_error=e,
                )

        return value

    def require_config_value(
        self,
        key: str,
        value_type: Optional[type] = None,
        validator: Optional[Callable[[Any], bool]] = None,
    ) -> Any:
        """Get a required configuration value with validation.

        Args:
            key: Configuration key to retrieve.
            value_type: Expected type for the configuration value.
            validator: Optional validation function.

        Returns:
            Configuration value.

        Raises:
            ReadError: If the configuration value is missing or invalid.
        """
        return self.get_config_value(
            key, required=True, value_type=value_type, validator=validator
        )

    def get_config_with_env_fallback(
        self,
        key: str,
        env_var: str,
        default: Any = None,
        value_type: Optional[type] = None,
    ) -> Any:
        """Get configuration value with environment variable fallback.

        Args:
            key: Configuration key to retrieve.
            env_var: Environment variable name to check as fallback.
            default: Default value if neither config nor env var is found.
            value_type: Expected type for the value.

        Returns:
            Configuration value, environment variable value, or default.
        """
        import os

        # First try configuration
        value = self.get_config_value(key)

        # If not found, try environment variable
        if value is None:
            env_value = os.getenv(env_var)
            if env_value is not None:
                value = env_value

        # Use default if still None
        if value is None:
            value = default

        # Type conversion if needed
        if (
            value is not None
            and value_type is not None
            and not isinstance(value, value_type)
        ):
            try:
                value = value_type(value)
            except (ValueError, TypeError):
                logger.warning(
                    f"Failed to convert {key} value '{value}' to {value_type.__name__}, using as-is"
                )

        return value

    def validate_configuration(self) -> "ValidationResultCollector":
        """Validate the entire configuration object.

        Returns:
            ValidationResultCollector with validation results.
        """
        validator = ValidationResultCollector(context=self._config_context)

        if not hasattr(self, "cfg") or not self.cfg:
            validator.add_result(
                "configuration", False, "Missing configuration object", "structure"
            )
            return validator

        # Validate configuration object using Pydantic if available
        try:
            if hasattr(self.cfg, "model_validate"):
                # It's a Pydantic model, validation already happened during creation
                validator.add_result(
                    "configuration", True, "Configuration object is valid", "structure"
                )
            else:
                validator.add_result(
                    "configuration",
                    True,
                    "Configuration object exists (non-Pydantic)",
                    "structure",
                )
        except Exception as e:
            validator.add_result(
                "configuration",
                False,
                f"Configuration validation failed: {e}",
                "validation",
            )

        return validator

    def get_effective_configuration(self) -> dict[str, Any]:
        """Get the effective configuration including overrides.

        Returns:
            Dictionary containing all configuration values with overrides applied.
        """
        config_dict = {}

        # Start with base configuration
        if hasattr(self, "cfg") and self.cfg:
            if hasattr(self.cfg, "model_dump"):
                # Pydantic model
                config_dict = self.cfg.model_dump()
            elif hasattr(self.cfg, "__dict__"):
                # Regular object
                config_dict = vars(self.cfg).copy()

        # Apply overrides
        config_dict.update(self._config_overrides)

        return config_dict

    def merge_configurations(self, *configs: Any) -> dict[str, Any]:
        """Merge multiple configuration objects with precedence.

        Args:
            *configs: Configuration objects to merge (later ones take precedence).

        Returns:
            Merged configuration dictionary.
        """
        merged = {}

        for config in configs:
            if config is None:
                continue

            if hasattr(config, "model_dump"):
                # Pydantic model
                config_dict = config.model_dump()
            elif hasattr(config, "__dict__"):
                # Regular object
                config_dict = vars(config).copy()
            elif isinstance(config, dict):
                # Dictionary
                config_dict = config.copy()
            else:
                logger.warning(f"Unsupported configuration type: {type(config)}")
                continue

            merged.update(config_dict)

        return merged


# ===== Writer Mixins =====


class ValueExtractionMixin:
    """Mixin for consistent value extraction from nodes.

    This mixin provides a standardized way to extract values from nodes,
    handling both calculated values and stored values with proper error
    handling.
    """

    def extract_node_value(
        self,
        node: Any,  # Avoid circular import with Node type
        period: str,
        calculate: bool = True,
    ) -> Optional[float]:
        """Extract value from node with consistent error handling.

        Args:
            node: The node to extract value from.
            period: The period to get the value for.
            calculate: If True, attempt to calculate the value using node.calculate().
                      If False, only look for stored values.

        Returns:
            The extracted value as a float, or None if no value could be extracted.
        """
        try:
            # First try calculation if enabled and method exists
            if calculate and hasattr(node, "calculate") and callable(node.calculate):
                value = node.calculate(period)
                if isinstance(value, int | float):
                    return float(value)

            # Fall back to stored values
            if hasattr(node, "values") and isinstance(node.values, dict):
                value = node.values.get(period)
                if isinstance(value, int | float):
                    return float(value)

            return None

        except Exception as e:
            logger.debug(
                f"Failed to extract value from node '{getattr(node, 'name', 'unknown')}' "
                f"for period '{period}': {e}"
            )
            return None


class DataFrameBasedWriter(ValueExtractionMixin):
    """Base class for writers that convert to DataFrame format.

    This base class provides common functionality for writers that
    need to extract data from a graph into a tabular format.

    Note: Subclasses should apply the @handle_write_errors() decorator
    to their write() method implementation for consistent error handling.
    """

    def extract_graph_data(
        self,
        graph: Graph,
        include_nodes: Optional[list[str]] = None,
        calculate: bool = True,
    ) -> dict[str, dict[str, float]]:
        """Extract data from graph nodes into a dictionary format.

        Args:
            graph: The graph to extract data from.
            include_nodes: Optional list of node names to include.
                          If None, includes all nodes.
            calculate: Whether to calculate values or just use stored values.

        Returns:
            Dictionary mapping node names to period-value dictionaries.
        """
        import numpy as np

        periods = sorted(graph.periods) if graph.periods else []
        data: dict[str, dict[str, float]] = {}

        # Determine which nodes to process
        nodes_to_process = include_nodes if include_nodes else list(graph.nodes.keys())

        # Validate requested nodes exist
        if include_nodes:
            missing_nodes = [n for n in include_nodes if n not in graph.nodes]
            if missing_nodes:
                logger.warning(f"Requested nodes not found in graph: {missing_nodes}")
                nodes_to_process = [n for n in include_nodes if n in graph.nodes]

        # Extract data for each node
        for node_id in nodes_to_process:
            node = graph.nodes[node_id]
            row: dict[str, float] = {}

            for period in periods:
                # Use the mixin's extract method for consistent value extraction
                value = self.extract_node_value(node, period, calculate=calculate)

                # Convert None to NaN for DataFrame compatibility
                if (
                    value is None
                    or not isinstance(value, int | float | np.number)
                    or not np.isfinite(value)
                ):
                    value = np.nan

                row[period] = float(value)

            data[node_id] = row

        return data

    @abstractmethod
    def write(self, graph: Graph, target: Any = None, **kwargs: Any) -> Any:
        """Write graph data to target.

        Subclasses must implement this method with their specific
        writing logic. It's recommended to apply the @handle_write_errors()
        decorator to the implementation.

        Args:
            graph: Graph containing data to write.
            target: Target for the output (file path, etc.).
            **kwargs: Additional writer-specific options.

        Returns:
            Writer-specific return value.
        """


# ===== Utility Classes =====


class ValidationResultCollector:
    """Enhanced utility class for collecting and summarizing validation results.

    Useful for batch operations where you want to collect all validation
    results and report them together. Supports categorization, context tracking,
    and detailed metrics.
    """

    def __init__(self, context: Optional[dict[str, Any]] = None):
        """Initialize the validation result collector.

        Args:
            context: Optional context information for validation.
        """
        self.results: list[
            tuple[str, bool, str, str]
        ] = []  # item, valid, message, category
        self.errors: list[str] = []
        self.warnings: list[str] = []
        self.context = context or {}
        self._categories: dict[str, int] = {}

    def add_result(
        self,
        item_name: str,
        is_valid: bool,
        message: str,
        category: str = "general",
    ) -> None:
        """Add a validation result with optional categorization.

        Args:
            item_name: Name/identifier of the item being validated.
            is_valid: Whether the validation passed.
            message: Validation message or error description.
            category: Category of validation (e.g., 'data_type', 'range', 'format').
        """
        self.results.append((item_name, is_valid, message, category))

        # Track categories
        if category not in self._categories:
            self._categories[category] = 0
        if not is_valid:
            self._categories[category] += 1

        if not is_valid:
            self.errors.append(f"{item_name}: {message}")
        elif "warning" in message.lower():
            self.warnings.append(f"{item_name}: {message}")

    def add_warning(
        self, item_name: str, message: str, category: str = "warning"
    ) -> None:
        """Add a warning (non-blocking validation issue).

        Args:
            item_name: Name/identifier of the item.
            message: Warning message.
            category: Category of the warning.
        """
        self.warnings.append(f"{item_name}: {message}")
        self.results.append((item_name, True, f"WARNING: {message}", category))

    def has_errors(self) -> bool:
        """Check if any errors were collected."""
        return len(self.errors) > 0

    def has_warnings(self) -> bool:
        """Check if any warnings were collected."""
        return len(self.warnings) > 0

    def get_error_count_by_category(self) -> dict[str, int]:
        """Get error counts grouped by category.

        Returns:
            Dictionary mapping category names to error counts.
        """
        return self._categories.copy()

    def get_items_with_errors(self) -> list[str]:
        """Get list of item names that had validation errors.

        Returns:
            List of item names with errors.
        """
        return [item for item, valid, _, _ in self.results if not valid]

    def get_summary(self) -> dict[str, Any]:
        """Get a comprehensive summary of all validation results.

        Returns:
            Dictionary containing validation metrics and summaries.
        """
        total = len(self.results)
        valid = sum(1 for _, is_valid, _, _ in self.results if is_valid)

        # Calculate category-specific metrics
        category_summary = {}
        for category, error_count in self._categories.items():
            category_total = sum(1 for _, _, _, cat in self.results if cat == category)
            category_summary[category] = {
                "total": category_total,
                "errors": error_count,
                "success_rate": (
                    (category_total - error_count) / category_total
                    if category_total > 0
                    else 1.0
                ),
            }

        return {
            "total": total,
            "valid": valid,
            "invalid": total - valid,
            "errors": self.errors.copy(),
            "warnings": self.warnings.copy(),
            "error_rate": (total - valid) / total if total > 0 else 0.0,
            "warning_count": len(self.warnings),
            "categories": category_summary,
            "context": self.context.copy(),
            "items_with_errors": self.get_items_with_errors(),
        }

    def clear(self) -> None:
        """Clear all collected results."""
        self.results.clear()
        self.errors.clear()
        self.warnings.clear()
        self._categories.clear()

    def merge(self, other: "ValidationResultCollector") -> None:
        """Merge results from another collector.

        Args:
            other: Another ValidationResultCollector to merge from.
        """
        for item, valid, message, category in other.results:
            self.add_result(item, valid, message, category)

    def get_detailed_report(self) -> str:
        """Generate a detailed text report of validation results.

        Returns:
            Formatted string report.
        """
        summary = self.get_summary()

        report_lines = [
            "=== Validation Report ===",
            f"Total items processed: {summary['total']}",
            f"Valid items: {summary['valid']}",
            f"Invalid items: {summary['invalid']}",
            f"Warnings: {summary['warning_count']}",
            f"Overall success rate: {(1 - summary['error_rate']) * 100:.1f}%",
        ]

        if summary["categories"]:
            report_lines.append("\n--- Category Breakdown ---")
            for category, stats in summary["categories"].items():
                report_lines.append(
                    f"{category}: {stats['total']} items, {stats['errors']} errors "
                    f"({stats['success_rate'] * 100:.1f}% success)"
                )

        if self.has_errors():
            report_lines.append("\n--- First 10 Errors ---")
            report_lines.extend(f"  • {error}" for error in self.errors[:10])
            if len(self.errors) > 10:
                report_lines.append(f"  ... and {len(self.errors) - 10} more errors")

        if self.has_warnings():
            report_lines.append("\n--- First 5 Warnings ---")
            report_lines.extend(f"  • {warning}" for warning in self.warnings[:5])
            if len(self.warnings) > 5:
                report_lines.append(f"  ... and {len(self.warnings) - 5} more warnings")

        return "\n".join(report_lines)



================================================================================
File: fin_statement_model/io/core/registry.py
================================================================================

"""Registry system for managing IO format handlers.

This module provides a generic registry implementation and specific registries
for readers and writers, along with registration decorators and access functions.
"""

import logging
from typing import TypeVar, Generic, Optional, Any, Union, cast, Type
from collections.abc import Callable

from pydantic import ValidationError
from pydantic import BaseModel

from fin_statement_model.io.core.base import DataReader, DataWriter
from fin_statement_model.io.exceptions import (
    FormatNotSupportedError,
    ReadError,
    WriteError,
    UnsupportedReaderError,
    UnsupportedWriterError,
)

logger = logging.getLogger(__name__)

# Type variable for the handler type (DataReader or DataWriter)
T = TypeVar("T")


# ===== Generic Registry Implementation =====


class HandlerRegistry(Generic[T]):
    """Generic registry for managing format handlers (readers or writers).

    This class provides a reusable registry pattern for registering and
    retrieving handler classes by format type.

    Attributes:
        _registry: Internal dictionary mapping format types to handler classes.
        _handler_type: String describing the handler type ('reader' or 'writer').
        _schema_map: Internal dictionary mapping format types to Pydantic schema classes.
    """

    def __init__(self, handler_type: str):
        """Initialize the registry.

        Args:
            handler_type: Type of handlers ('reader' or 'writer') for error messages.
        """
        self._registry: dict[str, type[T]] = {}
        self._handler_type = handler_type
        self._schema_map: dict[str, type[BaseModel]] = {}

    def register(
        self, format_type: str, *, schema: type[BaseModel] | None = None
    ) -> Callable[[type[T]], type[T]]:
        """Create a decorator to register a handler class for a format type.

        Args:
            format_type: The format identifier (e.g., 'excel', 'csv').
            schema: Optional Pydantic schema for the format.

        Returns:
            A decorator function that registers the class.

        Raises:
            ValueError: If the format is already registered to a different class.
        """

        def decorator(cls: type[T]) -> type[T]:
            if format_type in self._registry:
                # Allow re-registration of the same class (idempotent)
                if self._registry[format_type] is not cls:
                    raise ValueError(
                        f"{self._handler_type.capitalize()} format type '{format_type}' "
                        f"already registered to {self._registry[format_type]}."
                    )
                logger.debug(
                    f"Re-registering {self._handler_type} format type '{format_type}' "
                    f"to {cls.__name__}"
                )
            else:
                logger.debug(
                    f"Registering {self._handler_type} format type '{format_type}' "
                    f"to {cls.__name__}"
                )

            self._registry[format_type] = cls
            if schema is not None:
                self._schema_map[format_type] = schema
            return cls

        return decorator

    def get(self, format_type: str) -> type[T]:
        """Get the registered handler class for a format type.

        Args:
            format_type: The format identifier.

        Returns:
            The registered handler class.

        Raises:
            FormatNotSupportedError: If no handler is registered for the format.
        """
        if format_type not in self._registry:
            raise FormatNotSupportedError(
                format_type=format_type, operation=f"{self._handler_type} operations"
            )

        return self._registry[format_type]

    def list_formats(self) -> dict[str, type[T]]:
        """Return a copy of all registered format handlers.

        Returns:
            Dictionary mapping format types to handler classes.
        """
        return self._registry.copy()

    def is_registered(self, format_type: str) -> bool:
        """Check if a format type is registered.

        Args:
            format_type: The format identifier to check.

        Returns:
            True if the format is registered, False otherwise.
        """
        return format_type in self._registry

    def unregister(self, format_type: str) -> Optional[type[T]]:
        """Remove a format handler from the registry.

        This method is primarily useful for testing.

        Args:
            format_type: The format identifier to remove.

        Returns:
            The removed handler class, or None if not found.
        """
        return self._registry.pop(format_type, None)

    def clear(self) -> None:
        """Clear all registered handlers.

        This method is primarily useful for testing.
        """
        self._registry.clear()

    def __contains__(self, format_type: str) -> bool:
        """Check if a format type is registered using 'in' operator.

        Args:
            format_type: The format identifier to check.

        Returns:
            True if the format is registered, False otherwise.
        """
        return format_type in self._registry

    def __len__(self) -> int:
        """Return the number of registered formats.

        Returns:
            Number of registered format handlers.
        """
        return len(self._registry)

    def get_schema(self, format_type: str) -> Optional[type[BaseModel]]:
        """Return the Pydantic schema for a registered format type, if any."""
        return self._schema_map.get(format_type)


# ===== Registry Instances =====

# Create registry instances for readers and writers
_reader_registry = HandlerRegistry[DataReader]("reader")
_writer_registry = HandlerRegistry[DataWriter]("writer")


# ===== Registration Decorators =====


def register_reader(
    format_type: str,
    *,
    schema: Type[BaseModel] | None = None,
) -> Callable[[type[DataReader]], type[DataReader]]:
    """Decorator to register a DataReader class for a specific format type.

    Args:
        format_type: The string identifier for the format (e.g., 'excel', 'csv').
        schema: Optional Pydantic schema class for configuration validation.

    Returns:
        A decorator function that registers the class and returns it unmodified.

    Raises:
        ValueError: If the format_type is already registered for a reader.
    """
    # Require schema at registration time (legacy schema-less removed)
    if schema is None:
        raise ValueError(
            f"Schema required for reader '{format_type}'; legacy schema-less mode removed."
        )
    return _reader_registry.register(format_type, schema=schema)


def register_writer(
    format_type: str,
    *,
    schema: Type[BaseModel] | None = None,
) -> Callable[[type[DataWriter]], type[DataWriter]]:
    """Decorator to register a DataWriter class for a specific format type.

    Args:
        format_type: The string identifier for the format (e.g., 'excel', 'json').
        schema: Optional Pydantic schema class for configuration validation.

    Returns:
        A decorator function that registers the class and returns it unmodified.

    Raises:
        ValueError: If the format_type is already registered for a writer.
    """
    # Require schema at registration time (legacy schema-less removed)
    if schema is None:
        raise ValueError(
            f"Schema required for writer '{format_type}'; legacy schema-less mode removed."
        )
    return _writer_registry.register(format_type, schema=schema)


# ===== Generic Handler Function =====


def _get_handler(
    format_type: str,
    registry: HandlerRegistry[Any],
    handler_type: str,
    error_class: type[Union[ReadError, WriteError]],
    **kwargs: Any,
) -> Union[DataReader, DataWriter]:
    """Generic handler instantiation logic.

    This function encapsulates the common pattern for instantiating
    readers and writers, including configuration validation and error handling.

    Args:
        format_type: The format identifier (e.g., 'excel', 'csv').
        registry: The registry instance containing handler classes.
        handler_type: Either 'read' or 'write' for error messages.
        error_class: Either ReadError or WriteError class.
        **kwargs: Configuration parameters for the handler.

    Returns:
        An initialized handler instance.

    Raises:
        FormatNotSupportedError: If format_type is not in registry.
        ReadError/WriteError: If configuration validation or instantiation fails.
    """
    # Get handler class from registry (may raise FormatNotSupportedError)
    handler_class = registry.get(format_type)

    # Determine Pydantic schema from registry
    schema = registry.get_schema(format_type)
    # Enforce schema-only mode for readers (remove legacy schema-less support)
    if handler_type == "read" and schema is None:
        raise UnsupportedReaderError(
            f"Reader '{format_type}' requires a Pydantic schema; legacy schema-less mode removed.",
            source=kwargs.get("source"),
            reader_type=format_type,
        )
    # Enforce schema-only mode for writers (remove legacy schema-less support)
    if handler_type == "write" and schema is None:
        raise UnsupportedWriterError(
            f"Writer '{format_type}' requires a Pydantic schema; legacy schema-less mode removed.",
            target=kwargs.get("target"),
            writer_type=format_type,
        )

    # Prepare error context based on handler type
    error_context = {}
    if handler_type == "read":
        error_context["source"] = kwargs.get("source")
        error_context["reader_type"] = format_type
    else:  # write
        error_context["target"] = kwargs.get("target")
        error_context["writer_type"] = format_type

    if schema:
        # Validate configuration using Pydantic schema
        try:
            cfg = schema.model_validate({**kwargs, "format_type": format_type})
        except ValidationError as ve:
            raise error_class(
                message=f"Invalid {handler_type}er configuration",
                original_error=ve,
                **error_context,
            ) from ve

        # Instantiate handler with validated config
        try:
            return cast(Union[DataReader, DataWriter], handler_class(cfg))
        except Exception as e:
            logger.error(
                f"Failed to instantiate {handler_type}er for format '{format_type}' "
                f"({handler_class.__name__}): {e}",
                exc_info=True,
            )
            raise error_class(
                message=f"Failed to initialize {handler_type}er",
                original_error=e,
                **error_context,
            ) from e

    # Explicitly handle unreachable code paths
    raise RuntimeError(
        f"Unhandled handler instantiation for {handler_type} '{format_type}'"
    )


# ===== Registry Access Functions =====


def get_reader(format_type: str, **kwargs: Any) -> DataReader:
    """Get an instance of the registered DataReader for the given format type.

    Args:
        format_type: The string identifier for the format.
        **kwargs: Keyword arguments to pass to the reader's constructor.

    Returns:
        An initialized DataReader instance.

    Raises:
        FormatNotSupportedError: If no reader is registered for the format type.
        ReadError: If validation fails for known reader types.
    """
    return cast(
        DataReader,
        _get_handler(
            format_type=format_type,
            registry=_reader_registry,
            handler_type="read",
            error_class=ReadError,
            **kwargs,
        ),
    )


def get_writer(format_type: str, **kwargs: Any) -> DataWriter:
    """Get an instance of the registered DataWriter for the given format type.

    Args:
        format_type: The string identifier for the format.
        **kwargs: Keyword arguments to pass to the writer's constructor.

    Returns:
        An initialized DataWriter instance.

    Raises:
        FormatNotSupportedError: If no writer is registered for the format type.
        WriteError: If validation fails for known writer types.
    """
    return cast(
        DataWriter,
        _get_handler(
            format_type=format_type,
            registry=_writer_registry,
            handler_type="write",
            error_class=WriteError,
            **kwargs,
        ),
    )


def list_readers() -> dict[str, type[DataReader]]:
    """Return a copy of the registered reader classes."""
    return _reader_registry.list_formats()


def list_writers() -> dict[str, type[DataWriter]]:
    """Return a copy of the registered writer classes."""
    return _writer_registry.list_formats()


__all__ = [
    "HandlerRegistry",
    "get_reader",
    "get_writer",
    "list_readers",
    "list_writers",
    "register_reader",
    "register_writer",
]



================================================================================
File: fin_statement_model/io/core/utils.py
================================================================================

"""Utility functions for IO operations."""

from typing import Optional, Union, cast

# Type alias for mapping configurations
MappingConfig = Union[dict[str, str], dict[Optional[str], dict[str, str]]]


def normalize_mapping(
    mapping_config: Optional[MappingConfig] = None, context_key: Optional[str] = None
) -> dict[str, str]:
    """Turn a scoped MappingConfig into a unified flat dict with a required default mapping under None.

    Args:
        mapping_config: MappingConfig object defining name mappings.
        context_key: Optional key (e.g., sheet name or statement type) to select
            a scoped mapping within a scoped config.

    Returns:
        A flat dict mapping original names to canonical names.

    Raises:
        TypeError: If the provided mapping_config is not of a supported structure.
    """
    if mapping_config is None:
        return {}
    if not isinstance(mapping_config, dict):
        raise TypeError(
            f"mapping_config must be a dict, got {type(mapping_config).__name__}"
        )
    if None not in mapping_config:
        # Flat mapping: keys are source names
        return cast(dict[str, str], mapping_config)
    # Scoped mapping: mapping_config keys include Optional[str]
    scoped = cast(dict[Optional[str], dict[str, str]], mapping_config)
    # Default scope under None
    default_mapping = scoped[None]
    # Overlay context-specific mappings if provided
    if context_key and context_key in scoped:
        context_mapping = scoped[context_key]
        merged: dict[str, str] = {**default_mapping, **context_mapping}
        return merged
    return default_mapping


__all__ = ["MappingConfig", "normalize_mapping"]



================================================================================
File: fin_statement_model/io/exceptions.py
================================================================================

"""IO specific exceptions."""

from typing import Optional

# Use absolute import based on project structure
from fin_statement_model.core.errors import FinancialModelError


class IOError(FinancialModelError):
    """Base exception for all Input/Output errors in the IO package."""

    def __init__(
        self,
        message: str,
        source_or_target: Optional[str] = None,
        format_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the IOError.

        Args:
            message: The base error message.
            source_or_target: Optional identifier for the source (read) or target (write).
            format_type: Optional name of the format or handler involved.
            original_error: Optional underlying exception that caused the failure.
        """
        self.source_or_target = source_or_target
        self.format_type = format_type
        self.original_error = original_error

        context = []
        if source_or_target:
            context.append(f"source/target '{source_or_target}'")
        if format_type:
            context.append(f"format '{format_type}'")

        full_message = (
            f"{message} involving {' and '.join(context)}" if context else message
        )

        if original_error:
            full_message = f"{full_message}: {original_error!s}"

        super().__init__(full_message)


class ReadError(IOError):
    """Exception raised specifically for errors during data read/import operations."""

    def __init__(
        self,
        message: str,
        source: Optional[str] = None,
        reader_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the ReadError.

        Args:
            message: The base error message.
            source: Optional identifier for the data source (e.g., file path, URL).
            reader_type: Optional name of the reader class used for importing.
            original_error: Optional underlying exception that caused the import failure.
        """
        super().__init__(
            message=message,
            source_or_target=source,
            format_type=reader_type,
            original_error=original_error,
        )


class WriteError(IOError):
    """Exception raised specifically for errors during data write/export operations."""

    def __init__(
        self,
        message: str,
        target: Optional[str] = None,
        writer_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the WriteError.

        Args:
            message: The base error message.
            target: Optional identifier for the export destination (e.g., file path).
            writer_type: Optional name of the writer class being used.
            original_error: Optional underlying exception that caused the export failure.
        """
        super().__init__(
            message=message,
            source_or_target=target,
            format_type=writer_type,
            original_error=original_error,
        )


class FormatNotSupportedError(IOError):
    """Exception raised when a requested IO format is not registered or supported."""

    def __init__(self, format_type: str, operation: str = "read/write"):
        """Initializes the FormatNotSupportedError.

        Args:
            format_type: The requested format identifier (e.g., 'excel', 'json').
            operation: The operation being attempted ('read' or 'write').
        """
        message = f"Format '{format_type}' is not supported for {operation} operations."
        super().__init__(message=message, format_type=format_type)


# Add new exception for unsupported readers (schema-less mode removed)
class UnsupportedReaderError(ReadError):
    """Exception raised when a requested reader is unsupported (no schema)."""

    def __init__(
        self,
        message: str,
        source: Optional[str] = None,
        reader_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the UnsupportedReaderError."""
        super().__init__(
            message=message,
            source=source,
            reader_type=reader_type,
            original_error=original_error,
        )


# Add new exception for unsupported writers (schema-less mode removed)
class UnsupportedWriterError(WriteError):
    """Exception raised when a requested writer is unsupported (no schema)."""

    def __init__(
        self,
        message: str,
        target: Optional[str] = None,
        writer_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the UnsupportedWriterError."""
        super().__init__(
            message=message,
            target=target,
            writer_type=writer_type,
            original_error=original_error,
        )



================================================================================
File: fin_statement_model/io/formats/__init__.py
================================================================================

"""Format-specific IO implementations.

This module contains readers and writers for various data formats.
Each format is organized in its own submodule.
"""

# Import all format handlers to ensure they're registered
from .csv import CsvReader
from .dataframe import DataFrameReader, DataFrameWriter
from .dict import DictReader, DictWriter
from .excel import ExcelReader, ExcelWriter
from .api import FmpReader
from .markdown import MarkdownWriter

__all__ = [
    # CSV
    "CsvReader",
    # DataFrame
    "DataFrameReader",
    "DataFrameWriter",
    # Dict
    "DictReader",
    "DictWriter",
    # Excel
    "ExcelReader",
    "ExcelWriter",
    # API
    "FmpReader",
    # Markdown
    "MarkdownWriter",
]



================================================================================
File: fin_statement_model/io/formats/api/__init__.py
================================================================================

"""API format IO operations."""

from .fmp import FmpReader

__all__ = ["FmpReader"]



================================================================================
File: fin_statement_model/io/formats/api/fmp.py
================================================================================

"""Data reader for the Financial Modeling Prep (FMP) API."""

import logging
import requests
from typing import Optional, Any, cast
import numpy as np


from fin_statement_model.config import cfg
from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.base import DataReader
from fin_statement_model.io.core.mixins import (
    MappingAwareMixin,
    ConfigurationMixin,
)
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError

from fin_statement_model.io.config.models import FmpReaderConfig

logger = logging.getLogger(__name__)


@register_reader("fmp", schema=FmpReaderConfig)
class FmpReader(DataReader, ConfigurationMixin, MappingAwareMixin):
    """Reads financial statement data from the FMP API into a Graph.

    Fetches data for a specific ticker and statement type.
    Requires an API key, either passed directly or via the FMP_API_KEY env var.

    Supports a `mapping_config` constructor parameter for mapping API field names to canonical node names,
    accepting either a flat mapping or a statement-type keyed mapping.

    Configuration (api_key, statement_type, period_type, limit, mapping_config)
    is passed via an `FmpReaderConfig` object during initialization (typically by
    the `read_data` facade). The `.read()` method currently takes no specific
    keyword arguments beyond the `source` (ticker).

    Stateful Use:
        For advanced use cases involving repeated API calls, consider instantiating
        and reusing a single `FmpReader` instance to avoid redundant API key
        validations and improve performance.
    """

    BASE_URL = "https://financialmodelingprep.com/api/v3"

    @classmethod
    def _get_default_mapping_path(cls) -> Optional[str]:
        """Specify the default mapping file for FMP."""
        return "fmp_default_mappings.yaml"

    def __init__(self, cfg: FmpReaderConfig) -> None:
        """Initialize the FmpReader with validated configuration.

        Args:
            cfg: A validated `FmpReaderConfig` instance containing parameters like
                 `source` (ticker), `api_key`, `statement_type`, `period_type`,
                 `limit`, and `mapping_config`.
        """
        # Initialise mixins to set up configuration context and mapping caches.
        ConfigurationMixin.__init__(self)
        MappingAwareMixin.__init__(self)  # currently a no-op but future-proof

        self.cfg = cfg

    def _validate_api_key(self, api_key: str) -> None:
        """Perform a simple check if the provided API key is valid."""
        if not api_key:
            raise ReadError(
                "FMP API key is required for reading.",
                source="FMP API",
                reader_type="FmpReader",
            )
        try:
            # Use a cheap endpoint for validation
            test_url = f"{self.BASE_URL}/profile/AAPL?apikey={api_key}"  # Example
            response = requests.get(test_url, timeout=cfg("api.api_timeout"))
            response.raise_for_status()
            if not response.json():
                raise ReadError(
                    "API key validation returned empty response.",
                    source="FMP API",
                    reader_type="FmpReader",
                )
            logger.debug("FMP API key validated successfully.")
        except requests.exceptions.RequestException as e:
            logger.error(f"FMP API key validation failed: {e}", exc_info=True)
            raise ReadError(
                f"FMP API key validation failed: {e}",
                source="FMP API",
                reader_type="FmpReader",
                original_error=e,
            )

    def read(self, source: str, **kwargs: Any) -> Graph:
        """Fetch data from FMP API and return a Graph.

        Args:
            source (str): The stock ticker symbol (e.g., "AAPL").
            **kwargs: Optional runtime arguments overriding config defaults:
                statement_type (str): Type of statement to fetch ('income_statement', 'balance_sheet', 'cash_flow').
                period_type (str): Period type ('FY' or 'QTR').
                limit (int): Number of periods to fetch.
                api_key (str): API key for FMP, overrides env or config value.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If API key is missing/invalid, API request fails, or data format is unexpected.
        """
        ticker = source

        # Set configuration context for better error reporting
        self.set_config_context(ticker=ticker, operation="api_read")

        # Runtime overrides: kwargs override configuration defaults
        statement_type = kwargs.get("statement_type", self.cfg.statement_type)
        period_type_arg = kwargs.get("period_type", self.cfg.period_type)
        limit = kwargs.get("limit", self.cfg.limit)
        # Handle API key override or fallback to validated config
        api_key = (
            kwargs.get("api_key")
            if kwargs.get("api_key") is not None
            else self.cfg.api_key
        )

        # --- Validate Inputs ---
        if not ticker or not isinstance(ticker, str):
            raise ReadError(
                "Invalid source (ticker) provided. Expected a non-empty string.",
                source=ticker,
                reader_type="FmpReader",
            )
        # statement_type and period_type are validated by FmpReaderConfig

        # Validate API key (using any override)
        self._validate_api_key(cast(str, api_key))

        # Determine mapping for this operation, allowing override via kwargs
        try:
            mapping = self._get_mapping(statement_type)
        except TypeError as te:
            raise ReadError(
                "Invalid mapping_config provided.",
                source=ticker,
                reader_type="FmpReader",
                original_error=te,
            )
        logger.debug(f"Using mapping for {ticker} {statement_type}: {mapping}")

        # --- Fetch API Data ---
        # Correct endpoint construction based on FMP v3 docs
        # e.g., /income-statement/AAPL, not /income_statement-statement/AAPL
        endpoint_path = statement_type.replace("_", "-")
        endpoint = f"{self.BASE_URL}/{endpoint_path}/{ticker}"
        params = {"apikey": api_key, "limit": limit}
        if period_type_arg == "QTR":
            params["period"] = "quarter"

        try:
            logger.info(
                f"Fetching {period_type_arg} {statement_type} for {ticker} from FMP API (limit={limit})."
            )
            response = requests.get(
                endpoint, params=params, timeout=cfg("api.api_timeout")
            )
            response.raise_for_status()  # Check for HTTP errors
            api_data = response.json()

            if not isinstance(api_data, list):
                raise ReadError(
                    f"Unexpected API response format. Expected list, got {type(api_data)}. Response: {str(api_data)[:100]}...",
                    source=f"FMP API ({ticker})",
                    reader_type="FmpReader",
                )
            if not api_data:
                logger.warning(
                    f"FMP API returned empty list for {ticker} {statement_type}."
                )
                # Return empty graph or raise? Returning empty for now.
                return Graph(periods=[])

        except requests.exceptions.RequestException as e:
            logger.error(
                f"FMP API request failed for {ticker} {statement_type}: {e}",
                exc_info=True,
            )
            raise ReadError(
                f"FMP API request failed: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            )
        except Exception as e:
            logger.error(f"Failed to process FMP API response: {e}", exc_info=True)
            raise ReadError(
                f"Failed to process FMP API response: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            )

        # --- Process Data and Populate Graph ---
        try:
            # FMP data is usually newest first, reverse to process chronologically
            api_data.reverse()

            # Extract periods (e.g., 'date' or 'fillingDate')
            # Using 'date' as it usually represents the period end date
            periods = [item.get("date") for item in api_data if item.get("date")]
            if not periods:
                raise ReadError(
                    "Could not extract periods ('date' field) from FMP API response.",
                    source=f"FMP API ({ticker})",
                    reader_type="FmpReader",
                )

            graph = Graph(periods=periods)
            all_item_data: dict[str, dict[str, float]] = {}

            # Collect data for all items across all periods
            for period_data in api_data:
                period = period_data.get("date")
                if not period:
                    continue  # Skip records without a date

                for api_field, value in period_data.items():
                    node_name = self._apply_mapping(api_field, mapping)

                    # Initialize node data dict if first time seeing this node
                    if node_name not in all_item_data:
                        all_item_data[node_name] = {
                            p: np.nan for p in periods
                        }  # Pre-fill with NaN

                    # Store value for this period
                    if isinstance(value, int | float):
                        all_item_data[node_name][period] = float(value)

            # Create nodes from collected data
            nodes_added = 0
            for node_name, period_values in all_item_data.items():
                # Filter out periods that only have NaN
                valid_period_values = {
                    p: v for p, v in period_values.items() if not np.isnan(v)
                }
                if valid_period_values:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=valid_period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

            logger.info(
                f"Successfully created graph with {nodes_added} nodes from FMP API for {ticker} {statement_type}."
            )
            return graph

        except Exception as e:
            logger.error(
                f"Failed to parse FMP data and build graph: {e}", exc_info=True
            )
            raise ReadError(
                message=f"Failed to parse FMP data: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            ) from e



================================================================================
File: fin_statement_model/io/formats/csv/__init__.py
================================================================================

"""CSV format IO operations."""

from .reader import CsvReader

__all__ = ["CsvReader"]



================================================================================
File: fin_statement_model/io/formats/csv/reader.py
================================================================================

"""Data reader for CSV files."""

import logging
from typing import Any, cast

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.mixins import (
    FileBasedReader,
    ConfigurationMixin,
    MappingAwareMixin,
    ValidationMixin,
    handle_read_errors,
    ValidationResultCollector,
)
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import CsvReaderConfig

logger = logging.getLogger(__name__)


@register_reader("csv", schema=CsvReaderConfig)
class CsvReader(
    FileBasedReader, ConfigurationMixin, MappingAwareMixin, ValidationMixin
):
    """Reads financial statement data from a CSV file into a Graph.

    Assumes a 'long' format where each row represents a single data point
    (item, period, value).
    Requires specifying the columns containing item names, period identifiers,
    and values.

    Supports a `mapping_config` constructor parameter for name mapping,
    accepting either a flat mapping or a statement-type scoped mapping.

    Configuration (delimiter, header_row, index_col, mapping_config) is passed
    via a `CsvReaderConfig` object during initialization (typically by the `read_data` facade).
    Method-specific options (`item_col`, `period_col`, `value_col`, `pandas_read_csv_kwargs`)
    are passed as keyword arguments to the `read()` method.
    """

    def __init__(self, cfg: CsvReaderConfig) -> None:
        """Initialize the CsvReader with validated configuration.

        Args:
            cfg: A validated `CsvReaderConfig` instance containing parameters like
                 `source`, `delimiter`, `header_row`, `index_col`, and `mapping_config`.
        """
        self.cfg = cfg

    @handle_read_errors()
    def read(self, source: str, **kwargs: Any) -> Graph:
        """Read data from a CSV file into a new Graph.

        Args:
            source (str): Path to the CSV file.
            **kwargs: Optional runtime arguments overriding config defaults:
                statement_type (str): Statement type ('income_statement', 'balance_sheet', 'cash_flow').
                item_col (str): Name of the column containing item identifiers.
                period_col (str): Name of the column containing period identifiers.
                value_col (str): Name of the column containing numeric values.
                pandas_read_csv_kwargs (dict): Additional kwargs for pandas.read_csv.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the file cannot be read or required columns are missing.
        """
        file_path = source
        logger.info(f"Starting import from CSV file: {file_path}")

        # Set configuration context for better error reporting
        self.set_config_context(file_path=file_path, operation="read")

        # Use base class validation
        self.validate_file_exists(file_path)
        self.validate_file_extension(file_path, (".csv", ".txt"))

        # Runtime overrides: kwargs override configured defaults (statement_type handled in _process_dataframe)
        item_col = kwargs.get("item_col", self.cfg.item_col)
        period_col = kwargs.get("period_col", self.cfg.period_col)
        value_col = kwargs.get("value_col", self.cfg.value_col)
        pandas_read_csv_kwargs = (
            kwargs.get("pandas_read_csv_kwargs")
            or self.cfg.pandas_read_csv_kwargs
            or {}
        )

        if not all([item_col, period_col, value_col]):
            raise ReadError(
                "Missing required arguments: 'item_col', 'period_col', 'value_col' must be provided.",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        # Cast columns to str after validation
        item_col_str = cast(str, item_col)
        period_col_str = cast(str, period_col)
        value_col_str = cast(str, value_col)
        # Read CSV Data
        df = self._read_csv_file(file_path, pandas_read_csv_kwargs)

        # Validate columns
        self._validate_columns(
            df, item_col_str, period_col_str, value_col_str, file_path
        )

        # Process data
        return self._process_dataframe(
            df, item_col_str, period_col_str, value_col_str, file_path, kwargs
        )

    def _read_csv_file(
        self, file_path: str, user_options: dict[str, Any]
    ) -> pd.DataFrame:
        """Read CSV file with configuration options."""
        # Use configuration from self.cfg with enhanced validation
        from fin_statement_model.config.helpers import cfg

        delimiter = self.get_config_value(
            "delimiter",
            default=cfg("io.default_csv_delimiter"),
            value_type=str,
            validator=lambda x: len(x) >= 1,
        )
        header_row = self.get_config_value(
            "header_row", default=1, value_type=int, validator=lambda x: x >= 1
        )

        read_options = {
            "delimiter": delimiter,
            "header": header_row - 1,  # Convert to 0-indexed
        }

        # Handle optional index_col with validation
        index_col = self.get_config_value(
            "index_col",
            value_type=int,
            validator=lambda x: x is None or x >= 1,
        )
        if index_col is not None:
            read_options["index_col"] = index_col - 1  # Convert to 0-indexed

        # Merge user-provided kwargs, allowing them to override config
        read_options.update(user_options)

        return pd.read_csv(file_path, **read_options)

    def _validate_columns(
        self,
        df: pd.DataFrame,
        item_col: str,
        period_col: str,
        value_col: str,
        file_path: str,
    ) -> None:
        """Validate that required columns exist in the DataFrame."""
        # Use ValidationMixin for column validation
        self.validate_required_columns(df, [item_col, period_col, value_col], file_path)

    def _process_dataframe(
        self,
        df: pd.DataFrame,
        item_col: str,
        period_col: str,
        value_col: str,
        file_path: str,
        kwargs: dict[str, Any],
    ) -> Graph:
        """Process the DataFrame and create a Graph."""
        # Convert period column to string
        df[period_col] = df[period_col].astype(str)
        all_periods = sorted(df[period_col].unique().tolist())

        # Use ValidationMixin for periods validation
        self.validate_periods_exist(all_periods, file_path)

        logger.info(f"Identified periods: {all_periods}")
        graph = Graph(periods=all_periods)

        # Use validation collector for better error reporting
        validator = ValidationResultCollector()

        # Group data by item name
        grouped = df.groupby(item_col)
        nodes_added = 0

        # Determine mapping context
        context_key = kwargs.get("statement_type", self.cfg.statement_type)
        mapping = self._get_mapping(context_key)

        for item_name_csv, group in grouped:
            if pd.isna(item_name_csv) or not item_name_csv:
                logger.debug("Skipping group with empty item name.")
                continue

            item_name_csv_str = str(item_name_csv).strip()
            node_name = self._apply_mapping(item_name_csv_str, mapping)

            period_values = self._extract_period_values(
                group, period_col, value_col, item_name_csv_str, node_name, validator
            )

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' (from CSV item '{item_name_csv_str}') already exists. "
                        "Overwriting data is not standard for readers."
                    )
                else:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

        # Check for validation errors
        if validator.has_errors():
            summary = validator.get_summary()
            raise ReadError(
                f"Validation errors occurred while reading {file_path}: {'; '.join(summary['errors'])}",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        logger.info(
            f"Successfully created graph with {nodes_added} nodes from {file_path}."
        )
        return graph

    def _extract_period_values(
        self,
        group: pd.DataFrame,
        period_col: str,
        value_col: str,
        item_name_csv: str,
        node_name: str,
        validator: ValidationResultCollector,
    ) -> dict[str, float]:
        """Extract period values from a group with validation."""
        period_values: dict[str, float] = {}

        for _, row in group.iterrows():
            period = row[period_col]
            value = row[value_col]

            if pd.isna(value):
                continue  # Skip missing values

            # Use ValidationMixin for numeric validation
            is_valid, converted_value = self.validate_numeric_value(
                value, item_name_csv, period, validator, allow_conversion=True
            )

            if not is_valid or converted_value is None:
                continue

            value = converted_value

            if period in period_values:
                logger.warning(
                    f"Duplicate value found for node '{node_name}' "
                    f"(from CSV item '{item_name_csv}') period '{period}'. "
                    "Using the last one found."
                )

            period_values[period] = float(value)

        return period_values



================================================================================
File: fin_statement_model/io/formats/dataframe/__init__.py
================================================================================

"""DataFrame format IO operations."""

from .reader import DataFrameReader
from .writer import DataFrameWriter

__all__ = ["DataFrameReader", "DataFrameWriter"]



================================================================================
File: fin_statement_model/io/formats/dataframe/reader.py
================================================================================

"""Data reader for pandas DataFrames."""

import logging
import pandas as pd
import numpy as np
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.base import DataReader
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import DataFrameReaderConfig

logger = logging.getLogger(__name__)


@register_reader("dataframe", schema=DataFrameReaderConfig)
class DataFrameReader(DataReader):
    """Reads data from a pandas DataFrame into a Graph.

    Assumes the DataFrame index contains node names and columns contain periods.
    Values should be numeric.
    """

    def __init__(self, cfg: Optional[DataFrameReaderConfig] = None) -> None:
        """Initialize the DataFrameReader.

        Args:
            cfg: Optional validated `DataFrameReaderConfig` instance.
                 Currently unused but kept for registry symmetry and future options.
        """
        self.cfg = cfg  # For future use; currently no configuration options.

    def read(self, source: pd.DataFrame, **kwargs: Any) -> Graph:
        """Read data from a pandas DataFrame into a new Graph.

        Assumes DataFrame index = node names, columns = periods.

        Args:
            source (pd.DataFrame): The DataFrame to read data from.
            **kwargs: Optional runtime argument overriding config defaults:
                periods (list[str], optional): List of periods (columns) to include. Overrides `cfg.periods`.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the source is not a DataFrame or has invalid structure.
        """
        df = source
        logger.info("Starting import from DataFrame.")

        # --- Validate Inputs ---
        if not isinstance(df, pd.DataFrame):
            raise ReadError(
                "Source is not a pandas DataFrame.",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        if df.index.name is None and df.index.empty:
            logger.warning(
                "DataFrame index is unnamed and empty, assuming columns are nodes if periods kwarg is provided."
            )
            # Handle case where DF might be oriented differently if periods kwarg is present?
            # For now, stick to index=nodes assumption.

        # Determine periods: runtime kwargs override config, else config defaults, else infer
        graph_periods_arg = kwargs.get(
            "periods", self.cfg.periods if self.cfg else None
        )
        if graph_periods_arg:
            if not isinstance(graph_periods_arg, list):
                raise ReadError("'periods' argument must be a list of column names.")
            missing_cols = [p for p in graph_periods_arg if p not in df.columns]
            if missing_cols:
                raise ReadError(
                    f"Specified periods (columns) not found in DataFrame: {missing_cols}"
                )
            graph_periods = sorted(graph_periods_arg)
            df_subset = df[graph_periods]  # Select only specified period columns
        else:
            # No explicit periods provided; infer from columns
            graph_periods = sorted(df.columns.astype(str).tolist())
            df_subset = df

        if not graph_periods:
            raise ReadError(
                "No periods identified in DataFrame columns.",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        logger.info(f"Using periods (columns): {graph_periods}")
        graph = Graph(periods=graph_periods)

        # --- Populate Graph ---
        validation_errors = []
        nodes_added = 0
        for node_name_df, row in df_subset.iterrows():
            if pd.isna(node_name_df) or not node_name_df:
                logger.debug("Skipping row with empty index name.")
                continue

            node_name = str(node_name_df).strip()
            period_values: dict[str, float] = {}
            for period in graph_periods:
                value = row[period]
                if pd.isna(value):
                    continue  # Skip NaN values

                if not isinstance(value, int | float | np.number):
                    try:
                        value = float(value)
                        logger.warning(
                            f"Converted non-numeric value '{row[period]}' to float for node '{node_name}' period '{period}'"
                        )
                    except (ValueError, TypeError):
                        validation_errors.append(
                            f"Node '{node_name}': Non-numeric value '{value}' for period '{period}'"
                        )
                        continue  # Skip invalid value

                period_values[period] = float(value)

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' already exists. Overwriting data is not standard for readers."
                    )
                    # Update existing? Log for now.
                else:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

        if validation_errors:
            raise ReadError(
                f"Validation errors occurred while reading DataFrame: {'; '.join(validation_errors)}",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        logger.info(
            f"Successfully created graph with {nodes_added} nodes from DataFrame."
        )
        return graph

        # No specific file operations, so less need for broad Exception catch
        # Specific errors handled above (TypeError, ValueError from float conversion)



================================================================================
File: fin_statement_model/io/formats/dataframe/writer.py
================================================================================

"""Data writer for pandas DataFrames."""

import logging
from typing import Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.mixins import (
    DataFrameBasedWriter,
    ConfigurationMixin,
    handle_write_errors,
)
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.io.config.models import DataFrameWriterConfig

logger = logging.getLogger(__name__)


@register_writer("dataframe", schema=DataFrameWriterConfig)  # type: ignore[arg-type]
class DataFrameWriter(DataFrameBasedWriter, ConfigurationMixin):
    """Writes graph data to a pandas DataFrame.

    Converts the graph to a DataFrame with node names as index and periods as columns.

    Configuration options `recalculate` and `include_nodes` are controlled by
    the `DataFrameWriterConfig` object passed during initialization.
    """

    def __init__(self, cfg: DataFrameWriterConfig) -> None:
        """Initialize the DataFrameWriter.

        Args:
            cfg: Optional validated `DataFrameWriterConfig` instance.
        """
        super().__init__()
        self.cfg = cfg

    @handle_write_errors()
    def write(self, graph: Graph, target: Any = None, **kwargs: Any) -> pd.DataFrame:
        """Convert the graph data to a pandas DataFrame based on instance configuration.

        Args:
            graph (Graph): The Graph instance to export.
            target (Any): Ignored by this writer; the DataFrame is returned directly.
            **kwargs: Optional runtime overrides of configured defaults:
                recalculate (bool): Whether to recalculate graph before export.
                include_nodes (list[str]): List of node names to include in export.

        Returns:
            pd.DataFrame: DataFrame with node names as index and periods as columns.

        Raises:
            WriteError: If an error occurs during conversion.
        """
        # Runtime overrides: kwargs override configured defaults
        recalculate = kwargs.get("recalculate", self.cfg.recalculate)
        include_nodes = kwargs.get("include_nodes", self.cfg.include_nodes)

        logger.info("Exporting graph to DataFrame format.")

        # Handle recalculation if requested
        if recalculate:
            self._recalculate_graph(graph)

        # Extract data using base class method
        data = self.extract_graph_data(
            graph, include_nodes=include_nodes, calculate=True
        )

        # Convert to DataFrame
        periods = sorted(graph.periods) if graph.periods else []
        df = pd.DataFrame.from_dict(data, orient="index", columns=periods)
        df.index.name = "node_name"

        logger.info(f"Successfully exported {len(df)} nodes to DataFrame.")
        return df

    def _recalculate_graph(self, graph: Graph) -> None:
        """Recalculate the graph if it has periods defined.

        Args:
            graph: The graph to recalculate.
        """
        try:
            if graph.periods:
                graph.recalculate_all(periods=graph.periods)
                logger.info("Recalculated graph before exporting to DataFrame.")
            else:
                logger.warning("Graph has no periods defined, skipping recalculation.")
        except Exception as e:
            logger.error(
                f"Error during recalculation for DataFrame export: {e}",
                exc_info=True,
            )
            logger.warning(
                "Proceeding to export DataFrame without successful recalculation."
            )



================================================================================
File: fin_statement_model/io/formats/dict/__init__.py
================================================================================

"""Dictionary format IO operations."""

from .reader import DictReader
from .writer import DictWriter

__all__ = ["DictReader", "DictWriter"]



================================================================================
File: fin_statement_model/io/formats/dict/reader.py
================================================================================

"""Data reader for Python dictionaries."""

import logging
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.base import DataReader
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import DictReaderConfig

logger = logging.getLogger(__name__)


@register_reader("dict", schema=DictReaderConfig)
class DictReader(DataReader):
    """Reads data from a Python dictionary to create a new Graph.

    Expects a dictionary format: {node_name: {period: value, ...}, ...}
    Creates FinancialStatementItemNode instances for each entry.

    Note:
        Configuration is handled via `DictReaderConfig` during initialization.
        The `read()` method takes the source dictionary directly and an optional
        `periods` keyword argument.
    """

    def __init__(self, cfg: Optional[DictReaderConfig] = None) -> None:
        """Initialize the DictReader.

        Args:
            cfg: Optional validated `DictReaderConfig` instance.
                 Currently unused but kept for registry symmetry and future options.
        """
        self.cfg = cfg

    def read(self, source: dict[str, dict[str, float]], **kwargs: Any) -> Graph:
        """Create a new Graph from a dictionary.

        Args:
            source: Dictionary mapping node names to period-value dictionaries.
                    Format: {node_name: {period: value, ...}, ...}
            **kwargs: Optional runtime argument overriding config defaults:
                periods (list[str], optional): List of periods to include. Overrides `cfg.periods`.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the source data format is invalid or processing fails.
            # DataValidationError: If data values are not numeric.
        """
        logger.info("Starting import from dictionary to create a new graph.")

        if not isinstance(source, dict):
            raise ReadError(
                message="Invalid source type for DictReader. Expected dict.",
                source="dict_input",
                reader_type="DictReader",
            )

        # Validate data structure and collect all periods
        all_periods = set()
        validation_errors: list[str] = []
        try:
            for node_name, period_values in source.items():
                if not isinstance(period_values, dict):
                    validation_errors.append(
                        f"Node '{node_name}': Invalid format - expected dict, got {type(period_values).__name__}"
                    )
                    continue  # Skip further checks for this node
                for period, value in period_values.items():
                    # Basic type checks - can be expanded
                    if not isinstance(period, str):
                        validation_errors.append(
                            f"Node '{node_name}': Invalid period format '{period}' - expected string."
                        )
                    if not isinstance(value, int | float):
                        validation_errors.append(
                            f"Node '{node_name}' period '{period}': Invalid value type {type(value).__name__} - expected number."
                        )
                    all_periods.add(str(period))

            if validation_errors:
                # Use core DataValidationError if it exists and is suitable
                # Otherwise, stick to ReadError or a specific IOValidationError
                # raise DataValidationError(
                #     message="Input dictionary failed validation",
                #     validation_errors=validation_errors
                # )
                raise ReadError(
                    f"Input dictionary failed validation: {'; '.join(validation_errors)}",
                    source="dict_input",
                    reader_type="DictReader",
                )

        except Exception as e:
            # Catch unexpected validation errors
            raise ReadError(
                message=f"Error validating input dictionary: {e}",
                source="dict_input",
                reader_type="DictReader",
                original_error=e,
            ) from e

        # Determine graph periods: runtime kwargs override config defaults
        graph_periods = kwargs.get("periods", self.cfg.periods if self.cfg else None)
        if graph_periods is None:
            graph_periods = sorted(list(all_periods))
            logger.debug(f"Inferred graph periods from data: {graph_periods}")
        # Optional: Validate if all data periods are within the provided list
        elif not all_periods.issubset(set(graph_periods)):
            missing = all_periods - set(graph_periods)
            logger.warning(
                f"Data contains periods not in specified graph periods: {missing}"
            )
            # Decide whether to error or just ignore extra data

        # Create graph and add nodes
        try:
            graph = Graph(periods=graph_periods)
            for node_name, period_values in source.items():
                # Filter values to only include those matching graph_periods
                filtered_values = {
                    p: v for p, v in period_values.items() if p in graph_periods
                }
                if filtered_values:
                    # Create FinancialStatementItemNode directly
                    # Assumes FinancialStatementItemNode takes name and values dict
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=filtered_values.copy()
                    )
                    graph.add_node(new_node)
                else:
                    logger.debug(
                        f"Node '{node_name}' has no data for specified graph periods. Skipping."
                    )

            logger.info(
                f"Successfully created graph with {len(graph.nodes)} nodes from dictionary."
            )
            return graph

        except Exception as e:
            # Catch errors during graph/node creation
            logger.error(f"Failed to create graph from dictionary: {e}", exc_info=True)
            raise ReadError(
                message="Failed to build graph from dictionary data",
                source="dict_input",
                reader_type="DictReader",
                original_error=e,
            ) from e



================================================================================
File: fin_statement_model/io/formats/dict/writer.py
================================================================================

"""Data writer for Python dictionaries."""

import logging
from typing import Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.mixins import (
    DataFrameBasedWriter,
    ConfigurationMixin,
    handle_write_errors,
)
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.io.config.models import DictWriterConfig

logger = logging.getLogger(__name__)


@register_writer("dict", schema=DictWriterConfig)  # type: ignore[arg-type]
class DictWriter(DataFrameBasedWriter, ConfigurationMixin):
    """Writes graph data to a Python dictionary.

    Extracts values for each node and period in the graph, attempting to
    calculate values where possible.

    Initialized via `DictWriterConfig` (typically by the `write_data` facade),
    although the config currently has no options.
    """

    def __init__(self, cfg: DictWriterConfig) -> None:
        """Initialize the DictWriter.

        Args:
            cfg: Validated `DictWriterConfig` instance.
        """
        super().__init__()
        self.cfg = cfg

    @handle_write_errors()
    def write(
        self, graph: Graph, target: Any = None, **kwargs: dict[str, Any]
    ) -> dict[str, dict[str, float]]:
        """Export calculated data from all graph nodes to a dictionary.

        Args:
            graph (Graph): The Graph instance to export data from.
            target (Any): Ignored by this writer; the dictionary is returned directly.
            **kwargs: Currently unused.

        Returns:
            Dict[str, Dict[str, float]]: Mapping node names to period-value dicts.
                                         Includes values for all nodes in the graph
                                         for all defined periods. NaN represents
                                         uncalculable values.

        Raises:
            WriteError: If an unexpected error occurs during export.
        """
        logger.info(f"Starting export of graph '{graph}' to dictionary format.")

        if not graph.periods:
            logger.warning(
                "Graph has no periods defined. Exported dictionary will be empty."
            )
            return {}

        # Use base class method to extract all data
        # This handles calculation attempts and error handling consistently
        result = self.extract_graph_data(graph, include_nodes=None, calculate=True)

        logger.info(f"Successfully exported {len(result)} nodes to dictionary.")
        return result



================================================================================
File: fin_statement_model/io/formats/excel/__init__.py
================================================================================

"""Excel format IO operations."""

from .reader import ExcelReader
from .writer import ExcelWriter

__all__ = ["ExcelReader", "ExcelWriter"]



================================================================================
File: fin_statement_model/io/formats/excel/reader.py
================================================================================

"""Data reader for Excel files."""

import logging
from typing import Optional, Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.mixins import (
    FileBasedReader,
    ConfigurationMixin,
    MappingAwareMixin,
    ValidationMixin,
    handle_read_errors,
    ValidationResultCollector,
)
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError

from fin_statement_model.io.config.models import ExcelReaderConfig

logger = logging.getLogger(__name__)


@register_reader("excel", schema=ExcelReaderConfig)
class ExcelReader(
    FileBasedReader, ConfigurationMixin, MappingAwareMixin, ValidationMixin
):
    """Reads financial statement data from an Excel file into a Graph.

    Expects data in a tabular format where rows typically represent items
    and columns represent periods, or vice-versa.
    Requires specifying sheet name, period identification, and item identification.

    Configuration (sheet_name, items_col, periods_row, mapping_config) is passed
    via an `ExcelReaderConfig` object during initialization (typically by the `read_data` facade).
    Method-specific options (`statement_type`, `header_row`, `nrows`, `skiprows`)
    are passed as keyword arguments to the `read()` method.
    """

    def __init__(self, cfg: ExcelReaderConfig) -> None:
        """Initialize the ExcelReader with validated configuration.

        Args:
            cfg: A validated `ExcelReaderConfig` instance containing parameters like
                 `source`, `sheet_name`, `items_col`, `periods_row`, and `mapping_config`.
        """
        self.cfg = cfg

    @handle_read_errors()
    def read(self, source: str, **kwargs: Any) -> Graph:
        """Read data from an Excel file sheet into a new Graph based on instance config.

        Args:
            source (str): Path to the Excel file.
            **kwargs: Optional runtime keyword arguments overriding configured defaults:
                statement_type (str): Type of statement ('income_statement', 'balance_sheet', 'cash_flow').
                header_row (int): 1-based index for pandas header reading.
                nrows (int): Number of rows to read from the sheet.
                skiprows (int): Number of rows to skip at the beginning.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the file cannot be read or the configuration is invalid.
        """
        file_path = source
        logger.info(f"Starting import from Excel file: {file_path}")

        # Use base class validation
        self.validate_file_exists(file_path)
        self.validate_file_extension(file_path, (".xls", ".xlsx", ".xlsm"))

        # Set configuration context for better error reporting
        self.set_config_context(file_path=file_path, operation="read")

        # Get configuration values with validation
        sheet_name = self.require_config_value("sheet_name", value_type=str)
        periods_row = self.require_config_value(
            "periods_row", value_type=int, validator=lambda x: x >= 1
        )
        items_col = self.require_config_value(
            "items_col", value_type=int, validator=lambda x: x >= 1
        )

        # Runtime options: kwargs override config defaults
        statement_type = kwargs.get("statement_type", self.cfg.statement_type)
        header_row = kwargs.get("header_row", self.cfg.header_row or periods_row)
        nrows = kwargs.get("nrows", self.cfg.nrows)
        skiprows = kwargs.get("skiprows", self.cfg.skiprows)

        # Get mapping
        mapping = self._get_mapping(statement_type)
        logger.debug(f"Using mapping for statement type '{statement_type}': {mapping}")

        # Read Excel data
        df, period_headers = self._read_excel_data(
            file_path, sheet_name, periods_row, items_col, header_row, nrows, skiprows
        )

        # Extract periods
        graph_periods = self._extract_periods(period_headers, items_col)

        # Create and populate graph
        return self._create_graph(
            df, graph_periods, items_col, mapping, file_path, sheet_name
        )

    def _read_excel_data(
        self,
        file_path: str,
        sheet_name: str,
        periods_row: int,
        items_col: int,
        header_row: int,
        nrows: Optional[int],
        skiprows: Optional[int],
    ) -> tuple[pd.DataFrame, list[str]]:
        """Read Excel file and extract data and period headers."""
        # Convert to 0-based indices for pandas
        periods_row_0idx = periods_row - 1
        items_col_0idx = items_col - 1
        header_row_0idx = header_row - 1

        # Read the main data
        df = pd.read_excel(
            file_path,
            sheet_name=sheet_name,
            header=header_row_0idx,
            skiprows=skiprows,
            nrows=nrows,
        )

        # Get period headers
        if header_row_0idx != periods_row_0idx:
            # Read periods row separately if different from header
            periods_df = pd.read_excel(
                file_path,
                sheet_name=sheet_name,
                header=None,
                skiprows=periods_row_0idx,
                nrows=1,
            )
            period_headers = periods_df.iloc[0].astype(str).tolist()
        else:
            # Periods are in the main header row
            period_headers = df.columns.astype(str).tolist()

        # Validate items column index using ValidationMixin
        self.validate_column_bounds(
            df, items_col_0idx, file_path, f"items_col ({items_col})"
        )

        return df, period_headers

    def _extract_periods(self, period_headers: list[str], items_col: int) -> list[str]:
        """Extract valid period names from headers."""
        items_col_0idx = items_col - 1

        # Filter period headers: exclude the item column and empty values
        graph_periods = [
            p
            for i, p in enumerate(period_headers)
            if i > items_col_0idx and p and p.strip()
        ]

        # Validate periods using ValidationMixin
        self.validate_periods_exist(graph_periods, "Excel file")

        logger.info(f"Identified periods: {graph_periods}")
        return graph_periods

    def _create_graph(
        self,
        df: pd.DataFrame,
        graph_periods: list[str],
        items_col: int,
        mapping: dict[str, str],
        file_path: str,
        sheet_name: str,
    ) -> Graph:
        """Create and populate the graph from DataFrame."""
        items_col_0idx = items_col - 1
        graph = Graph(periods=graph_periods)

        # Use validation collector
        validator = ValidationResultCollector()
        nodes_added = 0

        for index, row in df.iterrows():
            # Get item name
            item_name_excel = row.iloc[items_col_0idx]
            if pd.isna(item_name_excel) or not item_name_excel:
                continue

            item_name_excel = str(item_name_excel).strip()
            node_name = self._apply_mapping(item_name_excel, mapping)

            # Extract values for all periods
            period_values = self._extract_row_values(
                row, df, graph_periods, node_name, item_name_excel, index, validator
            )

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' (from Excel item '{item_name_excel}') already exists. "
                        "Overwriting data is not standard for readers."
                    )
                else:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

        # Check for validation errors
        if validator.has_errors():
            summary = validator.get_summary()
            raise ReadError(
                f"Validation errors occurred while reading {file_path} sheet '{sheet_name}': "
                f"{'; '.join(summary['errors'])}",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        logger.info(
            f"Successfully created graph with {nodes_added} nodes from {file_path} sheet '{sheet_name}'."
        )
        return graph

    def _extract_row_values(
        self,
        row: pd.Series,
        df: pd.DataFrame,
        graph_periods: list[str],
        node_name: str,
        item_name_excel: str,
        row_index: int,
        validator: ValidationResultCollector,
    ) -> dict[str, float]:
        """Extract period values from a row with validation."""
        period_values: dict[str, float] = {}

        for period in graph_periods:
            if period not in df.columns:
                logger.warning(
                    f"Period header '{period}' not found in DataFrame columns for row {row_index}."
                )
                continue

            value = row[period]

            if pd.isna(value):
                continue  # Skip NaN values

            # Use ValidationMixin for numeric validation
            is_valid, converted_value = self.validate_numeric_value(
                value, node_name, period, validator, allow_conversion=True
            )

            if is_valid and converted_value is not None:
                period_values[period] = converted_value

        return period_values



================================================================================
File: fin_statement_model/io/formats/excel/writer.py
================================================================================

"""Data writer for Excel files."""

import logging
from pathlib import Path
from typing import Any, Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.base import DataWriter
from fin_statement_model.io.core.mixins import (
    ConfigurationMixin,
    handle_write_errors,
)
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.io.formats.dataframe.writer import DataFrameWriter
from fin_statement_model.io.config.models import (
    ExcelWriterConfig,
    DataFrameWriterConfig,
)

logger = logging.getLogger(__name__)


@register_writer("excel", schema=ExcelWriterConfig)
class ExcelWriter(DataWriter, ConfigurationMixin):
    """Writes graph data to an Excel file.

    Converts the graph data to a pandas DataFrame first (using `DataFrameWriter`),
    then writes that DataFrame to an Excel file using `pandas.to_excel()`.

    Configuration (sheet_name, recalculate, include_nodes, excel_writer_kwargs) is
    provided via an `ExcelWriterConfig` object during initialization.
    """

    def __init__(self, cfg: ExcelWriterConfig) -> None:
        """Initialize the ExcelWriter.

        Args:
            cfg: Non-optional validated `ExcelWriterConfig` instance.
        """
        super().__init__()
        self.cfg = cfg

    @handle_write_errors()
    def write(self, graph: Graph, target: str, **kwargs: Any) -> None:
        """Write graph data to an Excel file, converting via DataFrame first.

        Args:
            graph (Graph): The Graph object containing the data to write.
            target (str): Path to the target Excel file.
            **kwargs: Optional runtime overrides of configured defaults:
                sheet_name (str): Excel sheet name.
                recalculate (bool): Whether to recalculate graph before export.
                include_nodes (list[str]): List of node names to include in export.
                excel_writer_kwargs (dict): Additional kwargs for pandas.DataFrame.to_excel.

        Raises:
            WriteError: If an error occurs during the writing process.
        """
        file_path = target

        # Runtime overrides: kwargs override configured defaults
        sheet_name = kwargs.get("sheet_name", self.cfg.sheet_name)
        recalculate = kwargs.get("recalculate", self.cfg.recalculate)
        include_nodes = kwargs.get("include_nodes", self.cfg.include_nodes)
        excel_writer_options = kwargs.get(
            "excel_writer_kwargs", self.cfg.excel_writer_kwargs
        )

        logger.info(f"Exporting graph to Excel file: {file_path}, sheet: {sheet_name}")

        # Convert graph to DataFrame
        df = self._create_dataframe(graph, recalculate, include_nodes)

        # Write DataFrame to Excel
        self._write_to_excel(df, file_path, sheet_name, excel_writer_options)

        logger.info(f"Successfully exported graph to {file_path}, sheet '{sheet_name}'")

    def _create_dataframe(
        self, graph: Graph, recalculate: bool, include_nodes: Optional[list[str]]
    ) -> Any:
        """Convert graph to DataFrame using DataFrameWriter.

        Args:
            graph: The graph to convert.
            recalculate: Whether to recalculate before export.
            include_nodes: Optional list of nodes to include.

        Returns:
            pandas DataFrame with the graph data.
        """
        # Create a config for DataFrameWriter
        df_config = DataFrameWriterConfig(
            target=None,
            format_type="dataframe",
            recalculate=recalculate,
            include_nodes=include_nodes,
        )

        df_writer = DataFrameWriter(df_config)
        return df_writer.write(graph=graph, target=None)

    def _write_to_excel(
        self,
        df: Any,
        file_path: str,
        sheet_name: str,
        excel_writer_options: dict[str, Any],
    ) -> None:
        """Write DataFrame to Excel file.

        Args:
            df: The pandas DataFrame to write.
            file_path: Path to the output file.
            sheet_name: Name of the Excel sheet.
            excel_writer_options: Additional options for pandas.to_excel().
        """
        output_path = Path(file_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        df.to_excel(
            output_path,
            sheet_name=sheet_name,
            index=True,  # Keep node names as index column
            **excel_writer_options,
        )



================================================================================
File: fin_statement_model/io/formats/markdown/__init__.py
================================================================================

"""Markdown format IO operations."""

from .writer import MarkdownWriter
from .renderer import MarkdownStatementRenderer
from .formatter import MarkdownTableFormatter
from .notes import MarkdownNotesBuilder
from .models import MarkdownStatementItem

__all__ = [
    "MarkdownNotesBuilder",
    "MarkdownStatementItem",
    "MarkdownStatementRenderer",
    "MarkdownTableFormatter",
    "MarkdownWriter",
]



================================================================================
File: fin_statement_model/io/formats/markdown/formatter.py
================================================================================

"""Formatter for converting statement items to markdown table format."""

import logging
from typing import Any, Optional, Union

from fin_statement_model.io.formats.markdown.models import MarkdownStatementItem

logger = logging.getLogger(__name__)


class MarkdownTableFormatter:
    """Formats statement items into markdown table format.

    This class handles the table layout, column width calculations,
    and markdown-specific formatting like bold text for subtotals.
    """

    def __init__(self, indent_spaces: int = 4):
        """Initialize the formatter.

        Args:
            indent_spaces: Number of spaces per indentation level.
        """
        self.indent_spaces = indent_spaces

    def format_table(
        self,
        items: list[MarkdownStatementItem],
        periods: list[str],
        historical_periods: Optional[list[str]] = None,
        forecast_periods: Optional[list[str]] = None,
    ) -> list[str]:
        """Format items into markdown table lines.

        Args:
            items: List of MarkdownStatementItem objects to format.
            periods: List of all periods in order.
            historical_periods: List of historical period names.
            forecast_periods: List of forecast period names.

        Returns:
            List of strings representing the markdown table lines.
        """
        if not items:
            logger.warning("No items to format into table")
            return []

        # Convert to sets for faster lookup
        historical_set = set(historical_periods or [])
        forecast_set = set(forecast_periods or [])

        # Calculate column widths and format data
        max_desc_width, period_max_widths, formatted_lines = (
            self._calculate_widths_and_format(items, periods)
        )

        # Build the table
        output_lines = []

        # Build header row
        header_parts = ["Description".ljust(max_desc_width)]
        for period in periods:
            period_label = period
            if period in historical_set:
                period_label += " (H)"
            elif period in forecast_set:
                period_label += " (F)"
            header_parts.append(period_label.rjust(period_max_widths[period]))

        output_lines.append(f"| {' | '.join(header_parts)} |")

        # Add separator line
        separator_parts = ["-" * max_desc_width]
        separator_parts.extend("-" * period_max_widths[period] for period in periods)
        output_lines.append(f"| {' | '.join(separator_parts)} |")

        # Build data rows
        for line_data in formatted_lines:
            row_parts = [line_data["name"].ljust(max_desc_width)]
            for period in periods:
                value = line_data["values"].get(period, "")
                row_parts.append(value.rjust(period_max_widths[period]))
            output_lines.append(f"| {' | '.join(row_parts)} |")

        return output_lines

    def _calculate_widths_and_format(
        self, items: list[MarkdownStatementItem], periods: list[str]
    ) -> tuple[int, dict[str, int], list[dict[str, Any]]]:
        """Calculate column widths and format all data.

        Args:
            items: List of items to format.
            periods: List of periods.

        Returns:
            Tuple of (max_desc_width, period_max_widths, formatted_lines).
        """
        max_desc_width = 0
        period_max_widths = {p: 0 for p in periods}
        formatted_lines = []

        # First pass: format data and calculate max widths
        for item in items:
            indent = " " * (item["level"] * self.indent_spaces)
            name = f"{indent}{item['name']}"
            is_subtotal = item["is_subtotal"]
            is_contra = item.get("is_contra", False)
            values_formatted = {}

            # Apply markdown formatting for subtotals
            if is_subtotal:
                name = f"**{name}**"

            # Apply contra formatting if needed
            if is_contra:
                name = f"_{name}_"  # Italic for contra items

            max_desc_width = max(max_desc_width, len(name))

            # Format values for each period
            for period in periods:
                raw_value = item["values"].get(period)
                value_str = self._format_value(raw_value, item)

                # Apply markdown formatting for subtotals
                if is_subtotal:
                    value_str = f"**{value_str}**"
                elif is_contra:
                    value_str = f"_{value_str}_"  # Italic for contra items

                values_formatted[period] = value_str
                period_max_widths[period] = max(
                    period_max_widths[period], len(value_str)
                )

            formatted_lines.append(
                {
                    "name": name,
                    "values": values_formatted,
                    "is_subtotal": is_subtotal,
                    "is_contra": is_contra,
                }
            )

        return max_desc_width, period_max_widths, formatted_lines

    def _format_value(
        self, value: Union[float, int, str, None], item: MarkdownStatementItem
    ) -> str:
        """Format a single value for display in the table.

        Args:
            value: The value to format.
            item: The item containing formatting information.

        Returns:
            Formatted string representation of the value.
        """
        if value is None:
            return ""

        if isinstance(value, str):
            # Keep error strings and other text as-is
            return value

        if isinstance(value, float | int):
            # Use custom format if specified
            display_format = item.get("display_format")
            if display_format:
                try:
                    return format(value, display_format)
                except (ValueError, TypeError) as e:
                    logger.warning(f"Invalid display format '{display_format}': {e}")
                    # Fall back to default formatting

            # Default number formatting
            if isinstance(value, float):
                return f"{value:,.2f}"
            else:
                return f"{value:,}"

        # Fallback for any other type
        return str(value)



================================================================================
File: fin_statement_model/io/formats/markdown/models.py
================================================================================

"""Data models for markdown formatting."""

from typing import Optional, TypedDict, Union


class MarkdownStatementItem(TypedDict):
    """Enhanced representation of a line item for Markdown output.

    This extends the basic StatementItem concept from the old implementation
    with additional formatting and display properties.
    """

    name: str
    values: dict[str, Union[float, int, str, None]]  # Values per period
    level: int
    is_subtotal: bool  # Indicates if the row is a subtotal or section header
    sign_convention: int  # 1 for normal, -1 for inverted
    display_format: Optional[str]  # Optional number format string
    units: Optional[str]  # Unit description
    display_scale_factor: float  # Factor to scale values for display
    is_contra: bool  # Whether this is a contra item for special formatting



================================================================================
File: fin_statement_model/io/formats/markdown/notes.py
================================================================================

"""Builder for markdown notes sections (forecast and adjustment notes)."""

import logging
from typing import Any, Optional

from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentFilter,
    DEFAULT_SCENARIO,
)
from fin_statement_model.core.graph import Graph

logger = logging.getLogger(__name__)


class MarkdownNotesBuilder:
    """Builds notes sections for markdown output.

    This class handles the creation of forecast notes and adjustment notes
    that are appended to the main statement table.
    """

    def build_notes(
        self,
        graph: Graph,
        forecast_configs: Optional[dict[str, Any]] = None,
        adjustment_filter: Optional[Any] = None,
    ) -> list[str]:
        """Build forecast and adjustment notes.

        Args:
            graph: The Graph object containing financial data.
            forecast_configs: Dictionary mapping node IDs to forecast configurations.
            adjustment_filter: Filter for adjustments to include.

        Returns:
            List of strings representing the notes sections.
        """
        lines = []

        # Add forecast notes
        if forecast_configs:
            forecast_lines = self._build_forecast_notes(forecast_configs)
            if forecast_lines:
                lines.extend(forecast_lines)

        # Add adjustment notes
        adjustment_lines = self._build_adjustment_notes(graph, adjustment_filter)
        if adjustment_lines:
            lines.extend(adjustment_lines)

        return lines

    def _build_forecast_notes(self, forecast_configs: dict[str, Any]) -> list[str]:
        """Build forecast notes section.

        Args:
            forecast_configs: Dictionary mapping node IDs to forecast configurations.

        Returns:
            List of strings for the forecast notes section.
        """
        if not forecast_configs:
            return []

        notes = ["", "## Forecast Notes"]  # Add blank line before header

        for node_id, config in forecast_configs.items():
            method = config.get("method", "N/A")
            cfg_details = config.get("config")
            desc = f"- **{node_id}**: Forecasted using method '{method}'"

            # Add method-specific details
            if method == "simple" and cfg_details is not None:
                desc += f" (e.g., simple growth rate: {cfg_details:.1%})."
            elif method == "curve" and cfg_details:
                rates_str = ", ".join([f"{r:.1%}" for r in cfg_details])
                desc += f" (e.g., specific growth rates: [{rates_str}])."
            elif method == "historical_growth":
                desc += " (based on average historical growth)."
            elif method == "average":
                desc += " (based on historical average value)."
            elif method == "statistical":
                dist_name = cfg_details.get("distribution", "unknown")
                params_dict = cfg_details.get("params", {})
                params_str = ", ".join(
                    [
                        f"{k}={v:.3f}" if isinstance(v, float) else f"{k}={v}"
                        for k, v in params_dict.items()
                    ]
                )
                desc += (
                    f" (using '{dist_name}' distribution with params: {params_str})."
                )
            else:
                desc += "."

            notes.append(desc)

        return notes

    def _build_adjustment_notes(
        self, graph: Graph, adjustment_filter: Optional[Any] = None
    ) -> list[str]:
        """Build adjustment notes section.

        Args:
            graph: The Graph object containing adjustments.
            adjustment_filter: Filter for adjustments to include.

        Returns:
            List of strings for the adjustment notes section.
        """
        all_adjustments: list[Adjustment] = graph.list_all_adjustments()
        if not all_adjustments:
            return []

        # Apply filter to adjustments
        filtered_adjustments = self._filter_adjustments(
            all_adjustments, adjustment_filter
        )
        if not filtered_adjustments:
            return []

        lines = [
            "",
            "## Adjustment Notes (Matching Filter)",
        ]  # Add blank line before header

        # Sort adjustments for consistent output
        sorted_adjustments = sorted(
            filtered_adjustments,
            key=lambda x: (x.node_name, x.period, x.priority, x.timestamp),
        )

        for adj in sorted_adjustments:
            tags_str = ", ".join(sorted(adj.tags)) if adj.tags else "None"
            details = (
                f"- **{adj.node_name}** ({adj.period}, Scenario: {adj.scenario}, "
                f"Prio: {adj.priority}): {adj.type.name.capitalize()} adjustment of {adj.value:.2f}. "
                f"Reason: {adj.reason}. Tags: [{tags_str}]. (ID: {adj.id})"
            )
            lines.append(details)

        return lines

    def _filter_adjustments(
        self, all_adjustments: list[Adjustment], adjustment_filter: Optional[Any]
    ) -> list[Adjustment]:
        """Filter adjustments based on the provided filter.

        Args:
            all_adjustments: List of all adjustments.
            adjustment_filter: Filter to apply.

        Returns:
            List of filtered adjustments.
        """
        # Create a filter instance based on the input
        filt: AdjustmentFilter

        if isinstance(adjustment_filter, AdjustmentFilter):
            filt = adjustment_filter.model_copy(
                update={"period": None}
            )  # Ignore period context
        elif isinstance(adjustment_filter, set):
            filt = AdjustmentFilter(
                include_tags=adjustment_filter,
                include_scenarios={
                    DEFAULT_SCENARIO
                },  # Assume default scenario for tag shorthand
                period=None,
            )
        else:  # Includes None or other types
            filt = AdjustmentFilter(include_scenarios={DEFAULT_SCENARIO}, period=None)

        # Apply the filter
        return [adj for adj in all_adjustments if filt.matches(adj)]



================================================================================
File: fin_statement_model/io/formats/markdown/renderer.py
================================================================================

"""Helper class for rendering StatementStructure to markdown format."""

import logging
from typing import Optional, Union

from fin_statement_model.core.graph import Graph
from fin_statement_model.statements.structure import (
    Section,
    StatementItem,
    StatementItemType,
    StatementStructure,
    LineItem,
    CalculatedLineItem,
    SubtotalLineItem,
    MetricLineItem,
)
from fin_statement_model.io.formats.markdown.models import MarkdownStatementItem
from fin_statement_model.core.nodes import standard_node_registry

logger = logging.getLogger(__name__)


class MarkdownStatementRenderer:
    """Helper class that renders StatementStructure to markdown format.

    This class handles the recursive traversal of a StatementStructure and
    extracts values from the graph to create formatted items for markdown output.
    """

    def __init__(self, graph: Graph, indent_spaces: int = 4):
        """Initialize the renderer.

        Args:
            graph: The Graph object containing financial data.
            indent_spaces: Number of spaces per indentation level.
        """
        self.graph = graph
        self.indent_spaces = indent_spaces
        self.periods = sorted(list(graph.periods))

    def render_structure(
        self,
        structure: StatementStructure,
        historical_periods: Optional[set[str]] = None,
        forecast_periods: Optional[set[str]] = None,
    ) -> list[MarkdownStatementItem]:
        """Traverse StatementStructure and extract formatted items.

        Args:
            structure: The StatementStructure to render.
            historical_periods: Set of historical period names.
            forecast_periods: Set of forecast period names.

        Returns:
            List of MarkdownStatementItem objects ready for formatting.
        """
        logger.debug(f"Rendering statement structure: {structure.id}")
        items = []

        for section in structure.sections:
            items.extend(self._render_section(section, level=0))

        logger.debug(f"Rendered {len(items)} items from structure")
        return items

    def _render_section(
        self, section: Section, level: int
    ) -> list[MarkdownStatementItem]:
        """Recursively render a section and its contents.

        Args:
            section: The Section to render.
            level: Current indentation level.

        Returns:
            List of rendered items from this section.
        """
        items = []

        # Process items within the section
        for item in section.items:
            if isinstance(item, Section):
                # Nested section - recurse
                items.extend(self._render_section(item, level + 1))
            else:
                # Statement item (LineItem, CalculatedLineItem, etc.)
                rendered_item = self._render_item(item, level + 1)
                if rendered_item:
                    items.append(rendered_item)

        # Process section subtotal if exists
        if hasattr(section, "subtotal") and section.subtotal:
            rendered_subtotal = self._render_item(section.subtotal, level + 1)
            if rendered_subtotal:
                items.append(rendered_subtotal)

        return items

    def _render_item(
        self, item: StatementItem, level: int
    ) -> Optional[MarkdownStatementItem]:
        """Render a single statement item with values from graph.

        Args:
            item: The StatementItem to render.
            level: Current indentation level.

        Returns:
            MarkdownStatementItem or None if item couldn't be rendered.
        """
        try:
            # Extract values based on item type
            values = self._extract_values(item)

            return MarkdownStatementItem(
                name=item.name,
                values=values,
                level=level,
                is_subtotal=(item.item_type == StatementItemType.SUBTOTAL),
                sign_convention=getattr(item, "sign_convention", 1),
                display_format=item.display_format,
                units=item.units,
                display_scale_factor=item.display_scale_factor,
                is_contra=item.is_contra,
            )
        except Exception as e:
            logger.warning(f"Failed to render item {item.id}: {e}")
            return None

    def _extract_values(
        self, item: StatementItem
    ) -> dict[str, Union[float, int, str, None]]:
        """Extract values for an item from the graph.

        Args:
            item: The StatementItem to extract values for.

        Returns:
            Dictionary mapping periods to values.
        """
        values = {}

        # Get the node ID based on item type
        node_id = self._get_node_id(item)
        if not node_id:
            logger.warning(f"Could not determine node ID for item: {item.id}")
            return {period: None for period in self.periods}

        try:
            node = self.graph.get_node(node_id)
            if node is None:
                logger.warning(f"Node '{node_id}' returned None for item: {item.id}")
                return {period: None for period in self.periods}

            for period in self.periods:
                raw_value = None

                # Calculation items use graph.calculate, pure line items use node.calculate
                if isinstance(
                    item, (CalculatedLineItem, SubtotalLineItem, MetricLineItem)
                ):
                    # Calculate value for derived items
                    try:
                        raw_value = self.graph.calculate(node_id, period)
                    except Exception as calc_error:
                        logger.warning(
                            f"Calculation failed for node '{node_id}' period '{period}': {calc_error}"
                        )
                        raw_value = "CALC_ERR"  # type: ignore[assignment]
                elif isinstance(item, LineItem):
                    # Direct value from node for basic line items
                    raw_value = node.calculate(period)
                else:
                    logger.warning(
                        f"Unsupported item type: {type(item)} for item: {item.id}"
                    )
                    raw_value = None

                # Apply sign convention and scaling
                values[period] = self._apply_formatting(raw_value, item)

        except KeyError:
            logger.warning(f"Node '{node_id}' not found in graph for item: {item.id}")
            values = {period: None for period in self.periods}
        except Exception:
            logger.exception(f"Error extracting values for item {item.id}")
            values = {period: "ERROR" for period in self.periods}

        return values

    def _get_node_id(self, item: StatementItem) -> Optional[str]:
        """Get the node ID for a statement item.

        Args:
            item: The StatementItem to get node ID for.

        Returns:
            The node ID or None if it couldn't be determined.
        """
        if isinstance(item, LineItem):
            # For LineItem, try to resolve node_id or standard_node_ref
            if item.node_id:
                return item.node_id
            elif item.standard_node_ref:
                return item.get_resolved_node_id(standard_node_registry)
            else:
                return None
        elif isinstance(item, CalculatedLineItem | SubtotalLineItem | MetricLineItem):
            # For calculated items, use the item ID as node ID
            return item.id
        else:
            logger.warning(f"Unknown item type for node ID resolution: {type(item)}")
            return None

    def _apply_formatting(
        self, raw_value: Union[float, int, str, None], item: StatementItem
    ) -> Union[float, int, str, None]:
        """Apply sign convention and scaling to a raw value.

        Args:
            raw_value: The raw value from the graph.
            item: The StatementItem containing formatting info.

        Returns:
            The formatted value.
        """
        if raw_value is None or isinstance(raw_value, str):
            # Keep None and error strings as-is
            return raw_value

        if isinstance(raw_value, int | float):
            # Apply sign convention
            sign_convention = getattr(item, "sign_convention", 1)
            formatted_value = raw_value * sign_convention

            # Apply display scale factor
            scale_factor = item.display_scale_factor
            if scale_factor != 1.0:
                formatted_value = formatted_value * scale_factor

            return formatted_value

        return raw_value



================================================================================
File: fin_statement_model/io/formats/markdown/writer.py
================================================================================

"""Writes a financial statement graph to a Markdown table."""

import logging
from typing import Any, Optional, TypedDict, Union

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.base import DataWriter
from fin_statement_model.io.config.models import MarkdownWriterConfig
from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.io.formats.markdown.renderer import MarkdownStatementRenderer
from fin_statement_model.io.formats.markdown.formatter import MarkdownTableFormatter
from fin_statement_model.io.formats.markdown.notes import MarkdownNotesBuilder

logger = logging.getLogger(__name__)


# Legacy structure for backward compatibility
class StatementItem(TypedDict):
    """Represents a line item with its values for Markdown output."""

    name: str
    # value: Union[float, int, str, None] # Replaced single value
    values: dict[str, Union[float, int, str, None]]  # Values per period
    level: int
    is_subtotal: bool  # Indicates if the row is a subtotal or section header


@register_writer("markdown", schema=MarkdownWriterConfig)
class MarkdownWriter(DataWriter):
    """Writes a financial statement structure to a Markdown table."""

    def __init__(self, config: Optional[MarkdownWriterConfig] = None):
        """Initializes the MarkdownWriter."""
        # Provide a default config; missing fields are handled by Pydantic defaults.
        self.config = config or MarkdownWriterConfig(
            format_type="markdown",
            target=None,  # type: ignore[call-arg]
        )
        logger.debug(f"Initialized MarkdownWriter with config: {self.config}")

    def _format_value(self, value: Union[float, int, str, None]) -> str:
        """Formats the value for display in the table."""
        if value is None:
            return ""
        if isinstance(value, float | int):
            # Basic number formatting, could be enhanced (e.g., commas)
            return f"{value:,.2f}" if isinstance(value, float) else str(value)
        return str(value)

    def write(self, graph: Graph, target: Any = None, **kwargs: Any) -> str:
        """Write financial statement to markdown.

        Args:
            graph: The Graph object containing the financial data.
            target: Ignored by this writer (returns string).
            **kwargs: Additional options including:
                - statement_structure: The StatementStructure to render

        Returns:
            String containing the formatted statement in Markdown.

        Raises:
            WriteError: If 'statement_structure' is not provided.
        """
        logger.info(
            f"Writing graph to Markdown format (target ignored: {target}) using kwargs: {kwargs.keys()}"
        )

        try:
            statement_structure = kwargs.get("statement_structure")

            if statement_structure is None:
                raise WriteError("Must provide 'statement_structure' argument.")

            filtered_kwargs = {
                k: v for k, v in kwargs.items() if k != "statement_structure"
            }
            return self._write_with_structure(
                graph, statement_structure, **filtered_kwargs
            )
        except NotImplementedError as nie:
            logger.exception("Markdown write failed")
            raise WriteError(
                message=f"Markdown writer requires graph traversal logic: {nie}",
                target=target,
                writer_type="markdown",
                original_error=nie,
            ) from nie
        except Exception as e:
            logger.exception("Error writing Markdown for graph", exc_info=True)
            raise WriteError(
                message=f"Failed to generate Markdown table: {e}",
                target=target,
                writer_type="markdown",
                original_error=e,
            ) from e

    def _write_with_structure(
        self, graph: Graph, statement_structure: StatementStructure, **kwargs: Any
    ) -> str:
        """Write using the new StatementStructure approach.

        Args:
            graph: The Graph object containing financial data.
            statement_structure: The StatementStructure to render.
            **kwargs: Additional options.

        Returns:
            Formatted markdown string.
        """
        # Use renderer to process structure
        renderer = MarkdownStatementRenderer(graph, self.config.indent_spaces)
        items = renderer.render_structure(
            statement_structure,
            historical_periods=set(kwargs.get("historical_periods", [])),
            forecast_periods=set(kwargs.get("forecast_periods", [])),
        )

        if not items:
            logger.warning("No statement items generated from structure.")
            return ""

        # Format into markdown table
        formatter = MarkdownTableFormatter(self.config.indent_spaces)
        table_lines = formatter.format_table(
            items,
            periods=renderer.periods,
            historical_periods=kwargs.get("historical_periods"),
            forecast_periods=kwargs.get("forecast_periods"),
        )

        # Add notes sections
        notes_builder = MarkdownNotesBuilder()
        notes_lines = notes_builder.build_notes(
            graph=graph,
            forecast_configs=kwargs.get("forecast_configs"),
            adjustment_filter=kwargs.get("adjustment_filter"),
        )

        return "\n".join(table_lines + notes_lines)



================================================================================
File: fin_statement_model/io/specialized/__init__.py
================================================================================

"""Specialized IO operations for domain-specific functionality."""

# Adjustments
from .adjustments import (
    read_excel,
    write_excel,
    load_adjustments_from_excel,
    export_adjustments_to_excel,
)

# Cells
from .cells import import_from_cells

# Graph serialization
from .graph import (
    GraphDefinitionReader,
    GraphDefinitionWriter,
    save_graph_definition,
    load_graph_definition,
)

# Statement utilities
from .statements import (
    list_available_builtin_configs,
    write_statement_to_excel,
    write_statement_to_json,
)

__all__ = [
    # Graph
    "GraphDefinitionReader",
    "GraphDefinitionWriter",
    # Adjustments
    "export_adjustments_to_excel",
    # Cells
    "import_from_cells",
    # Statements
    "list_available_builtin_configs",
    "write_statement_to_excel",
    "write_statement_to_json",
    # Graph serialization
    "save_graph_definition",
    "load_graph_definition",
    # File readers/writers for data sources
    "read_excel",
    "write_excel",
    "load_adjustments_from_excel",
]



================================================================================
File: fin_statement_model/io/specialized/adjustments.py
================================================================================

"""Functions for bulk import and export of adjustments via Excel files."""

import logging
from typing import Any, cast, Iterable
from pathlib import Path
from collections import defaultdict

import pandas as pd
from pydantic import ValidationError

from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentType,
)
from fin_statement_model.core.graph import Graph  # Needed for Graph convenience methods
from fin_statement_model.io.exceptions import ReadError, WriteError
from fin_statement_model.io.core import FileBasedReader, handle_read_errors
from fin_statement_model.io.specialized.row_models import AdjustmentRowModel

logger = logging.getLogger(__name__)

# Define expected column names (case-insensitive matching during read)
# Required columns per spec:
REQ_COLS = {"node_name", "period", "value", "reason"}
# Optional columns per spec:
OPT_COLS = {
    "type",
    "tags",
    "scale",
    "scenario",
    "start_period",
    "end_period",
    "priority",
    "user",
    "id",
}
ALL_COLS = REQ_COLS.union(OPT_COLS)

# Map DataFrame column names (lowercase) to Adjustment model fields
COL_TO_FIELD_MAP = {
    "node_name": "node_name",
    "period": "period",
    "value": "value",
    "reason": "reason",
    "type": "type",
    "tags": "tags",
    "scale": "scale",
    "scenario": "scenario",
    "start_period": "start_period",
    "end_period": "end_period",
    "priority": "priority",
    "user": "user",
    "id": "id",
    # Note: timestamp is not expected in input file, generated on creation
}


def _validate_required_columns(columns: Iterable[str]) -> None:
    missing = REQ_COLS - set(columns)
    if missing:
        raise ReadError(
            f"Missing required columns in adjustment Excel file: {missing}",
            source=str(columns),
        )


def _build_error_row(
    raw: dict[str, Any], ve: ValidationError, idx: int
) -> dict[str, Any]:
    error_detail = "; ".join(f"{err['loc'][0]}: {err['msg']}" for err in ve.errors())
    row = {**raw, "error": error_detail}
    logger.debug(f"Row {idx}: Validation failed - {error_detail}")
    return row


def _read_excel_impl(path: str | Path) -> tuple[list[Adjustment], pd.DataFrame]:
    """Read adjustments from an Excel file.

    Expects the first sheet to contain adjustment data.
    Validates required columns and parses each row via AdjustmentRowModel.
    Rows that fail validation are collected into an error report.
    """
    file_path = Path(path)
    logger.info(f"Reading adjustments from Excel file: {file_path}")

    try:
        df = pd.read_excel(file_path, sheet_name=0)
    except FileNotFoundError:
        raise ReadError(
            f"Adjustment Excel file not found: {file_path}", source=str(file_path)
        )
    except Exception as e:
        raise ReadError(
            f"Failed to read Excel file {file_path}: {e}",
            source=str(file_path),
            original_error=e,
        )

    # Normalize and validate columns
    df.columns = [str(col).lower().strip() for col in df.columns]
    _validate_required_columns(df.columns)

    records = df.to_dict(orient="records")
    valid_adjustments: list[Adjustment] = []
    error_rows: list[dict[str, Any]] = []

    for idx, raw in enumerate(records, start=2):
        try:
            row_model = AdjustmentRowModel(**raw)
            valid_adjustments.append(row_model.to_adjustment())
        except ValidationError as ve:
            error_rows.append(_build_error_row(raw, ve, idx))

    error_report_df = pd.DataFrame(error_rows)
    if not error_report_df.empty:
        logger.warning(
            f"Completed reading adjustments from {file_path}. "
            f"Found {len(valid_adjustments)} valid adjustments and {len(error_rows)} errors."
        )
    else:
        logger.info(
            f"Successfully read {len(valid_adjustments)} adjustments from {file_path} with no errors."
        )

    return valid_adjustments, error_report_df


# DataReader class for reading adjustments with standardized validation
class AdjustmentsExcelReader(FileBasedReader):
    """DataReader for reading adjustments from an Excel file with consistent validation and error handling."""

    @handle_read_errors()
    def read(  # type: ignore[override]
        self, source: str | Path, **kwargs: Any
    ) -> tuple[list[Adjustment], pd.DataFrame]:
        """Read adjustments using FileBasedReader validation."""
        path_str = str(source)
        self.validate_file_exists(path_str)
        self.validate_file_extension(path_str, (".xls", ".xlsx"))
        return _read_excel_impl(path_str)


# Public API: delegate to standardized reader
def read_excel(path: str | Path) -> tuple[list[Adjustment], pd.DataFrame]:
    """Read adjustments from an Excel file.

    Args:
        path: Path to the Excel file.

    Returns:
        Tuple of valid Adjustment list and error report DataFrame.
    """
    return AdjustmentsExcelReader().read(path)


def write_excel(adjustments: list[Adjustment], path: str | Path) -> None:
    """Write a list of adjustments to an Excel file.

    Writes adjustments to separate sheets based on their scenario.
    The columns will match the optional fields defined for reading.

    Args:
        adjustments: A list of Adjustment objects to write.
        path: Path for the output Excel file.

    Raises:
        WriteError: If writing to the file fails.
    """
    file_path = Path(path)
    logger.info(f"Writing {len(adjustments)} adjustments to Excel file: {file_path}")

    # Group adjustments by scenario
    grouped_by_scenario: dict[str, list[dict[str, Any]]] = defaultdict(list)
    for adj in adjustments:
        # Use model_dump for serialization, exclude fields we don't usually export
        adj_dict = adj.model_dump(exclude={"timestamp"})  # Exclude timestamp by default
        # Convert complex types to simple types for Excel
        adj_dict["id"] = str(adj_dict.get("id"))
        raw_type = adj_dict.get("type")
        # Cast to AdjustmentType to access .value safely if present
        adj_dict["type"] = cast(AdjustmentType, raw_type).value if raw_type else None
        adj_dict["tags"] = ",".join(sorted(adj_dict.get("tags", set())))
        grouped_by_scenario[adj.scenario].append(adj_dict)

    if not grouped_by_scenario:
        logger.warning("No adjustments provided to write_excel. Creating empty file.")
        # Create an empty file or handle as desired
        try:
            pd.DataFrame().to_excel(file_path, index=False)
        except Exception as e:
            raise WriteError(
                f"Failed to write empty Excel file {file_path}: {e}",
                target=str(file_path),
                original_error=e,
            )
        return

    try:
        with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
            for scenario, scenario_adjustments in grouped_by_scenario.items():
                df = pd.DataFrame(scenario_adjustments)
                # Reorder columns for consistency
                cols_ordered = [c for c in COL_TO_FIELD_MAP if c in df.columns]
                cols_ordered += [c for c in df.columns if c not in cols_ordered]
                df = df[cols_ordered]
                # Sheet names must be valid
                safe_scenario_name = (
                    scenario.replace(":", "-").replace("/", "-").replace("\\", "-")[:31]
                )
                df.to_excel(writer, sheet_name=safe_scenario_name, index=False)
        logger.info(f"Successfully wrote adjustments to {file_path}")
    except Exception as e:
        logger.error(
            f"Failed to write adjustments to Excel file {file_path}: {e}", exc_info=True
        )
        raise WriteError(
            f"Failed to write adjustments to Excel: {e}",
            target=str(file_path),
            original_error=e,
        )


# --- Graph Convenience Methods ---


def load_adjustments_from_excel(
    graph: Graph, path: str | Path, replace: bool = False
) -> pd.DataFrame:
    """Reads adjustments from Excel and adds them to the graph.

    Args:
        graph: The Graph instance to add adjustments to.
        path: Path to the Excel file.
        replace: If True, clear existing adjustments in the manager before adding new ones.

    Returns:
        pd.DataFrame: The error report DataFrame from `read_excel`.
                   Empty if no errors occurred.
    """
    logger.info(
        f"Loading adjustments from Excel ({path}) into graph. Replace={replace}"
    )
    valid_adjustments, error_report_df = read_excel(path)

    if replace:
        logger.debug("Clearing existing adjustments before loading.")
        graph.adjustment_manager.clear_all()

    added_count = 0
    for adj in valid_adjustments:
        try:
            graph.adjustment_manager.add_adjustment(adj)
            added_count += 1
        except Exception as e:
            logger.error(
                f"Failed to add valid adjustment {adj.id} to graph: {e}", exc_info=True
            )
            # Optionally add this failure to the error report?
            error_row = adj.model_dump(mode="json")
            error_row["error"] = f"Failed to add to graph: {e}"
            # Need to handle DataFrame append carefully if modifying during iteration
            # Simplest is to report read errors, log add errors.

    logger.info(f"Added {added_count} adjustments to the graph from {path}.")
    if not error_report_df.empty:
        logger.warning(
            f"Encountered {len(error_report_df)} errors during Excel read process."
        )

    return error_report_df


def export_adjustments_to_excel(graph: Graph, path: str | Path) -> None:
    """Exports all adjustments from the graph to an Excel file.

    Args:
        graph: The Graph instance containing adjustments.
        path: Path for the output Excel file.
    """
    logger.info(f"Exporting all adjustments from graph to Excel ({path}).")
    all_adjustments = graph.list_all_adjustments()
    write_excel(all_adjustments, path)


# Add convenience methods to Graph class directly?
# This uses module patching which can sometimes be debated, but keeps the Graph API clean.
# Alternatively, users would call fin_statement_model.io.adjustments_excel.load_adjustments_from_excel(graph, path)
setattr(Graph, "load_adjustments_from_excel", load_adjustments_from_excel)
setattr(Graph, "export_adjustments_to_excel", export_adjustments_to_excel)



================================================================================
File: fin_statement_model/io/specialized/cells.py
================================================================================

"""Importer module for reading cell-based financial statement data into a Graph."""

from typing import Any

# from fin_statement_model.statements.graph.financial_graph import FinancialStatementGraph # Removed
from fin_statement_model.core.graph import Graph  # Added

__all__ = ["import_from_cells"]


def import_from_cells(cells_info: list[dict[str, Any]]) -> Graph:  # Changed return type
    """Import a list of cell dictionaries into a core Graph.

    Each cell dict should include at minimum:
    - 'row_name': identifier for the line item (becomes node ID)
    - 'column_name': the period label
    - 'value': the numeric value

    Args:
        cells_info: List of cell metadata dictionaries.

    Returns:
        A core Graph populated with detected periods and data nodes.
    """
    # Group cells by row_name to aggregate values per financial statement item
    items: dict[str, dict[str, Any]] = {}
    unique_periods: set[str] = set()

    for cell in cells_info:
        # Clean the item name and period
        item_name = cell.get("row_name", "").strip()
        period = cell.get("column_name", "").strip()
        value = cell.get("value")

        if not item_name or not period:
            continue

        unique_periods.add(period)
        if item_name not in items:
            items[item_name] = {}
        items[item_name][period] = value

    # Sort periods and create the graph
    sorted_periods = sorted(list(unique_periods))  # Ensure list for Graph constructor
    graph = Graph(periods=sorted_periods)  # Changed to Graph

    # Add each financial statement item as a data node to the graph
    for name, values in items.items():
        # Use add_financial_statement_item based on Graph API
        graph.add_financial_statement_item(name, values)

    return graph



================================================================================
File: fin_statement_model/io/specialized/graph.py
================================================================================

"""Graph definition serialization and deserialization.

This module provides functionality to save and load complete graph definitions,
including all nodes, periods, and adjustments.
"""

import logging
from typing import Any, Optional, cast

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.adjustments.models import Adjustment
from fin_statement_model.core.nodes import (
    Node,
)
from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.io.core import (
    DataReader,
    DataWriter,
    register_reader,
    register_writer,
)
from fin_statement_model.io.exceptions import ReadError, WriteError
from fin_statement_model.io.config.models import BaseReaderConfig, BaseWriterConfig

logger = logging.getLogger(__name__)

# Define a type for the serialized node dictionary for clarity
SerializedNode = dict[str, Any]


# ===== Reader Implementation =====


@register_reader("graph_definition_dict", schema=BaseReaderConfig)
class GraphDefinitionReader(DataReader):
    """Reads a graph definition dictionary to reconstruct a Graph object.

    Handles reconstructing nodes based on their serialized type and configuration,
    and loads adjustments.
    """

    def __init__(self, cfg: Optional[Any] = None) -> None:
        """Initialize the GraphDefinitionReader. Config currently unused."""
        self.cfg = cfg

    def _get_node_dependencies(self, node_def: dict[str, Any]) -> list[str]:
        """Extract dependency names from a node definition.

        Args:
            node_def: Dictionary containing node definition.

        Returns:
            List of dependency node names.
        """
        node_type = node_def.get("type")

        if node_type == "financial_statement_item":
            return []  # No dependencies
        elif node_type in ["calculation", "formula_calculation"]:
            return cast(list[str], node_def.get("inputs", []))
        elif node_type == "forecast":
            base_node_name = cast(Optional[str], node_def.get("base_node_name"))
            return [base_node_name] if base_node_name is not None else []
        elif node_type == "custom_calculation":
            return cast(list[str], node_def.get("inputs", []))
        else:
            # Default: inputs should be list[str]
            return cast(list[str], node_def.get("inputs", []))

    def read(self, source: dict[str, Any], **kwargs: Any) -> Graph:
        """Reconstruct a Graph instance from its definition dictionary.

        Args:
            source: Dictionary containing the graph definition (periods, nodes, adjustments).
            **kwargs: Currently unused.

        Returns:
            A new Graph instance populated from the definition.

        Raises:
            ReadError: If the source format is invalid or graph reconstruction fails.
        """
        logger.info("Starting graph reconstruction from definition dictionary.")

        if (
            not isinstance(source, dict)
            or "periods" not in source
            or "nodes" not in source
        ):
            raise ReadError(
                message="Invalid source format for GraphDefinitionReader. Expected dict with 'periods' and 'nodes' keys.",
                source="graph_definition_dict",
                reader_type="GraphDefinitionReader",
            )

        try:
            # 1. Initialize Graph with Periods
            periods = source.get("periods", [])
            if not isinstance(periods, list):
                raise ReadError("Invalid format: 'periods' must be a list.")
            graph = Graph(periods=periods)

            # 2. Reconstruct Nodes using topological sort of definitions
            nodes_dict = source.get("nodes", {})
            if not isinstance(nodes_dict, dict):
                raise ReadError("Invalid format: 'nodes' must be a dictionary.")
            # Build dependency graph from definitions
            dep_graph: dict[str, list[str]] = {
                name: self._get_node_dependencies(defn)
                for name, defn in nodes_dict.items()
            }
            # Perform topological sort
            in_degree = {n: 0 for n in dep_graph}
            adjacency: dict[str, list[str]] = {n: [] for n in dep_graph}
            for n, deps in dep_graph.items():
                for dep in deps:
                    if dep not in dep_graph:
                        raise ReadError(
                            f"Dependency '{dep}' for node '{n}' not found in definitions.",
                            source="graph_definition_dict",
                        )
                    adjacency[dep].append(n)
                    in_degree[n] += 1
            queue = [n for n, d in in_degree.items() if d == 0]
            sorted_names: list[str] = []
            while queue:
                current = queue.pop()
                sorted_names.append(current)
                for neighbor in adjacency[current]:
                    in_degree[neighbor] -= 1
                    if in_degree[neighbor] == 0:
                        queue.append(neighbor)
            if len(sorted_names) != len(dep_graph):
                raise ReadError(
                    f"Circular dependency detected among nodes: {set(dep_graph) - set(sorted_names)}",
                    source="graph_definition_dict",
                )
            # Create and add nodes in sorted order
            for node_name in sorted_names:
                node_def = nodes_dict[node_name]
                existing_nodes = {name: graph.nodes[name] for name in graph.nodes}
                node = NodeFactory.create_from_dict(node_def, context=existing_nodes)
                graph.add_node(node)

            # 3. Load Adjustments
            adjustments_list = source.get("adjustments")  # Optional
            if adjustments_list is not None:
                if not isinstance(adjustments_list, list):
                    raise ReadError(
                        "Invalid format: 'adjustments' must be a list if present."
                    )

                deserialized_adjustments = []
                for i, adj_dict in enumerate(adjustments_list):
                    try:
                        # Use model_validate for Pydantic V2
                        adj = Adjustment.model_validate(adj_dict)
                        deserialized_adjustments.append(adj)
                    except Exception:
                        # Log error but try to continue with other nodes
                        logger.exception(
                            f"Failed to deserialize adjustment at index {i}: {adj_dict}. Skipping."
                        )
                        # Optionally raise ReadError here to fail fast

                if deserialized_adjustments:
                    graph.adjustment_manager.load_adjustments(deserialized_adjustments)
                    logger.info(
                        f"Loaded {len(deserialized_adjustments)} adjustments into the graph."
                    )

            logger.info(
                f"Successfully reconstructed graph with {len(graph.nodes)} nodes."
            )
            return graph

        except ReadError:  # Re-raise ReadErrors directly
            raise
        except Exception as e:
            logger.error(
                f"Failed to reconstruct graph from definition: {e}", exc_info=True
            )
            raise ReadError(
                message=f"Failed to reconstruct graph from definition: {e}",
                source="graph_definition_dict",
                reader_type="GraphDefinitionReader",
                original_error=e,
            ) from e


# ===== Writer Implementation =====


@register_writer("graph_definition_dict", schema=BaseWriterConfig)
class GraphDefinitionWriter(DataWriter):
    """Writes the full graph definition (nodes, periods, adjustments) to a dictionary.

    This writer serializes the structure and configuration of the graph, suitable
    for saving and reloading the entire model state.
    """

    def __init__(self, cfg: Optional[BaseWriterConfig] = None) -> None:
        """Initialize the GraphDefinitionWriter."""
        self.cfg = cfg

    def _serialize_node(self, node: Node) -> Optional[SerializedNode]:
        """Serialize a single node using its to_dict() method.

        Args:
            node: The node to serialize.

        Returns:
            Dictionary representation of the node, or None if serialization fails.
        """
        try:
            return node.to_dict()
        except Exception:
            logger.exception(f"Failed to serialize node '{node.name}'")
            logger.warning(f"Skipping node '{node.name}' due to serialization error.")
            return None

    def write(
        self, graph: Graph, target: Any = None, **kwargs: dict[str, Any]
    ) -> dict[str, Any]:
        """Export the full graph definition to a dictionary.

        Args:
            graph (Graph): The Graph instance to serialize.
            target (Any): Ignored by this writer; the dictionary is returned directly.
            **kwargs: Currently unused.

        Returns:
            Dict[str, Any]: Dictionary representing the graph definition, including
                            periods, node definitions, and adjustments.

        Raises:
            WriteError: If an unexpected error occurs during export.
        """
        logger.info(f"Starting export of graph definition for: {graph!r}")
        graph_definition: dict[str, Any] = {
            "periods": [],
            "nodes": {},
            "adjustments": [],
        }

        try:
            # 1. Serialize Periods
            graph_definition["periods"] = list(graph.periods)

            # 2. Serialize Nodes using their to_dict() methods
            serialized_nodes: dict[str, SerializedNode] = {}
            for node_name, node in graph.nodes.items():
                node_dict = self._serialize_node(node)
                if node_dict:
                    serialized_nodes[node_name] = node_dict
                else:
                    logger.warning(
                        f"Node '{node_name}' was not serialized due to errors."
                    )
            graph_definition["nodes"] = serialized_nodes

            # 3. Serialize Adjustments
            adjustments = graph.list_all_adjustments()
            serialized_adjustments = []
            for adj in adjustments:
                try:
                    # Use model_dump for Pydantic V2, ensure mode='json' for types like UUID/datetime
                    serialized_adjustments.append(adj.model_dump(mode="json"))
                except Exception as e:
                    logger.warning(
                        f"Failed to serialize adjustment {adj.id}: {e}. Skipping."
                    )
            graph_definition["adjustments"] = serialized_adjustments

            logger.info(
                f"Successfully created graph definition dictionary with {len(serialized_nodes)} nodes and {len(serialized_adjustments)} adjustments."
            )
            return graph_definition

        except Exception as e:
            logger.error(
                f"Failed to create graph definition dictionary: {e}", exc_info=True
            )
            raise WriteError(
                message=f"Failed to create graph definition dictionary: {e}",
                target="graph_definition_dict",
                writer_type="GraphDefinitionWriter",
                original_error=e,
            ) from e


# ===== Convenience Functions =====


def save_graph_definition(graph: Graph, filepath: str) -> None:
    """Save a graph definition to a JSON file.

    Args:
        graph: The graph to save.
        filepath: Path to the output JSON file.
    """
    import json

    writer = GraphDefinitionWriter()
    definition = writer.write(graph)

    with open(filepath, "w") as f:
        json.dump(definition, f, indent=2)

    logger.info(f"Saved graph definition to {filepath}")


def load_graph_definition(filepath: str) -> Graph:
    """Load a graph definition from a JSON file.

    Args:
        filepath: Path to the JSON file containing the graph definition.

    Returns:
        The reconstructed Graph object.
    """
    import json

    with open(filepath) as f:
        definition = json.load(f)

    reader = GraphDefinitionReader()
    graph = reader.read(definition)

    logger.info(f"Loaded graph definition from {filepath}")
    return graph


__all__ = [
    "GraphDefinitionReader",
    "GraphDefinitionWriter",
    "load_graph_definition",
    "save_graph_definition",
]



================================================================================
File: fin_statement_model/io/specialized/row_models.py
================================================================================

from __future__ import annotations

from uuid import UUID
from typing import Any, Optional, Set

from pydantic import BaseModel, ConfigDict, Field, field_validator

from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentType,
    AdjustmentTag,
    DEFAULT_SCENARIO,
)


class AdjustmentRowModel(BaseModel):
    """Model for validating a single adjustment row from Excel."""

    model_config = ConfigDict(populate_by_name=True, extra="ignore")

    # Required fields
    node_name: str
    period: str
    value: float
    reason: str

    # Optional or derived fields
    type: AdjustmentType = AdjustmentType.ADDITIVE
    tags: Set[AdjustmentTag] = Field(default_factory=set)
    scale: float = 1.0
    scenario: str = DEFAULT_SCENARIO
    start_period: Optional[str] = None
    end_period: Optional[str] = None
    priority: int = 0
    user: Optional[str] = None
    id: Optional[UUID] = None

    @field_validator("type", mode="before")
    @classmethod
    def _validate_type(cls, v: Any) -> Any:
        if isinstance(v, str):
            try:
                return AdjustmentType(v.lower())
            except ValueError:
                raise ValueError(f"Invalid AdjustmentType: {v}")
        return v

    @field_validator("tags", mode="before")
    @classmethod
    def _parse_tags(cls, v: Any) -> Set[AdjustmentTag]:
        if not v:
            return set()
        if isinstance(v, str):
            return {tag.strip() for tag in v.split(",") if tag.strip()}
        if isinstance(v, (list, set)):
            return set(v)
        raise ValueError(f"Invalid tags format: {v}")

    @field_validator("scale")
    @classmethod
    def _validate_scale(cls, v: float) -> float:
        if not 0.0 <= v <= 1.0:
            raise ValueError("Scale must be between 0.0 and 1.0")
        return v

    @field_validator("id", mode="before")
    @classmethod
    def _parse_id(cls, v: Any) -> Optional[UUID]:
        if v is None or v == "":
            return None
        if isinstance(v, UUID):
            return v
        try:
            return UUID(str(v))
        except Exception:
            raise ValueError(f"Invalid UUID for id: {v}")

    def to_adjustment(self) -> Adjustment:
        """
        Convert this row model into the core Adjustment model.
        """
        data = self.model_dump()
        # Remove None values to allow core model defaults
        filtered = {k: v for k, v in data.items() if v is not None}
        return Adjustment(**filtered)



================================================================================
File: fin_statement_model/io/specialized/statements.py
================================================================================

"""Statement-related IO utilities.

This module now only provides helpers for listing built-in statement configuration
filenames *and* writing formatted statement data to Excel or JSON. All former
file-loading utilities have been removed to enforce an in-memory configuration
workflow.
"""

import logging
import importlib.resources
from pathlib import Path
from typing import Any

import pandas as pd

from fin_statement_model.io.exceptions import WriteError

logger = logging.getLogger(__name__)

# Built-in config package path constant (directory containing YAML/JSON configs)
# Default base path is "fin_statement_model.statements.configs" – end-users can
# drop YAML or JSON files in that package (or a sub-package added to
# ``__init__.py``) and they will be discovered automatically.

_BUILTIN_CONFIG_PACKAGE = "fin_statement_model.statements.configs"


def list_available_builtin_configs() -> list[str]:
    """List names of all built-in statement configuration mappings."""
    package_path = _BUILTIN_CONFIG_PACKAGE
    try:
        resource_path = importlib.resources.files(package_path)
        if not resource_path.is_dir():
            logger.warning(
                f"Built-in config package path is not a directory: {package_path}"
            )
            return []
        names = [
            Path(res.name).stem
            for res in resource_path.iterdir()
            if res.is_file()
            and Path(res.name).suffix.lower() in (".yaml", ".yml", ".json")
        ]
        return sorted(names)
    except (ModuleNotFoundError, FileNotFoundError):
        logger.warning(f"Built-in statement config path not found: {package_path}")
        return []


# ===== Statement Writing =====


def write_statement_to_excel(
    statement_df: pd.DataFrame, file_path: str, **kwargs: Any
) -> None:
    """Write a statement DataFrame to an Excel file."""
    try:
        kwargs.setdefault("index", False)
        statement_df.to_excel(file_path, **kwargs)
    except Exception as e:
        raise WriteError(
            message="Failed to export statement DataFrame to Excel",
            target=file_path,
            writer_type="excel",
            original_error=e,
        ) from e


def write_statement_to_json(
    statement_df: pd.DataFrame,
    file_path: str,
    orient: str = "columns",
    **kwargs: Any,
) -> None:
    """Write a statement DataFrame to a JSON file."""
    try:
        statement_df.to_json(file_path, orient=orient, **kwargs)
    except Exception as e:
        raise WriteError(
            message="Failed to export statement DataFrame to JSON",
            target=file_path,
            writer_type="json",
            original_error=e,
        ) from e


__all__ = [
    # Built-in config helpers
    "list_available_builtin_configs",
    # Statement writing
    "write_statement_to_excel",
    "write_statement_to_json",
]



================================================================================
File: fin_statement_model/io/validation.py
================================================================================

"""Unified node name validation and standardization utilities.

This module provides a comprehensive validator that combines basic validation
with context-aware pattern recognition for financial statement nodes.
"""

import logging
import re
from typing import Optional, ClassVar, Any
from dataclasses import dataclass, field

from fin_statement_model.core.nodes import Node
from fin_statement_model.core.nodes.standard_registry import StandardNodeRegistry

logger = logging.getLogger(__name__)


@dataclass
class ValidationResult:
    """Result of a node name validation."""

    original_name: str
    standardized_name: str
    is_valid: bool
    message: str
    category: str
    confidence: float = 1.0
    suggestions: list[str] = field(default_factory=list)


class UnifiedNodeValidator:
    """Unified validator for node names with pattern recognition and standardization.

    This validator combines the functionality of NodeNameValidator and
    ContextAwareNodeValidator into a single, more efficient implementation.
    """

    # Common sub-node patterns
    SUBNODE_PATTERNS: ClassVar[list[tuple[str, str]]] = [
        (r"^(.+)_(q[1-4])$", "quarterly"),
        (r"^(.+)_(fy\d{4})$", "fiscal_year"),
        (r"^(.+)_(\d{4})$", "annual"),
        (r"^(.+)_(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)$", "monthly"),
        (r"^(.+)_(actual|budget|forecast)$", "scenario"),
    ]

    # Generic segment pattern - must be checked separately
    SEGMENT_PATTERN = r"^(.+)_([a-z_]+)$"

    # Formula patterns - check exact endings
    FORMULA_ENDINGS: ClassVar[list[str]] = [
        "_margin",
        "_ratio",
        "_growth",
        "_change",
        "_pct",
    ]

    def __init__(
        self,
        registry: StandardNodeRegistry,
        strict_mode: Optional[bool] = None,
        auto_standardize: Optional[bool] = None,
        warn_on_non_standard: Optional[bool] = None,
        enable_patterns: bool = True,
    ):
        """Initialize the unified validator.

        Args:
            registry: The StandardNodeRegistry instance.
            strict_mode: If True, only standard names are allowed.
                        If None, uses config.validation.strict_mode.
            auto_standardize: If True, convert alternate names to standard.
                            If None, uses config.validation.auto_standardize_names.
            warn_on_non_standard: If True, log warnings for non-standard names.
                                If None, uses config.validation.warn_on_non_standard.
            enable_patterns: If True, recognize sub-node and formula patterns.
        """
        from fin_statement_model import get_config

        config = get_config()

        self._registry = registry
        self.strict_mode = (
            strict_mode if strict_mode is not None else config.validation.strict_mode
        )
        self.auto_standardize = (
            auto_standardize
            if auto_standardize is not None
            else config.validation.auto_standardize_names
        )
        self.warn_on_non_standard = (
            warn_on_non_standard
            if warn_on_non_standard is not None
            else config.validation.warn_on_non_standard
        )
        self.enable_patterns = enable_patterns
        self._validation_cache: dict[str, ValidationResult] = {}

    def validate(
        self,
        name: str,
        node_type: Optional[str] = None,
        parent_nodes: Optional[list[str]] = None,
        use_cache: bool = True,
    ) -> ValidationResult:
        """Validate a node name with full context awareness.

        Args:
            name: The node name to validate.
            node_type: Optional node type hint.
            parent_nodes: Optional list of parent node names.
            use_cache: Whether to use cached results.

        Returns:
            ValidationResult with all validation details.
        """
        # Check cache first
        cache_key = f"{name}:{node_type}:{','.join(parent_nodes or [])}"
        if use_cache and cache_key in self._validation_cache:
            return self._validation_cache[cache_key]

        # Start validation
        result = self._perform_validation(name, node_type, parent_nodes)

        # Cache result
        if use_cache:
            self._validation_cache[cache_key] = result

        # Log warnings if configured
        if self.warn_on_non_standard and result.category in ["custom", "invalid"]:
            logger.warning(f"{result.message}")
            if result.suggestions:
                logger.info(
                    f"Suggestions for '{name}': {'; '.join(result.suggestions)}"
                )

        return result

    def _perform_validation(
        self,
        name: str,
        node_type: Optional[str],
        parent_nodes: Optional[list[str]],
    ) -> ValidationResult:
        """Perform the actual validation logic."""
        # Normalize name to lowercase for registry checks
        normalized_name = name.lower()

        # Check standard names first (using normalized name)
        if self._registry.is_standard_name(normalized_name):
            # If original name is different case, standardize to lowercase
            standardized = normalized_name if name != normalized_name else name
            return ValidationResult(
                original_name=name,
                standardized_name=standardized,
                is_valid=True,
                message=f"Standard node: {normalized_name}",
                category="standard",
                confidence=1.0,
            )

        # Check alternate names (using normalized name)
        if self._registry.is_alternate_name(normalized_name):
            standard_name = self._registry.get_standard_name(normalized_name)
            return ValidationResult(
                original_name=name,
                standardized_name=standard_name if self.auto_standardize else name,
                is_valid=True,
                message=f"{'Standardized' if self.auto_standardize else 'Alternate name for'} '{standard_name}'",
                category="alternate",
                confidence=1.0,
            )

        # Pattern recognition if enabled
        if self.enable_patterns:
            pattern_result = self._check_pattern_validations(
                name, node_type, parent_nodes
            )
            if pattern_result:
                return pattern_result

        # Generate suggestions for unrecognized names
        suggestions = self._generate_suggestions(name)

        # Default to custom/invalid
        return ValidationResult(
            original_name=name,
            standardized_name=name,
            is_valid=not self.strict_mode,
            message=f"{'Non-standard' if self.strict_mode else 'Custom'} node: '{name}'",
            category="invalid" if self.strict_mode else "custom",
            confidence=0.5,
            suggestions=suggestions,
        )

    def _check_pattern_validations(
        self,
        name: str,
        node_type: Optional[str],
        parent_nodes: Optional[list[str]],
    ) -> Optional[ValidationResult]:
        """Check all pattern-based validations."""
        # Check formula patterns first (more specific)
        if node_type in ["calculation", "formula", None]:
            pattern_result = self._check_formula_ending(name)
            if pattern_result:
                base_name, formula_type = pattern_result

                return ValidationResult(
                    original_name=name,
                    standardized_name=name,
                    is_valid=True,
                    message=f"Formula node: {formula_type} of '{base_name}'",
                    category="formula",
                    confidence=0.85,
                )

        # Check specific sub-node patterns
        subnode_match = self._check_patterns(name, self.SUBNODE_PATTERNS, "subnode")
        if subnode_match:
            base_name, suffix, pattern_type = subnode_match
            # Normalize base name for registry check
            is_base_standard = self._registry.is_recognized_name(base_name.lower())

            return ValidationResult(
                original_name=name,
                standardized_name=name,
                is_valid=not self.strict_mode or is_base_standard,
                message=f"{'Valid' if is_base_standard else 'Non-standard'} sub-node of '{base_name}' ({pattern_type})",
                category="subnode" if is_base_standard else "subnode_nonstandard",
                confidence=0.9 if is_base_standard else 0.7,
            )

        # Check generic segment pattern last
        match = re.match(self.SEGMENT_PATTERN, name.lower())
        if match and "_" in name:
            base_name = match.group(1)
            suffix = match.group(2)

            # Only treat as segment if it doesn't match other patterns
            # and has a reasonable structure (geographic/business segment)
            segment_keywords = [
                "america",
                "europe",
                "asia",
                "pacific",
                "africa",
                "region",
                "domestic",
                "international",
                "global",
                "local",
                "retail",
                "wholesale",
                "online",
                "digital",
                "services",
                "products",
                "solutions",
                "segment",
                "division",
                "unit",
            ]

            if len(suffix) > 2 and any(
                keyword in suffix.lower() for keyword in segment_keywords
            ):
                # Normalize base name for registry check
                is_base_standard = self._registry.is_recognized_name(base_name.lower())

                return ValidationResult(
                    original_name=name,
                    standardized_name=name,
                    is_valid=not self.strict_mode or is_base_standard,
                    message=f"{'Valid' if is_base_standard else 'Non-standard'} sub-node of '{base_name}' (segment)",
                    category="subnode" if is_base_standard else "subnode_nonstandard",
                    confidence=0.85 if is_base_standard else 0.65,
                )

        return None

    def _check_formula_ending(self, name: str) -> Optional[tuple[str, str]]:
        """Check if name ends with a formula pattern."""
        name_lower = name.lower()

        for ending in self.FORMULA_ENDINGS:
            if name_lower.endswith(ending):
                base_name = name[: -len(ending)]
                formula_type = ending[1:]  # Remove underscore
                return base_name, formula_type

        return None

    def _check_patterns(
        self,
        name: str,
        patterns: list[tuple[str, str]],
        pattern_category: str,
    ) -> Optional[tuple[str, str, str]]:
        """Check if name matches any pattern in the list."""
        name_lower = name.lower()

        for pattern, pattern_type in patterns:
            match = re.match(pattern, name_lower)
            if match:
                base_name = match.group(1)
                suffix = (
                    match.group(2)
                    if (match.lastindex is not None and match.lastindex > 1)
                    else ""
                )
                return base_name, suffix, pattern_type

        return None

    def _generate_suggestions(self, name: str) -> list[str]:
        """Generate improvement suggestions for non-standard names."""
        suggestions_with_scores = []
        name_lower = name.lower()

        # Find similar standard names
        for std_name in self._registry.list_standard_names():
            std_lower = std_name.lower()

            # Calculate similarity score
            score = 0.0

            # Exact prefix match gets highest score
            if std_lower.startswith(name_lower) or name_lower.startswith(std_lower):
                score = 0.9
            # Check character overlap
            elif self._is_similar(name_lower, std_lower):
                overlap = len(set(name_lower) & set(std_lower))
                min_len = min(len(name_lower), len(std_lower))
                score = overlap / min_len * 0.8

            if score > 0:
                suggestions_with_scores.append(
                    (score, f"Consider using standard name: '{std_name}'")
                )

        # Sort by score (highest first) and take top suggestions
        suggestions_with_scores.sort(key=lambda x: x[0], reverse=True)
        suggestions = [msg for _, msg in suggestions_with_scores[:3]]

        # Check for pattern improvements
        if "_" in name and len(suggestions) < 3:
            parts = name.split("_", 1)
            base = parts[0]

            # Suggest standardizing the base
            for std_name in self._registry.list_standard_names():
                if self._is_similar(base.lower(), std_name.lower()):
                    suggestions.append(
                        f"Consider using '{std_name}_{parts[1]}' for consistency"
                    )
                    break

        # Generic suggestions if nothing specific found
        if not suggestions:
            if any(
                suffix in name for suffix in ["_margin", "_ratio", "_growth", "_pct"]
            ):
                suggestions.append(
                    "Formula node detected - ensure base name follows standard conventions"
                )
            else:
                suggestions.append(
                    "Consider using a standard node name for better metric compatibility"
                )

        return suggestions[:3]  # Return top 3 suggestions

    def _is_similar(self, str1: str, str2: str, threshold: float = 0.6) -> bool:
        """Check if two strings are similar enough."""
        if len(str1) < 3 or len(str2) < 3:
            return False

        # Check if one is a prefix of the other
        if str1.startswith(str2) or str2.startswith(str1):
            return True

        # Check containment
        if str1 in str2 or str2 in str1:
            return True

        # Check character overlap
        overlap = len(set(str1) & set(str2))
        min_len = min(len(str1), len(str2))

        return overlap / min_len >= threshold

    def validate_batch(
        self,
        names: list[str],
        node_types: Optional[dict[str, str]] = None,
        parent_map: Optional[dict[str, list[str]]] = None,
    ) -> dict[str, ValidationResult]:
        """Validate multiple node names efficiently.

        Args:
            names: List of node names to validate.
            node_types: Optional mapping of names to node types.
            parent_map: Optional mapping of names to parent node lists.

        Returns:
            Dictionary mapping names to ValidationResults.
        """
        results = {}
        node_types = node_types or {}
        parent_map = parent_map or {}

        for name in names:
            result = self.validate(
                name,
                node_type=node_types.get(name),
                parent_nodes=parent_map.get(name),
            )
            results[name] = result

        return results

    def validate_graph(self, nodes: list[Node]) -> dict[str, Any]:
        """Validate all nodes in a graph with full context.

        Args:
            nodes: List of Node objects from the graph.

        Returns:
            Comprehensive validation report.
        """
        # Build context maps
        node_types = {}
        parent_map = {}

        for node in nodes:
            # Determine node type
            class_name = node.__class__.__name__
            if "Formula" in class_name:
                node_types[node.name] = "formula"
            elif "Calculation" in class_name:
                node_types[node.name] = "calculation"
            elif "Forecast" in class_name:
                node_types[node.name] = "forecast"
            else:
                node_types[node.name] = "data"

            # Extract parent nodes
            if hasattr(node, "inputs"):
                if isinstance(node.inputs, dict):
                    parent_map[node.name] = [p.name for p in node.inputs.values()]
                elif isinstance(node.inputs, list):
                    parent_map[node.name] = [p.name for p in node.inputs]

        # Validate all nodes
        node_names = [node.name for node in nodes]
        results = self.validate_batch(node_names, node_types, parent_map)

        # Build summarized report
        by_category: dict[str, list[str]] = {}
        by_validity: dict[str, int] = {"valid": 0, "invalid": 0}
        suggestions: dict[str, list[str]] = {}
        # Populate report sections
        for name, result in results.items():
            # Count by category
            cat = result.category
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(name)
            # Count by validity
            if result.is_valid:
                by_validity["valid"] += 1
            else:
                by_validity["invalid"] += 1
            # Collect suggestions
            if result.suggestions:
                suggestions[name] = result.suggestions
        return {
            "total": len(results),
            "by_category": by_category,
            "by_validity": by_validity,
            "suggestions": suggestions,
            "details": results,
        }

    def clear_cache(self) -> None:
        """Clear the validation cache."""
        self._validation_cache.clear()


def create_validator(
    registry: StandardNodeRegistry, **kwargs: Any
) -> UnifiedNodeValidator:
    """Create a validator instance with the given configuration."""
    return UnifiedNodeValidator(registry, **kwargs)



================================================================================
File: fin_statement_model/logging_config.py
================================================================================

"""Centralized logging configuration for the fin_statement_model library.

This module provides consistent logging configuration across the entire package.
It sets up appropriate formatters, handlers, and logging levels for different
components of the library.
"""

import logging
import logging.handlers
import os
import sys
from typing import Optional


# Default format for log messages
DEFAULT_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
DETAILED_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(funcName)s() - %(message)s"

# Environment variable to control logging level
LOG_LEVEL_ENV = "FSM_LOG_LEVEL"
LOG_FORMAT_ENV = "FSM_LOG_FORMAT"


def get_logger(name: str) -> logging.Logger:
    """Get a logger instance with the given name.

    This is a convenience function that ensures all loggers are properly
    namespaced under the fin_statement_model package.

    Args:
        name: The name of the logger, typically __name__

    Returns:
        A configured logger instance
    """
    if not name.startswith("fin_statement_model") and name.startswith("."):
        name = f"fin_statement_model{name}"
    return logging.getLogger(name)


def setup_logging(
    level: Optional[str] = None,
    format_string: Optional[str] = None,
    log_file_path: Optional[str] = None,
    detailed: bool = False,
) -> None:
    """Configure logging for the fin_statement_model library.

    This function should be called once at application startup to configure
    logging behavior. If not called, the library will use a NullHandler to
    avoid "no handler" warnings.

    Args:
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).
               If None, uses FSM_LOG_LEVEL env var or defaults to WARNING.
        format_string: Custom format string for log messages.
                      If None, uses FSM_LOG_FORMAT env var or default format.
        log_file_path: Optional file path to write logs to.
        detailed: If True, uses detailed format with file/line information.

    Example:
        >>> from fin_statement_model import logging_config
        >>> logging_config.setup_logging(level="INFO", detailed=True)
    """
    # Determine log level
    if level is None:
        level = os.environ.get(LOG_LEVEL_ENV, "WARNING")

    # Convert string level to logging constant
    numeric_level = getattr(logging, level.upper(), logging.WARNING)

    # Determine format
    if format_string is None:
        format_string = os.environ.get(LOG_FORMAT_ENV)
        if format_string is None:
            format_string = DETAILED_FORMAT if detailed else DEFAULT_FORMAT

    # Get the root logger for fin_statement_model
    root_logger = logging.getLogger("fin_statement_model")
    root_logger.setLevel(numeric_level)

    # Remove any existing handlers to avoid duplicates
    root_logger.handlers.clear()

    # Create formatter
    formatter = logging.Formatter(format_string)

    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    # Optional file handler
    if log_file_path:
        file_handler = logging.handlers.RotatingFileHandler(
            log_file_path,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5,
        )
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)

    # Set specific log levels for noisy sub-modules if needed
    # For example, reduce verbosity of certain components during normal operation
    logging.getLogger("fin_statement_model.io.formats").setLevel(
        max(numeric_level, logging.INFO)
    )
    logging.getLogger("fin_statement_model.core.graph.traverser").setLevel(
        max(numeric_level, logging.INFO)
    )


def configure_library_logging() -> None:
    """Configure default logging for library usage.

    This is called automatically when the library is imported and sets up
    a NullHandler to prevent "no handler" warnings. Users should call
    setup_logging() to enable actual logging output.
    """
    # Attach a NullHandler to the base fin_statement_model logger so that
    # all child loggers inherit it and avoid 'No handler' warnings by default.
    base_logger = logging.getLogger("fin_statement_model")

    # Only add NullHandler if no handlers exist
    if not base_logger.handlers:
        base_logger.addHandler(logging.NullHandler())

    # Prevent propagation to root logger by default
    base_logger.propagate = False


# Configure library logging on import
configure_library_logging()


# Logging best practices for fin_statement_model:
#
# 1. Always use logger = logging.getLogger(__name__) in modules
# 2. Use appropriate log levels:
#    - DEBUG: Detailed information for diagnosing problems
#    - INFO: General informational messages
#    - WARNING: Something unexpected happened but the app is still working
#    - ERROR: A serious problem occurred, function cannot proceed
#    - CRITICAL: A very serious error occurred, program may be unable to continue
#
# 3. Include context in log messages:
#    - Good: logger.info(f"Loaded {count} metrics from {filepath}")
#    - Bad: logger.info("Metrics loaded")
#
# 4. Use logger.exception() in except blocks to capture stack traces:
#    try:
#        risky_operation()
#    except Exception:
#        logger.exception("Failed to perform risky operation")
#
# 5. For performance-sensitive code, check log level before expensive operations:
#    if logger.isEnabledFor(logging.DEBUG):
#        logger.debug(f"Expensive debug info: {expensive_function()}")



================================================================================
File: fin_statement_model/preprocessing/__init__.py
================================================================================

"""Export DataTransformer, CompositeTransformer, and TransformerFactory for preprocessing.

This module exposes core transformer interfaces and factory for the preprocessing layer.
"""

from .base_transformer import DataTransformer, CompositeTransformer
from .transformer_service import TransformerFactory, TransformationService
from .errors import (
    PreprocessingError,
    TransformerRegistrationError,
    TransformerConfigurationError,
    PeriodConversionError,
    NormalizationError,
    TimeSeriesError,
)

## Trigger transformer discovery on package import
TransformerFactory.discover_transformers(
    "fin_statement_model.preprocessing.transformers"
)

__all__ = [
    "CompositeTransformer",
    "DataTransformer",
    "NormalizationError",
    "PeriodConversionError",
    "PreprocessingError",
    "TimeSeriesError",
    "TransformationService",
    "TransformerConfigurationError",
    "TransformerFactory",
    "TransformerRegistrationError",
]



================================================================================
File: fin_statement_model/preprocessing/base_transformer.py
================================================================================

"""Define base DataTransformer interface for preprocessing layer.

This module provides the DataTransformer abstract base class and CompositeTransformer.
"""

from abc import ABC, abstractmethod
import pandas as pd
from typing import Optional, Union
import logging

from fin_statement_model.core.errors import TransformationError

logger = logging.getLogger(__name__)


class DataTransformer(ABC):
    """Define base class for data transformers.

    Data transformers convert data between formats and apply business rules.

    This separation follows the Single Responsibility Principle for maintainability.
    """

    def __init__(self, config: Optional[dict[str, object]] = None):
        """Initialize the transformer with optional configuration.

        Args:
            config: Optional configuration dictionary for the transformer
        """
        self.config = config or {}
        logger.debug(
            f"Initialized {self.__class__.__name__} with config: {self.config}"
        )

    def transform(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Transform the input data.

        Args:
            data: The data to transform.

        Returns:
            The transformed data.

        Raises:
            TransformationError: If transformation fails.
        """
        try:
            logger.debug(f"Transforming data with {self.__class__.__name__}")
            return self._transform_impl(data)
        except Exception as e:
            logger.exception(f"Error transforming data with {self.__class__.__name__}")
            raise TransformationError(
                "Error transforming data",
                transformer_type=self.__class__.__name__,
            ) from e

    @abstractmethod
    def _transform_impl(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Apply the transformation logic.

        This method must be implemented by subclasses to define the specific
        transformation logic.

        Args:
            data: The data to transform.

        Returns:
            The transformed data.

        Raises:
            TransformationError: If transformation fails.
        """

    def validate_input(self, data: object) -> bool:
        """Validate that the input data is a pandas DataFrame by default.

        This performs a basic DataFrame type check and can be overridden by subclasses with more specific validation logic.

        Args:
            data (object): The input data to validate.

        Returns:
            bool: True if data is a pandas.DataFrame, False otherwise.
        """
        return isinstance(data, pd.DataFrame)

    def _pre_transform_hook(self, data: object) -> object:
        """Hook method called before transformation.

        Args:
            data: The input data

        Returns:
            Processed data to be passed to the transform method

        This method can be overridden by subclasses to add pre-processing steps.
        """
        return data

    def _post_transform_hook(self, data: object) -> object:
        """Hook method called after transformation.

        Args:
            data: The transformed data

        Returns:
            Final processed data

        This method can be overridden by subclasses to add post-processing steps.
        """
        return data

    def execute(self, data: object) -> object:
        """Execute the complete transformation pipeline.

        Args:
            data: The input data to transform

        Returns:
            Transformed data

        Raises:
            ValueError: If the data is invalid or cannot be transformed
        """
        if not self.validate_input(data):
            raise ValueError(f"Invalid input data for {self.__class__.__name__}")

        try:
            # Apply pre-transform hook
            processed_data = self._pre_transform_hook(data)

            # Perform transformation
            result = self.transform(processed_data)
            result = self._post_transform_hook(result)
            logger.debug(
                f"Successfully transformed data with {self.__class__.__name__}"
            )
        except Exception as e:
            logger.exception(f"Error transforming data with {self.__class__.__name__}")
            raise ValueError("Error transforming data") from e
        else:
            return result

    def validate_config(self) -> None:
        """Validate the transformer configuration.

        This method can be overridden by subclasses to add specific validation logic.

        Raises:
            TransformationError: If the configuration is invalid.
        """
        if self.config is None:
            raise TransformationError(
                f"Invalid input data for {self.__class__.__name__}",
                transformer_type=self.__class__.__name__,
            )


class CompositeTransformer(DataTransformer):
    """Compose multiple transformers into a pipeline.

    This allows building complex transformation chains from simple steps.
    """

    def __init__(
        self,
        transformers: list[DataTransformer],
        config: Optional[dict[str, object]] = None,
    ):
        """Initialize with a list of transformers.

        Args:
            transformers: List of transformers to apply in sequence
            config: Optional configuration dictionary
        """
        super().__init__(config)
        self.transformers = transformers

    def _transform_impl(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Apply each transformer in sequence.

        Args:
            data: The input data to transform

        Returns:
            Data transformed by the pipeline
        """
        result = data
        for transformer in self.transformers:
            result = transformer.execute(result)
        return result

    def add_transformer(self, transformer: DataTransformer) -> None:
        """Add a transformer to the pipeline.

        Args:
            transformer: The transformer to add
        """
        self.transformers.append(transformer)

    def remove_transformer(self, index: int) -> Optional[DataTransformer]:
        """Remove a transformer from the pipeline.

        Args:
            index: Index of the transformer to remove

        Returns:
            The removed transformer or None if index is invalid
        """
        if 0 <= index < len(self.transformers):
            return self.transformers.pop(index)
        return None

    def validate_input(self, data: object) -> bool:
        """Validate input for the composite transformer.

        If the pipeline is empty, accepts any data; otherwise, delegates validation to the first transformer.

        Args:
            data (object): Input data to validate.

        Returns:
            bool: True if input is valid for the pipeline.
        """
        if not hasattr(self, "transformers") or not self.transformers:
            return True
        return self.transformers[0].validate_input(data)



================================================================================
File: fin_statement_model/preprocessing/config.py
================================================================================

"""Configuration models and enums for preprocessing transformers.

This module contains Pydantic models and Enums for configuring preprocessing transformations.
"""

from enum import Enum
from typing import Optional
from pydantic import BaseModel


class NormalizationType(Enum):
    """Available normalization types for NormalizationTransformer."""

    PERCENT_OF = "percent_of"
    MINMAX = "minmax"
    STANDARD = "standard"
    SCALE_BY = "scale_by"


class TransformationType(Enum):
    """Available transformation types for TimeSeriesTransformer."""

    GROWTH_RATE = "growth_rate"
    MOVING_AVG = "moving_avg"
    CAGR = "cagr"
    YOY = "yoy"
    QOQ = "qoq"


class ConversionType(Enum):
    """Available conversion types for PeriodConversionTransformer."""

    QUARTERLY_TO_ANNUAL = "quarterly_to_annual"
    MONTHLY_TO_QUARTERLY = "monthly_to_quarterly"
    MONTHLY_TO_ANNUAL = "monthly_to_annual"
    QUARTERLY_TO_TTM = "quarterly_to_ttm"


class StatementType(Enum):
    """Available statement types for StatementFormattingTransformer."""

    INCOME_STATEMENT = "income_statement"
    BALANCE_SHEET = "balance_sheet"
    CASH_FLOW = "cash_flow"


class NormalizationConfig(BaseModel):
    """Configuration for normalization transformations."""

    normalization_type: Optional[str] = None
    reference: Optional[str] = None
    scale_factor: Optional[float] = None


class TimeSeriesConfig(BaseModel):
    """Configuration for time series transformations."""

    transformation_type: Optional[str] = None
    periods: Optional[int] = None
    window_size: Optional[int] = None


class PeriodConversionConfig(BaseModel):
    """Configuration for period conversion transformations."""

    conversion_type: Optional[str] = None
    aggregation: Optional[str] = None


class StatementFormattingConfig(BaseModel):
    """Configuration for formatting statement output."""

    statement_type: Optional[str] = None
    add_subtotals: Optional[bool] = None
    apply_sign_convention: Optional[bool] = None


__all__ = [
    "ConversionType",
    "NormalizationConfig",
    "NormalizationType",
    "PeriodConversionConfig",
    "StatementFormattingConfig",
    "StatementType",
    "TimeSeriesConfig",
    "TransformationType",
]



================================================================================
File: fin_statement_model/preprocessing/errors.py
================================================================================

"""Custom Exception classes for the preprocessing package.

These exceptions provide specific error information related to data
preprocessing, transformation, and validation operations.
"""

from typing import Optional, Any
from fin_statement_model.core.errors import (
    FinancialModelError,
    TransformationError,
)

__all__ = [
    "NormalizationError",
    "PeriodConversionError",
    "PreprocessingError",
    "TimeSeriesError",
    "TransformerConfigurationError",
    "TransformerRegistrationError",
]


class PreprocessingError(FinancialModelError):
    """Base exception for all preprocessing-related errors."""


class TransformerRegistrationError(PreprocessingError):
    """Exception raised for transformer registration issues.

    This includes attempts to register duplicate transformers or
    register invalid transformer classes.
    """

    def __init__(
        self,
        message: str,
        transformer_name: Optional[str] = None,
        existing_class: Optional[type] = None,
    ):
        """Initialize a TransformerRegistrationError.

        Args:
            message: The primary error message.
            transformer_name: Optional name of the transformer.
            existing_class: Optional existing class that's already registered.
        """
        self.transformer_name = transformer_name
        self.existing_class = existing_class

        full_message = message
        if transformer_name:
            full_message = f"{message} for transformer '{transformer_name}'"
        if existing_class:
            full_message = (
                f"{full_message} (already registered as {existing_class.__name__})"
            )

        super().__init__(full_message)


class TransformerConfigurationError(PreprocessingError):
    """Exception raised for invalid transformer configuration.

    This includes missing required parameters, invalid parameter values,
    or incompatible configuration options.
    """

    def __init__(
        self,
        message: str,
        transformer_name: Optional[str] = None,
        config: Optional[dict[str, Any]] = None,
        missing_params: Optional[list[str]] = None,
    ):
        """Initialize a TransformerConfigurationError.

        Args:
            message: The primary error message.
            transformer_name: Optional name of the transformer.
            config: Optional configuration dictionary that caused the error.
            missing_params: Optional list of missing required parameters.
        """
        self.transformer_name = transformer_name
        self.config = config
        self.missing_params = missing_params or []

        details = []
        if transformer_name:
            details.append(f"Transformer: {transformer_name}")
        if missing_params:
            details.append(f"Missing params: {', '.join(missing_params)}")

        full_message = message
        if details:
            full_message = f"{message} ({'; '.join(details)})"

        super().__init__(full_message)


class PeriodConversionError(TransformationError):
    """Exception raised for period conversion failures.

    This includes invalid period formats, unsupported conversion types,
    or missing date/period columns.
    """

    def __init__(
        self,
        message: str,
        source_period: Optional[str] = None,
        target_period: Optional[str] = None,
        date_column: Optional[str] = None,
    ):
        """Initialize a PeriodConversionError.

        Args:
            message: The primary error message.
            source_period: Optional source period type.
            target_period: Optional target period type.
            date_column: Optional name of the date column.
        """
        self.source_period = source_period
        self.target_period = target_period
        self.date_column = date_column

        details: dict[str, Any] = {}
        if source_period and target_period:
            details["conversion"] = f"{source_period} -> {target_period}"
        if date_column:
            details["date_column"] = date_column

        super().__init__(
            message,
            transformer_type="PeriodConversionTransformer",
            parameters=details,
        )


class NormalizationError(TransformationError):
    """Exception raised for normalization failures.

    This includes missing reference columns, invalid normalization methods,
    or data type incompatibilities.
    """

    def __init__(
        self,
        message: str,
        method: Optional[str] = None,
        reference_field: Optional[str] = None,
        scale_factor: Optional[float] = None,
    ):
        """Initialize a NormalizationError.

        Args:
            message: The primary error message.
            method: Optional normalization method.
            reference_field: Optional reference field for percent_of method.
            scale_factor: Optional scale factor for scale_by method.
        """
        self.method = method
        self.reference_field = reference_field
        self.scale_factor = scale_factor

        params: dict[str, Any] = {}
        if method:
            params["method"] = method
        if reference_field:
            params["reference"] = reference_field
        if scale_factor is not None:
            params["scale_factor"] = scale_factor

        super().__init__(
            message,
            transformer_type="NormalizationTransformer",
            parameters=params,
        )


class TimeSeriesError(TransformationError):
    """Exception raised for time series transformation failures.

    This includes invalid window sizes, missing columns, or
    incompatible aggregation methods.
    """

    def __init__(
        self,
        message: str,
        operation: Optional[str] = None,
        window_size: Optional[int] = None,
        column: Optional[str] = None,
    ):
        """Initialize a TimeSeriesError.

        Args:
            message: The primary error message.
            operation: Optional operation type (e.g., 'rolling_mean', 'lag').
            window_size: Optional window size parameter.
            column: Optional column being processed.
        """
        self.operation = operation
        self.window_size = window_size
        self.column = column

        params: dict[str, Any] = {}
        if operation:
            params["operation"] = operation
        if window_size is not None:
            params["window_size"] = window_size
        if column:
            params["column"] = column

        super().__init__(
            message,
            transformer_type="TimeSeriesTransformer",
            parameters=params,
        )



================================================================================
File: fin_statement_model/preprocessing/transformer_service.py
================================================================================

"""Provide TransformerFactory and TransformationService for preprocessing.

This module merges the transformer factory and transformation service into a single module for simplicity.
"""

import importlib
import inspect
import logging
import pkgutil
import re
from typing import Any, ClassVar, Optional, Union

import pandas as pd

from fin_statement_model.preprocessing.base_transformer import (
    DataTransformer,
    CompositeTransformer,
)
from fin_statement_model.preprocessing.errors import (
    TransformerRegistrationError,
    TransformerConfigurationError,
)
from fin_statement_model.config.helpers import cfg

logger = logging.getLogger(__name__)


class TransformerFactory:
    """Create and manage transformer instances.

    Centralizes transformer registration, discovery, and instantiation.
    """

    _transformers: ClassVar[dict[str, type[DataTransformer]]] = {}

    @classmethod
    def register_transformer(
        cls, name: str, transformer_class: type[DataTransformer]
    ) -> None:
        """Register a transformer class with the factory."""
        if name in cls._transformers:
            raise TransformerRegistrationError(
                f"Transformer name '{name}' is already registered",
                transformer_name=name,
                existing_class=cls._transformers[name],
            )
        if not issubclass(transformer_class, DataTransformer):
            raise TransformerRegistrationError(
                "Transformer class must be a subclass of DataTransformer",
                transformer_name=name,
            )
        cls._transformers[name] = transformer_class
        logger.info(f"Registered transformer '{name}'")

    @classmethod
    def create_transformer(cls, name: str, **kwargs: Any) -> DataTransformer:
        """Create a transformer instance by name."""
        if name not in cls._transformers:
            raise TransformerConfigurationError(
                f"No transformer registered with name '{name}'",
                transformer_name=name,
            )
        transformer_class = cls._transformers[name]
        transformer = transformer_class(**kwargs)
        logger.debug(f"Created transformer '{name}'")
        return transformer

    @classmethod
    def list_transformers(cls) -> list[str]:
        """List all registered transformer names."""
        return list(cls._transformers.keys())

    @classmethod
    def get_transformer_class(cls, name: str) -> type[DataTransformer]:
        """Get a transformer class by name."""
        if name not in cls._transformers:
            raise TransformerConfigurationError(
                f"No transformer registered with name '{name}'",
                transformer_name=name,
            )
        return cls._transformers[name]

    @classmethod
    def discover_transformers(cls, package_name: str) -> None:
        """Discover and register all transformers in a package."""
        try:
            package = importlib.import_module(package_name)
            package_path = package.__path__
            for _, module_name, _ in pkgutil.iter_modules(package_path):
                full_module_name = f"{package_name}.{module_name}"
                module = importlib.import_module(full_module_name)
                for obj_name, obj in inspect.getmembers(module):
                    if (
                        inspect.isclass(obj)
                        and issubclass(obj, DataTransformer)
                        and obj is not DataTransformer
                    ):
                        cls.register_transformer(obj_name, obj)
                        snake = re.sub(r"(.)([A-Z][a-z]+)", r"\1_\2", obj_name)
                        snake = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", snake).lower()
                        alias = snake.replace("_transformer", "")
                        if alias not in cls._transformers:
                            cls.register_transformer(alias, obj)
            logger.info(f"Discovered transformers from package '{package_name}'")
        except ImportError:
            logger.exception(
                f"Error discovering transformers from package '{package_name}'"
            )

    @classmethod
    def create_composite_transformer(
        cls, transformer_names: list[str], **kwargs: Any
    ) -> DataTransformer:
        """Create a composite transformer from a list of transformer names."""
        transformers = [
            cls.create_transformer(name, **kwargs) for name in transformer_names
        ]
        return CompositeTransformer(transformers)


class TransformationService:
    """Service for managing and applying data transformations."""

    def __init__(self) -> None:
        """Initialize the transformation service."""
        logger.info("TransformationService initialized")

    def normalize_data(
        self,
        data: Union[pd.DataFrame, dict[str, Any]],
        normalization_type: Optional[str] = None,
        reference: Optional[str] = None,
        scale_factor: Optional[float] = None,
    ) -> Union[pd.DataFrame, dict[str, Any]]:
        """Normalize financial data."""
        # Determine normalization type default
        default_norm_type = cfg("preprocessing.default_normalization_type")
        norm_type = (
            normalization_type if normalization_type is not None else default_norm_type
        ) or "percent_of"
        transformer = TransformerFactory.create_transformer(
            "normalization",
            normalization_type=norm_type,
            reference=reference,
            scale_factor=scale_factor,
        )
        return transformer.execute(data)

    def transform_time_series(
        self,
        data: Union[pd.DataFrame, dict[str, Any]],
        transformation_type: Optional[str] = None,
        periods: Optional[int] = None,
        window_size: Optional[int] = None,
    ) -> Union[pd.DataFrame, dict[str, Any]]:
        """Apply time series transformations to financial data."""
        # Determine defaults from config
        default_transform_type = cfg("preprocessing.default_transformation_type")
        transform_type = (
            transformation_type
            if transformation_type is not None
            else default_transform_type
        )
        default_periods = cfg("preprocessing.default_time_series_periods")
        num_periods = periods if periods is not None else default_periods
        default_window = cfg("preprocessing.default_time_series_window_size")
        win_size = window_size if window_size is not None else default_window
        transformer = TransformerFactory.create_transformer(
            "time_series",
            transformation_type=transform_type,
            periods=num_periods,
            window_size=win_size,
        )
        return transformer.execute(data)

    def convert_periods(
        self,
        data: pd.DataFrame,
        conversion_type: str,
        aggregation: Optional[str] = None,
    ) -> pd.DataFrame:
        """Convert data between different period types."""
        default_agg = cfg("preprocessing.default_conversion_aggregation")
        agg = aggregation if aggregation is not None else default_agg
        transformer = TransformerFactory.create_transformer(
            "period_conversion",
            conversion_type=conversion_type,
            aggregation=agg,
        )
        return transformer.execute(data)

    def format_statement(
        self,
        data: pd.DataFrame,
        statement_type: Optional[str] = None,
        add_subtotals: Optional[bool] = None,
        apply_sign_convention: Optional[bool] = None,
    ) -> pd.DataFrame:
        """Format a financial statement DataFrame."""
        # Determine defaults from preprocessing config
        stmt_cfg = cfg("preprocessing.statement_formatting")
        default_stmt_type = stmt_cfg.statement_type or "income_statement"
        stmt_type = statement_type if statement_type is not None else default_stmt_type
        default_add = (
            stmt_cfg.add_subtotals if hasattr(stmt_cfg, "add_subtotals") else True
        )
        sub = add_subtotals if add_subtotals is not None else default_add
        default_sign = (
            stmt_cfg.apply_sign_convention
            if hasattr(stmt_cfg, "apply_sign_convention")
            else True
        )
        sign = (
            apply_sign_convention if apply_sign_convention is not None else default_sign
        )
        transformer = TransformerFactory.create_transformer(
            "statement_formatting",
            statement_type=stmt_type,
            add_subtotals=sub,
            apply_sign_convention=sign,
        )
        return transformer.execute(data)

    def create_transformation_pipeline(
        self, transformers_config: list[dict[str, Any]]
    ) -> DataTransformer:
        """Create a composite transformer from configurations."""
        transformers = []
        for config in transformers_config:
            if "name" not in config:
                raise ValueError(
                    "Each transformer configuration must have a 'name' field"
                )
            name = config.pop("name")
            transformer = TransformerFactory.create_transformer(name, **config)
            transformers.append(transformer)
        return CompositeTransformer(transformers)

    def apply_transformation_pipeline(
        self, data: object, transformers_config: list[dict[str, Any]]
    ) -> object:
        """Apply a transformation pipeline to data."""
        pipeline = self.create_transformation_pipeline(transformers_config)
        return pipeline.execute(data)

    def register_custom_transformer(
        self, name: str, transformer_class: type[DataTransformer]
    ) -> None:
        """Register a custom transformer with the factory."""
        TransformerFactory.register_transformer(name, transformer_class)
        logger.info(f"Registered custom transformer: {name}")

    def list_available_transformers(self) -> list[str]:
        """List all available transformer types."""
        return TransformerFactory.list_transformers()



================================================================================
File: fin_statement_model/preprocessing/transformers/__init__.py
================================================================================

"""Package for preprocessing transformers.

This package exports built-in data transformer classes for the preprocessing layer.
"""

from .normalization import NormalizationTransformer
from .time_series import TimeSeriesTransformer
from .period_conversion import PeriodConversionTransformer

__all__ = [
    "NormalizationTransformer",
    "PeriodConversionTransformer",
    "TimeSeriesTransformer",
]



================================================================================
File: fin_statement_model/preprocessing/transformers/normalization.py
================================================================================

"""Provide a NormalizationTransformer to normalize financial data.

Transforms data by percent_of, minmax, standard, or scale_by methods.

This module implements the NormalizationTransformer for the preprocessing layer.
"""

from typing import Optional, Union, ClassVar
import logging

import numpy as np
import pandas as pd

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.config import (
    NormalizationConfig,
    NormalizationType,
)
from fin_statement_model.preprocessing.errors import NormalizationError
from fin_statement_model.core.errors import DataValidationError

logger = logging.getLogger(__name__)


class NormalizationTransformer(DataTransformer):
    """Transformer that normalizes financial data using various methods.

    This transformer provides multiple normalization strategies commonly used in
    financial analysis to make data comparable across different scales or to
    express values as percentages of a reference metric.

    Supported normalization types:
        - **percent_of**: Express values as percentages of a reference column
          (e.g., all items as % of revenue)
        - **minmax**: Scale values to [0, 1] range based on min/max values
        - **standard**: Standardize using (x - mean) / std deviation
        - **scale_by**: Multiply all values by a fixed scale factor

    Examples:
        Express all income statement items as percentage of revenue:

        >>> import pandas as pd
        >>> from fin_statement_model.preprocessing.transformers import NormalizationTransformer
        >>>
        >>> # Sample income statement data
        >>> data = pd.DataFrame({
        ...     'revenue': [1000, 1100, 1200],
        ...     'cogs': [600, 650, 700],
        ...     'operating_expenses': [200, 220, 250]
        ... }, index=['2021', '2022', '2023'])
        >>>
        >>> # Create transformer to express as % of revenue
        >>> normalizer = NormalizationTransformer(
        ...     normalization_type='percent_of',
        ...     reference='revenue'
        ... )
        >>>
        >>> # Transform the data
        >>> normalized = normalizer.transform(data)
        >>> print(normalized)
        #       revenue  cogs  operating_expenses
        # 2021    100.0  60.0               20.0
        # 2022    100.0  59.1               20.0
        # 2023    100.0  58.3               20.8

        Scale financial data to millions:

        >>> # Scale values to millions (divide by 1,000,000)
        >>> scaler = NormalizationTransformer(
        ...     normalization_type='scale_by',
        ...     scale_factor=0.000001
        ... )
        >>> scaled = scaler.transform(data)

    Note:
        For 'percent_of' normalization, if a reference value is 0 or NaN,
        the corresponding output for that row will be NaN to avoid division
        by zero errors.
    """

    NORMALIZATION_TYPES: ClassVar[list[str]] = [t.value for t in NormalizationType]

    def __init__(
        self,
        normalization_type: Union[
            str, NormalizationType
        ] = NormalizationType.PERCENT_OF,
        reference: Optional[str] = None,
        scale_factor: Optional[float] = None,
        config: Optional[NormalizationConfig] = None,
    ):
        """Initialize the normalizer with specified parameters.

        Args:
            normalization_type: Type of normalization to apply. Can be either
                a string or NormalizationType enum value:
                - 'percent_of': Express values as percentage of reference column
                - 'minmax': Scale to [0,1] range
                - 'standard': Apply z-score normalization
                - 'scale_by': Multiply by scale_factor
            reference: Name of the reference column for 'percent_of' normalization.
                Required when normalization_type is 'percent_of'.
            scale_factor: Multiplication factor for 'scale_by' normalization.
                Required when normalization_type is 'scale_by'.
                Common values: 0.001 (to thousands), 0.000001 (to millions)
            config: Optional NormalizationConfig object containing configuration.
                If provided, overrides other parameters.

        Raises:
            NormalizationError: If normalization_type is invalid, or if required
                parameters are missing for the selected normalization type.
        """
        super().__init__(config.model_dump() if config else None)
        # Normalize enum to string
        if isinstance(normalization_type, NormalizationType):
            norm_type = normalization_type.value
        else:
            norm_type = normalization_type
        if norm_type not in self.NORMALIZATION_TYPES:
            raise NormalizationError(
                f"Invalid normalization type: {norm_type}. "
                f"Must be one of {self.NORMALIZATION_TYPES}",
                method=norm_type,
            )
        self.normalization_type = norm_type

        self.reference = reference
        self.scale_factor = scale_factor

        # Validation
        if (
            self.normalization_type == NormalizationType.PERCENT_OF.value
            and not reference
        ):
            raise NormalizationError(
                "Reference field must be provided for percent_of normalization",
                method=self.normalization_type,
            )

        if (
            self.normalization_type == NormalizationType.SCALE_BY.value
            and scale_factor is None
        ):
            raise NormalizationError(
                "Scale factor must be provided for scale_by normalization",
                method=self.normalization_type,
            )

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Normalize the data based on the configured normalization type.

        Args:
            data: DataFrame containing financial data to normalize.
                All columns will be normalized except the reference column
                in 'percent_of' normalization.

        Returns:
            DataFrame with normalized values. Original column names are preserved
            for all normalization types.

        Raises:
            DataValidationError: If data is not a pandas DataFrame.
            NormalizationError: If reference column is not found in DataFrame
                (for 'percent_of' normalization).
        """
        if not isinstance(data, pd.DataFrame):
            raise DataValidationError(
                f"Unsupported data type: {type(data)}. Expected pandas.DataFrame",
                validation_errors=[f"Got type: {type(data).__name__}"],
            )
        return super().transform(data)

    def validate_config(self) -> None:
        """Validate the transformer configuration.

        Raises:
            NormalizationError: If the configuration is invalid.
        """
        super().validate_config()

        if self.normalization_type not in self.NORMALIZATION_TYPES:
            raise NormalizationError(
                f"Unknown normalization method: {self.normalization_type}. "
                f"Supported methods are: {self.NORMALIZATION_TYPES}",
                method=self.normalization_type,
            )

        if (
            self.normalization_type == NormalizationType.PERCENT_OF.value
            and not self.reference
        ):
            raise NormalizationError(
                "Reference field must be provided for percent_of normalization",
                method=self.normalization_type,
            )

        if (
            self.normalization_type == NormalizationType.SCALE_BY.value
            and self.scale_factor is None
        ):
            raise NormalizationError(
                "Scale factor must be provided for scale_by normalization",
                method=self.normalization_type,
            )

    def _transform_impl(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Apply the normalization transformation.

        Args:
            data: The data to transform.

        Returns:
            The normalized data.

        Raises:
            DataValidationError: If the data type is not supported.
            NormalizationError: If there are issues during normalization.
        """
        if not isinstance(data, pd.DataFrame | pd.Series):
            raise DataValidationError(
                f"Unsupported data type: {type(data)}. Expected pandas.DataFrame or pandas.Series",
                validation_errors=[f"Got type: {type(data).__name__}"],
            )

        # Handle Series by converting to DataFrame temporarily
        if isinstance(data, pd.Series):
            temp_df = data.to_frame()
            result_df = self._normalize_dataframe(temp_df)
            return result_df.iloc[:, 0]  # Return as Series
        else:
            return self._normalize_dataframe(data)

    def _normalize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply normalization to a DataFrame.

        Args:
            df: DataFrame to normalize.

        Returns:
            Normalized DataFrame.

        Raises:
            NormalizationError: If reference column is not found or other normalization issues.
        """
        result = df.copy()

        if self.normalization_type == NormalizationType.PERCENT_OF.value:
            if self.reference not in df.columns:
                raise NormalizationError(
                    f"Reference column '{self.reference}' not found in DataFrame",
                    method=self.normalization_type,
                    reference_field=self.reference,
                )

            for col in df.columns:
                if col != self.reference:
                    # Replace 0 with NaN in the denominator to ensure division by zero results in NaN
                    reference_series = df[self.reference].replace(0, np.nan)
                    if (
                        reference_series.isnull().all()
                    ):  # If all reference values are NaN (or were 0)
                        result[col] = np.nan
                        logger.warning(
                            f"All reference values for '{self.reference}' are zero or NaN. '{col}' will be NaN."
                        )
                    else:
                        result[col] = (df[col] / reference_series) * 100

        elif (
            self.normalization_type == NormalizationType.MINMAX.value
        ):  # pragma: no cover
            for col in df.columns:
                min_val = df[col].min()
                max_val = df[col].max()

                if max_val > min_val:
                    result[col] = (df[col] - min_val) / (
                        max_val - min_val
                    )  # pragma: no cover
                elif max_val == min_val:  # Handles constant columns
                    result[col] = (
                        0.0  # Or np.nan, depending on desired behavior for constant series
                    )
                # else: max_val < min_val (should not happen with .min()/.max())

        elif self.normalization_type == NormalizationType.STANDARD.value:
            for col in df.columns:
                mean = df[col].mean()
                std = df[col].std()

                if std > 0:
                    result[col] = (df[col] - mean) / std
                elif std == 0:  # Handles constant columns
                    result[col] = 0.0  # Or np.nan, depending on desired behavior
                # else: std < 0 (not possible)

        elif self.normalization_type == NormalizationType.SCALE_BY.value:
            for col in df.columns:
                result[col] = df[col] * self.scale_factor

        return result



================================================================================
File: fin_statement_model/preprocessing/transformers/period_conversion.py
================================================================================

"""Financial data transformers for the Financial Statement Model.

This module provides the PeriodConversionTransformer for converting between period types:
quarterly_to_annual, monthly_to_quarterly, monthly_to_annual, and quarterly_to_ttm.
"""

import logging
import pandas as pd
from typing import Optional, Union, ClassVar

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.config import (
    ConversionType,
    PeriodConversionConfig,
)
from fin_statement_model.core.errors import DataValidationError

# Configure logging
logger = logging.getLogger(__name__)


class PeriodConversionTransformer(DataTransformer):
    """Transformer for converting between different financial reporting periods.

    This transformer aggregates financial data from higher-frequency periods
    (e.g., monthly, quarterly) to lower-frequency periods (e.g., quarterly, annual)
    or calculates trailing metrics like TTM (Trailing Twelve Months).

    Supported conversion types:
        - **quarterly_to_annual**: Aggregate 4 quarters into annual data
        - **monthly_to_quarterly**: Aggregate 3 months into quarterly data
        - **monthly_to_annual**: Aggregate 12 months into annual data
        - **quarterly_to_ttm**: Calculate trailing twelve months from quarterly data

    Input Data Requirements:
        - Data must have a DatetimeIndex or an index convertible to datetime
        - The index should represent the period-end dates
        - Data frequency should match the conversion type (e.g., quarterly data
          for quarterly_to_annual conversion)

    Aggregation Methods:
        - **sum**: Total values (default) - use for flow items like revenue, expenses
        - **mean**: Average values - use for rates, ratios, or average balances
        - **last**: Take last value - use for balance sheet items (point-in-time)
        - **first**: Take first value - use for opening balances
        - **max/min**: Maximum/minimum values - use for peak/trough analysis

    Examples:
        Convert quarterly revenue to annual totals:

        >>> import pandas as pd
        >>> from fin_statement_model.preprocessing.transformers import PeriodConversionTransformer
        >>>
        >>> # Quarterly revenue and expense data
        >>> quarterly_data = pd.DataFrame({
        ...     'revenue': [100, 110, 120, 130, 140, 150, 160, 170],
        ...     'expenses': [80, 85, 90, 95, 100, 105, 110, 115]
        ... }, index=pd.date_range('2022-03-31', periods=8, freq='Q'))
        >>>
        >>> # Convert to annual data (sum 4 quarters)
        >>> annual_converter = PeriodConversionTransformer(
        ...     conversion_type='quarterly_to_annual',
        ...     aggregation='sum'
        ... )
        >>> annual_data = annual_converter.transform(quarterly_data)
        >>> print(annual_data)
        #       revenue  expenses
        # 2022      460       350
        # 2023      620       430

        Convert monthly balance sheet to quarterly (taking last value):

        >>> # Monthly balance sheet data
        >>> monthly_bs = pd.DataFrame({
        ...     'total_assets': [1000, 1020, 1050, 1080, 1100, 1150],
        ...     'total_equity': [600, 610, 620, 630, 640, 650]
        ... }, index=pd.date_range('2023-01-31', periods=6, freq='M'))
        >>>
        >>> # Convert to quarterly, taking last month's value
        >>> quarterly_converter = PeriodConversionTransformer(
        ...     conversion_type='monthly_to_quarterly',
        ...     aggregation='last'
        ... )
        >>> quarterly_bs = quarterly_converter.transform(monthly_bs)
        >>> print(quarterly_bs)
        #                  total_assets  total_equity
        # (2023, 1)              1050           620
        # (2023, 2)              1150           650

        Calculate trailing twelve months (TTM) from quarterly data:

        >>> # Quarterly earnings data
        >>> quarterly_earnings = pd.DataFrame({
        ...     'net_income': [25, 30, 35, 40, 45, 50, 55, 60]
        ... }, index=pd.date_range('2022-03-31', periods=8, freq='Q'))
        >>>
        >>> # Calculate TTM (rolling 4-quarter sum)
        >>> ttm_converter = PeriodConversionTransformer(
        ...     conversion_type='quarterly_to_ttm',
        ...     aggregation='sum'
        ... )
        >>> ttm_data = ttm_converter.transform(quarterly_earnings)
        >>> print(ttm_data.iloc[3:])  # First 3 periods will be NaN
        #             net_income
        # 2023-03-31       130.0
        # 2023-06-30       150.0
        # 2023-09-30       170.0
        # 2023-12-31       190.0
        # 2024-03-31       210.0

    Note:
        - The resulting index format depends on the conversion type
        - Annual conversions group by year (integer index)
        - Quarterly conversions group by (year, quarter) tuple
        - TTM conversions maintain the original datetime index
        - Ensure your aggregation method matches the financial item type
    """

    # All valid conversion types
    CONVERSION_TYPES: ClassVar[list[str]] = [t.value for t in ConversionType]

    def __init__(
        self,
        conversion_type: Union[
            str, ConversionType
        ] = ConversionType.QUARTERLY_TO_ANNUAL,
        aggregation: str = "sum",
        config: Optional[PeriodConversionConfig] = None,
    ):
        """Initialize the period conversion transformer.

        Args:
            conversion_type: Type of period conversion to apply. Can be either
                a string or ConversionType enum value:
                - 'quarterly_to_annual': Convert 4 quarters to 1 year
                - 'monthly_to_quarterly': Convert 3 months to 1 quarter
                - 'monthly_to_annual': Convert 12 months to 1 year
                - 'quarterly_to_ttm': Calculate trailing twelve months
            aggregation: How to aggregate data within each period:
                - 'sum': Add up all values (default) - for flow items
                - 'mean': Calculate average - for rates/ratios
                - 'last': Take last value - for balance sheet items
                - 'first': Take first value - for opening balances
                - 'max': Take maximum value
                - 'min': Take minimum value
                - 'std': Calculate standard deviation
                - 'count': Count non-null values
            config: Optional PeriodConversionConfig object containing configuration.
                If provided, overrides other parameters.

        Raises:
            ValueError: If conversion_type is invalid.

        Examples:
            >>> # Annual totals from quarterly data
            >>> converter = PeriodConversionTransformer('quarterly_to_annual', 'sum')
            >>>
            >>> # Quarter-end balances from monthly data
            >>> converter = PeriodConversionTransformer('monthly_to_quarterly', 'last')
            >>>
            >>> # TTM revenue from quarterly data
            >>> converter = PeriodConversionTransformer('quarterly_to_ttm', 'sum')
        """
        super().__init__(config.model_dump() if config else None)
        # Normalize enum to string
        if isinstance(conversion_type, ConversionType):
            ctype = conversion_type.value
        else:
            ctype = conversion_type
        if ctype not in self.CONVERSION_TYPES:
            raise ValueError(
                f"Invalid conversion type: {ctype}. Must be one of {self.CONVERSION_TYPES}"
            )
        self.conversion_type = ctype
        self.aggregation = aggregation

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform data by converting between period types.

        Args:
            data: DataFrame with time-based data to convert. Must have either:
                - A DatetimeIndex
                - An index containing date/time strings parsable by pd.to_datetime
                - Period labels that can be converted to datetime

                The data frequency should match the source period type (e.g.,
                quarterly data for 'quarterly_to_annual' conversion).

        Returns:
            DataFrame with converted periods:
            - For annual conversions: Index will be years (integers)
            - For quarterly conversions: Index will be (year, quarter) tuples
            - For TTM conversions: Original datetime index is preserved

            All columns are aggregated according to the specified method.

        Raises:
            TypeError: If data is not a pandas DataFrame.
            ValueError: If index cannot be converted to datetime or if
                aggregation='sum' is used with 'quarterly_to_ttm' and a
                different aggregation method is specified.

        Examples:
            >>> # Convert quarterly data to annual
            >>> df = pd.DataFrame({
            ...     'revenue': [100, 110, 120, 130],
            ...     'costs': [60, 65, 70, 75]
            ... }, index=['2023-Q1', '2023-Q2', '2023-Q3', '2023-Q4'])
            >>>
            >>> converter = PeriodConversionTransformer('quarterly_to_annual')
            >>> annual = converter.transform(df)
            >>> print(annual)
            #       revenue  costs
            # 2023      460    270
        """
        # Ensure we have a DataFrame
        if not isinstance(data, pd.DataFrame):
            raise TypeError("Period conversion requires a pandas DataFrame")

        return super().transform(data)

    def _transform_impl(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Apply the period conversion transformation.

        Args:
            data: The data to transform.

        Returns:
            The transformed data.

        Raises:
            DataValidationError: If data is not a DataFrame or Series.
            ValueError: If conversion fails.
        """
        if not isinstance(data, pd.DataFrame | pd.Series):
            raise DataValidationError(
                "Period conversion requires a pandas DataFrame or Series",
                validation_errors=[f"Got type: {type(data).__name__}"],
            )

        # Handle Series by converting to DataFrame temporarily
        if isinstance(data, pd.Series):
            temp_df = data.to_frame()
            result_df = self._convert_periods(temp_df)
            return result_df.iloc[:, 0]  # Return as Series
        else:
            return self._convert_periods(data)

    def _convert_periods(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply period conversion to a DataFrame.

        Args:
            df: DataFrame to convert.

        Returns:
            Converted DataFrame.
        """
        df_copy = df.copy()

        # Try to convert index to datetime if it's not already
        if not isinstance(df_copy.index, pd.DatetimeIndex):
            try:
                df_copy.index = pd.to_datetime(df_copy.index)
                logger.debug("Successfully converted DataFrame index to DatetimeIndex.")
            except Exception as e:
                logger.exception(
                    "Failed to convert DataFrame index to DatetimeIndex. Ensure index contains standard date/time strings or is already a DatetimeIndex."
                )
                raise ValueError(
                    f"Index must be convertible to datetime for period conversion: {e}"
                )

        if self.conversion_type == ConversionType.QUARTERLY_TO_ANNUAL.value:
            # Group by year and aggregate
            return df_copy.groupby(df_copy.index.year).agg(self.aggregation)

        elif self.conversion_type == ConversionType.MONTHLY_TO_QUARTERLY.value:
            # Group by year and quarter
            return df_copy.groupby([df_copy.index.year, df_copy.index.quarter]).agg(
                self.aggregation
            )

        elif self.conversion_type == ConversionType.MONTHLY_TO_ANNUAL.value:
            # Group by year
            return df_copy.groupby(df_copy.index.year).agg(self.aggregation)

        elif self.conversion_type == ConversionType.QUARTERLY_TO_TTM.value:
            # Implement TTM as rolling sum with window=4 for quarterly data
            if self.aggregation == "sum":
                return df_copy.rolling(window=4, min_periods=4).sum()
            else:
                # For other aggregation methods, we need custom logic
                raise ValueError(
                    "QUARTERLY_TO_TTM conversion currently only supports 'sum' aggregation for TTM. "
                    "TTM typically represents the sum of the last 4 quarters for flow items like revenue."
                )
        else:
            raise NotImplementedError(
                f"Conversion type '{self.conversion_type}' is defined in ConversionType enum but not implemented in PeriodConversionTransformer."
            )



================================================================================
File: fin_statement_model/preprocessing/transformers/time_series.py
================================================================================

"""Financial data transformers for the Financial Statement Model.

This module provides the TimeSeriesTransformer which applies growth rates,
moving averages, CAGR, year-over-year, and quarter-over-quarter conversions.
"""

import logging
import numpy as np
import pandas as pd
from typing import Union, Optional, ClassVar

from fin_statement_model.preprocessing.config import (
    TimeSeriesConfig,
    TransformationType,
)
from fin_statement_model.preprocessing.base_transformer import DataTransformer

logger = logging.getLogger(__name__)


class TimeSeriesTransformer(DataTransformer):
    """Transformer for time series financial data analysis.

    This transformer provides common time series transformations used in financial
    analysis to identify trends, growth patterns, and period-over-period changes.

    Supported transformation types:
        - **growth_rate**: Calculate period-to-period growth rates (%)
        - **moving_avg**: Calculate moving averages over specified window
        - **cagr**: Compute compound annual growth rate
        - **yoy**: Year-over-year comparison (%)
        - **qoq**: Quarter-over-quarter comparison (%)

    Data Frequency Assumptions:
        The transformer makes no assumptions about the frequency of your data.
        You must specify the appropriate 'periods' parameter based on your data:

        - For **monthly data**:
            - YoY: use periods=12 (compare to same month last year)
            - QoQ: use periods=3 (compare to same month last quarter)

        - For **quarterly data**:
            - YoY: use periods=4 (compare to same quarter last year)
            - QoQ: use periods=1 (compare to previous quarter)

        - For **annual data**:
            - YoY: use periods=1 (compare to previous year)

    Examples:
        Calculate year-over-year growth for quarterly revenue data:

        >>> import pandas as pd
        >>> from fin_statement_model.preprocessing.transformers import TimeSeriesTransformer
        >>>
        >>> # Quarterly revenue data
        >>> data = pd.DataFrame({
        ...     'revenue': [100, 105, 110, 115, 120, 125, 130, 135],
        ...     'costs': [60, 62, 65, 68, 70, 73, 75, 78]
        ... }, index=pd.date_range('2022-Q1', periods=8, freq='Q'))
        >>>
        >>> # Calculate YoY growth (comparing to same quarter previous year)
        >>> yoy_transformer = TimeSeriesTransformer(
        ...     transformation_type='yoy',
        ...     periods=4  # 4 quarters back for quarterly data
        ... )
        >>> yoy_growth = yoy_transformer.transform(data)
        >>> print(yoy_growth[['revenue_yoy', 'costs_yoy']].iloc[4:])  # First 4 periods will be NaN
        #             revenue_yoy  costs_yoy
        # 2023-Q1           20.0      16.67
        # 2023-Q2           19.05     17.74
        # 2023-Q3           18.18     15.38
        # 2023-Q4           17.39     14.71

        Calculate 3-month moving average for monthly data:

        >>> # Monthly sales data
        >>> monthly_data = pd.DataFrame({
        ...     'sales': [100, 95, 105, 110, 108, 115, 120, 118, 125]
        ... }, index=pd.date_range('2023-01', periods=9, freq='M'))
        >>>
        >>> # Calculate 3-month moving average
        >>> ma_transformer = TimeSeriesTransformer(
        ...     transformation_type='moving_avg',
        ...     window_size=3
        ... )
        >>> ma_result = ma_transformer.transform(monthly_data)
        >>> print(ma_result['sales_ma3'].round(2))
        # 2023-01-31       NaN
        # 2023-02-28       NaN
        # 2023-03-31    100.00
        # 2023-04-30    103.33
        # 2023-05-31    107.67
        # 2023-06-30    111.00
        # 2023-07-31    114.33
        # 2023-08-31    117.67
        # 2023-09-30    121.00

    Note:
        - Growth rate calculations will return NaN for periods without valid
          comparison data (e.g., first 4 periods for YoY with quarterly data)
        - CAGR requires at least 2 data points and positive starting values
        - Moving averages will have NaN values for the first (window_size - 1) periods
    """

    TRANSFORMATION_TYPES: ClassVar[list[str]] = [t.value for t in TransformationType]

    def __init__(
        self,
        transformation_type: Union[
            str, TransformationType
        ] = TransformationType.GROWTH_RATE,
        periods: int = 1,
        window_size: int = 3,
        config: Optional[TimeSeriesConfig] = None,
    ):
        """Initialize the time series transformer.

        Args:
            transformation_type: Type of transformation to apply. Can be either
                a string or TransformationType enum value:
                - 'growth_rate': Period-to-period growth rate
                - 'moving_avg': Rolling window average
                - 'cagr': Compound annual growth rate
                - 'yoy': Year-over-year growth rate
                - 'qoq': Quarter-over-quarter growth rate
            periods: Number of periods for lag calculations. Critical for YoY/QoQ:
                - For YoY with quarterly data: use periods=4
                - For YoY with monthly data: use periods=12
                - For QoQ with quarterly data: use periods=1
                - For QoQ with monthly data: use periods=3
                - For growth_rate: use periods=1 for consecutive period growth
            window_size: Size of the moving average window (only used for 'moving_avg').
                Default is 3.
            config: Optional TimeSeriesConfig object containing configuration.
                If provided, overrides other parameters.

        Raises:
            ValueError: If transformation_type is invalid.

        Examples:
            >>> # YoY for quarterly data
            >>> transformer = TimeSeriesTransformer('yoy', periods=4)
            >>>
            >>> # 3-month moving average
            >>> transformer = TimeSeriesTransformer('moving_avg', window_size=3)
            >>>
            >>> # Quarter-over-quarter for monthly data
            >>> transformer = TimeSeriesTransformer('qoq', periods=3)
        """
        super().__init__(config.model_dump() if config else None)
        # Normalize to string
        if isinstance(transformation_type, TransformationType):
            ttype = transformation_type.value
        else:
            ttype = transformation_type
        if ttype not in self.TRANSFORMATION_TYPES:
            raise ValueError(
                f"Invalid transformation type: {ttype}. Must be one of {self.TRANSFORMATION_TYPES}"
            )
        self.transformation_type = ttype

        self.periods = periods
        self.window_size = window_size

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform time series data based on the configured transformation type.

        Args:
            data: DataFrame containing time series financial data. The DataFrame
                should have a time-based index (DatetimeIndex, PeriodIndex, or
                sequential numeric index) for meaningful time series analysis.

        Returns:
            DataFrame with new columns containing transformed values:
            - For 'growth_rate': adds '{column}_growth' columns
            - For 'moving_avg': adds '{column}_ma{window_size}' columns
            - For 'cagr': adds '{column}_cagr' columns (single value repeated)
            - For 'yoy': adds '{column}_yoy' columns
            - For 'qoq': adds '{column}_qoq' columns

            Original columns are preserved in all cases.

        Raises:
            TypeError: If data is not a pandas DataFrame.

        Examples:
            >>> df = pd.DataFrame({'revenue': [100, 110, 120, 130]})
            >>> transformer = TimeSeriesTransformer('growth_rate')
            >>> result = transformer.transform(df)
            >>> print(result)
            #    revenue  revenue_growth
            # 0      100             NaN
            # 1      110            10.0
            # 2      120            9.09
            # 3      130            8.33
        """
        if not isinstance(data, pd.DataFrame):
            raise TypeError(
                f"Unsupported data type: {type(data)}. Expected pandas.DataFrame"
            )
        return super().transform(data)

    def _transform_impl(
        self, data: Union[pd.DataFrame, pd.Series]
    ) -> Union[pd.DataFrame, pd.Series]:
        """Transform time series data.

        Internal method that performs the actual transformation based on
        the configured transformation type.

        Args:
            data: DataFrame or Series containing time series data.

        Returns:
            Transformed data with the same type as input.
        """
        # Handle Series by converting to DataFrame temporarily
        if isinstance(data, pd.Series):
            temp_df = data.to_frame()
            result_df = self._transform_dataframe(temp_df)
            return result_df.iloc[:, 0]  # Return as Series
        else:
            return self._transform_dataframe(data)

    def _transform_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform a DataFrame with time series data.

        Internal method that performs the actual transformation on DataFrames.
        """
        result = df.copy()

        if self.transformation_type == "growth_rate":
            for col in df.columns:
                result[f"{col}_growth"] = df[col].pct_change(periods=self.periods) * 100

        elif self.transformation_type == "moving_avg":
            for col in df.columns:
                result[f"{col}_ma{self.window_size}"] = (
                    df[col].rolling(window=self.window_size).mean()
                )

        elif self.transformation_type == "cagr":
            # Assuming the index represents time periods
            n_periods_for_cagr = len(df) - 1

            if n_periods_for_cagr < 1:
                logger.warning(
                    "CAGR requires at least 2 periods. Returning NaN for all columns."
                )
                for col in df.columns:
                    result[f"{col}_cagr"] = pd.NA
            else:
                for col in df.columns:
                    start_val = df[col].iloc[0]
                    end_val = df[col].iloc[-1]

                    if pd.isna(start_val) or pd.isna(end_val) or start_val == 0:
                        result[f"{col}_cagr"] = pd.NA
                        continue

                    ratio = end_val / start_val
                    # Check for negative base with fractional exponent leading to complex numbers
                    if ratio < 0 and (1 / n_periods_for_cagr) % 1 != 0:
                        result[f"{col}_cagr"] = pd.NA
                    else:
                        try:
                            # Ensure result is float, np.power can handle negative base if exponent is integer
                            power_val = np.power(ratio, (1 / n_periods_for_cagr))
                            if np.iscomplex(
                                power_val
                            ):  # Should be caught by above, but defensive
                                result[f"{col}_cagr"] = pd.NA
                            else:
                                result[f"{col}_cagr"] = (float(power_val) - 1) * 100
                        except (
                            ValueError,
                            TypeError,
                            ZeroDivisionError,
                        ):  # Catch any math errors
                            result[f"{col}_cagr"] = pd.NA

        elif self.transformation_type == "yoy":
            if self.periods not in [
                4,
                12,
            ]:  # Assuming YoY is typically for quarterly (lag 4) or monthly (lag 12)
                logger.warning(
                    f"For YoY transformation, 'periods' parameter is {self.periods}. "
                    f"This will calculate change over {self.periods} periods. "
                    f"Commonly, 4 (for quarterly data) or 12 (for monthly data) is used for YoY."
                )
            for col in df.columns:
                result[f"{col}_yoy"] = df[col].pct_change(periods=self.periods) * 100

        elif self.transformation_type == "qoq":
            if self.periods not in [
                1,
                3,
            ]:  # Assuming QoQ is typically for quarterly (lag 1) or monthly (lag 3)
                logger.warning(
                    f"For QoQ transformation, 'periods' parameter is {self.periods}. "
                    f"This will calculate change over {self.periods} periods. "
                    f"Commonly, 1 (for quarterly data) or 3 (for monthly data) is used for QoQ."
                )
            for col in df.columns:
                result[f"{col}_qoq"] = df[col].pct_change(periods=self.periods) * 100

        else:
            # This case should ideally be caught by the __init__ validation,
            # but as a safeguard during development:
            raise NotImplementedError(
                f"Transformation type '{self.transformation_type}' is defined in TransformationType enum but not implemented in TimeSeriesTransformer."
            )

        return result



================================================================================
File: fin_statement_model/statements/__init__.py
================================================================================

"""Financial Statements Layer (`fin_statement_model.statements`).

This package provides domain-specific abstractions for defining, building,
managing, and presenting financial statements (like Income Statement,
Balance Sheet, Cash Flow Statement) based on underlying configurations.

It sits above the `core` layer and orchestrates the use of core components
(like `Graph`, `Node`) within the context of financial statement structures.
It utilizes configurations (often YAML files) to define the layout, items,
and calculations of a statement.

Key functionalities include:
  - Defining statement structure (`StatementStructure`, `Section`, `LineItem` etc.)
  - Loading and validating statement configurations (`StatementConfig`).
  - Building `StatementStructure` objects from configurations
    (`StatementStructureBuilder`).
  - Managing multiple loaded statements (`StatementRegistry`).
  - Populating a `core.graph.Graph` with calculation nodes based on statement
    definitions (`populate_graph_from_statement`).
  - Formatting statement data retrieved from a graph into user-friendly formats,
    primarily pandas DataFrames (`StatementFormatter`).
  - High-level functions to streamline common workflows like generating a
    statement DataFrame or exporting statements to files (`create_statement_dataframe`,
    `export_statements_to_excel`).
  - Centralizing ID resolution logic between statement items and graph nodes
    (`IDResolver`).

This package imports from `core` and `io` (indirectly via `factory`), but should
not be imported by `core`.
"""

# Core statement structure components
from .structure import (
    StatementStructure,
    Section,
    LineItem,
    CalculatedLineItem,
    SubtotalLineItem,
    StatementItemType,
    StatementItem,  # Added base item type if needed
)

# Configuration related classes
from .configs.validator import StatementConfig
from .configs.models import AdjustmentFilterSpec

# Building
from .structure.builder import StatementStructureBuilder

# Registry
from .registry import StatementRegistry

# ID Resolution
from .population.id_resolver import IDResolver

# Data Fetching
from .formatting.data_fetcher import DataFetcher, FetchResult, NodeData

# Item Processors
from .population.item_processors import (
    ProcessorResult,
    ItemProcessor,
    MetricItemProcessor,
    CalculatedItemProcessor,
    SubtotalItemProcessor,
    ItemProcessorManager,
)

# Result Types for Error Handling
from .utilities.result_types import (
    Result,
    Success,
    Failure,
    ErrorDetail,
    ErrorSeverity,
    ErrorCollector,
    OperationResult,
    ValidationResult,
    ProcessingResult,
    combine_results,
)

# Retry Handler
from .utilities.retry_handler import (
    RetryHandler,
    RetryConfig,
    RetryStrategy,
    RetryResult,
    BackoffStrategy,
    ExponentialBackoff,
    LinearBackoff,
    ConstantBackoff,
    retry_with_exponential_backoff,
    retry_on_specific_errors,
)

# Populator
from .population.populator import populate_graph_from_statement

# Formatting
from .formatting.formatter import StatementFormatter

# High-level orchestration functions
from .orchestration.orchestrator import create_statement_dataframe
from .orchestration.exporter import (
    export_statements_to_excel,
    export_statements_to_json,
)

# Errors specific to statements
from .errors import StatementError, ConfigurationError
from typing import Any, Optional

# Import UnifiedNodeValidator for convenience
from fin_statement_model.io.validation import UnifiedNodeValidator
from fin_statement_model.core.nodes import standard_node_registry
from .utilities.cli_formatters import pretty_print_errors


# Node validation convenience functions
def create_validated_statement_config(
    config_data: dict[str, Any],
    enable_node_validation: bool = True,
    strict_mode: bool = False,
    node_validator: Optional[UnifiedNodeValidator] = None,
) -> StatementConfig:
    """Create a StatementConfig with optional node validation enabled.

    Args:
        config_data: Dictionary containing the raw configuration data.
        enable_node_validation: If True, validates node IDs using UnifiedNodeValidator.
        strict_mode: If True, treats node validation failures as errors.
        node_validator: Optional pre-configured UnifiedNodeValidator instance.

    Returns:
        StatementConfig instance with validation configured.

    Example:
        >>> config_data = {...}  # Your YAML/JSON config as dict
        >>> config = create_validated_statement_config(
        ...     config_data,
        ...     enable_node_validation=True,
        ...     strict_mode=True
        ... )
        >>> errors = config.validate_config()
        >>> if errors:
        ...     print("Validation failed:", errors)
    """
    return StatementConfig(
        config_data=config_data,
        enable_node_validation=enable_node_validation,
        node_validation_strict=strict_mode,
        node_validator=node_validator,
    )


def create_validated_statement_builder(
    enable_node_validation: bool = True,
    strict_mode: bool = False,
    node_validator: Optional[UnifiedNodeValidator] = None,
) -> StatementStructureBuilder:
    """Create a StatementStructureBuilder with optional node validation enabled.

    Args:
        enable_node_validation: If True, validates node IDs during build.
        strict_mode: If True, treats node validation failures as errors.
        node_validator: Optional pre-configured UnifiedNodeValidator instance.

    Returns:
        StatementStructureBuilder instance with validation configured.

    Example:
        >>> builder = create_validated_statement_builder(
        ...     enable_node_validation=True,
        ...     strict_mode=False  # Warnings only
        ... )
        >>> statement = builder.build(validated_config)
    """
    return StatementStructureBuilder(
        enable_node_validation=enable_node_validation,
        node_validation_strict=strict_mode,
        node_validator=node_validator,
    )


def validate_statement_config_with_nodes(
    config_path_or_data: str | dict[str, Any],
    strict_mode: bool = False,
    auto_standardize: bool = True,
) -> tuple[StatementConfig, list[ErrorDetail]]:
    """Validate a statement configuration with comprehensive node validation.

    This is a high-level convenience function that handles the entire validation
    process including node ID validation.

    Args:
        config_path_or_data: Path to config file or config data dict.
        strict_mode: If True, treats node validation failures as errors.
        auto_standardize: If True, auto-standardize alternate node names.

    Returns:
        Tuple of (StatementConfig, validation_errors), where validation_errors is a list of ErrorDetail.
        If the list is empty, validation was successful.

    Example:
        >>> config, errors = validate_statement_config_with_nodes(
        ...     "path/to/income_statement.yaml",
        ...     strict_mode=True
        ... )
        >>> if errors:
        ...     print("Validation failed:", errors)
        >>> else:
        ...     print("Validation passed!")
    """
    # File-based loading is no longer supported; only in-memory dicts
    if not isinstance(config_path_or_data, dict):
        raise ConfigurationError(
            message="File-based loading of statement configs is no longer supported; please pass a configuration dictionary."
        )
    config_data = config_path_or_data

    # Create validator
    node_validator = UnifiedNodeValidator(
        standard_node_registry,
        strict_mode=strict_mode,
        auto_standardize=auto_standardize,
        warn_on_non_standard=True,
        enable_patterns=True,
    )

    # Create and validate config
    config = StatementConfig(
        config_data=config_data,
        enable_node_validation=True,
        node_validation_strict=strict_mode,
        node_validator=node_validator,
    )

    errors = config.validate_config()
    return config, errors


def build_validated_statement_from_config(
    config_path_or_data: str | dict[str, Any],
    strict_mode: bool = False,
    auto_standardize: bool = True,
) -> StatementStructure:
    """Build a complete validated StatementStructure from configuration.

    This is the highest-level convenience function that handles the entire
    process from config to built statement with comprehensive validation.

    Args:
        config_path_or_data: Path to config file or config data dict.
        strict_mode: If True, treats node validation failures as errors.
        auto_standardize: If True, auto-standardize alternate node names.

    Returns:
        StatementStructure instance.

    Raises:
        ConfigurationError: If validation fails in strict mode.
        ValueError: If config validation fails.

    Example:
        >>> try:
        ...     statement = build_validated_statement_from_config(
        ...         "path/to/income_statement.yaml",
        ...         strict_mode=True
        ...     )
        ...     print(f"Built statement: {statement.name}")
        ... except ConfigurationError as e:
        ...     print(f"Validation failed: {e}")
    """
    # Validate config
    config, errors = validate_statement_config_with_nodes(
        config_path_or_data, strict_mode, auto_standardize
    )

    if errors:
        raise ConfigurationError(
            message="Statement configuration validation failed",
            errors=errors,
        )

    # Create builder with validation
    builder = create_validated_statement_builder(
        enable_node_validation=True,
        strict_mode=strict_mode,
    )

    # Build statement
    return builder.build(config)


# Public API definition
__all__ = [
    # Core components
    "AdjustmentFilterSpec",
    "BackoffStrategy",
    "CalculatedItemProcessor",
    "CalculatedLineItem",
    "ConfigurationError",
    "ConstantBackoff",
    "DataFetcher",
    "ErrorCollector",
    "ErrorDetail",
    "ErrorSeverity",
    "ExponentialBackoff",
    "Failure",
    "FetchResult",
    "IDResolver",
    "ItemProcessor",
    "ItemProcessorManager",
    "LineItem",
    "LinearBackoff",
    "MetricItemProcessor",
    "MetricLineItem",
    "NodeData",
    "OperationResult",
    "ProcessingResult",
    "ProcessorResult",
    "Result",
    "RetryConfig",
    "RetryHandler",
    "RetryResult",
    "RetryStrategy",
    "Section",
    "StatementConfig",
    "StatementError",
    "StatementFormatter",
    "StatementItem",
    "StatementItemType",
    "StatementRegistry",
    "StatementStructure",
    "StatementStructureBuilder",
    "SubtotalItemProcessor",
    "SubtotalLineItem",
    "Success",
    "UnifiedNodeValidator",
    "ValidationResult",
    # High-level functions
    "build_validated_statement_from_config",
    "combine_results",
    "create_statement_dataframe",
    "create_validated_statement_builder",
    "create_validated_statement_config",
    "export_statements_to_excel",
    "export_statements_to_json",
    "populate_graph_from_statement",
    "retry_on_specific_errors",
    "retry_with_exponential_backoff",
    "validate_statement_config_with_nodes",
    "pretty_print_errors",
]

# Note: FinancialStatementGraph removed as part of refactor, assuming its
# responsibilities are covered by core Graph and statement-specific components.



================================================================================
File: fin_statement_model/statements/configs/__init__.py
================================================================================

"""Configuration handling for financial statements.

This package provides:
- Pydantic models for configuration validation
- Configuration file loading utilities
- StatementConfig class for managing configurations
"""

from .models import (
    AdjustmentFilterSpec,
    BaseItemModel,
    CalculatedItemModel,
    CalculationSpec,
    LineItemModel,
    MetricItemModel,
    SectionModel,
    StatementModel,
    SubtotalModel,
)
from .validator import StatementConfig

__all__ = [
    # Models
    "AdjustmentFilterSpec",
    "BaseItemModel",
    "CalculatedItemModel",
    "CalculationSpec",
    "LineItemModel",
    "MetricItemModel",
    "SectionModel",
    # Validator
    "StatementConfig",
    "StatementModel",
    "SubtotalModel",
]



================================================================================
File: fin_statement_model/statements/configs/models.py
================================================================================

"""Define Pydantic models for statement configuration.

This module defines Pydantic models for validating statement configuration data,
including statements, sections, line items, calculations, and subtotals.
"""

from __future__ import annotations

from typing import Any, Optional, Union, Literal

from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator


class CalculationSpec(BaseModel):
    """Define a calculation specification.

    Args:
        type: Type identifier for the calculation (e.g., 'addition', 'subtraction').
        inputs: List of input node or line item IDs referenced by this calculation.
    """

    type: str = Field(
        ...,
        description="Type identifier for the calculation (e.g., 'addition', 'subtraction').",
    )
    inputs: list[str] = Field(
        ...,
        description="List of input node or line item IDs referenced by this calculation.",
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class AdjustmentFilterSpec(BaseModel):
    """Define an adjustment filter specification for configuration.

    This model represents the adjustment filter options that can be specified
    in configuration files. It maps to the core AdjustmentFilter model but
    uses serializable types suitable for YAML/JSON.

    Args:
        include_scenarios: Only include adjustments from these scenarios.
        exclude_scenarios: Exclude adjustments from these scenarios.
        include_tags: Include adjustments matching any of these tag prefixes.
        exclude_tags: Exclude adjustments matching any of these tag prefixes.
        require_all_tags: Include only adjustments having all these exact tags.
        include_types: Only include adjustments of these types.
        exclude_types: Exclude adjustments of these types.
        period: The specific period context for effective window checks.
    """

    include_scenarios: Optional[list[str]] = Field(
        None, description="Only include adjustments from these scenarios."
    )
    exclude_scenarios: Optional[list[str]] = Field(
        None, description="Exclude adjustments from these scenarios."
    )
    include_tags: Optional[list[str]] = Field(
        None, description="Include adjustments matching any of these tag prefixes."
    )
    exclude_tags: Optional[list[str]] = Field(
        None, description="Exclude adjustments matching any of these tag prefixes."
    )
    require_all_tags: Optional[list[str]] = Field(
        None, description="Include only adjustments having all these exact tags."
    )
    include_types: Optional[list[str]] = Field(
        None,
        description="Only include adjustments of these types (additive, multiplicative, replacement).",
    )
    exclude_types: Optional[list[str]] = Field(
        None,
        description="Exclude adjustments of these types (additive, multiplicative, replacement).",
    )
    period: Optional[str] = Field(
        None, description="The specific period context for effective window checks."
    )

    @field_validator("include_types", "exclude_types", mode="before")
    def validate_adjustment_types(cls, value: list[str] | None) -> list[str] | None:
        """Validate adjustment types are valid."""
        if value is not None:
            valid_types = {"additive", "multiplicative", "replacement"}
            for adj_type in value:
                if adj_type not in valid_types:
                    raise ValueError(
                        f"Invalid adjustment type '{adj_type}'. Must be one of: {valid_types}"
                    )
        return value

    model_config = ConfigDict(extra="forbid", frozen=True)


class BaseItemModel(BaseModel):
    """Define common fields for all statement items.

    Args:
        id: Unique identifier for the item. Must not contain spaces.
        name: Human-readable name of the item.
        description: Optional description for the item.
        metadata: Optional metadata dictionary for the item.
        sign_convention: Sign convention for the item (1 or -1).
        default_adjustment_filter: Optional default adjustment filter for this item.
        display_format: Optional specific number format string (e.g., ",.2f", ",.0f").
        hide_if_all_zero: Whether to hide this item if all values are zero.
        css_class: Optional CSS class name for HTML/web outputs.
        notes_references: List of footnote/note IDs referenced by this item.
        units: Optional unit description (e.g., "USD Thousands", "Percentage").
        display_scale_factor: Factor to scale values for display (e.g., 0.001 for thousands).
    """

    id: str = Field(
        ...,
        description="Unique identifier for the item. Must not contain spaces.",
    )
    name: str = Field(..., description="Human-readable name of the item.")
    description: Optional[str] = Field(
        "", description="Optional description for the item."
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Optional metadata for the item."
    )
    sign_convention: int = Field(
        1, description="Sign convention for the item (1 or -1)."
    )
    default_adjustment_filter: Optional[Union[AdjustmentFilterSpec, list[str]]] = Field(
        None,
        description="Optional default adjustment filter for this item. Can be a filter specification or list of tags.",
    )

    # Enhanced Display Control Fields
    display_format: Optional[str] = Field(
        None,
        description="Specific number format string for this item (e.g., ',.2f', ',.0f', '.1%').",
    )
    hide_if_all_zero: bool = Field(
        False,
        description="Whether to hide this item from display if all values are zero or null.",
    )
    css_class: Optional[str] = Field(
        None,
        description="CSS class name to apply to this item in HTML/web outputs.",
    )
    notes_references: list[str] = Field(
        default_factory=list,
        description="List of footnote or note IDs that reference this item.",
    )

    # Contra Item Support
    is_contra: bool = Field(
        False,
        description="Whether this is a contra item (e.g., Accumulated Depreciation, Treasury Stock, Sales Returns) that naturally reduces the balance of its category for display purposes.",
    )

    # Units and Scaling Fields
    units: Optional[str] = Field(
        None,
        description="Unit description for this item (e.g., 'USD Thousands', 'Percentage', 'Days').",
    )
    display_scale_factor: float = Field(
        1.0,
        description="Factor to scale values for display purposes (e.g., 0.001 to show in thousands).",
    )

    @field_validator("id", mode="before")
    def id_must_not_contain_spaces(cls, value: str) -> str:
        """Ensure that 'id' does not contain spaces."""
        if " " in value:
            raise ValueError("must not contain spaces")
        return value

    @field_validator("display_scale_factor", mode="before")
    def validate_display_scale_factor(cls, value: float) -> float:
        """Ensure display_scale_factor is positive and non-zero."""
        if value <= 0:
            raise ValueError("display_scale_factor must be positive and non-zero")
        return value

    @field_validator("display_format", mode="before")
    def validate_display_format(cls, value: Optional[str]) -> Optional[str]:
        """Validate that display_format is a reasonable format string."""
        if value is not None:
            # Basic validation - try to format a test number
            try:
                test_format = f"{12345.67:{value}}"
                # Basic sanity check that it produced something reasonable
                if not test_format or len(test_format) > 50:
                    raise ValueError("Invalid or problematic format string")
            except (ValueError, TypeError) as e:
                raise ValueError(f"Invalid display_format '{value}': {e}") from e
        return value

    model_config = ConfigDict(extra="forbid", frozen=True)


class LineItemModel(BaseItemModel):
    """Define a basic line item configuration model.

    Args:
        type: Must be 'line_item' for this model.
        node_id: ID of the core node this line item maps to.
        standard_node_ref: Optional reference to a standard node name from the registry.
    """

    type: Literal["line_item"] = Field(
        "line_item", description="Discriminator for basic line items."
    )
    node_id: Optional[str] = Field(
        None, description="ID of the core node this line item maps to."
    )
    standard_node_ref: Optional[str] = Field(
        None,
        description="Reference to a standard node name from the standard_node_registry.",
    )

    @model_validator(mode="before")
    def exactly_one_node_reference(cls, values: dict[str, Any]) -> dict[str, Any]:
        """Ensure exactly one of 'node_id' or 'standard_node_ref' is provided."""
        node_id = values.get("node_id")
        standard_ref = values.get("standard_node_ref")

        if not node_id and not standard_ref:
            raise ValueError("must provide either 'node_id' or 'standard_node_ref'")
        if node_id and standard_ref:
            raise ValueError(
                "cannot provide both 'node_id' and 'standard_node_ref' - use only one"
            )
        return values

    model_config = ConfigDict(extra="forbid", frozen=True)


class MetricItemModel(BaseItemModel):
    """Define a metric-based line item configuration model.

    Args:
        type: Must be 'metric' for this model.
        metric_id: ID of the metric in the core registry.
        inputs: Mapping of metric input names to statement item IDs.
    """

    type: Literal["metric"] = Field(
        "metric", description="Discriminator for metric-based items."
    )
    metric_id: str = Field(
        ..., description="ID of the metric in the core.metrics.registry."
    )
    inputs: dict[str, str] = Field(
        ..., description="Mapping of metric input names to statement item IDs."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class CalculatedItemModel(BaseItemModel):
    """Define a calculated line item configuration model.

    Args:
        type: Must be 'calculated' for this model.
        calculation: Calculation specification for the calculated item.
    """

    type: Literal["calculated"] = Field(
        "calculated", description="Discriminator for calculated items."
    )
    calculation: CalculationSpec = Field(
        ..., description="Calculation specification for the calculated item."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class SubtotalModel(BaseItemModel):
    """Define a subtotal configuration model.

    Args:
        type: Must be 'subtotal' for this model.
        calculation: Optional calculation specification for the subtotal.
        items_to_sum: Optional list of item IDs to sum for the subtotal.
    """

    type: Literal["subtotal"] = Field(
        "subtotal", description="Discriminator for subtotal items."
    )
    calculation: Optional[CalculationSpec] = Field(
        None, description="Calculation specification for the subtotal."
    )
    items_to_sum: Optional[list[str]] = Field(
        None, description="List of item IDs to sum for the subtotal."
    )

    @model_validator(mode="before")
    def exactly_one_of_calculation_or_items(
        cls, values: dict[str, Any]
    ) -> dict[str, Any]:
        """Ensure exactly one of 'calculation' or 'items_to_sum' is provided."""
        calc, items = values.get("calculation"), values.get("items_to_sum")
        if bool(calc) == bool(items):
            raise ValueError(
                "must provide exactly one of 'calculation' or 'items_to_sum'"
            )
        return values

    model_config = ConfigDict(extra="forbid", frozen=True)


class SectionModel(BaseItemModel):
    """Define a nested section within the statement configuration.

    Args:
        type: Must be 'section' for this model.
        items: List of line items, calculated items, subtotals, or nested sections.
        subsections: List of nested sections.
        subtotal: Optional subtotal configuration for this section.
        default_adjustment_filter: Optional default adjustment filter for this section.
    """

    type: Literal["section"] = Field(
        "section", description="Discriminator for nested sections."
    )
    items: list[
        Union[
            LineItemModel,
            CalculatedItemModel,
            MetricItemModel,
            SubtotalModel,
            SectionModel,
        ]
    ] = Field(
        default_factory=list,
        description=(
            "List of line items, calculated items, subtotals, or nested sections."
        ),
    )
    subsections: list[SectionModel] = Field(
        default_factory=list,
        description="List of nested sections.",
    )
    subtotal: Optional[SubtotalModel] = Field(
        None, description="Optional subtotal configuration for this section."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)

    @model_validator(mode="after")
    def check_unique_item_ids(section: SectionModel) -> SectionModel:
        """Ensure that item and subsection IDs within a section are unique and subtotal refs valid."""
        ids = [item.id for item in section.items] + [
            sub.id for sub in section.subsections
        ]
        duplicates = {item_id for item_id in ids if ids.count(item_id) > 1}
        if duplicates:
            raise ValueError(
                f"Duplicate item id(s) in section '{section.id}': {', '.join(duplicates)}"
            )
        if section.subtotal and section.subtotal.items_to_sum is not None:
            valid_ids = [item.id for item in section.items]
            missing = [i for i in section.subtotal.items_to_sum if i not in valid_ids]
            if missing:
                raise ValueError(
                    f"Section '{section.id}' subtotal references undefined ids: {', '.join(missing)}"
                )
        return section


SectionModel.model_rebuild(force=True)


class StatementModel(BaseModel):
    """Define the top-level statement configuration model.

    Args:
        id: Unique identifier for the statement. Must not contain spaces.
        name: Human-readable name of the statement.
        description: Optional description of the statement.
        metadata: Optional metadata dictionary.
        sections: List of top-level sections in the statement.
        units: Optional default unit description for the entire statement.
        display_scale_factor: Optional default scale factor for the entire statement.
    """

    id: str = Field(
        ..., description="Unique statement identifier. Must not contain spaces."
    )
    name: str = Field(..., description="Human-readable statement name.")
    description: Optional[str] = Field(
        "", description="Optional statement description."
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Optional metadata dictionary."
    )
    sections: list[SectionModel] = Field(
        ..., description="List of top-level sections in the statement."
    )

    # Statement-level units and scaling
    units: Optional[str] = Field(
        None,
        description="Default unit description for the statement (e.g., 'USD Thousands').",
    )
    display_scale_factor: float = Field(
        1.0,
        description="Default scale factor for displaying values in this statement.",
    )

    @field_validator("id", mode="before")
    def id_must_not_contain_spaces(cls, value: str) -> str:
        """Ensure that statement 'id' does not contain spaces."""
        if " " in value:
            raise ValueError("must not contain spaces")
        return value

    @field_validator("display_scale_factor", mode="before")
    def validate_display_scale_factor(cls, value: float) -> float:
        """Ensure display_scale_factor is positive and non-zero."""
        if value <= 0:
            raise ValueError("display_scale_factor must be positive and non-zero")
        return value

    model_config = ConfigDict(extra="forbid", frozen=True)

    @model_validator(mode="after")
    def check_unique_section_ids(model: StatementModel) -> StatementModel:
        """Ensure that top-level section IDs are unique."""
        ids = [section.id for section in model.sections]
        duplicates = {sec_id for sec_id in ids if ids.count(sec_id) > 1}
        if duplicates:
            raise ValueError(f"Duplicate section id(s): {', '.join(duplicates)}")
        return model



================================================================================
File: fin_statement_model/statements/configs/validator.py
================================================================================

"""Statement configuration handling for Financial Statement Model.

This module provides utilities for parsing and validating statement configuration data
(provided as a dictionary) and building StatementStructure objects.
"""

# Removed json, yaml, Path imports as file loading moved to IO
import logging
from typing import Any, Optional

# Use absolute imports
# Import Pydantic models for building from validated configuration
from fin_statement_model.statements.configs.models import (
    StatementModel,
    BaseItemModel,
    LineItemModel,
    CalculatedItemModel,
    MetricItemModel,
    SubtotalModel,
    SectionModel,
    CalculationSpec,
)
from pydantic import ValidationError  # Import directly

# Import UnifiedNodeValidator for node ID validation
from fin_statement_model.io.validation import UnifiedNodeValidator
from fin_statement_model.core.nodes import standard_node_registry

# Import Result types for enhanced error handling
from fin_statement_model.statements.utilities.result_types import (
    ErrorCollector,
    ErrorSeverity,
    ErrorDetail,
)

# Configure logging
logger = logging.getLogger(__name__)


class StatementConfig:
    """Manages configuration parsing and building for financial statement structures.

    This class handles validating statement configuration data (provided as a dictionary)
    and building StatementStructure objects from these configurations.
    It does NOT handle file loading.
    """

    def __init__(
        self,
        config_data: dict[str, Any],
        enable_node_validation: bool = False,
        node_validation_strict: bool = False,
        node_validator: Optional[UnifiedNodeValidator] = None,
    ):
        """Initialize a statement configuration processor.

        Args:
            config_data: Dictionary containing the raw configuration data.
            enable_node_validation: If True, validates node IDs using UnifiedNodeValidator.
            node_validation_strict: If True, treats node validation failures as errors.
                                   If False, treats them as warnings.
            node_validator: Optional pre-configured UnifiedNodeValidator instance.
                           If None and enable_node_validation is True, creates a default instance.

        Raises:
            ValueError: If config_data is not a non-empty dictionary.
        """
        if not config_data or not isinstance(config_data, dict):
            raise ValueError("config_data must be a non-empty dictionary.")

        self.config_data = config_data
        self.model: Optional[StatementModel] = None  # Store validated model

        # Node validation configuration
        self.enable_node_validation = enable_node_validation
        self.node_validation_strict = node_validation_strict

        # Initialize node_validator attribute
        self.node_validator: Optional[UnifiedNodeValidator] = None
        if enable_node_validation:
            if node_validator is not None:
                self.node_validator = node_validator
            else:
                # Create default validator
                self.node_validator = UnifiedNodeValidator(
                    standard_node_registry,
                    strict_mode=node_validation_strict,
                    auto_standardize=True,
                    warn_on_non_standard=True,
                    enable_patterns=True,
                )

    def validate_config(self) -> list[ErrorDetail]:
        """Validate the configuration data using Pydantic models and optional node validation.

        Returns:
            list[ErrorDetail]: List of validation errors, or empty list if valid.
                     Stores the validated model in self.model on success.
        """
        error_collector = ErrorCollector()

        try:
            # First perform Pydantic validation
            self.model = StatementModel.model_validate(self.config_data)

            # If node validation is enabled, perform additional validation
            if self.enable_node_validation and self.node_validator:
                self._validate_node_ids(self.model, error_collector)

            # Collect structured errors (always include errors)
            errors: list[ErrorDetail] = error_collector.get_errors()
            warnings: list[ErrorDetail] = error_collector.get_warnings()
            if self.node_validation_strict:
                return errors + warnings
            # Log warnings but exclude from returned errors
            for warning in warnings:
                logger.warning(f"Node validation warning: {warning}")
            return errors

        except ValidationError as ve:
            # Convert Pydantic errors to structured ErrorDetail list
            error_details: list[ErrorDetail] = []
            for err in ve.errors():
                loc = ".".join(str(x) for x in err.get("loc", []))
                msg = err.get("msg", "")
                error_details.append(
                    ErrorDetail(
                        code="pydantic_validation",
                        message=msg,
                        context=loc,
                        severity=ErrorSeverity.ERROR,
                    )
                )
            self.model = None  # Ensure model is not set on validation error
            return error_details
        except Exception as e:
            # Catch other potential validation issues
            logger.exception("Unexpected error during configuration validation")
            self.model = None
            return [
                ErrorDetail(
                    code="unexpected_validation_error",
                    message=str(e),
                    severity=ErrorSeverity.ERROR,
                )
            ]

    def _validate_node_ids(
        self, model: StatementModel, error_collector: ErrorCollector
    ) -> None:
        """Validate all node IDs in the statement model using UnifiedNodeValidator.

        Args:
            model: The validated StatementModel to check.
            error_collector: ErrorCollector to accumulate validation issues.
        """
        logger.debug(f"Starting node ID validation for statement '{model.id}'")

        # Validate statement ID itself
        self._validate_single_node_id(
            model.id, "statement", "statement.id", error_collector
        )

        # Validate all sections recursively
        for section in model.sections:
            self._validate_section_node_ids(
                section, error_collector, f"statement.{model.id}"
            )

    def _validate_section_node_ids(
        self,
        section: SectionModel,
        error_collector: ErrorCollector,
        parent_context: str,
    ) -> None:
        """Validate node IDs within a section and its items.

        Args:
            section: The section model to validate.
            error_collector: ErrorCollector to accumulate validation issues.
            parent_context: Context string for error reporting.
        """
        section_context = f"{parent_context}.section.{section.id}"

        # Validate section ID
        self._validate_single_node_id(
            section.id, "section", f"{section_context}.id", error_collector
        )

        # Validate all items in the section
        for item in section.items:
            self._validate_item_node_ids(item, error_collector, section_context)

        # Validate subsections recursively
        for subsection in section.subsections:
            self._validate_section_node_ids(
                subsection, error_collector, section_context
            )

        # Validate section subtotal if present
        if section.subtotal:
            self._validate_item_node_ids(
                section.subtotal, error_collector, section_context
            )

    def _validate_item_node_ids(
        self, item: BaseItemModel, error_collector: ErrorCollector, parent_context: str
    ) -> None:
        """Validate node IDs within a specific item.

        Args:
            item: The item model to validate.
            error_collector: ErrorCollector to accumulate validation issues.
            parent_context: Context string for error reporting.
        """
        item_context = f"{parent_context}.item.{item.id}"

        # Validate the item ID itself
        self._validate_single_node_id(
            item.id, "item", f"{item_context}.id", error_collector
        )

        # Type-specific validation
        if isinstance(item, LineItemModel):
            # Validate node_id if present
            if item.node_id:
                self._validate_single_node_id(
                    item.node_id, "node", f"{item_context}.node_id", error_collector
                )

            # Validate standard_node_ref if present
            if item.standard_node_ref:
                self._validate_single_node_id(
                    item.standard_node_ref,
                    "standard_node",
                    f"{item_context}.standard_node_ref",
                    error_collector,
                )

        elif isinstance(item, CalculatedItemModel):
            # Validate calculation inputs
            self._validate_calculation_inputs(
                item.calculation, error_collector, item_context
            )

        elif isinstance(item, MetricItemModel):
            # Validate metric inputs (the values, not the keys)
            for input_key, input_id in item.inputs.items():
                self._validate_single_node_id(
                    input_id,
                    "metric_input",
                    f"{item_context}.inputs.{input_key}",
                    error_collector,
                )

        elif isinstance(item, SubtotalModel):
            # Validate items_to_sum if present
            if item.items_to_sum:
                for i, input_id in enumerate(item.items_to_sum):
                    self._validate_single_node_id(
                        input_id,
                        "subtotal_input",
                        f"{item_context}.items_to_sum[{i}]",
                        error_collector,
                    )

            # Validate calculation inputs if present
            if item.calculation:
                self._validate_calculation_inputs(
                    item.calculation, error_collector, item_context
                )

        elif isinstance(item, SectionModel):
            # Recursive validation for nested sections
            self._validate_section_node_ids(item, error_collector, parent_context)

    def _validate_calculation_inputs(
        self,
        calculation: CalculationSpec,
        error_collector: ErrorCollector,
        parent_context: str,
    ) -> None:
        """Validate inputs within a calculation specification.

        Args:
            calculation: The calculation specification to validate.
            error_collector: ErrorCollector to accumulate validation issues.
            parent_context: Context string for error reporting.
        """
        for i, input_id in enumerate(calculation.inputs):
            self._validate_single_node_id(
                input_id,
                "calculation_input",
                f"{parent_context}.calculation.inputs[{i}]",
                error_collector,
            )

    def _validate_single_node_id(
        self,
        node_id: str,
        node_type: str,
        context: str,
        error_collector: ErrorCollector,
    ) -> None:
        """Validate a single node ID using the UnifiedNodeValidator.

        Args:
            node_id: The node ID to validate.
            node_type: Type description for error messages.
            context: Context string for error reporting.
            error_collector: ErrorCollector to accumulate validation issues.
        """
        if not self.node_validator:
            return

        try:
            validation_result = self.node_validator.validate(
                node_id,
                node_type=node_type,
                parent_nodes=None,  # Could be enhanced to track parent context
                use_cache=True,
            )

            # Determine severity based on validation result and configuration
            if not validation_result.is_valid:
                severity = (
                    ErrorSeverity.ERROR
                    if self.node_validation_strict
                    else ErrorSeverity.WARNING
                )
                message = (
                    f"Invalid {node_type} ID '{node_id}': {validation_result.message}"
                )
                if severity == ErrorSeverity.ERROR:
                    error_collector.add_error(
                        code="invalid_node_id",
                        message=message,
                        context=context,
                        source=node_id,
                    )
                else:
                    error_collector.add_warning(
                        code="invalid_node_id",
                        message=message,
                        context=context,
                        source=node_id,
                    )

            elif validation_result.category in [
                "alternate",
                "subnode_nonstandard",
                "custom",
            ]:
                # These are valid but could be improved
                error_collector.add_warning(
                    code="non_standard_node_id",
                    message=f"Non-standard {node_type} ID '{node_id}': {validation_result.message}",
                    context=context,
                    source=node_id,
                )

            # Add suggestions if available
            if validation_result.suggestions:
                suggestion_msg = f"Suggestions for {node_type} ID '{node_id}': {'; '.join(validation_result.suggestions)}"
                error_collector.add_warning(
                    code="node_id_suggestions",
                    message=suggestion_msg,
                    context=context,
                    source=node_id,
                )

        except Exception as e:
            logger.exception(
                f"Error validating node ID '{node_id}' in context '{context}'"
            )
            error_collector.add_warning(
                code="node_validation_error",
                message=f"Failed to validate {node_type} ID '{node_id}': {e}",
                context=context,
                source=node_id,
            )



================================================================================
File: fin_statement_model/statements/errors.py
================================================================================

"""Custom Exception classes for the `fin_statement_model.statements` package.

These exceptions provide more specific error information related to statement
definition, configuration, building, and processing, inheriting from the base
`FinancialModelError` defined in `fin_statement_model.core.errors`.
"""

from typing import Optional
from fin_statement_model.core.errors import (
    StatementError,
    ConfigurationError,
)

__all__ = [
    "ConfigurationError",
    "StatementBuilderError",
    "StatementError",
    "StatementValidationError",
]


class StatementBuilderError(StatementError):
    """Exception raised during statement structure building.

    This includes errors encountered while constructing nodes from statement
    definitions or creating graph relationships.
    """

    def __init__(
        self,
        message: str,
        item_id: Optional[str] = None,
        statement_type: Optional[str] = None,
    ):
        """Initialize a StatementBuilderError.

        Args:
            message: The primary error message.
            item_id: Optional ID of the item causing the error.
            statement_type: Optional type of statement being built.
        """
        self.item_id = item_id
        self.statement_type = statement_type

        details = []
        if statement_type:
            details.append(f"Statement type: {statement_type}")
        if item_id:
            details.append(f"Item ID: {item_id}")

        full_message = message
        if details:
            full_message = f"{message} ({', '.join(details)})"

        super().__init__(full_message)


class StatementValidationError(StatementError):
    """Exception raised during statement validation.

    This includes structural validation errors, consistency checks,
    and statement-specific rule violations.
    """

    def __init__(
        self,
        message: str,
        validation_errors: Optional[list[str]] = None,
        statement_id: Optional[str] = None,
    ):
        """Initialize a StatementValidationError.

        Args:
            message: The primary error message.
            validation_errors: Optional list of specific validation failures.
            statement_id: Optional ID of the statement being validated.
        """
        self.validation_errors = validation_errors or []

        full_message = message
        if statement_id:
            full_message = f"{message} for statement '{statement_id}'"
        if validation_errors:
            full_message = f"{full_message}:\n" + "\n".join(
                f"  - {error}" for error in validation_errors
            )

        super().__init__(full_message, statement_id=statement_id)



================================================================================
File: fin_statement_model/statements/formatting/__init__.py
================================================================================

"""Formatting and data fetching for financial statements.

This package provides tools for:
- Fetching data from graphs for statement display
- Formatting statements as DataFrames
- Applying formatting rules and conventions
"""

from .data_fetcher import DataFetcher, FetchResult, NodeData
from .formatter import StatementFormatter

__all__ = [
    # Data Fetching
    "DataFetcher",
    "FetchResult",
    "NodeData",
    # Formatting
    "StatementFormatter",
]



================================================================================
File: fin_statement_model/statements/formatting/_formatting_utils.py
================================================================================

"""Utility functions for formatting statement DataFrames."""

import pandas as pd
from typing import Optional, Any  # Keep necessary imports
from pandas.api.types import is_numeric_dtype


def apply_sign_convention(df: pd.DataFrame, period_columns: list[str]) -> pd.DataFrame:
    """Apply sign conventions to the statement values across periods."""
    result = df.copy()
    if "sign_convention" in result.columns:
        for col in period_columns:
            if col in result.columns and is_numeric_dtype(result[col]):
                mask = result[col].notna()
                # Ensure sign_convention is treated as numeric if needed
                sign_col = pd.to_numeric(
                    result.loc[mask, "sign_convention"], errors="coerce"
                ).fillna(1)
                result.loc[mask, col] = result.loc[mask, col] * sign_col
    return result


def format_numbers(
    df: pd.DataFrame,
    default_formats: dict[str, Any],  # Pass defaults needed
    number_format: Optional[str] = None,
    period_columns: Optional[list[str]] = None,
) -> pd.DataFrame:
    """Format numeric values in the statement.

    Args:
        df: DataFrame to format numbers in
        default_formats: Dictionary containing default formatting options
                         (e.g., 'precision', 'use_thousands_separator').
        number_format: Optional format string
        period_columns: List of columns containing period data to format.
                        If None, attempts to format all numeric columns
                        except metadata/indicators.

    Returns:
        pd.DataFrame: DataFrame with formatted numbers
    """
    result = df.copy()

    if period_columns:
        numeric_cols = [
            col
            for col in period_columns
            if col in result.columns and is_numeric_dtype(result[col])
        ]
    else:
        # Original logic if period_columns not specified
        numeric_cols = [
            col
            for col in result.columns
            if is_numeric_dtype(result[col])
            and col not in ("sign_convention", "depth", "ID")  # Added ID
            and not col.startswith("meta_")
            and col != "Line Item"  # Ensure Line Item name is not formatted
        ]

    # Get defaults from the passed dictionary
    precision = default_formats.get("precision", 2)  # Provide fallback default
    use_thousands = default_formats.get("use_thousands_separator", True)

    if number_format:
        # Use provided format string
        for col in numeric_cols:
            # Check if column exists before applying format
            if col in result.columns:
                result[col] = result[col].apply(
                    lambda x: f"{x:{number_format}}" if pd.notna(x) else ""
                )
    else:
        # Use default formatting based on passed defaults
        for col in numeric_cols:
            # Check if column exists before applying format
            if col in result.columns:
                result[col] = result[col].apply(
                    lambda x: (
                        (f"{x:,.{precision}f}" if pd.notna(x) else "")
                        if use_thousands
                        else (f"{x:.{precision}f}" if pd.notna(x) else "")
                    )
                )

    return result


def render_values(
    df: pd.DataFrame,
    period_columns: list[str],
    default_formats: dict[str, Any],
    number_format: Optional[str] = None,
    contra_display_style: Optional[str] = "parentheses",
) -> pd.DataFrame:
    """Render values by applying sign conventions, number formatting, and contra styling."""
    # 1. Apply sign conventions to numeric data
    signed_df = apply_sign_convention(df, period_columns)

    # 2. Format numbers to strings
    formatted_df = format_numbers(
        signed_df,
        default_formats,
        number_format=number_format,
        period_columns=period_columns,
    )

    # 3. Apply contra formatting to formatted strings
    if "is_contra" in df.columns:
        contra_mask = df["is_contra"].fillna(False)
        for col in period_columns:
            mask = contra_mask & formatted_df[col].astype(bool)
            if contra_display_style == "parentheses":
                formatted_df.loc[mask, col] = "(" + formatted_df.loc[mask, col] + ")"
            elif contra_display_style == "negative_sign":
                formatted_df.loc[mask, col] = "-" + formatted_df.loc[mask, col]
            elif contra_display_style == "brackets":
                formatted_df.loc[mask, col] = "[" + formatted_df.loc[mask, col] + "]"
            else:
                formatted_df.loc[mask, col] = "(" + formatted_df.loc[mask, col] + ")"

    # 4. Assign formatted strings back to DataFrame
    result = df.copy()
    result[period_columns] = formatted_df[period_columns].astype("object")
    return result



================================================================================
File: fin_statement_model/statements/formatting/data_fetcher.py
================================================================================

"""Data fetching functionality for financial statements.

This module provides the DataFetcher class that handles retrieving data from
the graph for statement formatting. It encapsulates the logic for resolving
item IDs to node IDs and fetching values with proper error handling.
"""

import logging
from dataclasses import dataclass
from typing import Optional, cast

import numpy as np
import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import NodeError, CalculationError
from fin_statement_model.core.adjustments.models import AdjustmentFilterInput
from fin_statement_model.statements.structure import (
    StatementStructure,
    Section,
    StatementItem,
)
from fin_statement_model.statements.population.id_resolver import IDResolver
from fin_statement_model.core.nodes import standard_node_registry
from fin_statement_model.statements.utilities.result_types import (
    Result,
    Success,
    Failure,
    ErrorDetail,
    ErrorSeverity,
    ErrorCollector,
)

logger = logging.getLogger(__name__)

__all__ = ["DataFetcher", "FetchResult", "NodeData"]


@dataclass
class NodeData:
    """Data for a single node across all periods.

    Attributes:
        node_id: The graph node ID
        values: Dict mapping period to value
        is_adjusted: Dict mapping period to bool indicating if adjusted
        errors: Any errors encountered during fetching
    """

    node_id: str
    values: dict[str, float]
    is_adjusted: dict[str, bool]
    errors: list[ErrorDetail]

    @property
    def has_data(self) -> bool:
        """Check if any non-NaN values exist."""
        return any(pd.notna(v) for v in self.values.values())


@dataclass
class FetchResult:
    """Result of fetching data for a statement.

    Attributes:
        data: Dict mapping node_id to period values
        errors: ErrorCollector with any errors/warnings
        node_count: Number of nodes successfully fetched
        missing_nodes: List of node IDs that couldn't be found
    """

    data: dict[str, dict[str, float]]
    errors: ErrorCollector
    node_count: int
    missing_nodes: list[str]

    def to_result(self) -> Result[dict[str, dict[str, float]]]:
        """Convert to Result type."""
        if self.errors.has_errors():
            return Failure(errors=self.errors.get_errors())
        return Success(value=self.data)


class DataFetcher:
    """Fetches data from graph for statement formatting.

    This class encapsulates the logic for:
    - Resolving statement item IDs to graph node IDs
    - Fetching values from the graph with error handling
    - Applying adjustment filters if specified
    - Collecting errors and warnings during the process
    """

    def __init__(self, statement: StatementStructure, graph: Graph):
        """Initialize the data fetcher.

        Args:
            statement: The statement structure to fetch data for
            graph: The graph containing the data
        """
        self.statement = statement
        self.graph = graph
        self.id_resolver = IDResolver(statement, standard_node_registry)

    def _resolve_adjustment_filter(
        self,
        item: StatementItem,
        global_filter: Optional[AdjustmentFilterInput] = None,
    ) -> Optional[AdjustmentFilterInput]:
        """Resolve which adjustment filter to use for an item.

        Precedence order:
        1. Global filter passed to fetch method (highest priority)
        2. Item's default adjustment filter
        3. Parent section's default adjustment filter
        4. None (no filter)

        Args:
            item: The statement item to get the filter for.
            global_filter: Optional global filter that overrides everything.

        Returns:
            The resolved adjustment filter to use, or None.
        """
        # Global filter has highest priority
        if global_filter is not None:
            return global_filter

        # Check item's own default filter
        if (
            hasattr(item, "default_adjustment_filter")
            and item.default_adjustment_filter is not None
        ):
            return cast(AdjustmentFilterInput, item.default_adjustment_filter)

        # Check parent section's default filter
        # We need to find which section contains this item
        parent_section = self._find_parent_section(item)
        if (
            parent_section
            and hasattr(parent_section, "default_adjustment_filter")
            and parent_section.default_adjustment_filter is not None
        ):
            return cast(AdjustmentFilterInput, parent_section.default_adjustment_filter)

        # No filter
        return None

    def _find_parent_section(self, target_item: StatementItem) -> Optional[Section]:
        """Find the parent section that contains the given item.

        Args:
            target_item: The item to find the parent section for.

        Returns:
            The parent Section object, or None if not found.
        """

        def search_in_section(section: Section) -> Optional[Section]:
            # Check direct items
            for item in section.items:
                if item is target_item or (
                    hasattr(item, "id")
                    and hasattr(target_item, "id")
                    and item.id == target_item.id
                ):
                    return section
                # Check nested sections
                if isinstance(item, Section):
                    result = search_in_section(item)
                    if result:
                        return result

            # Check subtotal
            if hasattr(section, "subtotal") and section.subtotal is target_item:
                return section

            return None

        # Search through all top-level sections
        for section in self.statement.sections:
            result = search_in_section(section)
            if result:
                return result

        return None

    def fetch_all_data(
        self,
        adjustment_filter: Optional[AdjustmentFilterInput] = None,
        include_missing: bool = False,
    ) -> FetchResult:
        """Fetch data for all items in the statement.

        Args:
            adjustment_filter: Optional global filter for adjustments (overrides item defaults)
            include_missing: If True, include nodes that don't exist in graph
                           with NaN values

        Returns:
            FetchResult containing the fetched data and any errors
        """
        error_collector = ErrorCollector()
        data: dict[str, dict[str, float]] = {}
        missing_nodes: list[str] = []

        # Check if graph has periods
        periods = self.graph.periods
        if not periods:
            error_collector.add_error(
                code="no_periods",
                message=f"Graph has no periods defined for statement '{self.statement.id}'",
                source=self.statement.id,
            )
            return FetchResult(
                data={}, errors=error_collector, node_count=0, missing_nodes=[]
            )

        logger.debug(
            f"Fetching data for statement '{self.statement.id}' across {len(periods)} periods"
        )

        # Get all items and resolve their node IDs
        all_items = self.statement.get_all_items()
        processed_node_ids = set()

        for item in all_items:
            # Resolve item ID to node ID
            node_id = self.id_resolver.resolve(item.id, self.graph)

            if not node_id:
                error_collector.add_warning(
                    code="unresolvable_item",
                    message=f"Cannot resolve item '{item.id}' to a node ID",
                    source=item.id,
                    context="IDResolver.resolve",
                )
                continue

            if node_id in processed_node_ids:
                continue  # Skip already processed nodes

            processed_node_ids.add(node_id)

            # Resolve adjustment filter for this specific item
            item_filter = self._resolve_adjustment_filter(item, adjustment_filter)

            # Fetch data for this node
            node_result = self._fetch_node_data(
                node_id, periods, item_filter, item_id=item.id
            )

            if node_result.is_success():
                node_data = cast(NodeData, node_result.get_value())
                if node_data.has_data or include_missing:
                    data[node_id] = node_data.values

                # Add any warnings from node fetching
                for error in node_data.errors:
                    if error.severity == ErrorSeverity.WARNING:
                        error_collector.add_warning(
                            error.code,
                            error.message,
                            error.context,
                            error.source or item.id,
                        )
            else:
                # Node doesn't exist in graph
                missing_nodes.append(node_id)
                if include_missing:
                    # Fill with NaN values
                    data[node_id] = {period: np.nan for period in periods}

                error_collector.add_from_result(node_result, source=item.id)

        logger.info(
            f"Fetched data for {len(data)} nodes from statement '{self.statement.id}'. "
            f"Missing: {len(missing_nodes)}, Warnings: {len(error_collector.get_warnings())}"
        )

        return FetchResult(
            data=data,
            errors=error_collector,
            node_count=len(data),
            missing_nodes=missing_nodes,
        )

    def _fetch_node_data(
        self,
        node_id: str,
        periods: list[str],
        adjustment_filter: Optional[AdjustmentFilterInput],
        item_id: Optional[str] = None,
    ) -> Result[NodeData]:
        """Fetch data for a single node across all periods.

        Args:
            node_id: The graph node ID to fetch
            periods: List of periods to fetch
            adjustment_filter: Optional adjustment filter
            item_id: Optional statement item ID for error context

        Returns:
            Result containing NodeData or error details
        """
        # Check if node exists
        if not self.graph.has_node(node_id):
            return Failure(
                [
                    ErrorDetail(
                        code="node_not_found",
                        message=f"Node '{node_id}' not found in graph",
                        source=item_id or node_id,
                        severity=ErrorSeverity.WARNING,
                    )
                ]
            )

        values = {}
        is_adjusted = {}
        errors = []

        for period in periods:
            try:
                # Fetch value with optional adjustments
                value = self.graph.get_adjusted_value(
                    node_id,
                    period,
                    filter_input=adjustment_filter,
                    return_flag=False,  # Only need the value
                )
                # Ensure value is float or NaN
                values[period] = float(value) if pd.notna(value) else np.nan
                is_adjusted[period] = bool(value)

            except (NodeError, CalculationError) as e:
                # Expected errors - log as warning
                logger.warning(
                    f"Error calculating node '{node_id}' for period '{period}': {e}"
                )
                values[period] = np.nan
                is_adjusted[period] = False
                errors.append(
                    ErrorDetail(
                        code="calculation_error",
                        message=f"Failed to calculate value: {e}",
                        context=f"period={period}",
                        severity=ErrorSeverity.WARNING,
                        source=item_id or node_id,
                    )
                )

            except TypeError as e:
                # Filter/adjustment errors
                logger.warning(
                    f"Type error for node '{node_id}', period '{period}': {e}"
                )
                values[period] = np.nan
                is_adjusted[period] = False
                errors.append(
                    ErrorDetail(
                        code="filter_error",
                        message=f"Invalid adjustment filter: {e}",
                        context=f"period={period}",
                        severity=ErrorSeverity.WARNING,
                        source=item_id or node_id,
                    )
                )

            except Exception as e:
                # Unexpected errors - log as error
                logger.error(
                    f"Unexpected error for node '{node_id}', period '{period}': {e}",
                    exc_info=True,
                )
                values[period] = np.nan
                is_adjusted[period] = False
                errors.append(
                    ErrorDetail(
                        code="unexpected_error",
                        message=f"Unexpected error: {e}",
                        context=f"period={period}, error_type={type(e).__name__}",
                        severity=ErrorSeverity.ERROR,
                        source=item_id or node_id,
                    )
                )

        return Success(
            NodeData(
                node_id=node_id, values=values, is_adjusted=is_adjusted, errors=errors
            )
        )

    def check_adjustments(
        self,
        node_ids: list[str],
        periods: list[str],
        adjustment_filter: Optional[AdjustmentFilterInput] = None,
    ) -> dict[str, dict[str, bool]]:
        """Check which node/period combinations have adjustments.

        Args:
            node_ids: List of node IDs to check
            periods: List of periods to check
            adjustment_filter: Filter to check adjustments against

        Returns:
            Dict mapping node_id -> period -> was_adjusted boolean
        """
        results = {}

        for node_id in node_ids:
            if not self.graph.has_node(node_id):
                results[node_id] = {period: False for period in periods}
                continue

            period_results = {}
            for period in periods:
                try:
                    was_adjusted = self.graph.was_adjusted(
                        node_id, period, adjustment_filter
                    )
                    period_results[period] = bool(was_adjusted)
                except Exception as e:
                    logger.warning(
                        f"Error checking adjustments for {node_id}/{period}: {e}"
                    )
                    period_results[period] = False

            results[node_id] = period_results

        return results



================================================================================
File: fin_statement_model/statements/formatting/formatter.py
================================================================================

"""Formatter for financial statements.

This module provides functionality for formatting financial statements
for display or reporting, including applying formatting rules, adding subtotals,
and applying sign conventions with enhanced display control.
"""

import pandas as pd
import numpy as np  # Added numpy for NaN handling
from typing import Optional, Any, Union
from collections.abc import Callable
import logging
import re
from dataclasses import dataclass, field

from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.statements.structure import (
    Section,
    CalculatedLineItem,
    SubtotalLineItem,
    StatementItem,
)

# Add core Graph and errors
from fin_statement_model.core.graph import Graph

# Import adjustment types for filtering
from fin_statement_model.core.adjustments.models import AdjustmentFilterInput

# Import the ID resolver
from fin_statement_model.statements.population.id_resolver import IDResolver
from fin_statement_model.core.nodes import standard_node_registry

# Import the data fetcher
from fin_statement_model.statements.formatting.data_fetcher import DataFetcher

# Import formatting utilities
from ._formatting_utils import render_values

# Configure logging
logger = logging.getLogger(__name__)


@dataclass
class FormattingContext:
    """Encapsulates all formatting configuration and state for statement generation.

    This dataclass holds all the configuration options and runtime state needed
    for formatting financial statements, providing a clean interface for passing
    formatting parameters between methods.
    """

    # Core formatting options
    should_apply_signs: bool = True
    include_empty_items: bool = False
    number_format: Optional[str] = None
    include_metadata_cols: bool = False

    # Adjustment options
    adjustment_filter: Optional[AdjustmentFilterInput] = None
    add_is_adjusted_column: bool = False

    # Enhanced display options
    include_units_column: bool = False
    include_css_classes: bool = False
    include_notes_column: bool = False
    apply_item_scaling: bool = True
    apply_item_formatting: bool = True
    respect_hide_flags: Optional[bool] = None

    # Contra item options
    contra_display_style: Optional[str] = None
    apply_contra_formatting: bool = True
    add_contra_indicator_column: bool = False

    # Runtime state (populated during processing)
    all_periods: list[str] = field(default_factory=list)
    items_to_hide: set[str] = field(default_factory=set)
    default_formats: dict[str, Any] = field(default_factory=dict)

    # Derived flags (computed after initialization)
    should_include_enhanced_metadata: bool = field(init=False)

    def __post_init__(self) -> None:
        """Compute derived flags after initialization."""
        self.should_include_enhanced_metadata = any(
            [
                self.include_units_column,
                self.include_css_classes,
                self.include_notes_column,
                self.add_contra_indicator_column,
            ]
        )


class StatementFormatter:
    """Formats financial statements for display or reporting.

    This class provides methods to transform raw financial data into
    formatted financial statements with proper headers, indentation,
    subtotals, sign conventions, and enhanced display control.
    """

    def __init__(self, statement: StatementStructure):
        """Initialize a statement formatter.

        Args:
            statement: The statement structure to format
        """
        self.statement = statement

        # --- Build default display formats from global config ---
        from fin_statement_model import get_config  # Local import to avoid circular dep

        cfg_display = get_config().display
        num_format = cfg_display.default_number_format or ",.2f"
        # Detect precision from format string like ',.2f' or '.3f'
        precision_match = re.search(r"\.([0-9]+)f$", num_format)
        precision = int(precision_match.group(1)) if precision_match else 2
        use_thousands_sep = "," in num_format.split(".")[0]

        self.default_formats = {
            "precision": precision,
            "use_thousands_separator": use_thousands_sep,
            "show_zero_values": not cfg_display.hide_zero_rows,
            "show_negative_sign": cfg_display.show_negative_sign,
            "indent_character": cfg_display.indent_character,
            "subtotal_style": cfg_display.subtotal_style,
            "total_style": cfg_display.total_style,
            "header_style": cfg_display.header_style,
            # Contra item display options
            "contra_display_style": cfg_display.contra_display_style,
            "contra_css_class": cfg_display.contra_css_class,
        }

    def _resolve_hierarchical_attribute(
        self,
        item: Union[StatementItem, Section],
        attribute_name: str,
        default_value: Any = None,
        config_path: Optional[str] = None,
        skip_default_check: Optional[Callable[[Any], bool]] = None,
    ) -> Any:
        """Resolve an attribute value using hierarchical lookup.

        Precedence: Item > Parent Section > Statement > Config/Default

        Args:
            item: The item or section to resolve the attribute for
            attribute_name: Name of the attribute to look up
            default_value: Default value if not found anywhere
            config_path: Optional config path to check before using default_value
            skip_default_check: Optional function to determine if a value should be
                              considered "default" and skipped (e.g., scale_factor == 1.0)

        Returns:
            The resolved attribute value
        """
        # Check item-specific attribute
        if hasattr(item, attribute_name):
            item_value = getattr(item, attribute_name)
            if skip_default_check is None or not skip_default_check(item_value):
                return item_value

        # Check if item is part of a section with the attribute
        if isinstance(item, StatementItem):
            parent_section = self._find_parent_section_for_item(item)
            if parent_section and hasattr(parent_section, attribute_name):
                section_value = getattr(parent_section, attribute_name)
                if skip_default_check is None or not skip_default_check(section_value):
                    return section_value

        # Check statement-level attribute
        if hasattr(self.statement, attribute_name):
            statement_value = getattr(self.statement, attribute_name)
            if skip_default_check is None or not skip_default_check(statement_value):
                return statement_value

        # Check config if path provided
        if config_path:
            from fin_statement_model.config.helpers import cfg

            return cfg(config_path, default_value)

        # Return default value
        return default_value

    def _resolve_display_scale_factor(
        self, item: Union[StatementItem, Section]
    ) -> float:
        """Resolve the display scale factor for an item, considering hierarchy.

        Precedence: Item > Section > Statement > Default (from config)

        Args:
            item: The item or section to get the scale factor for

        Returns:
            The resolved scale factor
        """
        result = self._resolve_hierarchical_attribute(
            item=item,
            attribute_name="display_scale_factor",
            default_value=1.0,
            config_path="display.scale_factor",
            skip_default_check=lambda x: x == 1.0,
        )
        return float(result)

    def _resolve_units(self, item: Union[StatementItem, Section]) -> Optional[str]:
        """Resolve the unit description for an item, considering hierarchy.

        Precedence: Item > Section > Statement > None

        Args:
            item: The item or section to get the units for

        Returns:
            The resolved unit description or None
        """
        result = self._resolve_hierarchical_attribute(
            item=item,
            attribute_name="units",
            default_value=None,
            skip_default_check=lambda x: not x,  # Skip empty strings/None
        )
        return result if result is not None else None

    def _find_parent_section_for_item(
        self, target_item: StatementItem
    ) -> Optional[Section]:
        """Find the parent section that contains the given item.

        Args:
            target_item: The item to find the parent section for.

        Returns:
            The parent Section object, or None if not found.
        """

        def search_in_section(section: Section) -> Optional[Section]:
            # Check direct items
            for item in section.items:
                if item is target_item or (
                    hasattr(item, "id")
                    and hasattr(target_item, "id")
                    and item.id == target_item.id
                ):
                    return section
                # Check nested sections
                if isinstance(item, Section):
                    result = search_in_section(item)
                    if result:
                        return result

            # Check subtotal
            if hasattr(section, "subtotal") and section.subtotal is target_item:
                return section

            return None

        # Search through all top-level sections
        for section in self.statement.sections:
            result = search_in_section(section)
            if result:
                return result

        return None

    def _should_hide_item(
        self, item: Union[StatementItem, Section], values: dict[str, float]
    ) -> bool:
        """Check if an item should be hidden based on hide_if_all_zero setting.

        Args:
            item: The item to check
            values: Dictionary of period values for the item

        Returns:
            True if the item should be hidden
        """
        # Check if the item has hide_if_all_zero enabled
        hide_if_zero = getattr(item, "hide_if_all_zero", False)
        if not hide_if_zero:
            return False

        # Check if all values are zero or NaN
        return all(not (pd.notna(value) and value != 0) for value in values.values())

    def _apply_item_scaling(
        self, values: dict[str, float], scale_factor: float
    ) -> dict[str, float]:
        """Apply scaling to item values.

        Args:
            values: Dictionary of period values
            scale_factor: Factor to scale by

        Returns:
            Dictionary of scaled values
        """
        if scale_factor == 1.0:
            return values

        scaled_values = {}
        for period, value in values.items():
            if pd.notna(value):
                scaled_values[period] = value * scale_factor
            else:
                scaled_values[period] = value

        return scaled_values

    def _format_item_values(
        self,
        item: Union[StatementItem, Section],
        values: dict[str, float],
        period_columns: list[str],
    ) -> dict[str, str]:
        """Format values for an item using its specific display format if available.

        Args:
            item: The item to format values for
            values: Dictionary of period values
            period_columns: List of period column names

        Returns:
            Dictionary of formatted values
        """
        # Get item-specific display format
        item_format = getattr(item, "display_format", None)

        formatted_values = {}
        for period in period_columns:
            value = values.get(period, np.nan)

            if pd.notna(value):
                if item_format:
                    try:
                        formatted_values[period] = f"{value:{item_format}}"
                    except (ValueError, TypeError):
                        # Fall back to default if format is invalid
                        logger.warning(
                            f"Invalid display format '{item_format}' for item '{getattr(item, 'id', 'unknown')}', using default"
                        )
                        formatted_values[period] = str(
                            value
                        )  # Convert to string for consistency
                else:
                    formatted_values[period] = str(
                        value
                    )  # Convert to string for consistency
            else:
                formatted_values[period] = ""

        return formatted_values

    def _format_contra_value(
        self, value: float, display_style: str | None = None
    ) -> str:
        """Format a contra item value according to the specified display style.

        Args:
            value: The numeric value to format
            display_style: Style for contra display ("parentheses", "negative_sign", "brackets")

        Returns:
            Formatted string representation of the contra value
        """
        if pd.isna(value) or value == 0:
            return ""

        # For contra items, we typically want to show the absolute value with special formatting
        # regardless of the underlying sign, since sign_convention handles calculation logic
        from fin_statement_model.config.helpers import cfg

        style = display_style or self.default_formats.get(
            "contra_display_style", cfg("display.contra_display_style", "parentheses")
        )
        abs_value = abs(value)

        # Use dictionary for style formatting
        style_formats = {
            "parentheses": f"({abs_value:,.2f})",
            "negative_sign": f"-{abs_value:,.2f}",
            "brackets": f"[{abs_value:,.2f}]",
        }

        if style and isinstance(style, str) and style in style_formats:
            return style_formats[style]
        return f"({abs_value:,.2f})"  # Default fallback

    def _apply_contra_formatting(
        self,
        item: Union[StatementItem, Section],
        values: dict[str, float],
        period_columns: list[str],
        display_style: str | None = None,
    ) -> dict[str, str]:
        """Apply contra-specific formatting to item values.

        Args:
            item: The item to format
            values: Dictionary of period values
            period_columns: List of period column names
            display_style: Optional override for contra display style

        Returns:
            Dictionary of formatted contra values
        """
        contra_formatted = {}
        for period in period_columns:
            value = values.get(period, np.nan)
            contra_formatted[period] = self._format_contra_value(value, display_style)

        return contra_formatted

    def _prepare_formatting_context(self, **kwargs: Any) -> FormattingContext:
        """Prepare formatting context with config defaults.

        Args:
            **kwargs: All formatting parameters passed to generate_dataframe (overrides config)

        Returns:
            FormattingContext: Configured context object
        """
        from fin_statement_model import get_config

        config = get_config()

        # Create context with provided kwargs
        context = FormattingContext(
            should_apply_signs=kwargs.get(
                "should_apply_signs", config.display.apply_sign_conventions
            ),
            include_empty_items=kwargs.get(
                "include_empty_items", config.display.include_empty_items
            ),
            number_format=kwargs.get("number_format"),
            include_metadata_cols=kwargs.get(
                "include_metadata_cols", config.display.include_metadata_cols
            ),
            adjustment_filter=kwargs.get("adjustment_filter"),
            add_is_adjusted_column=kwargs.get(
                "add_is_adjusted_column", config.display.add_is_adjusted_column
            ),
            include_units_column=kwargs.get(
                "include_units_column", config.display.include_units_column
            ),
            include_css_classes=kwargs.get(
                "include_css_classes", config.display.include_css_classes
            ),
            include_notes_column=kwargs.get(
                "include_notes_column", config.display.include_notes_column
            ),
            apply_item_scaling=kwargs.get(
                "apply_item_scaling", config.display.apply_item_scaling
            ),
            apply_item_formatting=kwargs.get(
                "apply_item_formatting", config.display.apply_item_formatting
            ),
            respect_hide_flags=kwargs.get("respect_hide_flags"),
            contra_display_style=kwargs.get("contra_display_style"),
            apply_contra_formatting=kwargs.get(
                "apply_contra_formatting", config.display.apply_contra_formatting
            ),
            add_contra_indicator_column=kwargs.get(
                "add_contra_indicator_column",
                config.display.add_contra_indicator_column,
            ),
        )

        # Apply config defaults for None values
        if context.should_apply_signs is None:
            context.should_apply_signs = (
                True  # This is a calculation default, not display
            )
        if context.include_empty_items is None:
            context.include_empty_items = False  # Preserve historical default
        if context.respect_hide_flags is None:
            context.respect_hide_flags = config.display.hide_zero_rows
        if context.contra_display_style is None:
            context.contra_display_style = config.display.contra_display_style
        if context.number_format is None:
            context.number_format = config.display.default_number_format

        # Set default formats
        context.default_formats = self.default_formats

        return context

    def _fetch_statement_data(
        self, graph: Graph, context: FormattingContext
    ) -> tuple[dict[str, dict[str, float]], Any]:
        """Fetch data from graph using DataFetcher.

        Args:
            graph: The core.graph.Graph instance containing the data
            context: Formatting context with fetch parameters

        Returns:
            Tuple of (data dictionary, fetch errors)
        """
        data_fetcher = DataFetcher(self.statement, graph)
        fetch_result = data_fetcher.fetch_all_data(
            adjustment_filter=context.adjustment_filter,
            include_missing=context.include_empty_items,
        )

        # Log any warnings/errors
        if fetch_result.errors.has_warnings() or fetch_result.errors.has_errors():
            fetch_result.errors.log_all(
                prefix=f"Statement '{self.statement.id}' data fetch: "
            )

        # Update context with periods from graph
        context.all_periods = graph.periods

        return fetch_result.data, fetch_result.errors

    def _create_empty_dataframe(self, context: FormattingContext) -> pd.DataFrame:
        """Create an empty DataFrame with appropriate columns.

        Args:
            context: Formatting context with column configuration

        Returns:
            Empty DataFrame with proper column structure
        """
        base_cols = ["Line Item", "ID", *context.all_periods]
        if context.include_units_column:
            base_cols.append("units")
        return pd.DataFrame(columns=base_cols)

    def _build_row_data(
        self,
        graph: Graph,
        data: dict[str, dict[str, float]],
        context: FormattingContext,
    ) -> list[dict[str, Any]]:
        """Build row data recursively from statement structure.

        Args:
            graph: The core.graph.Graph instance
            data: Fetched data dictionary
            context: Formatting context

        Returns:
            List of row dictionaries
        """
        rows: list[dict[str, Any]] = []
        id_resolver = IDResolver(self.statement, standard_node_registry)

        # Process all sections
        self._process_items_recursive(
            items=list(self.statement.sections),
            depth=0,
            data=data,
            rows=rows,
            context=context,
            id_resolver=id_resolver,
            graph=graph,
        )

        # Filter hidden items if needed
        if context.respect_hide_flags:
            rows = [row for row in rows if row["ID"] not in context.items_to_hide]

        return rows

    def _process_items_recursive(
        self,
        items: list[Union[Section, StatementItem]],
        depth: int,
        data: dict[str, dict[str, float]],
        rows: list[dict[str, Any]],
        context: FormattingContext,
        id_resolver: IDResolver,
        graph: Graph,
    ) -> None:
        """Recursively process items and sections.

        Args:
            items: List of items or sections to process
            depth: Current indentation depth
            data: Fetched data dictionary
            rows: List to append row data to
            context: Formatting context
            id_resolver: ID resolver instance
            graph: Graph instance
        """
        for item in items:
            if isinstance(item, Section):
                self._process_section(
                    item, depth, data, rows, context, id_resolver, graph
                )
            elif isinstance(item, StatementItem):
                self._process_item(item, depth, data, rows, context, id_resolver, graph)

    def _process_section(
        self,
        section: Section,
        depth: int,
        data: dict[str, dict[str, float]],
        rows: list[dict[str, Any]],
        context: FormattingContext,
        id_resolver: IDResolver,
        graph: Graph,
    ) -> None:
        """Process a section and its items.

        Args:
            section: Section to process
            depth: Current indentation depth
            data: Fetched data dictionary
            rows: List to append row data to
            context: Formatting context
            id_resolver: ID resolver instance
            graph: Graph instance
        """
        # Process section items first to collect data for hide check
        self._process_items_recursive(
            section.items, depth + 1, data, rows, context, id_resolver, graph
        )

        # Process subtotal if it exists
        if hasattr(section, "subtotal") and section.subtotal:
            self._process_items_recursive(
                [section.subtotal], depth + 1, data, rows, context, id_resolver, graph
            )

        # Check if section should be hidden
        if context.respect_hide_flags and getattr(section, "hide_if_all_zero", False):
            # For sections, check if all contained items are hidden or zero
            section_has_data = False
            for section_item in section.items:
                node_id = id_resolver.resolve(section_item.id, graph)
                if node_id and node_id in data:
                    item_data = data[node_id]
                    if any(pd.notna(v) and v != 0 for v in item_data.values()):
                        section_has_data = True
                        break
            if not section_has_data:
                context.items_to_hide.add(section.id)

    def _process_item(
        self,
        item: StatementItem,
        depth: int,
        data: dict[str, dict[str, float]],
        rows: list[dict[str, Any]],
        context: FormattingContext,
        id_resolver: IDResolver,
        graph: Graph,
    ) -> None:
        """Process a single statement item.

        Args:
            item: Statement item to process
            depth: Current indentation depth
            data: Fetched data dictionary
            rows: List to append row data to
            context: Formatting context
            id_resolver: ID resolver instance
            graph: Graph instance
        """
        # Use ID resolver to get the correct node ID
        node_id = id_resolver.resolve(item.id, graph)
        if not node_id:
            return

        item_data = data.get(node_id, {})
        numeric_values: dict[str, float] = {
            p: item_data.get(p, np.nan) for p in context.all_periods
        }

        # Apply item-specific scaling if enabled
        if context.apply_item_scaling:
            numeric_values = self._apply_scaling(numeric_values, context, item)

        # Check if item should be hidden
        if context.respect_hide_flags and self._should_hide_item(item, numeric_values):
            context.items_to_hide.add(item.id)
            return

        # Start with numeric values and add formatted strings as needed
        row_values: dict[str, Union[float, str]] = dict(numeric_values)

        # Apply item-specific formatting if enabled (but only if not using global format)
        if context.apply_item_formatting and not context.number_format:
            formatted_values = self._format_item_values(
                item, numeric_values, context.all_periods
            )
            # Only apply if we got actual formatted strings
            if any(isinstance(v, str) for v in formatted_values.values()):
                for period in context.all_periods:
                    if period in formatted_values and isinstance(
                        formatted_values[period], str
                    ):
                        # Keep numeric value for calculations, store formatted for display
                        row_values[f"{period}_formatted"] = formatted_values[period]

        # Apply contra formatting if enabled and item is marked as contra
        if context.apply_contra_formatting and getattr(item, "is_contra", False):
            contra_formatted = self._apply_contra_formatting(
                item, numeric_values, context.all_periods, context.contra_display_style
            )
            # Store contra formatted values for later use
            for period in context.all_periods:
                if contra_formatted.get(period):
                    row_values[f"{period}_contra"] = contra_formatted[period]

        if context.include_empty_items or any(pd.notna(v) for v in row_values.values()):
            row = self._create_row_dict(item, node_id, row_values, depth, context)
            rows.append(row)

    def _create_row_dict(
        self,
        item: StatementItem,
        node_id: str,
        row_values: dict[str, Union[float, str]],
        depth: int,
        context: FormattingContext,
    ) -> dict[str, Any]:
        """Create a row dictionary for a statement item.

        Args:
            item: Statement item
            node_id: Resolved node ID
            row_values: Period values for the item
            depth: Indentation depth
            context: Formatting context

        Returns:
            Row dictionary
        """
        indent_char = context.default_formats["indent_character"]

        row = {
            "Line Item": indent_char * depth + item.name,
            "ID": item.id,
            **row_values,
            # Metadata
            "line_type": self._get_item_type(item),
            "node_id": node_id,
            "sign_convention": getattr(item, "sign_convention", 1),
            "is_subtotal": isinstance(item, SubtotalLineItem),
            "is_calculated": isinstance(item, CalculatedLineItem),
            "is_contra": getattr(item, "is_contra", False),
        }

        # Add enhanced metadata columns if requested
        if context.include_units_column:
            row["units"] = self._resolve_units(item)

        if context.include_css_classes:
            # Get item's CSS class and add contra class if applicable
            item_css_class = getattr(item, "css_class", None)
            if getattr(item, "is_contra", False):
                contra_css = context.default_formats.get(
                    "contra_css_class", "contra-item"
                )
                if item_css_class:
                    row["css_class"] = f"{item_css_class} {contra_css}"
                else:
                    row["css_class"] = contra_css
            else:
                row["css_class"] = item_css_class

        if context.include_notes_column:
            notes = getattr(item, "notes_references", [])
            row["notes"] = "; ".join(notes) if notes else ""

        return row

    def _apply_scaling(
        self, values: dict[str, float], context: FormattingContext, item: StatementItem
    ) -> dict[str, float]:
        """Apply item-specific scaling if enabled.

        Args:
            values: Dictionary of period values
            context: Formatting context
            item: Statement item

        Returns:
            Dictionary of scaled values
        """
        if not context.apply_item_scaling:
            return values

        scale_factor = self._resolve_display_scale_factor(item)
        return self._apply_item_scaling(values, scale_factor)

    def _organize_dataframe_columns(
        self, df: pd.DataFrame, context: FormattingContext
    ) -> pd.DataFrame:
        """Organize DataFrame columns in the correct order.

        Args:
            df: DataFrame to organize
            context: Formatting context

        Returns:
            DataFrame with organized columns
        """
        base_cols = ["Line Item", "ID"]
        metadata_cols = [
            "line_type",
            "node_id",
            "sign_convention",
            "is_subtotal",
            "is_calculated",
            "is_contra",
        ]

        # Enhanced metadata columns
        enhanced_cols = []
        if context.include_units_column:
            enhanced_cols.append("units")
        if context.include_css_classes:
            enhanced_cols.append("css_class")
        if context.include_notes_column:
            enhanced_cols.append("notes")
        if context.add_contra_indicator_column:
            enhanced_cols.append("is_contra")

        # Add adjustment columns if they exist
        adjusted_flag_cols = []
        if context.add_is_adjusted_column:
            adjusted_flag_cols = [
                f"{period}_is_adjusted" for period in context.all_periods
            ]

        final_cols = base_cols + context.all_periods
        if adjusted_flag_cols:
            final_cols += adjusted_flag_cols
        if enhanced_cols:
            final_cols += enhanced_cols
        if context.include_metadata_cols:
            # Add metadata cols (excluding adjustment flags if they are already added)
            final_cols += [
                m_col for m_col in metadata_cols if m_col not in adjusted_flag_cols
            ]

        # Ensure contra formatting columns are available temporarily (will be removed later)
        all_available_cols = final_cols.copy()
        if context.apply_contra_formatting:
            contra_formatted_cols = [
                f"{period}_contra" for period in context.all_periods
            ]
            all_available_cols += contra_formatted_cols

        for col in all_available_cols:
            if col not in df.columns:
                df[col] = (
                    np.nan
                    if col in context.all_periods
                    else ("" if col == "Line Item" else None)
                )

        return df[all_available_cols]

    def _add_adjustment_columns(
        self, df: pd.DataFrame, graph: Graph, context: FormattingContext
    ) -> pd.DataFrame:
        """Add adjustment status columns to the DataFrame.

        Args:
            df: DataFrame to add columns to
            graph: Graph instance
            context: Formatting context

        Returns:
            DataFrame with adjustment columns added
        """
        if not context.add_is_adjusted_column or not context.all_periods:
            return df

        # Get node IDs from the dataframe
        node_ids_to_check = []
        for _, row in df.iterrows():
            node_id = row.get("node_id")
            is_calc_or_subtotal = row.get("is_calculated", False) or row.get(
                "is_subtotal", False
            )
            if node_id and not is_calc_or_subtotal:
                node_ids_to_check.append(node_id)

        # Use DataFetcher to check adjustments
        if node_ids_to_check:
            data_fetcher = DataFetcher(self.statement, graph)
            adjustment_status = data_fetcher.check_adjustments(
                node_ids_to_check, context.all_periods, context.adjustment_filter
            )
        else:
            adjustment_status = {}

        # Build adjustment columns
        is_adjusted_data = []
        for _, row in df.iterrows():
            node_id = row.get("node_id")
            is_calc_or_subtotal = row.get("is_calculated", False) or row.get(
                "is_subtotal", False
            )

            if node_id and not is_calc_or_subtotal and node_id in adjustment_status:
                row_adj_flags = {
                    f"{period}_is_adjusted": adjustment_status[node_id].get(
                        period, False
                    )
                    for period in context.all_periods
                }
            else:
                # For calculated/subtotal items or missing nodes, flags are False
                row_adj_flags = {
                    f"{period}_is_adjusted": False for period in context.all_periods
                }
            is_adjusted_data.append(row_adj_flags)

        if is_adjusted_data:
            adj_df = pd.DataFrame(is_adjusted_data, index=df.index)
            df = pd.concat([df, adj_df], axis=1)

        return df

    def _cleanup_temporary_columns(
        self, df: pd.DataFrame, context: FormattingContext
    ) -> pd.DataFrame:
        """Clean up temporary columns from the dataframe.

        Args:
            df: DataFrame to clean up
            context: Formatting context

        Returns:
            DataFrame with temporary columns removed
        """
        # Remove the contra formatting columns from the final output
        if context.apply_contra_formatting:
            contra_cols_to_remove = [
                f"{period}_contra" for period in context.all_periods
            ]
            df = df.drop(
                columns=[col for col in contra_cols_to_remove if col in df.columns]
            )

        # Build final column list
        base_cols = ["Line Item", "ID"]
        metadata_cols = [
            "line_type",
            "node_id",
            "sign_convention",
            "is_subtotal",
            "is_calculated",
            "is_contra",
        ]

        enhanced_cols = []
        if context.include_units_column:
            enhanced_cols.append("units")
        if context.include_css_classes:
            enhanced_cols.append("css_class")
        if context.include_notes_column:
            enhanced_cols.append("notes")
        if context.add_contra_indicator_column:
            enhanced_cols.append("is_contra")

        adjusted_flag_cols = []
        if context.add_is_adjusted_column:
            adjusted_flag_cols = [
                f"{period}_is_adjusted" for period in context.all_periods
            ]

        final_cols = base_cols + context.all_periods
        if context.add_is_adjusted_column:
            final_cols += adjusted_flag_cols
        if enhanced_cols:
            final_cols += enhanced_cols
        if context.include_metadata_cols:
            # Add metadata cols (excluding adjustment flags if they are already added)
            final_cols += [
                m_col for m_col in metadata_cols if m_col not in adjusted_flag_cols
            ]

        # Select only the final columns for output
        return df[final_cols]

    def _apply_all_formatting(
        self, df: pd.DataFrame, context: FormattingContext
    ) -> pd.DataFrame:
        """Apply consolidated formatting in a single vectorized pass."""
        # Combined sign, contra, and number formatting
        df = render_values(
            df=df,
            period_columns=context.all_periods,
            default_formats=context.default_formats,
            number_format=context.number_format,
            contra_display_style=context.contra_display_style,
        )
        # Clean up temporary columns
        df = self._cleanup_temporary_columns(df, context)
        return df

    def generate_dataframe(
        self,
        graph: Graph,
        context: Optional[FormattingContext] = None,
    ) -> pd.DataFrame:
        """Generate a formatted DataFrame for the statement.

        Args:
            graph: Graph instance containing the data to render.
            context: FormattingContext object containing all display / formatting
                configuration. If *None*, a context will be created from project
                configuration.

        Returns:
            A fully-formatted ``pandas.DataFrame`` representing the statement.
        """
        if context is None:
            # Build context purely from project defaults (no legacy kwargs path).
            context = self._prepare_formatting_context()

        # 2. Fetch statement data -------------------------------------------------------------------
        data, _ = self._fetch_statement_data(graph, context)

        # 3. Build row dictionaries -----------------------------------------------------------------
        rows = self._build_row_data(graph, data, context)

        # 4. Assemble DataFrame ---------------------------------------------------------------------
        if not rows:
            return self._create_empty_dataframe(context)

        df = pd.DataFrame(rows)

        # 5. Adjustment columns (must precede column organisation)
        if context.add_is_adjusted_column:
            df = self._add_adjustment_columns(df, graph, context)

        # 6. Column ordering / injection ------------------------------------------------------------
        df = self._organize_dataframe_columns(df, context)

        # 7. Post-processing formatting -------------------------------------------------------------
        df = self._apply_all_formatting(df, context)

        return df

    def _get_item_type(self, item: StatementItem) -> str:
        """Get the type of a statement item.

        Args:
            item: Statement item to get type for

        Returns:
            str: Item type identifier
        """
        if isinstance(item, Section):
            return "section"
        elif isinstance(item, SubtotalLineItem):
            return "subtotal"
        elif isinstance(item, CalculatedLineItem):
            return "calculated"
        else:
            return "item"

    def format_html(
        self,
        graph: Graph,
        should_apply_signs: Optional[bool] = None,
        include_empty_items: Optional[bool] = None,
        css_styles: Optional[dict[str, str]] = None,
        use_item_css_classes: Optional[bool] = None,
        **kwargs: Any,
    ) -> str:
        """Format the statement data as HTML with enhanced styling support.

        Args:
            graph: The core.graph.Graph instance containing the data.
            should_apply_signs: Whether to apply sign conventions (override config).
            include_empty_items: Whether to include items with no data (override config).
            css_styles: Optional dict of CSS styles for the HTML.
            use_item_css_classes: Whether to use item-specific CSS classes (override config).
            **kwargs: Additional arguments passed to generate_dataframe.

        Returns:
            str: HTML string representing the statement with enhanced styling.
        """
        # Determine defaults for unspecified parameters -------------------------------------------
        from fin_statement_model import get_config

        cfg = get_config()

        should_apply_signs = (
            should_apply_signs
            if should_apply_signs is not None
            else cfg.display.apply_sign_conventions
        )
        include_empty_items = (
            include_empty_items
            if include_empty_items is not None
            else cfg.display.include_empty_items
        )
        use_item_css_classes = (
            use_item_css_classes
            if use_item_css_classes is not None
            else cfg.display.include_css_classes
        )

        # Ensure the correct flag for CSS class inclusion is propagated
        if use_item_css_classes:
            kwargs["include_css_classes"] = True

        # Build a FormattingContext with the specified overrides and any additional
        # keyword arguments provided by the caller.
        context = self._prepare_formatting_context(
            should_apply_signs=should_apply_signs,
            include_empty_items=include_empty_items,
            **kwargs,
        )

        df = self.generate_dataframe(graph=graph, context=context)

        html: str = df.to_html(
            index=False, classes="statement-table", table_id="financial-statement"
        )

        if css_styles or use_item_css_classes:
            style_str = "<style>\n"

            # Add default styles for statement tables
            style_str += """
            .statement-table { border-collapse: collapse; width: 100%; }
            .statement-table th, .statement-table td { padding: 8px; text-align: right; border: 1px solid #ddd; }
            .statement-table th { background-color: #f2f2f2; font-weight: bold; }
            .statement-table .Line.Item { text-align: left; }
            .contra-item { font-style: italic; color: #666; }
            """

            # Add custom styles
            if css_styles:
                for selector, style in css_styles.items():
                    style_str += f"{selector} {{ {style} }}\n"

            style_str += "</style>\n"
            html = style_str + html

        return html



================================================================================
File: fin_statement_model/statements/orchestration/__init__.py
================================================================================

"""High-level orchestration functions for statement processing.

This package provides the main public API for:
- Creating statement DataFrames from configurations
- Exporting statements to various formats
- Coordinating the overall workflow
"""

from .exporter import export_statements_to_excel, export_statements_to_json
from .orchestrator import create_statement_dataframe, populate_graph

__all__ = [
    # Main API
    "create_statement_dataframe",
    "export_statements_to_excel",
    "export_statements_to_json",
    # Internal helpers
    "populate_graph",
]



================================================================================
File: fin_statement_model/statements/orchestration/exporter.py
================================================================================

"""Statement export functionality.

This module handles exporting financial statements to various file formats
including Excel and JSON. It provides both internal helper functions and
public API functions for exporting statements.
"""

import logging
from pathlib import Path
from typing import Any, Callable, Optional

import pandas as pd

from fin_statement_model.core.errors import FinancialModelError
from fin_statement_model.core.graph import Graph
from fin_statement_model.io import write_statement_to_excel, write_statement_to_json
from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.statements.orchestration.orchestrator import (
    create_statement_dataframe,
)

logger = logging.getLogger(__name__)

__all__ = [
    "export_statements",
    "export_statements_to_excel",
    "export_statements_to_json",
]


def export_statements(
    dfs: dict[str, pd.DataFrame],
    output_dir: str,
    writer_func: Callable[..., None],
    writer_kwargs: dict[str, Any],
    file_suffix: str,
) -> None:
    """Export pre-generated statement DataFrames using the provided writer.

    This function is intentionally *dumb*: it assumes the heavy-lifting has
    already been done by :pyfunc:`create_statement_dataframe` and focuses solely
    on persisting the resulting pandas objects to disk.

    Args:
        dfs: A mapping of ``statement_id`` to pandas ``DataFrame`` instances
            returned by :pyfunc:`create_statement_dataframe`.
        output_dir: Target directory where the files will be written. The
            directory (and any missing parents) is created if it does not yet
            exist.
        writer_func: Concrete low-level writer (e.g.
            :pyfunc:`write_statement_to_excel`). Must accept the dataframe as
            first argument and the destination path as second argument.
        writer_kwargs: Arbitrary keyword arguments forwarded verbatim to
            *writer_func*.
        file_suffix: Extension used for the generated files (e.g. ``".xlsx"``).

    Raises:
        TypeError: If *dfs* is not a mapping of ``str -> DataFrame``.
        WriteError: Propagated if the underlying *writer_func* fails for any
            individual statement.
    """

    if not isinstance(dfs, dict):
        raise TypeError(
            "'dfs' must be a mapping of statement_id to pandas DataFrame. "
            "Call 'create_statement_dataframe' first and pass the result here."
        )

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    if not dfs:
        logger.warning("Received an empty mapping of DataFrames – nothing to export.")
        return

    export_errors: list[tuple[str, str]] = []

    for stmt_id, df in dfs.items():
        # Ensure stmt_id is filename-safe (replace path separators)
        safe_stmt_id = stmt_id.replace("/", "_").replace("\\", "_")
        file_path = output_path / f"{safe_stmt_id}{file_suffix}"

        try:
            writer_func(df, str(file_path), **writer_kwargs)
            logger.info(
                "Successfully exported statement '%s' to %s", stmt_id, file_path
            )
        except WriteError as e:
            logger.exception(
                "Failed to write %s file for statement '%s'", file_suffix, stmt_id
            )
            export_errors.append((stmt_id, str(e)))
        except Exception as e:  # noqa: BLE001 – re-wrap unknown exceptions
            logger.exception(
                "Unexpected error exporting statement '%s' to %s", stmt_id, file_path
            )
            export_errors.append((stmt_id, f"Unexpected export error: {e!s}"))

    if export_errors:
        error_summary = "; ".join(f"{sid}: {err}" for sid, err in export_errors)
        raise WriteError(
            f"Encountered {len(export_errors)} errors during {file_suffix} export: "
            f"{error_summary}"
        )


def export_statements_to_excel(
    graph: Graph,
    raw_configs: dict[str, dict[str, Any]],
    output_dir: str,
    format_kwargs: Optional[dict[str, Any]] = None,
    writer_kwargs: Optional[dict[str, Any]] = None,
) -> None:
    """Generate statement DataFrames and export them to individual Excel files.

    Loads configurations, builds statements, populates the graph (if necessary),
    generates DataFrames for each statement, and saves each DataFrame to a
    separate `.xlsx` file in the specified `output_dir`.

    Args:
        graph: The core.graph.Graph instance containing necessary data.
        raw_configs: Mapping of statement IDs to configuration dictionaries.
        output_dir: The directory where the resulting Excel files will be saved.
            File names will be derived from the statement IDs (e.g.,
            'income_statement.xlsx').
        format_kwargs: Optional dictionary of arguments passed to
            `StatementFormatter.generate_dataframe` when creating the DataFrames.
        writer_kwargs: Optional dictionary of arguments passed to the underlying
            Excel writer (`write_statement_to_excel`), such as
            `sheet_name` or engine options.

    Raises:
        ConfigurationError: If loading or validating configurations fails.
        StatementError: If processing statements fails critically.
        TypeError: If `raw_configs` is not a valid mapping.
        WriteError: If writing any of the Excel files fails.
        FinancialModelError: Potentially other errors from graph operations.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> # Assume 'my_graph' is a pre-populated Graph instance
        >>> # Assume configs exist in './statement_configs/'
        >>> try:
        ...     export_statements_to_excel(
        ...         graph=my_graph,
        ...         raw_configs=my_configs,
        ...         output_dir='./output_excel/',
        ...         writer_kwargs={'freeze_panes': (1, 1)} # Freeze header row/col
        ...     )
        ...     # Use logger.info
        ...     logger.info("Statements exported to ./output_excel/")
        ... except (FileNotFoundError, ConfigurationError, StatementError, WriteError) as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Export failed: {e}")
    """
    try:
        dfs = create_statement_dataframe(graph, raw_configs, format_kwargs or {})
    except FinancialModelError:
        logger.exception("Failed to generate statement DataFrames for Excel export:")
        raise

    export_statements(
        dfs=dfs,
        output_dir=output_dir,
        writer_func=write_statement_to_excel,
        writer_kwargs=writer_kwargs or {},
        file_suffix=".xlsx",
    )


def export_statements_to_json(
    graph: Graph,
    raw_configs: dict[str, dict[str, Any]],
    output_dir: str,
    format_kwargs: Optional[dict[str, Any]] = None,
    writer_kwargs: Optional[dict[str, Any]] = None,
) -> None:
    """Generate statement DataFrames and export them to individual JSON files.

    Loads configurations, builds statements, populates the graph (if necessary),
    generates DataFrames for each statement, and saves each DataFrame to a
    separate `.json` file in the specified `output_dir`.

    Args:
        graph: The core.graph.Graph instance containing necessary data.
        raw_configs: Mapping of statement IDs to configuration dictionaries.
        output_dir: The directory where the resulting JSON files will be saved.
            File names will be derived from the statement IDs (e.g.,
            'balance_sheet.json').
        format_kwargs: Optional dictionary of arguments passed to
            `StatementFormatter.generate_dataframe` when creating the DataFrames.
        writer_kwargs: Optional dictionary of arguments passed to the underlying
            JSON writer (`write_statement_to_json`). Common options
            include `orient` (e.g., 'records', 'columns', 'split') and `indent`.
            Defaults to 'records' orient and indent=2 if not provided.

    Raises:
        ConfigurationError: If loading or validating configurations fails.
        StatementError: If processing statements fails critically.
        TypeError: If `raw_configs` is not a valid mapping.
        WriteError: If writing any of the JSON files fails.
        FinancialModelError: Potentially other errors from graph operations.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> # Assume 'my_graph' is a pre-populated Graph instance
        >>> # Assume configs exist in './statement_configs/'
        >>> try:
        ...     export_statements_to_json(
        ...         graph=my_graph,
        ...         raw_configs=my_configs,
        ...         output_dir='./output_json/',
        ...         writer_kwargs={'orient': 'split', 'indent': 4}
        ...     )
        ...     # Use logger.info
        ...     logger.info("Statements exported to ./output_json/")
        ... except (FileNotFoundError, ConfigurationError, StatementError, WriteError) as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Export failed: {e}")
    """
    final_writer_kwargs = writer_kwargs or {}
    # Set JSON specific defaults if not provided
    final_writer_kwargs.setdefault("orient", "records")
    final_writer_kwargs.setdefault("indent", 2)

    try:
        dfs = create_statement_dataframe(graph, raw_configs, format_kwargs or {})
    except FinancialModelError:
        logger.exception("Failed to generate statement DataFrames for JSON export:")
        raise

    export_statements(
        dfs=dfs,
        output_dir=output_dir,
        writer_func=write_statement_to_json,
        writer_kwargs=final_writer_kwargs,
        file_suffix=".json",
    )



================================================================================
File: fin_statement_model/statements/orchestration/loader.py
================================================================================

"""Statement loading and building functionality.

This module handles validation, building, and registration of statement structures
from in-memory configuration dictionaries.
"""

import logging
from typing import Any, Optional

from fin_statement_model.core.errors import ConfigurationError, StatementError
from fin_statement_model.statements.structure.builder import StatementStructureBuilder
from fin_statement_model.statements.configs.validator import StatementConfig
from fin_statement_model.statements.registry import StatementRegistry
from fin_statement_model.io.validation import UnifiedNodeValidator

logger = logging.getLogger(__name__)

__all__ = ["load_build_register_statements"]


def load_build_register_statements(
    raw_configs: dict[str, dict[str, Any]],
    registry: StatementRegistry,
    builder: Optional[StatementStructureBuilder] = None,
    enable_node_validation: bool = False,
    node_validation_strict: bool = False,
    node_validator: Optional[UnifiedNodeValidator] = None,
) -> list[str]:
    """Load, validate, build, and register statement structures from config dicts.

    Args:
        raw_configs: Mapping of statement IDs to configuration dicts.
        registry: StatementRegistry instance to register loaded statements.
        builder: Optional StatementStructureBuilder instance.
        enable_node_validation: If True, validate node IDs during config and build.
        node_validation_strict: If True, treat validation failures as errors.
        node_validator: Optional pre-configured UnifiedNodeValidator.

    Returns:
        List of statement IDs successfully loaded and registered.

    Raises:
        ConfigurationError: If config validation fails.
        StatementError: If registration fails.
    """
    loaded_statement_ids: list[str] = []
    errors: list[tuple[str, str]] = []

    # Enforce modern API: raw_configs must be an in-memory mapping.
    if not isinstance(raw_configs, dict):
        raise TypeError(
            "'raw_configs' must be a mapping of statement_id to configuration dict. "
            "Loading configurations from file paths has been removed. "
            "Load the YAML/JSON into memory first (e.g., with yaml.safe_load) "
            "and pass the resulting mapping instead."
        )

    if builder is None:
        builder = StatementStructureBuilder(
            enable_node_validation=enable_node_validation,
            node_validation_strict=node_validation_strict,
            node_validator=node_validator,
        )

    if not raw_configs:
        logger.warning("No statement configurations provided.")
        return []

    for stmt_id, raw_data in raw_configs.items():
        try:
            config = StatementConfig(
                config_data=raw_data,
                enable_node_validation=enable_node_validation,
                node_validation_strict=node_validation_strict,
                node_validator=node_validator,
            )
            validation_errors = config.validate_config()
            if validation_errors:
                raise ConfigurationError(
                    f"Invalid configuration for statement '{stmt_id}'",
                    errors=validation_errors,
                )
            statement = builder.build(config)
            registry.register(statement)
            loaded_statement_ids.append(statement.id)
        except (ConfigurationError, StatementError, ValueError) as e:
            logger.exception(f"Failed to process/register statement '{stmt_id}'.")
            errors.append((stmt_id, str(e)))
        except Exception as e:
            logger.exception(f"Unexpected error processing statement '{stmt_id}'.")
            errors.append((stmt_id, f"Unexpected error: {e!s}"))

    if errors:
        error_details = "; ".join(f"{sid}: {msg}" for sid, msg in errors)
        logger.warning(
            f"Encountered {len(errors)} errors during statement loading/building: {error_details}"
        )

    return loaded_statement_ids



================================================================================
File: fin_statement_model/statements/orchestration/orchestrator.py
================================================================================

"""Main orchestration for statement processing.

This module coordinates the workflow of building statements from in-memory
configurations, populating graphs, and generating DataFrames.
"""

import logging
from typing import Any, Optional

import pandas as pd

from fin_statement_model.core.errors import StatementError
from fin_statement_model.core.graph import Graph

from fin_statement_model.statements.structure.builder import StatementStructureBuilder
from fin_statement_model.statements.formatting.formatter import StatementFormatter
from fin_statement_model.statements.orchestration.loader import (
    load_build_register_statements,
)
from fin_statement_model.statements.population.populator import (
    populate_graph_from_statement,
)
from fin_statement_model.statements.registry import StatementRegistry

logger = logging.getLogger(__name__)

__all__ = ["create_statement_dataframe", "populate_graph"]


def populate_graph(registry: StatementRegistry, graph: Graph) -> list[tuple[str, str]]:
    """Populate the graph with nodes based on registered statements."""
    all_errors: list[tuple[str, str]] = []
    statements = registry.get_all_statements()
    if not statements:
        logger.warning("No statements registered to populate the graph.")
        return []

    for statement in statements:
        errors = populate_graph_from_statement(statement, graph)
        for item_id, msg in errors:
            all_errors.append((item_id, msg))

    if all_errors:
        logger.warning(f"Encountered {len(all_errors)} errors during graph population.")
    return all_errors


def create_statement_dataframe(
    graph: Graph,
    raw_configs: dict[str, dict[str, Any]],
    format_kwargs: Optional[dict[str, Any]] = None,
    enable_node_validation: Optional[bool] = None,
    node_validation_strict: Optional[bool] = None,
) -> dict[str, pd.DataFrame]:
    """Build statements from configurations, populate graph, and format DataFrames.

    Args:
        graph: Graph instance to populate.
        raw_configs: Mapping of statement IDs to configuration dicts.
        format_kwargs: Optional kwargs for formatter.
        enable_node_validation: If True, validate node IDs.
        node_validation_strict: If True, treat validation failures as errors.

    Returns:
        Mapping of statement IDs to pandas DataFrames.

    Raises:
        StatementError: If loading or formatting fails.
    """
    registry = StatementRegistry()
    enable_node_validation = (
        enable_node_validation if enable_node_validation is not None else False
    )
    node_validation_strict = (
        node_validation_strict if node_validation_strict is not None else False
    )
    builder = StatementStructureBuilder(
        enable_node_validation=enable_node_validation,
        node_validation_strict=node_validation_strict,
    )
    format_kwargs = format_kwargs or {}

    # Step 1: Load, build, register
    loaded_ids = load_build_register_statements(
        raw_configs,
        registry,
        builder,
        enable_node_validation=enable_node_validation,
        node_validation_strict=node_validation_strict,
    )
    if not loaded_ids:
        raise StatementError("No valid statements could be loaded.")

    # Step 2: Populate graph
    populate_graph(registry, graph)

    # Step 3: Format results
    results: dict[str, pd.DataFrame] = {}
    for stmt_id in loaded_ids:
        statement = registry.get(stmt_id)
        if statement is None:
            logger.error(f"Statement '{stmt_id}' not found in registry.")
            raise StatementError(f"Statement '{stmt_id}' not found in registry.")
        formatter = StatementFormatter(statement)
        if format_kwargs:
            context = formatter._prepare_formatting_context(**format_kwargs)
        else:
            # Use default project configuration
            context = formatter._prepare_formatting_context()

        df = formatter.generate_dataframe(graph, context=context)
        results[stmt_id] = df

    return results



================================================================================
File: fin_statement_model/statements/population/__init__.py
================================================================================

"""Graph population functionality for financial statements.

This package handles the conversion of statement structures into graph nodes:
- ID resolution between statement items and graph nodes
- Processing different item types (metrics, calculations, subtotals)
- Managing dependencies and retry logic
"""

from .id_resolver import IDResolver
from .item_processors import (
    CalculatedItemProcessor,
    ItemProcessor,
    ItemProcessorManager,
    MetricItemProcessor,
    ProcessorResult,
    SubtotalItemProcessor,
)
from .populator import populate_graph_from_statement

__all__ = [
    "CalculatedItemProcessor",
    # ID Resolution
    "IDResolver",
    # Item Processors
    "ItemProcessor",
    "ItemProcessorManager",
    "MetricItemProcessor",
    "ProcessorResult",
    "SubtotalItemProcessor",
    # Main Function
    "populate_graph_from_statement",
]



================================================================================
File: fin_statement_model/statements/population/id_resolver.py
================================================================================

"""ID resolution for statement items to graph nodes.

This module provides centralized logic for resolving statement item IDs to their
corresponding graph node IDs, handling the complexity of different item types
having different ID mapping rules, including standard node references.
"""

import logging
from typing import Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes.standard_registry import StandardNodeRegistry
from fin_statement_model.statements.structure import (
    StatementStructure,
    LineItem,
)

logger = logging.getLogger(__name__)

__all__ = ["IDResolver"]


class IDResolver:
    """Centralizes ID resolution from statement items to graph nodes.

    This class handles the complexity of mapping statement item IDs to graph
    node IDs, accounting for the fact that:
    - LineItems can have either a direct node_id property OR a standard_node_ref
      that gets resolved through the provided registry
    - Other items (CalculatedLineItem, SubtotalLineItem, MetricLineItem) use
      their ID directly as the node ID
    - Some nodes may exist directly in the graph without being statement items

    The resolver caches mappings for performance and provides both single and
    batch resolution methods. Standard node references are resolved at cache
    build time for optimal performance.
    """

    def __init__(self, statement: StatementStructure, registry: StandardNodeRegistry):
        """Initialize the resolver with a statement structure and a registry.

        Args:
            statement: The statement structure containing items to resolve.
            registry: The standard node registry for resolving references.
        """
        self.statement = statement
        self._registry = registry
        self._item_to_node_cache: dict[str, str] = {}
        self._node_to_items_cache: dict[str, list[str]] = {}
        self._build_cache()

    def _build_cache(self) -> None:
        """Pre-build ID mappings for all items in the statement."""
        logger.debug(f"Building ID cache for statement '{self.statement.id}'")

        for item in self.statement.get_all_items():
            if isinstance(item, LineItem):
                # Get the resolved node ID (handles both direct node_id and standard_node_ref)
                resolved_node_id = item.get_resolved_node_id(self._registry)
                if resolved_node_id:
                    # LineItems map their ID to their resolved node_id
                    self._item_to_node_cache[item.id] = resolved_node_id
                    self._node_to_items_cache.setdefault(resolved_node_id, []).append(
                        item.id
                    )

                    # Log if using standard node reference for debugging
                    if item.standard_node_ref:
                        logger.debug(
                            f"Resolved standard node reference '{item.standard_node_ref}' "
                            f"to '{resolved_node_id}' for item '{item.id}'"
                        )
                else:
                    logger.warning(
                        f"Could not resolve node reference for LineItem '{item.id}'. "
                        f"node_id: {item.node_id}, standard_node_ref: {item.standard_node_ref}"
                    )
            else:
                # Other items use their ID directly as the node ID
                self._item_to_node_cache[item.id] = item.id
                self._node_to_items_cache.setdefault(item.id, []).append(item.id)

        logger.debug(
            f"ID cache built: {len(self._item_to_node_cache)} item->node mappings, "
            f"{len(self._node_to_items_cache)} unique nodes"
        )

    def resolve(self, item_id: str, graph: Optional[Graph] = None) -> Optional[str]:
        """Resolve a statement item ID to its graph node ID.

        Resolution process:
        1. Check the pre-built cache for the item ID
        2. If not found and a graph is provided, check if the ID exists
           directly as a node in the graph
        3. Return None if not found anywhere

        Args:
            item_id: The statement item ID to resolve.
            graph: Optional graph to check for direct node existence.

        Returns:
            The resolved graph node ID if found, None otherwise.
        """
        # Rebuild cache if it's empty (e.g., after invalidation)
        if not self._item_to_node_cache:
            self._build_cache()

        # Check cache first
        if item_id in self._item_to_node_cache:
            return self._item_to_node_cache[item_id]

        # Check if it exists directly in graph
        if graph and graph.has_node(item_id):
            # Cache this discovery for future lookups
            self._item_to_node_cache[item_id] = item_id
            self._node_to_items_cache.setdefault(item_id, []).append(item_id)
            return item_id

        return None

    def resolve_multiple(
        self, item_ids: list[str], graph: Optional[Graph] = None
    ) -> dict[str, Optional[str]]:
        """Resolve multiple item IDs at once.

        Args:
            item_ids: List of statement item IDs to resolve.
            graph: Optional graph to check for direct node existence.

        Returns:
            Dictionary mapping each item ID to its resolved node ID (or None).
        """
        return {item_id: self.resolve(item_id, graph) for item_id in item_ids}

    def get_items_for_node(self, node_id: str) -> list[str]:
        """Get all statement item IDs that map to a given node ID.

        This reverse lookup can be useful for debugging and understanding
        which statement items contribute to a particular graph node.

        Args:
            node_id: The graph node ID to look up.

        Returns:
            List of statement item IDs that map to this node (may be empty).
        """
        # Rebuild cache if it's empty
        if not self._node_to_items_cache:
            self._build_cache()
        return self._node_to_items_cache.get(node_id, [])

    def get_all_mappings(self) -> dict[str, str]:
        """Get all item ID to node ID mappings.

        Returns:
            Dictionary of all cached mappings.
        """
        # Rebuild cache if it's empty
        if not self._item_to_node_cache:
            self._build_cache()
        return self._item_to_node_cache.copy()

    def invalidate_cache(self) -> None:
        """Clear the cache, forcing a rebuild on next resolution.

        This should be called if the statement structure changes after
        the resolver was created.
        """
        self._item_to_node_cache.clear()
        self._node_to_items_cache.clear()
        logger.debug(f"ID cache invalidated for statement '{self.statement.id}'")

    def refresh_cache(self) -> None:
        """Rebuild the cache from the current statement structure."""
        self.invalidate_cache()
        self._build_cache()



================================================================================
File: fin_statement_model/statements/population/item_processors.py
================================================================================

"""Item processors for converting statement items into graph nodes.

This module provides a processor hierarchy that handles the conversion of different
statement item types (MetricLineItem, CalculatedLineItem, SubtotalLineItem) into
graph nodes. Each processor encapsulates the logic for its specific item type,
reducing complexity and improving testability.
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import (
    NodeError,
    CircularDependencyError,
    CalculationError,
    ConfigurationError,
    MetricError,
)
from fin_statement_model.core.metrics import metric_registry
from fin_statement_model.statements.structure import (
    StatementStructure,
    StatementItem,
    MetricLineItem,
    CalculatedLineItem,
    SubtotalLineItem,
)
from fin_statement_model.statements.population.id_resolver import IDResolver
from fin_statement_model.statements.utilities.result_types import (
    Result,
    Success,
    Failure,
    ErrorDetail,
    ErrorSeverity,
)

logger = logging.getLogger(__name__)

__all__ = [
    "CalculatedItemProcessor",
    "ItemProcessor",
    "ItemProcessorManager",
    "MetricItemProcessor",
    "ProcessorResult",
    "SubtotalItemProcessor",
]


@dataclass
class ProcessorResult:
    """Result of processing a statement item.

    Attributes:
        success: Whether the processing was successful.
        node_added: Whether a new node was added to the graph.
        error_message: Error message if processing failed.
        missing_inputs: List of missing input details (item_id, resolved_node_id).
    """

    success: bool
    node_added: bool = False
    error_message: Optional[str] = None
    missing_inputs: Optional[list[tuple[str, Optional[str]]]] = None

    def to_result(self) -> Result[bool]:
        """Convert to the new Result type."""
        if self.success:
            return Success(value=self.node_added)

        errors = []
        if self.error_message:
            errors.append(
                ErrorDetail(
                    code="processing_error",
                    message=self.error_message,
                    severity=ErrorSeverity.ERROR,
                )
            )

        if self.missing_inputs:
            for item_id, node_id in self.missing_inputs:
                msg = (
                    f"Missing input: item '{item_id}' needs node '{node_id}'"
                    if node_id
                    else f"Missing input: item '{item_id}' not found/mappable"
                )
                errors.append(
                    ErrorDetail(
                        code="missing_input",
                        message=msg,
                        context=f"item_id={item_id}, node_id={node_id}",
                        severity=ErrorSeverity.ERROR,
                    )
                )

        return Failure(errors=errors)


class ItemProcessor(ABC):
    """Abstract base class for processing statement items into graph nodes.

    This base class provides common functionality for resolving input IDs
    and handling missing inputs across different item types.
    """

    def __init__(
        self, id_resolver: IDResolver, graph: Graph, statement: StatementStructure
    ):
        """Initialize the processor.

        Args:
            id_resolver: ID resolver for mapping statement IDs to graph node IDs.
            graph: The graph to add nodes to.
            statement: The statement structure being processed.
        """
        self.id_resolver = id_resolver
        self.graph = graph
        self.statement = statement

    @abstractmethod
    def can_process(self, item: StatementItem) -> bool:
        """Check if this processor can handle the given item type.

        Args:
            item: The statement item to check.

        Returns:
            True if this processor can handle the item type.
        """

    @abstractmethod
    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process the item and add it to the graph if needed.

        Args:
            item: The statement item to process.
            is_retry: Whether this is a retry attempt (affects error logging).

        Returns:
            ProcessorResult indicating success/failure and details.
        """

    def resolve_inputs(
        self, input_ids: list[str]
    ) -> tuple[list[str], list[tuple[str, Optional[str]]]]:
        """Resolve input IDs to graph node IDs.

        Args:
            input_ids: List of statement item IDs to resolve.

        Returns:
            Tuple of (resolved_node_ids, missing_details).
            missing_details contains tuples of (item_id, resolved_node_id_or_none).
        """
        resolved = []
        missing = []

        for input_id in input_ids:
            node_id = self.id_resolver.resolve(input_id, self.graph)
            if node_id and self.graph.has_node(node_id):
                resolved.append(node_id)
            else:
                missing.append((input_id, node_id))

        return resolved, missing

    def _handle_missing_inputs(
        self,
        item: StatementItem,
        missing: list[tuple[str, Optional[str]]],
        is_retry: bool,
    ) -> ProcessorResult:
        """Handle missing input nodes consistently across processors.

        Args:
            item: The item being processed.
            missing: List of missing input details.
            is_retry: Whether this is a retry attempt.

        Returns:
            ProcessorResult with appropriate error details.
        """
        missing_summary = [
            (
                f"item '{i_id}' needs node '{n_id}'"
                if n_id
                else f"item '{i_id}' not found/mappable"
            )
            for i_id, n_id in missing
        ]

        if is_retry:
            logger.error(
                f"Retry failed for {type(item).__name__} '{item.id}' in statement '{self.statement.id}': "
                f"missing required inputs: {'; '.join(missing_summary)}"
            )
            return ProcessorResult(
                success=False,
                error_message=f"Missing inputs on retry: {missing_summary}",
                missing_inputs=missing,
            )
        else:
            # Don't log on first attempt - allows dependency resolution
            return ProcessorResult(success=False, missing_inputs=missing)


class MetricItemProcessor(ItemProcessor):
    """Processor for MetricLineItem objects.

    Handles the creation of metric-based calculation nodes by:
    1. Looking up the metric in the registry
    2. Validating input mappings
    3. Resolving input IDs to graph nodes
    4. Adding the metric node to the graph
    """

    def can_process(self, item: StatementItem) -> bool:
        """Check if item is a MetricLineItem."""
        return isinstance(item, MetricLineItem)

    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process a MetricLineItem and add it to the graph."""
        # Early validation
        if not isinstance(item, MetricLineItem):
            return ProcessorResult(success=False, error_message="Invalid item type")

        # Check if node already exists
        if self.graph.has_node(item.id):
            return ProcessorResult(success=True, node_added=False)

        # Initialize result variables
        error_message = None
        node_added = False

        # Get metric from registry
        try:
            metric = metric_registry.get(item.metric_id)
        except MetricError as e:
            logger.exception(
                f"Cannot populate item '{item.id}': Metric '{item.metric_id}' not found in registry"
            )
            error_message = f"Metric '{item.metric_id}' not found: {e}"

        # Validate input mappings if no error yet
        if not error_message:
            error_message = self._validate_metric_inputs(metric, item)

        # Resolve metric inputs if no error yet
        if not error_message:
            resolved_map, missing = self._resolve_metric_inputs(metric, item)
            if missing:
                return self._handle_missing_inputs(item, missing, is_retry)

            # Add to graph
            try:
                self.graph.add_metric(
                    metric_name=item.metric_id,
                    node_name=item.id,
                    input_node_map=resolved_map,
                )
                node_added = True
            except Exception as e:
                logger.exception(f"Failed to add metric node '{item.id}'")
                error_message = f"Failed to add metric node: {e}"

        # Single exit point
        if error_message:
            return ProcessorResult(success=False, error_message=error_message)
        return ProcessorResult(success=True, node_added=node_added)

    def _validate_metric_inputs(
        self, metric: Any, item: MetricLineItem
    ) -> Optional[str]:
        """Validate that the item provides all required metric inputs."""
        provided_inputs = set(item.inputs.keys())
        required_inputs = set(metric.inputs)

        if provided_inputs != required_inputs:
            missing_req = required_inputs - provided_inputs
            extra_prov = provided_inputs - required_inputs
            error_msg = f"Input mapping mismatch for metric '{item.metric_id}' in item '{item.id}'."

            if missing_req:
                error_msg += f" Missing required metric inputs: {missing_req}."
            if extra_prov:
                error_msg += f" Unexpected inputs provided: {extra_prov}."

            logger.error(error_msg)
            return error_msg

        return None

    def _resolve_metric_inputs(
        self, metric: Any, item: MetricLineItem
    ) -> tuple[dict[str, str], list[tuple[str, Optional[str]]]]:
        """Resolve metric input mappings to graph node IDs."""
        resolved_map = {}
        missing = []

        for metric_input_name in metric.inputs:
            input_item_id = item.inputs[metric_input_name]
            node_id = self.id_resolver.resolve(input_item_id, self.graph)

            if node_id and self.graph.has_node(node_id):
                resolved_map[metric_input_name] = node_id
            else:
                missing.append((input_item_id, node_id))

        return resolved_map, missing


class CalculatedItemProcessor(ItemProcessor):
    """Processor for CalculatedLineItem objects.

    Handles the creation of calculation nodes with specific operations by:
    1. Resolving input IDs to graph nodes
    2. Getting sign conventions from input items
    3. Creating the calculation node with proper sign handling
    """

    def can_process(self, item: StatementItem) -> bool:
        """Check if item is a CalculatedLineItem."""
        return isinstance(item, CalculatedLineItem)

    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process a CalculatedLineItem and add it to the graph."""
        if not isinstance(item, CalculatedLineItem):
            return ProcessorResult(success=False, error_message="Invalid item type")

        # Check if node already exists
        if self.graph.has_node(item.id):
            return ProcessorResult(success=True, node_added=False)

        # Command: create any needed signed nodes
        neg_base_ids: list[str] = []
        for input_id in item.input_ids:
            input_item = self.statement.find_item_by_id(input_id)
            if input_item and getattr(input_item, "sign_convention", 1) == -1:
                node_id = self.id_resolver.resolve(input_id, self.graph)
                if node_id:
                    neg_base_ids.append(node_id)
        if neg_base_ids:
            self.graph.ensure_signed_nodes(neg_base_ids)

        # Query: resolve inputs without mutating graph
        resolved_inputs, missing = self._resolve_inputs(item)
        if missing:
            return self._handle_missing_inputs(item, missing, is_retry)

        # Add calculation node
        error_message = None
        try:
            self.graph.add_calculation(
                name=item.id,
                input_names=resolved_inputs,
                operation_type=item.calculation_type,
                **item.parameters,
            )
        except (
            NodeError,
            CircularDependencyError,
            CalculationError,
            ConfigurationError,
        ) as e:
            error_msg = f"Failed to add calculation node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = str(e)
        except Exception as e:
            error_msg = f"Unexpected error adding calculation node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = f"Unexpected error: {e}"

        if error_message:
            return ProcessorResult(success=False, error_message=error_message)
        return ProcessorResult(success=True, node_added=True)

    def _resolve_inputs(
        self, item: CalculatedLineItem
    ) -> tuple[list[str], list[tuple[str, Optional[str]]]]:
        """Resolve input IDs to graph node or signed-node IDs without side effects.

        Args:
            item: The CalculatedLineItem being processed.

        Returns:
            Tuple of (resolved_node_ids, missing_details).
        """
        resolved: list[str] = []
        missing: list[tuple[str, Optional[str]]] = []

        for input_id in item.input_ids:
            node_id = self.id_resolver.resolve(input_id, self.graph)
            # Missing base node
            if not node_id or not self.graph.has_node(node_id):
                missing.append((input_id, node_id))
                continue
            # Determine sign
            input_item = self.statement.find_item_by_id(input_id)
            sign = getattr(input_item, "sign_convention", 1) if input_item else 1
            if sign == -1:
                signed_id = f"{node_id}_signed"
                # Represent signed node if exists, else missing
                if signed_id in self.graph.nodes:
                    resolved.append(signed_id)
                else:
                    missing.append((input_id, signed_id))
            else:
                resolved.append(node_id)
        return resolved, missing


class SubtotalItemProcessor(ItemProcessor):
    """Processor for SubtotalLineItem objects.

    Handles the creation of subtotal (addition) nodes by:
    1. Resolving input IDs to graph nodes
    2. Adding an addition calculation node
    """

    def can_process(self, item: StatementItem) -> bool:
        """Check if item is a SubtotalLineItem."""
        return isinstance(item, SubtotalLineItem)

    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process a SubtotalLineItem and add it to the graph."""
        if not isinstance(item, SubtotalLineItem):
            return ProcessorResult(success=False, error_message="Invalid item type")

        # Check if node already exists
        if self.graph.has_node(item.id):
            return ProcessorResult(success=True, node_added=False)

        # Handle empty subtotals
        if not item.item_ids:
            logger.debug(f"Subtotal item '{item.id}' has no input items")
            return ProcessorResult(success=True, node_added=False)

        # Resolve inputs
        resolved, missing = self.resolve_inputs(item.item_ids)
        if missing:
            return self._handle_missing_inputs(item, missing, is_retry)

        # Add subtotal as addition calculation
        error_message = None
        try:
            self.graph.add_calculation(
                name=item.id, input_names=resolved, operation_type="addition"
            )
        except (
            NodeError,
            CircularDependencyError,
            CalculationError,
            ConfigurationError,
        ) as e:
            error_msg = f"Failed to add subtotal node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = str(e)
        except Exception as e:
            error_msg = f"Unexpected error adding subtotal node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = f"Unexpected error: {e}"

        if error_message:
            return ProcessorResult(success=False, error_message=error_message)
        return ProcessorResult(success=True, node_added=True)


class ItemProcessorManager:
    """Manages the collection of item processors.

    This class coordinates the processing of different statement item types
    by delegating to the appropriate processor based on the item type.
    """

    def __init__(
        self, id_resolver: IDResolver, graph: Graph, statement: StatementStructure
    ):
        """Initialize the processor manager with all available processors.

        Args:
            id_resolver: ID resolver for mapping statement IDs to graph node IDs.
            graph: The graph to add nodes to.
            statement: The statement structure being processed.
        """
        self.processors = [
            MetricItemProcessor(id_resolver, graph, statement),
            CalculatedItemProcessor(id_resolver, graph, statement),
            SubtotalItemProcessor(id_resolver, graph, statement),
        ]

    def process_item(
        self, item: StatementItem, is_retry: bool = False
    ) -> ProcessorResult:
        """Process a statement item using the appropriate processor.

        Args:
            item: The statement item to process.
            is_retry: Whether this is a retry attempt.

        Returns:
            ProcessorResult from the appropriate processor, or a success result
            if no processor handles the item type (e.g., for LineItem).
        """
        for processor in self.processors:
            if processor.can_process(item):
                return processor.process(item, is_retry)

        # No processor found - this is OK for non-calculation items like LineItem
        logger.debug(
            f"No processor for item type {type(item).__name__} with ID '{item.id}'. "
            "This is expected for non-calculation items."
        )
        return ProcessorResult(success=True, node_added=False)



================================================================================
File: fin_statement_model/statements/population/populator.py
================================================================================

"""Populates a `fin_statement_model.core.graph.Graph` with calculation nodes.

This module provides the function `populate_graph_from_statement`, which is
responsible for translating the calculation logic defined within a
`StatementStructure` (specifically `CalculatedLineItem`, `SubtotalLineItem`,
and `MetricLineItem`) into actual calculation nodes within a `Graph` instance.

Key Concepts:
- **ID Resolution**: Statement configurations use item IDs that must be resolved
  to graph node IDs. This is handled by the `IDResolver` class.
- **Dependency Ordering**: Items may depend on other items. The populator handles
  this by retrying failed items after successful ones, allowing dependencies to
  be satisfied.
- **Idempotency**: If a node already exists in the graph, it will be skipped.
"""

import logging

from fin_statement_model.core.graph import Graph
from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.statements.population.id_resolver import IDResolver
from fin_statement_model.core.nodes import standard_node_registry
from fin_statement_model.statements.population.item_processors import (
    ItemProcessorManager,
)

logger = logging.getLogger(__name__)

__all__ = ["populate_graph_from_statement"]


def populate_graph_from_statement(
    statement: StatementStructure, graph: Graph
) -> list[tuple[str, str]]:
    """Add calculation nodes defined in a StatementStructure to a Graph.

    This function bridges the gap between static statement definitions and the
    dynamic calculation graph. It processes three types of items:

    1. **CalculatedLineItem**: Creates calculation nodes with specified operations
    2. **SubtotalLineItem**: Creates addition nodes that sum multiple items
    3. **MetricLineItem**: Creates metric-based calculation nodes

    ID Resolution Logic:
    - Input IDs in statement configurations are resolved to graph node IDs using
      the `IDResolver` class
    - This handles the mapping between statement item IDs and actual graph nodes
    - Resolution accounts for LineItem.node_id vs other items using their ID directly

    Dependency Handling:
    - Items may depend on other items that haven't been created yet
    - The function uses a retry mechanism: failed items are retried after
      successful ones, allowing dependencies to be resolved
    - Circular dependencies are detected and reported as errors

    Idempotency:
    - If a node already exists in the graph, it will be skipped
    - This allows the function to be called multiple times safely

    Args:
        statement: The `StatementStructure` object containing the definitions
            of calculated items, subtotals, and metrics.
        graph: The `core.graph.Graph` instance that will be populated with
            the calculation nodes.

    Returns:
        A list of tuples, where each tuple contains `(item_id, error_message)`
        for any items that could not be successfully added to the graph. An
        empty list indicates that all applicable items were added (or already
        existed) without critical errors.

    Raises:
        TypeError: If `statement` is not a `StatementStructure` or `graph` is
            not a `Graph` instance.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> from fin_statement_model.statements.structure import StatementStructure
        >>>
        >>> # Create graph with data nodes
        >>> graph = Graph()
        >>> graph.add_financial_statement_item('revenue_node', {'2023': 1000})
        >>> graph.add_financial_statement_item('cogs_node', {'2023': 600})
        >>>
        >>> # Create statement with calculations
        >>> statement = StatementStructure(id="IS", name="Income Statement")
        >>> # Add a LineItem that maps to 'revenue_node'
        >>> revenue_item = LineItem(id='revenue', name='Revenue', node_id='revenue_node')
        >>> # Add a CalculatedLineItem that references the LineItem
        >>> gross_profit = CalculatedLineItem(
        ...     id='gross_profit',
        ...     name='Gross Profit',
        ...     calculation_type='subtraction',
        ...     input_ids=['revenue', 'cogs']  # Uses LineItem IDs
        ... )
        >>>
        >>> errors = populate_graph_from_statement(statement, graph)
        >>> # The function will:
        >>> # 1. Resolve 'revenue' to 'revenue_node' via LineItem.node_id
        >>> # 2. Resolve 'cogs' to 'cogs_node' (if it exists in statement or graph)
        >>> # 3. Create a calculation node 'gross_profit' with the resolved inputs
    """
    # Validate inputs
    if not isinstance(statement, StatementStructure):
        raise TypeError("statement must be a StatementStructure instance")
    if not isinstance(graph, Graph):
        raise TypeError("graph must be a Graph instance")

    # Initialize components
    id_resolver = IDResolver(statement, standard_node_registry)
    processor_manager = ItemProcessorManager(id_resolver, graph, statement)

    # Get all items to process
    calculation_items = statement.get_calculation_items()
    metric_items = statement.get_metric_items()
    all_items_to_process = calculation_items + metric_items

    # Track results
    errors_encountered: list[tuple[str, str]] = []
    nodes_added_count = 0

    logger.info(
        f"Starting graph population for statement '{statement.id}'. "
        f"Processing {len(all_items_to_process)} calculation/metric items."
    )

    # Process items with retry mechanism
    items_to_process = list(all_items_to_process)
    processed_in_pass = -1  # Initialize to enter loop

    while items_to_process and processed_in_pass != 0:
        items_failed_this_pass = []
        processed_in_pass = 0

        logger.debug(f"Population loop: Processing {len(items_to_process)} items...")

        for item in items_to_process:
            # Determine if this is a retry (not the first overall pass)
            is_retry = len(items_to_process) < len(all_items_to_process)

            # Process the item
            result = processor_manager.process_item(item, is_retry)

            if result.success:
                processed_in_pass += 1
                if result.node_added:
                    nodes_added_count += 1
            else:
                items_failed_this_pass.append(item)
                # Only record errors on retry or for non-dependency errors
                if is_retry and result.error_message:
                    errors_encountered.append((item.id, result.error_message))

        # Prepare for next iteration
        items_to_process = items_failed_this_pass

        # Check for stalled progress
        if processed_in_pass == 0 and items_to_process:
            logger.warning(
                f"Population loop stalled. {len(items_to_process)} items could not be processed: "
                f"{[item.id for item in items_to_process]}"
            )
            # Add errors for items that couldn't be processed
            for item in items_to_process:
                if not any(err[0] == item.id for err in errors_encountered):
                    errors_encountered.append(
                        (
                            item.id,
                            "Failed to process due to unresolved dependencies or circular reference.",
                        )
                    )
            break

    # Log results
    if errors_encountered:
        logger.warning(
            f"Graph population for statement '{statement.id}' completed with "
            f"{len(errors_encountered)} persistent errors."
        )
    else:
        log_level = logging.INFO if nodes_added_count > 0 else logging.DEBUG
        logger.log(
            log_level,
            f"Graph population for statement '{statement.id}' completed. "
            f"Added {nodes_added_count} new nodes.",
        )

    return errors_encountered



================================================================================
File: fin_statement_model/statements/registry.py
================================================================================

"""Registry for managing loaded and validated financial statement structures.

This module provides the `StatementRegistry` class, which acts as a central
store for `StatementStructure` objects after they have been loaded from
configurations and built. It ensures uniqueness of statement IDs and provides
methods for retrieving registered statements.
"""

import logging
from typing import Optional

# Assuming StatementStructure is defined here or imported appropriately
# We might need to adjust this import based on the actual location
try:
    from .structure import StatementStructure
except ImportError:
    # Handle cases where structure might be in a different sub-package later if needed
    # For now, assume it's available via relative import
    from fin_statement_model.statements.structure import StatementStructure

from .errors import StatementError  # Assuming StatementError is in statements/errors.py

logger = logging.getLogger(__name__)

__all__ = ["StatementRegistry"]


class StatementRegistry:
    """Manages a collection of loaded financial statement structures.

    This registry holds instances of `StatementStructure`, keyed by their unique
    IDs. It prevents duplicate registrations and provides methods to access
    registered statements individually or collectively.

    Attributes:
        _statements: A dictionary mapping statement IDs (str) to their
                     corresponding `StatementStructure` objects.
    """

    def __init__(self) -> None:
        """Initialize an empty statement registry."""
        self._statements: dict[str, StatementStructure] = {}
        logger.debug("StatementRegistry initialized.")

    def register(self, statement: StatementStructure) -> None:
        """Register a statement structure with the registry.

        Ensures the provided object is a `StatementStructure` with a valid ID
        and that the ID is not already present in the registry.

        Args:
            statement: The `StatementStructure` instance to register.

        Raises:
            TypeError: If the `statement` argument is not an instance of
                `StatementStructure`.
            ValueError: If the `statement` has an invalid or empty ID.
            StatementError: If a statement with the same ID (`statement.id`) is
                already registered.
        """
        if not isinstance(statement, StatementStructure):
            raise TypeError("Only StatementStructure objects can be registered.")

        statement_id = statement.id
        if not statement_id:
            raise ValueError(
                "StatementStructure must have a valid non-empty id to be registered."
            )

        if statement_id in self._statements:
            # Policy: Raise error on conflict
            logger.error(
                f"Attempted to register duplicate statement ID: '{statement_id}'"
            )
            raise StatementError(
                message=f"Statement with ID '{statement_id}' is already registered.",
                # statement_id=statement_id # Add if StatementError accepts this arg
            )

        self._statements[statement_id] = statement
        logger.info(f"Registered statement '{statement.name}' with ID '{statement_id}'")

    def get(self, statement_id: str) -> Optional[StatementStructure]:
        """Get a registered statement by its ID.

        Returns:
            The `StatementStructure` instance associated with the given ID if
            it exists, otherwise returns `None`.

        Example:
            >>> registry = StatementRegistry()
            >>> # Assume 'income_statement' is a valid StatementStructure instance
            >>> # registry.register(income_statement)
            >>> retrieved_statement = registry.get("income_statement_id")
            >>> if retrieved_statement:
            ...     logger.info(f"Found: {retrieved_statement.name}")
            ... else:
            ...     logger.info("Statement not found.")
        """
        return self._statements.get(statement_id)

    def get_all_ids(self) -> list[str]:
        """Get the IDs of all registered statements.

        Returns:
            A list containing the unique IDs of all statements currently held
            in the registry.
        """
        return list(self._statements.keys())

    def get_all_statements(self) -> list[StatementStructure]:
        """Get all registered statement structure objects.

        Returns:
            A list containing all `StatementStructure` objects currently held
            in the registry.
        """
        return list(self._statements.values())

    def clear(self) -> None:
        """Remove all statement structures from the registry.

        Resets the registry to an empty state.
        """
        self._statements = {}
        logger.info("StatementRegistry cleared.")

    def __len__(self) -> int:
        """Return the number of registered statements."""
        return len(self._statements)

    def __contains__(self, statement_id: str) -> bool:
        """Check if a statement ID exists in the registry.

        Allows using the `in` operator with the registry.

        Args:
            statement_id: The statement ID to check for.

        Returns:
            `True` if a statement with the given ID is registered, `False` otherwise.

        Example:
            >>> registry = StatementRegistry()
            >>> # Assume 'income_statement' is registered with ID 'IS_2023'
            >>> # registry.register(income_statement)
            >>> print("IS_2023" in registry)  # Output: True
            >>> print("BS_2023" in registry)  # Output: False
        """
        return statement_id in self._statements



================================================================================
File: fin_statement_model/statements/structure/__init__.py
================================================================================

"""Statement structure package.

Re-export domain model classes from submodules.
"""

from .items import (
    StatementItem,
    StatementItemType,
    LineItem,
    CalculatedLineItem,
    MetricLineItem,
    SubtotalLineItem,
)
from .containers import Section, StatementStructure

__all__ = [
    "CalculatedLineItem",
    "LineItem",
    "MetricLineItem",
    "Section",
    "StatementItem",
    "StatementItemType",
    "StatementStructure",
    "SubtotalLineItem",
]



================================================================================
File: fin_statement_model/statements/structure/builder.py
================================================================================

"""Builds StatementStructure objects from validated StatementConfig models.

This module provides the `StatementStructureBuilder`, which translates the
deserialized and validated configuration (represented by `StatementConfig`
containing Pydantic models) into the hierarchical `StatementStructure` object
used internally for representing the layout and components of a financial
statement.
"""

import logging
from typing import Union, Optional, Any, cast

# Assuming config and structure modules are accessible
from fin_statement_model.statements.configs.validator import StatementConfig
from fin_statement_model.statements.configs.models import (
    SectionModel,
    BaseItemModel,
    LineItemModel,
    CalculatedItemModel,
    MetricItemModel,
    SubtotalModel,
    StatementModel,
    AdjustmentFilterSpec,
)
from fin_statement_model.statements.structure import (
    StatementStructure,
    Section,
    LineItem,
    MetricLineItem,
    CalculatedLineItem,
    SubtotalLineItem,
)
from fin_statement_model.statements.errors import ConfigurationError

# Import UnifiedNodeValidator for optional node validation during build
from fin_statement_model.io.validation import UnifiedNodeValidator
from fin_statement_model.config.helpers import cfg
from fin_statement_model.core.nodes import standard_node_registry

# Import Result types for enhanced error handling
from fin_statement_model.statements.utilities.result_types import (
    ErrorCollector,
    ErrorSeverity,
)

# Import adjustment types for filter conversion
from fin_statement_model.core.adjustments.models import AdjustmentFilter, AdjustmentType

logger = logging.getLogger(__name__)

__all__ = ["StatementStructureBuilder"]


class StatementStructureBuilder:
    """Constructs a `StatementStructure` object from a validated configuration.

    Takes a `StatementConfig` instance (which should have successfully passed
    validation, populating its `.model` attribute) and recursively builds the
    corresponding `StatementStructure`, including its sections, line items,
    calculated items, subtotals, and nested sections.
    """

    def __init__(
        self,
        enable_node_validation: Optional[bool] = None,
        node_validation_strict: Optional[bool] = None,
        node_validator: Optional[UnifiedNodeValidator] = None,
    ) -> None:
        """Initialize the StatementStructureBuilder.

        Defaults for validation flags come from the global statements config, but can be overridden locally.
        Args:
            enable_node_validation: If True, validates node IDs during build.
            node_validation_strict: If True, treats validation failures as errors.
            node_validator: Optional pre-configured UnifiedNodeValidator instance.
        """
        # Pull defaults from global config if not provided
        if enable_node_validation is None:
            enable_node_validation = cfg("statements.enable_node_validation")
        if node_validation_strict is None:
            node_validation_strict = cfg("statements.node_validation_strict")
        self.enable_node_validation = enable_node_validation
        self.node_validation_strict = node_validation_strict

        # Initialize node_validator
        self.node_validator: Optional[UnifiedNodeValidator] = None
        if self.enable_node_validation:
            if node_validator is not None:
                self.node_validator = node_validator
            else:
                # Create default validator using strict flag
                self.node_validator = UnifiedNodeValidator(
                    standard_node_registry,
                    strict_mode=self.node_validation_strict,
                    auto_standardize=True,
                    warn_on_non_standard=True,
                    enable_patterns=True,
                )
        else:
            self.node_validator = None

    def _convert_adjustment_filter(
        self, filter_input: Optional[Union[AdjustmentFilterSpec, list[str]]]
    ) -> Optional[Any]:
        """Convert configuration adjustment filter to core AdjustmentFilter or tag set.

        Args:
            filter_input: The filter specification from configuration.

        Returns:
            AdjustmentFilter instance, set of tags, or None.
        """
        # Use global default adjustment filter if none provided
        if filter_input is None:
            default_filter = cfg("statements.default_adjustment_filter")
            if default_filter is None:
                return None
            filter_input = default_filter
        # Simple list of tags - convert to set
        elif isinstance(filter_input, list):
            return set(filter_input)
        # Configuration spec to core filter conversion
        elif isinstance(filter_input, AdjustmentFilterSpec):
            # Convert AdjustmentFilterSpec to AdjustmentFilter
            kwargs: dict[str, Any] = {}

            # Convert list fields to sets
            if filter_input.include_scenarios:
                kwargs["include_scenarios"] = set(filter_input.include_scenarios)
            if filter_input.exclude_scenarios:
                kwargs["exclude_scenarios"] = set(filter_input.exclude_scenarios)
            if filter_input.include_tags:
                kwargs["include_tags"] = set(filter_input.include_tags)
            if filter_input.exclude_tags:
                kwargs["exclude_tags"] = set(filter_input.exclude_tags)
            if filter_input.require_all_tags:
                kwargs["require_all_tags"] = set(filter_input.require_all_tags)

            # Convert string types to AdjustmentType enums
            if filter_input.include_types:
                kwargs["include_types"] = cast(
                    set[AdjustmentType],
                    {
                        AdjustmentType(type_str)
                        for type_str in filter_input.include_types
                    },
                )
            if filter_input.exclude_types:
                kwargs["exclude_types"] = cast(
                    set[AdjustmentType],
                    {
                        AdjustmentType(type_str)
                        for type_str in filter_input.exclude_types
                    },
                )

            # Pass through period
            if filter_input.period:
                kwargs["period"] = filter_input.period

            # Create AdjustmentFilter from kwargs
            return AdjustmentFilter(**kwargs)

        # Unknown type - log warning and return None
        logger.warning(f"Unknown adjustment filter type: {type(filter_input)}")
        return None

    def build(self, config: StatementConfig) -> StatementStructure:
        """Build a `StatementStructure` from a validated `StatementConfig`.

        This is the main public method of the builder. It orchestrates the
        conversion process, calling internal helper methods to build sections
        and items.

        Args:
            config: A `StatementConfig` instance whose `.validate_config()`
                method has been successfully called, populating `config.model`.

        Returns:
            The fully constructed `StatementStructure` object, ready to be
            registered or used.

        Raises:
            ValueError: If the provided `config` object has not been validated
                (i.e., `config.model` is `None`).
            ConfigurationError: If an unexpected error occurs during the building
                process, potentially indicating an issue not caught by the
                initial Pydantic validation or an internal inconsistency.
        """
        if config.model is None:
            # Ensure validation has run successfully before building
            raise ValueError(
                "StatementConfig must be validated (config.model must be set) "
                "before building the structure."
            )

        # Build from the validated Pydantic model stored in config.model
        try:
            stmt_model = config.model  # Use validated model from config

            # Optional node validation during build
            if self.enable_node_validation and self.node_validator:
                error_collector = ErrorCollector()
                self._validate_structure_node_ids(stmt_model, error_collector)

                # Handle validation results
                if error_collector.has_errors() and self.node_validation_strict:
                    # Fail build on validation errors in strict mode
                    error_messages = [
                        str(error) for error in error_collector.get_errors()
                    ]
                    raise ConfigurationError(
                        message=f"Node validation failed for statement '{stmt_model.id}'",
                        errors=error_messages,
                    )
                elif error_collector.has_warnings() or error_collector.has_errors():
                    # Log warnings and non-strict errors
                    for warning in error_collector.get_warnings():
                        logger.warning(f"Build-time node validation: {warning}")
                    if not self.node_validation_strict:
                        for error in error_collector.get_errors():
                            logger.warning(f"Build-time node validation: {error}")

            statement = StatementStructure(
                id=stmt_model.id,
                name=stmt_model.name,
                description=cast(str, stmt_model.description),
                metadata=stmt_model.metadata,
                units=stmt_model.units,
                display_scale_factor=stmt_model.display_scale_factor,
            )
            for sec_model in stmt_model.sections:
                section = self._build_section_model(sec_model)
                statement.add_section(section)
            logger.info(
                f"Successfully built StatementStructure for ID '{statement.id}'"
            )
            return statement
        except Exception as e:
            # Catch potential errors during the building process itself
            logger.exception(
                f"Error building statement structure from validated model for ID '{config.model.id}'"
            )
            raise ConfigurationError(
                message=f"Failed to build statement structure from validated config: {e}",
                errors=[str(e)],
            ) from e

    def _validate_structure_node_ids(
        self, stmt_model: StatementModel, error_collector: ErrorCollector
    ) -> None:
        """Validate node IDs in the statement structure during build.

        This is a simpler validation focused on the final structure,
        complementing the config-level validation.

        Args:
            stmt_model: The StatementModel to validate.
            error_collector: ErrorCollector to accumulate validation issues.
        """
        logger.debug(f"Build-time node validation for statement '{stmt_model.id}'")

        # Validate key node references that will be used in the built structure
        collected_node_refs = set()

        # Collect all node references from the structure
        def collect_node_refs(items: list[Any]) -> None:
            for item in items:
                if isinstance(item, LineItemModel):
                    if item.node_id:
                        collected_node_refs.add(
                            (item.node_id, "line_item_node", f"item.{item.id}.node_id")
                        )
                    if item.standard_node_ref:
                        collected_node_refs.add(
                            (
                                item.standard_node_ref,
                                "standard_node",
                                f"item.{item.id}.standard_node_ref",
                            )
                        )
                elif isinstance(item, CalculatedItemModel):
                    collected_node_refs.add(
                        (item.id, "calculated_node", f"item.{item.id}.id")
                    )
                elif isinstance(item, MetricItemModel):
                    collected_node_refs.add(
                        (item.id, "metric_node", f"item.{item.id}.id")
                    )
                elif isinstance(item, SubtotalModel):
                    collected_node_refs.add(
                        (item.id, "subtotal_node", f"item.{item.id}.id")
                    )
                elif isinstance(item, SectionModel):
                    collect_node_refs(item.items)
                    collect_node_refs(item.subsections)
                    if item.subtotal:
                        collected_node_refs.add(
                            (
                                item.subtotal.id,
                                "subtotal_node",
                                f"section.{item.id}.subtotal.id",
                            )
                        )

        # Collect all node references
        for section in stmt_model.sections:
            collect_node_refs([section, *section.items, *section.subsections])

        # Validate collected references
        for node_id, node_type, context in collected_node_refs:
            self._validate_single_build_node_id(
                node_id, node_type, context, error_collector
            )

    def _validate_single_build_node_id(
        self,
        node_id: str,
        node_type: str,
        context: str,
        error_collector: ErrorCollector,
    ) -> None:
        """Validate a single node ID during build process.

        Args:
            node_id: The node ID to validate.
            node_type: Type description for error messages.
            context: Context string for error reporting.
            error_collector: ErrorCollector to accumulate validation issues.
        """
        if not self.node_validator:
            return

        try:
            validation_result = self.node_validator.validate(
                node_id,
                node_type=node_type,
                use_cache=True,
            )

            # Only report significant issues during build
            if not validation_result.is_valid:
                severity = (
                    ErrorSeverity.ERROR
                    if self.node_validation_strict
                    else ErrorSeverity.WARNING
                )
                message = f"Build validation: Invalid {node_type} '{node_id}': {validation_result.message}"

                if severity == ErrorSeverity.ERROR:
                    error_collector.add_error(
                        code="build_invalid_node_id",
                        message=message,
                        context=context,
                        source=node_id,
                    )
                else:
                    error_collector.add_warning(
                        code="build_invalid_node_id",
                        message=message,
                        context=context,
                        source=node_id,
                    )

        except Exception as e:
            logger.exception(
                f"Error during build-time validation of node ID '{node_id}'"
            )
            error_collector.add_warning(
                code="build_node_validation_error",
                message=f"Build validation error for {node_type} '{node_id}': {e}",
                context=context,
                source=node_id,
            )

    def _build_section_model(self, section_model: SectionModel) -> Section:
        """Build a `Section` object from a `SectionModel`.

        Recursively builds the items and subsections within this section.

        Args:
            section_model: The Pydantic model representing the section configuration.

        Returns:
            A `Section` instance corresponding to the model.
        """
        # Convert adjustment filter
        adjustment_filter = self._convert_adjustment_filter(
            section_model.default_adjustment_filter
        )

        section = Section(
            id=section_model.id,
            name=section_model.name,
            description=cast(str, section_model.description),
            metadata=section_model.metadata,
            default_adjustment_filter=adjustment_filter,
            display_format=section_model.display_format,
            hide_if_all_zero=section_model.hide_if_all_zero,
            css_class=section_model.css_class,
            notes_references=section_model.notes_references,
            units=section_model.units,
            display_scale_factor=section_model.display_scale_factor,
        )
        for item in section_model.items:
            section.add_item(self._build_item_model(item))
        for sub in section_model.subsections:
            # Recursively build subsections
            section.add_item(self._build_section_model(sub))
        if section_model.subtotal:
            section.subtotal = self._build_subtotal_model(section_model.subtotal)
        return section

    def _build_item_model(
        self, item_model: BaseItemModel
    ) -> Union[LineItem, CalculatedLineItem, MetricLineItem, SubtotalLineItem, Section]:
        """Build a statement item object from its corresponding Pydantic model.

        Dispatches the building process based on the specific type of the input
        model (`LineItemModel`, `CalculatedItemModel`, `MetricItemModel`,
        `SubtotalModel`, or `SectionModel` for nested sections).

        Args:
            item_model: The Pydantic model representing a line item, calculated
                item, metric item, subtotal, or nested section.

        Returns:
            The corresponding `StatementStructure` component (`LineItem`,
            `CalculatedLineItem`, `MetricLineItem`, `SubtotalLineItem`, or `Section`).

        Raises:
            TypeError: If an unexpected model type is encountered.
        """
        # Convert adjustment filter for all item types
        adjustment_filter = self._convert_adjustment_filter(
            item_model.default_adjustment_filter
        )

        # Dispatch by model instance type
        if isinstance(item_model, SectionModel):
            # Handle nested sections directly
            return self._build_section_model(item_model)
        if isinstance(item_model, LineItemModel):
            return LineItem(
                id=item_model.id,
                name=item_model.name,
                node_id=item_model.node_id,
                standard_node_ref=item_model.standard_node_ref,
                description=cast(str, item_model.description),
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
                default_adjustment_filter=adjustment_filter,
                display_format=item_model.display_format,
                hide_if_all_zero=item_model.hide_if_all_zero,
                css_class=item_model.css_class,
                notes_references=item_model.notes_references,
                units=item_model.units,
                display_scale_factor=item_model.display_scale_factor,
                is_contra=item_model.is_contra,
            )
        if isinstance(item_model, CalculatedItemModel):
            # Pass the calculation model directly or its dict representation
            return CalculatedLineItem(
                id=item_model.id,
                name=item_model.name,
                # Pass the nested Pydantic model if structure expects dict
                calculation=item_model.calculation.model_dump(),
                description=cast(str, item_model.description),
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
                default_adjustment_filter=adjustment_filter,
                display_format=item_model.display_format,
                hide_if_all_zero=item_model.hide_if_all_zero,
                css_class=item_model.css_class,
                notes_references=item_model.notes_references,
                units=item_model.units,
                display_scale_factor=item_model.display_scale_factor,
                is_contra=item_model.is_contra,
            )
        if isinstance(item_model, MetricItemModel):
            return MetricLineItem(
                id=item_model.id,
                name=item_model.name,
                metric_id=item_model.metric_id,
                inputs=item_model.inputs,
                description=cast(str, item_model.description),
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
                default_adjustment_filter=adjustment_filter,
                display_format=item_model.display_format,
                hide_if_all_zero=item_model.hide_if_all_zero,
                css_class=item_model.css_class,
                notes_references=item_model.notes_references,
                units=item_model.units,
                display_scale_factor=item_model.display_scale_factor,
                is_contra=item_model.is_contra,
            )
        if isinstance(item_model, SubtotalModel):
            return self._build_subtotal_model(item_model)

        # Should be unreachable if Pydantic validation works
        raise TypeError(f"Unhandled type: {type(item_model)}")

    def _build_subtotal_model(self, subtotal_model: SubtotalModel) -> SubtotalLineItem:
        """Build a `SubtotalLineItem` object from a `SubtotalModel`.

        Extracts the relevant item IDs to be summed, either from the explicit
        `items_to_sum` list or from the `calculation.inputs` if provided.

        Args:
            subtotal_model: The Pydantic model representing the subtotal configuration.

        Returns:
            A `SubtotalLineItem` instance.
        """
        # Convert adjustment filter
        adjustment_filter = self._convert_adjustment_filter(
            subtotal_model.default_adjustment_filter
        )

        # Consolidate logic for getting item IDs
        item_ids = (
            subtotal_model.calculation.inputs
            if subtotal_model.calculation and subtotal_model.calculation.inputs
            else subtotal_model.items_to_sum
        )
        if not item_ids:
            logger.warning(
                f"Subtotal '{subtotal_model.id}' has no items_to_sum or calculation inputs defined."
            )
            # Decide handling: error or allow empty subtotal?
            # Allowing for now, may need adjustment based on desired behavior.

        return SubtotalLineItem(
            id=subtotal_model.id,
            name=subtotal_model.name,
            item_ids=item_ids or [],  # Ensure it's a list
            description=cast(str, subtotal_model.description),
            sign_convention=subtotal_model.sign_convention,
            metadata=subtotal_model.metadata,
            default_adjustment_filter=adjustment_filter,
            display_format=subtotal_model.display_format,
            hide_if_all_zero=subtotal_model.hide_if_all_zero,
            css_class=subtotal_model.css_class,
            notes_references=subtotal_model.notes_references,
            units=subtotal_model.units,
            display_scale_factor=subtotal_model.display_scale_factor,
            is_contra=subtotal_model.is_contra,
        )



================================================================================
File: fin_statement_model/statements/structure/containers.py
================================================================================

"""Container classes for defining hierarchical financial statement structures.

This module provides Section and StatementStructure, which organize
LineItem and CalculatedLineItem objects into nested groups.
"""

from __future__ import annotations
from typing import Any, Optional, Union, Sequence

from fin_statement_model.core.errors import StatementError
from fin_statement_model.statements.structure.items import (
    StatementItem,
    StatementItemType,
    CalculatedLineItem,
    MetricLineItem,
    SubtotalLineItem,
)

"""Configuration helper will be imported inside methods to avoid circular imports."""

__all__ = ["Section", "StatementStructure"]


class Section:
    """Represents a section in a financial statement.

    Sections group related items and subsections into a hierarchical container
    with enhanced display control and units metadata.
    """

    def __init__(
        self,
        id: str,
        name: str,
        description: str = "",
        metadata: Optional[dict[str, Any]] = None,
        default_adjustment_filter: Optional[Any] = None,
        display_format: Optional[str] = None,
        hide_if_all_zero: bool = False,
        css_class: Optional[str] = None,
        notes_references: Optional[list[str]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
    ):
        """Initialize a section.

        Args:
            id: Unique identifier for the section.
            name: Display name for the section.
            description: Optional description text.
            metadata: Optional additional metadata.
            default_adjustment_filter: Optional default adjustment filter for this section.
            display_format: Optional specific number format string for section items.
            hide_if_all_zero: Whether to hide this section if all values are zero.
            css_class: Optional CSS class name for HTML/web outputs.
            notes_references: List of footnote/note IDs referenced by this section.
            units: Optional unit description for this section.
            display_scale_factor: Factor to scale values for display in this section.
                                If not provided, uses config default from display.scale_factor.

        Raises:
            StatementError: If id or name is invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(f"Invalid section ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(f"Invalid section name: {name} for ID: {id}")

        # Use config default if not provided (import only when needed)
        from fin_statement_model.config.helpers import cfg_or_param

        display_scale_factor = cfg_or_param(
            "display.scale_factor", display_scale_factor
        )

        if display_scale_factor is None or display_scale_factor <= 0:
            raise StatementError(
                f"display_scale_factor must be positive for section: {id}"
            )

        self._id = id
        self._name = name
        self._description = description
        self._metadata = metadata or {}
        self._default_adjustment_filter = default_adjustment_filter
        self._display_format = display_format
        self._hide_if_all_zero = hide_if_all_zero
        self._css_class = css_class
        self._notes_references = notes_references or []
        self._units = units
        self._display_scale_factor = display_scale_factor
        self._items: list[Union[Section, StatementItem]] = []
        # Optional subtotal for this section (may be set by builder)
        self.subtotal: Optional[StatementItem] = None

    @property
    def id(self) -> str:
        """Get the section identifier."""
        return self._id

    @property
    def name(self) -> str:
        """Get the section display name."""
        return self._name

    @property
    def description(self) -> str:
        """Get the section description."""
        return self._description

    @property
    def metadata(self) -> dict[str, Any]:
        """Get the section metadata."""
        return self._metadata

    @property
    def default_adjustment_filter(self) -> Optional[Any]:
        """Get the default adjustment filter for this section."""
        return self._default_adjustment_filter

    @property
    def display_format(self) -> Optional[str]:
        """Get the display format string for this section."""
        return self._display_format

    @property
    def hide_if_all_zero(self) -> bool:
        """Get whether to hide this section if all values are zero."""
        return self._hide_if_all_zero

    @property
    def css_class(self) -> Optional[str]:
        """Get the CSS class for this section."""
        return self._css_class

    @property
    def notes_references(self) -> list[str]:
        """Get the list of note references for this section."""
        return list(self._notes_references)

    @property
    def units(self) -> Optional[str]:
        """Get the unit description for this section."""
        return self._units

    @property
    def display_scale_factor(self) -> float:
        """Get the display scale factor for this section."""
        return self._display_scale_factor

    @property
    def items(self) -> list[Union["Section", StatementItem]]:
        """Get the child items and subsections."""
        return list(self._items)

    @property
    def item_type(self) -> StatementItemType:
        """Get the item type (SECTION)."""
        return StatementItemType.SECTION

    def add_item(self, item: Union["Section", StatementItem]) -> None:
        """Add a child item or subsection to this section.

        Args:
            item: The Section or StatementItem to add.

        Raises:
            StatementError: If an item with the same id already exists.
        """
        if any(existing.id == item.id for existing in self._items):
            raise StatementError(f"Duplicate item ID: {item.id} in section: {self.id}")
        self._items.append(item)

    def find_item_by_id(
        self, item_id: str
    ) -> Optional[Union["Section", StatementItem]]:
        """Recursively find an item by its identifier within this section.

        Args:
            item_id: Identifier of the item to search for.

        Returns:
            The found Section or StatementItem, or None if not found.
        """
        if self.id == item_id:
            return self
        for child in self._items:
            if child.id == item_id:
                return child
            if isinstance(child, Section):
                found = child.find_item_by_id(item_id)
                if found:
                    return found
        if hasattr(self, "subtotal") and self.subtotal and self.subtotal.id == item_id:
            return self.subtotal
        return None


class StatementStructure:
    """Top-level container for a financial statement structure.

    Manages a hierarchy of Section objects with statement-level display
    and units metadata.
    """

    def __init__(
        self,
        id: str,
        name: str,
        description: str = "",
        metadata: Optional[dict[str, Any]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
    ):
        """Initialize a statement structure.

        Args:
            id: Unique identifier for the statement.
            name: Display name for the statement.
            description: Optional description text.
            metadata: Optional additional metadata.
            units: Optional default unit description for the statement.
            display_scale_factor: Default scale factor for displaying values.
                                If not provided, uses config default from display.scale_factor.

        Raises:
            StatementError: If id or name is invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(f"Invalid statement ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(f"Invalid statement name: {name} for ID: {id}")

        # Use config default if not provided (import only when needed)
        from fin_statement_model.config.helpers import cfg_or_param

        display_scale_factor = cfg_or_param(
            "display.scale_factor", display_scale_factor
        )

        if display_scale_factor is None or display_scale_factor <= 0:
            raise StatementError(
                f"display_scale_factor must be positive for statement: {id}"
            )

        self._id = id
        self._name = name
        self._description = description
        self._metadata = metadata or {}
        self._units = units
        self._display_scale_factor = display_scale_factor
        self._sections: list[Section] = []

    @property
    def id(self) -> str:
        """Get the statement identifier."""
        return self._id

    @property
    def name(self) -> str:
        """Get the statement display name."""
        return self._name

    @property
    def description(self) -> str:
        """Get the statement description."""
        return self._description

    @property
    def metadata(self) -> dict[str, Any]:
        """Get the statement metadata."""
        return self._metadata

    @property
    def units(self) -> Optional[str]:
        """Get the default unit description for the statement."""
        return self._units

    @property
    def display_scale_factor(self) -> float:
        """Get the default display scale factor for the statement."""
        return self._display_scale_factor

    @property
    def sections(self) -> list[Section]:
        """Get the top-level sections."""
        return list(self._sections)

    @property
    def items(self) -> list[Section]:
        """Alias for sections to ease iteration."""
        return self.sections

    def add_section(self, section: Section) -> None:
        """Add a section to the statement.

        Args:
            section: Section to add.

        Raises:
            StatementError: If a section with the same id already exists.
        """
        if any(s.id == section.id for s in self._sections):
            raise StatementError(
                f"Duplicate section ID: {section.id} in statement: {self.id}"
            )
        self._sections.append(section)

    def find_item_by_id(self, item_id: str) -> Optional[Union[Section, StatementItem]]:
        """Find an item by its ID in the statement structure.

        Args:
            item_id: The ID of the item to find.

        Returns:
            Optional[Union[Section, StatementItem]]: The found item or None if not found.
        """
        for section in self._sections:
            item = section.find_item_by_id(item_id)
            if item:
                return item
        return None

    def get_calculation_items(
        self,
    ) -> list[Union[CalculatedLineItem, SubtotalLineItem]]:
        """Get all calculation items from the statement structure.

        Returns:
            List[Union[CalculatedLineItem, SubtotalLineItem]]: List of calculation items.
        """
        calculation_items = []

        def collect_calculation_items(
            items: Sequence[Union[Section, StatementItem]],
        ) -> None:
            for item in items:
                if isinstance(item, CalculatedLineItem | SubtotalLineItem):
                    calculation_items.append(item)
                elif isinstance(item, Section):
                    collect_calculation_items(item.items)
                    if hasattr(item, "subtotal") and item.subtotal:
                        if isinstance(item.subtotal, SubtotalLineItem):
                            calculation_items.append(item.subtotal)
                        else:
                            pass

        collect_calculation_items(self._sections)
        return calculation_items

    def get_metric_items(self) -> list[MetricLineItem]:
        """Get all metric items from the statement structure.

        Returns:
            List[MetricLineItem]: List of metric items.
        """
        metric_items = []

        def collect_metric_items(
            items: Sequence[Union[Section, StatementItem]],
        ) -> None:
            for item in items:
                if isinstance(item, MetricLineItem):
                    metric_items.append(item)
                elif isinstance(item, Section):
                    collect_metric_items(item.items)
                    # Subtotals are handled by get_calculation_items, not relevant here

        collect_metric_items(self._sections)
        return metric_items

    def get_all_items(self) -> list[StatementItem]:
        """Get all StatementItem instances recursively from the structure.

        Traverses all sections and nested sections, collecting only objects that
        are subclasses of StatementItem (e.g., LineItem, CalculatedLineItem),
        excluding Section objects themselves.

        Returns:
            List[StatementItem]: A flat list of all statement items found.
        """
        all_statement_items: list[StatementItem] = []

        def _collect_items_recursive(
            items_or_sections: Sequence[Union[Section, StatementItem]],
        ) -> None:
            for item in items_or_sections:
                if isinstance(item, Section):
                    _collect_items_recursive(item.items)
                    # Also collect the section's subtotal if it exists and is a StatementItem
                    if hasattr(item, "subtotal") and isinstance(
                        item.subtotal, StatementItem
                    ):
                        all_statement_items.append(item.subtotal)
                elif isinstance(item, StatementItem):
                    all_statement_items.append(item)

        _collect_items_recursive(self._sections)

        return all_statement_items



================================================================================
File: fin_statement_model/statements/structure/items.py
================================================================================

"""Statement structure items module defining line items, calculated items, and subtotals."""

from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Optional, cast

from fin_statement_model.core.errors import StatementError
from fin_statement_model.core.nodes.standard_registry import StandardNodeRegistry

__all__ = [
    "CalculatedLineItem",
    "LineItem",
    "MetricLineItem",
    "StatementItem",
    "StatementItemType",
    "SubtotalLineItem",
]


class StatementItemType(Enum):
    """Types of statement structure items.

    Attributes:
      SECTION: Section container
      LINE_ITEM: Basic financial line item
      SUBTOTAL: Subtotal of multiple items
      CALCULATED: Derived calculation item
      METRIC: Derived metric item from registry
    """

    SECTION = "section"
    LINE_ITEM = "line_item"
    SUBTOTAL = "subtotal"
    CALCULATED = "calculated"
    METRIC = "metric"


class StatementItem(ABC):
    """Abstract base class for all statement structure items.

    Defines a common interface: id, name, item_type, default_adjustment_filter,
    and enhanced display control and units metadata.
    """

    @property
    @abstractmethod
    def id(self) -> str:
        """Get the unique identifier of the item."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Get the display name of the item."""

    @property
    @abstractmethod
    def item_type(self) -> StatementItemType:
        """Get the type of this statement item."""

    @property
    @abstractmethod
    def default_adjustment_filter(self) -> Optional[Any]:
        """Get the default adjustment filter for this item."""

    @property
    @abstractmethod
    def display_format(self) -> Optional[str]:
        """Get the display format string for this item."""

    @property
    @abstractmethod
    def hide_if_all_zero(self) -> bool:
        """Get whether to hide this item if all values are zero."""

    @property
    @abstractmethod
    def css_class(self) -> Optional[str]:
        """Get the CSS class for this item."""

    @property
    @abstractmethod
    def notes_references(self) -> list[str]:
        """Get the list of note references for this item."""

    @property
    @abstractmethod
    def units(self) -> Optional[str]:
        """Get the unit description for this item."""

    @property
    @abstractmethod
    def display_scale_factor(self) -> float:
        """Get the display scale factor for this item."""

    @property
    @abstractmethod
    def is_contra(self) -> bool:
        """Get whether this is a contra item for special display formatting."""


class LineItem(StatementItem):
    """Represents a basic line item in a financial statement.

    Args:
      id: Unique ID for the line item
      name: Display name for the line item
      node_id: ID of the core graph node that holds values (optional if standard_node_ref provided)
      standard_node_ref: Reference to a standard node name from the registry (optional if node_id provided)
      description: Optional explanatory text
      sign_convention: 1 for normal values, -1 for inverted
      metadata: Optional additional attributes
      default_adjustment_filter: Optional default adjustment filter for this item
      display_format: Optional specific number format string
      hide_if_all_zero: Whether to hide this item if all values are zero
      css_class: Optional CSS class name for HTML/web outputs
      notes_references: List of footnote/note IDs referenced by this item
      units: Optional unit description
      display_scale_factor: Factor to scale values for display
      is_contra: Whether this is a contra item for special display formatting

    Raises:
      StatementError: If inputs are invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        node_id: Optional[str] = None,
        standard_node_ref: Optional[str] = None,
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
        default_adjustment_filter: Optional[Any] = None,
        display_format: Optional[str] = None,
        hide_if_all_zero: bool = False,
        css_class: Optional[str] = None,
        notes_references: Optional[list[str]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
        is_contra: bool = False,
    ):
        """Initialize a basic LineItem.

        Args:
            id: Unique ID for the line item.
            name: Display name for the line item.
            node_id: ID of the core graph node holding values (optional if standard_node_ref provided).
            standard_node_ref: Reference to a standard node name (optional if node_id provided).
            description: Optional explanatory text.
            sign_convention: Sign convention (1 for positive, -1 for negative).
            metadata: Optional additional attributes.
            default_adjustment_filter: Optional default adjustment filter for this item.
            display_format: Optional specific number format string (e.g., ",.2f").
            hide_if_all_zero: Whether to hide this item if all values are zero.
            css_class: Optional CSS class name for HTML/web outputs.
            notes_references: List of footnote/note IDs referenced by this item.
            units: Optional unit description (e.g., "USD Thousands").
            display_scale_factor: Factor to scale values for display (e.g., 0.001 for thousands).
                                If not provided, uses config default from display.scale_factor.
            is_contra: Whether this is a contra item for special display formatting.

        Raises:
            StatementError: If inputs are invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(f"Invalid line item ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(f"Invalid line item name: {name} for ID: {id}")

        # Validate that exactly one of node_id or standard_node_ref is provided
        if not node_id and not standard_node_ref:
            raise StatementError(
                f"Must provide either 'node_id' or 'standard_node_ref' for line item: {id}"
            )
        if node_id and standard_node_ref:
            raise StatementError(
                f"Cannot provide both 'node_id' and 'standard_node_ref' for line item: {id}"
            )

        if sign_convention not in (1, -1):
            raise StatementError(
                f"Invalid sign convention {sign_convention} for item: {id}"
            )

        # Use config default if not provided (import only when needed)
        from fin_statement_model.config.helpers import cfg_or_param

        display_scale_factor = cfg_or_param(
            "display.scale_factor", display_scale_factor
        )

        if display_scale_factor is None or display_scale_factor <= 0:
            raise StatementError(
                f"display_scale_factor must be positive for item: {id}"
            )

        self._id = id
        self._name = name
        self._node_id = node_id
        self._standard_node_ref = standard_node_ref
        self._description = description
        self._sign_convention = sign_convention
        self._metadata = metadata or {}
        self._default_adjustment_filter = default_adjustment_filter
        self._display_format = display_format
        self._hide_if_all_zero = hide_if_all_zero
        self._css_class = css_class
        self._notes_references = notes_references or []
        self._units = units
        self._display_scale_factor = display_scale_factor
        self._is_contra = is_contra

    @property
    def id(self) -> str:
        """Get the unique identifier of the line item."""
        return self._id

    @property
    def name(self) -> str:
        """Get the display name of the line item."""
        return self._name

    @property
    def node_id(self) -> Optional[str]:
        """Get the core graph node ID for this line item (if provided directly)."""
        return self._node_id

    @property
    def standard_node_ref(self) -> Optional[str]:
        """Get the standard node reference for this line item (if provided)."""
        return self._standard_node_ref

    @property
    def description(self) -> str:
        """Get the description for this line item."""
        return self._description

    @property
    def sign_convention(self) -> int:
        """Get the sign convention (1 or -1)."""
        return self._sign_convention

    @property
    def metadata(self) -> dict[str, Any]:
        """Get custom metadata associated with this item."""
        return self._metadata

    @property
    def default_adjustment_filter(self) -> Optional[Any]:
        """Get the default adjustment filter for this item."""
        return self._default_adjustment_filter

    @property
    def display_format(self) -> Optional[str]:
        """Get the display format string for this item."""
        return self._display_format

    @property
    def hide_if_all_zero(self) -> bool:
        """Get whether to hide this item if all values are zero."""
        return self._hide_if_all_zero

    @property
    def css_class(self) -> Optional[str]:
        """Get the CSS class for this item."""
        return self._css_class

    @property
    def notes_references(self) -> list[str]:
        """Get the list of note references for this item."""
        return list(self._notes_references)

    @property
    def units(self) -> Optional[str]:
        """Get the unit description for this item."""
        return self._units

    @property
    def display_scale_factor(self) -> float:
        """Get the display scale factor for this item."""
        return self._display_scale_factor

    @property
    def is_contra(self) -> bool:
        """Get whether this is a contra item for special display formatting."""
        return self._is_contra

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (LINE_ITEM)."""
        return StatementItemType.LINE_ITEM

    def get_resolved_node_id(self, registry: StandardNodeRegistry) -> Optional[str]:
        """Get the resolved node ID, handling both direct node_id and standard_node_ref.

        Args:
            registry: Standard node registry for resolving references.

        Returns:
            The resolved node ID, or None if no node ID could be resolved.
        """
        if self._node_id:
            return self._node_id

        if self._standard_node_ref:
            # Try to get the standard name (handles alternate names too)
            return registry.get_standard_name(self._standard_node_ref)

        return None


class MetricLineItem(LineItem):
    """Represents a line item whose calculation is defined by a core metric.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      metric_id: ID of the metric in the core.metrics.registry
      inputs: Dict mapping metric input names to statement item IDs
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata
      default_adjustment_filter: Optional default adjustment filter for this item
      display_format: Optional specific number format string
      hide_if_all_zero: Whether to hide this item if all values are zero
      css_class: Optional CSS class name for HTML/web outputs
      notes_references: List of footnote/note IDs referenced by this item
      units: Optional unit description
      display_scale_factor: Factor to scale values for display
      is_contra: Whether this is a contra item for special display formatting

    Raises:
      StatementError: If metric_id or inputs are invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        metric_id: str,
        inputs: dict[str, str],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
        default_adjustment_filter: Optional[Any] = None,
        display_format: Optional[str] = None,
        hide_if_all_zero: bool = False,
        css_class: Optional[str] = None,
        notes_references: Optional[list[str]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
        is_contra: bool = False,
    ):
        """Initialize a MetricLineItem referencing a registered metric.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            metric_id: ID of the metric in the core.metrics.registry.
            inputs: Dict mapping metric input names to statement item IDs.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.
            default_adjustment_filter: Optional default adjustment filter for this item.
            display_format: Optional specific number format string.
            hide_if_all_zero: Whether to hide this item if all values are zero.
            css_class: Optional CSS class name for HTML/web outputs.
            notes_references: List of footnote/note IDs referenced by this item.
            units: Optional unit description.
            display_scale_factor: Factor to scale values for display.
                                If not provided, uses config default from display.scale_factor.
            is_contra: Whether this is a contra item for special display formatting.

        Raises:
            StatementError: If metric_id or inputs are invalid.
        """
        super().__init__(
            id=id,
            name=name,
            node_id=id,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
            default_adjustment_filter=default_adjustment_filter,
            display_format=display_format,
            hide_if_all_zero=hide_if_all_zero,
            css_class=css_class,
            notes_references=notes_references,
            units=units,
            display_scale_factor=display_scale_factor,
            is_contra=is_contra,
        )
        if not metric_id or not isinstance(metric_id, str):
            raise StatementError(f"Invalid metric_id '{metric_id}' for item: {id}")
        if not isinstance(inputs, dict) or not inputs:
            raise StatementError(
                f"Metric inputs must be a non-empty dictionary for item: {id}"
            )
        if not all(
            isinstance(k, str) and isinstance(v, str) for k, v in inputs.items()
        ):
            raise StatementError(
                f"Metric input keys and values must be strings for item: {id}"
            )

        self._metric_id = metric_id
        self._inputs = inputs

    @property
    def metric_id(self) -> str:
        """Get the ID of the metric referenced from the core registry."""
        return self._metric_id

    @property
    def inputs(self) -> dict[str, str]:
        """Get the mapping from metric input names to statement item IDs."""
        return self._inputs

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (METRIC)."""
        return StatementItemType.METRIC


class CalculatedLineItem(LineItem):
    """Represents a calculated line item whose values come from graph calculations.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      calculation: Dict with 'type', 'inputs', optional 'parameters'
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata
      default_adjustment_filter: Optional default adjustment filter for this item
      display_format: Optional specific number format string
      hide_if_all_zero: Whether to hide this item if all values are zero
      css_class: Optional CSS class name for HTML/web outputs
      notes_references: List of footnote/note IDs referenced by this item
      units: Optional unit description
      display_scale_factor: Factor to scale values for display
      is_contra: Whether this is a contra item for special display formatting

    Raises:
      StatementError: If calculation dictionary is invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        calculation: dict[str, Any],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
        default_adjustment_filter: Optional[Any] = None,
        display_format: Optional[str] = None,
        hide_if_all_zero: bool = False,
        css_class: Optional[str] = None,
        notes_references: Optional[list[str]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
        is_contra: bool = False,
    ):
        """Initialize a CalculatedLineItem based on calculation specification.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            calculation: Calculation spec dict with 'type', 'inputs', optional 'parameters'.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.
            default_adjustment_filter: Optional default adjustment filter for this item.
            display_format: Optional specific number format string.
            hide_if_all_zero: Whether to hide this item if all values are zero.
            css_class: Optional CSS class name for HTML/web outputs.
            notes_references: List of footnote/note IDs referenced by this item.
            units: Optional unit description.
            display_scale_factor: Factor to scale values for display.
                                If not provided, uses config default from display.scale_factor.
            is_contra: Whether this is a contra item for special display formatting.

        Raises:
            StatementError: If calculation dictionary is invalid.
        """
        super().__init__(
            id=id,
            name=name,
            node_id=id,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
            default_adjustment_filter=default_adjustment_filter,
            display_format=display_format,
            hide_if_all_zero=hide_if_all_zero,
            css_class=css_class,
            notes_references=notes_references,
            units=units,
            display_scale_factor=display_scale_factor,
            is_contra=is_contra,
        )
        if not isinstance(calculation, dict):
            raise StatementError(f"Invalid calculation spec for item: {id}")
        if "type" not in calculation:
            raise StatementError(f"Missing calculation type for item: {id}")
        inputs = calculation.get("inputs")
        if not isinstance(inputs, list) or not inputs:
            raise StatementError(
                f"Calculation inputs must be a non-empty list for item: {id}"
            )
        self._calculation = calculation

    @property
    def calculation_type(self) -> str:
        """Get the calculation operation type (e.g., 'addition')."""
        return cast(str, self._calculation["type"])

    @property
    def input_ids(self) -> list[str]:
        """Get the list of input item IDs for this calculation."""
        return cast(list[str], self._calculation["inputs"])

    @property
    def parameters(self) -> dict[str, Any]:
        """Get optional parameters for the calculation."""
        return cast(dict[str, Any], self._calculation.get("parameters", {}))

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (CALCULATED)."""
        return StatementItemType.CALCULATED


class SubtotalLineItem(CalculatedLineItem):
    """Represents a subtotal line item summing multiple other items.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      item_ids: List of IDs to sum
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata
      default_adjustment_filter: Optional default adjustment filter for this item
      display_format: Optional specific number format string
      hide_if_all_zero: Whether to hide this item if all values are zero
      css_class: Optional CSS class name for HTML/web outputs
      notes_references: List of footnote/note IDs referenced by this item
      units: Optional unit description
      display_scale_factor: Factor to scale values for display

    Raises:
      StatementError: If item_ids is empty or not a list
    """

    def __init__(
        self,
        id: str,
        name: str,
        item_ids: list[str],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
        default_adjustment_filter: Optional[Any] = None,
        display_format: Optional[str] = None,
        hide_if_all_zero: bool = False,
        css_class: Optional[str] = None,
        notes_references: Optional[list[str]] = None,
        units: Optional[str] = None,
        display_scale_factor: Optional[float] = None,
        is_contra: bool = False,
    ):
        """Initialize a SubtotalLineItem summing multiple items.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            item_ids: List of IDs to sum.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.
            default_adjustment_filter: Optional default adjustment filter for this item.
            display_format: Optional specific number format string.
            hide_if_all_zero: Whether to hide this item if all values are zero.
            css_class: Optional CSS class name for HTML/web outputs.
            notes_references: List of footnote/note IDs referenced by this item.
            units: Optional unit description.
            display_scale_factor: Factor to scale values for display.
                                If not provided, uses config default from display.scale_factor.
            is_contra: Whether this is a contra item for special display formatting.

        Raises:
            StatementError: If item_ids is empty or not a list.
        """
        if not isinstance(item_ids, list) or not item_ids:
            raise StatementError(f"Invalid or empty item IDs for subtotal: {id}")
        calculation = {"type": "addition", "inputs": item_ids, "parameters": {}}
        super().__init__(
            id=id,
            name=name,
            calculation=calculation,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
            default_adjustment_filter=default_adjustment_filter,
            display_format=display_format,
            hide_if_all_zero=hide_if_all_zero,
            css_class=css_class,
            notes_references=notes_references,
            units=units,
            display_scale_factor=display_scale_factor,
            is_contra=is_contra,
        )
        self._item_ids = item_ids

    @property
    def item_ids(self) -> list[str]:
        """Get the IDs of items summed by this subtotal."""
        return self._item_ids

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (SUBTOTAL)."""
        return StatementItemType.SUBTOTAL



================================================================================
File: fin_statement_model/statements/utilities/__init__.py
================================================================================

"""Cross-cutting utilities for the statements package.

This package provides reusable components:
- Result types for functional error handling
- Retry mechanisms for transient failures
- Common error codes and handling patterns
"""

from .result_types import (
    ErrorCollector,
    ErrorDetail,
    ErrorSeverity,
    Failure,
    OperationResult,
    ProcessingResult,
    Result,
    Success,
    ValidationResult,
    combine_results,
)
from .retry_handler import (
    BackoffStrategy,
    ConstantBackoff,
    ExponentialBackoff,
    LinearBackoff,
    RetryConfig,
    RetryHandler,
    RetryResult,
    RetryStrategy,
    retry_on_specific_errors,
    retry_with_exponential_backoff,
)

__all__ = [
    "BackoffStrategy",
    "ConstantBackoff",
    "ErrorCollector",
    "ErrorDetail",
    "ErrorSeverity",
    "ExponentialBackoff",
    "Failure",
    "LinearBackoff",
    "OperationResult",
    "ProcessingResult",
    # Result Types
    "Result",
    "RetryConfig",
    # Retry Handler
    "RetryHandler",
    "RetryResult",
    "RetryStrategy",
    "Success",
    "ValidationResult",
    "combine_results",
    "retry_on_specific_errors",
    "retry_with_exponential_backoff",
]



================================================================================
File: fin_statement_model/statements/utilities/cli_formatters.py
================================================================================

from fin_statement_model.statements.utilities.result_types import ErrorDetail


def pretty_print_errors(errors: list[ErrorDetail]) -> None:
    """
    Pretty-print a list of ErrorDetail objects as a table to the console.

    Args:
        errors: List of ErrorDetail instances to display.
    """
    if not errors:
        return

    # Prepare table headers
    headers = ["SEVERITY", "CODE", "CONTEXT", "SOURCE", "MESSAGE"]
    rows: list[list[str]] = []
    for err in errors:
        rows.append(
            [
                err.severity.value.upper(),
                err.code,
                err.context or "",
                err.source or "",
                err.message,
            ]
        )

    # Calculate column widths
    col_widths = [max(len(str(val)) for val in col) for col in zip(headers, *rows)]

    # Build header row and separator
    header_row = " | ".join(
        headers[i].ljust(col_widths[i]) for i in range(len(headers))
    )
    separator = "-+-".join("".ljust(col_widths[i], "-") for i in range(len(headers)))

    # Print table
    print(header_row)
    print(separator)
    for row in rows:
        line = " | ".join(str(row[i]).ljust(col_widths[i]) for i in range(len(headers)))
        print(line)



================================================================================
File: fin_statement_model/statements/utilities/result_types.py
================================================================================

"""Common result types for standardized error handling in the statements module.

This module provides consistent result types and error collection utilities
to standardize error handling across the statements package. These types
enable functional error handling without exceptions for operations that
can fail in expected ways.
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Generic, Optional, TypeVar, Any, cast

logger = logging.getLogger(__name__)

__all__ = [
    "ErrorCollector",
    "ErrorDetail",
    "ErrorSeverity",
    "Failure",
    "OperationResult",
    "ProcessingResult",
    "Result",
    "Success",
    "ValidationResult",
]

T = TypeVar("T")


class ErrorSeverity(Enum):
    """Severity levels for errors."""

    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass(frozen=True)
class ErrorDetail:
    """Detailed information about an error.

    Attributes:
        code: Error code for programmatic handling
        message: Human-readable error message
        context: Optional context about where/what caused the error
        severity: Severity level of the error
        source: Optional source identifier (e.g., item ID, file path)
    """

    code: str
    message: str
    context: Optional[str] = None
    severity: ErrorSeverity = ErrorSeverity.ERROR
    source: Optional[str] = None

    def __str__(self) -> str:
        """Format error as string."""
        parts = [f"[{self.severity.value.upper()}]"]
        if self.source:
            parts.append(f"{self.source}:")
        parts.append(self.message)
        if self.context:
            parts.append(f"({self.context})")
        return " ".join(parts)


class Result(ABC, Generic[T]):
    """Abstract base class for operation results.

    Provides a functional approach to error handling, allowing
    operations to return either success or failure without exceptions.
    """

    @abstractmethod
    def is_success(self) -> bool:
        """Check if the result represents success."""

    @abstractmethod
    def is_failure(self) -> bool:
        """Check if the result represents failure."""

    @abstractmethod
    def get_value(self) -> Optional[T]:
        """Get the success value if available."""

    @abstractmethod
    def get_errors(self) -> list[ErrorDetail]:
        """Get error details if this is a failure."""

    def unwrap(self) -> T:
        """Get the value or raise an exception if failed.

        Raises:
            ValueError: If the result is a failure.
        """
        if self.is_failure():
            errors_str = "\n".join(str(e) for e in self.get_errors())
            raise ValueError(f"Cannot unwrap failed result:\n{errors_str}")
        return cast(T, self.get_value())

    def unwrap_or(self, default: T) -> T:
        """Get the value or return a default if failed."""
        return cast(T, self.get_value()) if self.is_success() else default


@dataclass(frozen=True)
class Success(Result[T]):
    """Represents a successful operation result."""

    value: T

    def is_success(self) -> bool:
        """Always returns True for Success."""
        return True

    def is_failure(self) -> bool:
        """Always returns False for Success."""
        return False

    def get_value(self) -> Optional[T]:
        """Return the success value."""
        return self.value

    def get_errors(self) -> list[ErrorDetail]:
        """Return empty list for Success."""
        return []


@dataclass(frozen=True)
class Failure(Result[T]):
    """Represents a failed operation result."""

    errors: list[ErrorDetail] = field(default_factory=list)

    def __post_init__(self) -> None:
        """Ensure at least one error is present."""
        if not self.errors:
            # Add a default error if none provided
            object.__setattr__(
                self,
                "errors",
                [
                    ErrorDetail(
                        code="unknown",
                        message="Operation failed with no specific error",
                    )
                ],
            )

    def is_success(self) -> bool:
        """Always returns False for Failure."""
        return False

    def is_failure(self) -> bool:
        """Always returns True for Failure."""
        return True

    def get_value(self) -> Optional[T]:
        """Always returns None for Failure."""
        return None

    def get_errors(self) -> list[ErrorDetail]:
        """Return the error details."""
        return self.errors

    @classmethod
    def from_exception(cls, exc: Exception, code: str = "exception") -> "Failure[T]":
        """Create a Failure from an exception."""
        return cls(
            errors=[
                ErrorDetail(
                    code=code,
                    message=str(exc),
                    context=type(exc).__name__,
                    severity=ErrorSeverity.ERROR,
                )
            ]
        )


class ErrorCollector:
    """Collects errors during multi-step operations.

    Useful for operations that should continue collecting errors
    rather than failing fast on the first error.
    """

    def __init__(self) -> None:
        """Initialize an empty error collector."""
        self._errors: list[ErrorDetail] = []
        self._warnings: list[ErrorDetail] = []

    def add_error(
        self,
        code: str,
        message: str,
        context: Optional[str] = None,
        source: Optional[str] = None,
    ) -> None:
        """Add an error to the collector."""
        self._errors.append(
            ErrorDetail(
                code=code,
                message=message,
                context=context,
                severity=ErrorSeverity.ERROR,
                source=source,
            )
        )

    def add_warning(
        self,
        code: str,
        message: str,
        context: Optional[str] = None,
        source: Optional[str] = None,
    ) -> None:
        """Add a warning to the collector."""
        self._warnings.append(
            ErrorDetail(
                code=code,
                message=message,
                context=context,
                severity=ErrorSeverity.WARNING,
                source=source,
            )
        )

    def add_from_result(
        self, result: Result[Any], source: Optional[str] = None
    ) -> None:
        """Add errors from a Result object."""
        if result.is_failure():
            for error in result.get_errors():
                # Override source if provided
                if source:
                    new_error = ErrorDetail(
                        code=error.code,
                        message=error.message,
                        context=error.context,
                        severity=error.severity,
                        source=source,
                    )
                    if new_error.severity == ErrorSeverity.WARNING:
                        self._warnings.append(new_error)
                    else:
                        self._errors.append(new_error)
                elif error.severity == ErrorSeverity.WARNING:
                    self._warnings.append(error)
                else:
                    self._errors.append(error)

    def has_errors(self) -> bool:
        """Check if any errors have been collected."""
        return len(self._errors) > 0

    def has_warnings(self) -> bool:
        """Check if any warnings have been collected."""
        return len(self._warnings) > 0

    def get_errors(self) -> list[ErrorDetail]:
        """Get all collected errors (not warnings)."""
        return list(self._errors)

    def get_warnings(self) -> list[ErrorDetail]:
        """Get all collected warnings."""
        return list(self._warnings)

    def get_all(self) -> list[ErrorDetail]:
        """Get all collected errors and warnings."""
        return self._errors + self._warnings

    def to_result(self, value: Optional[T] = None) -> Result[T]:
        """Convert collector state to a Result.

        If there are errors, returns Failure.
        Otherwise returns Success with the provided value.
        """
        if self.has_errors():
            return Failure(errors=self._errors)
        # Cast value to T for Success
        return Success(value=cast(T, value))

    def log_all(self, prefix: str = "") -> None:
        """Log all collected errors and warnings."""
        for warning in self._warnings:
            logger.warning(f"{prefix}{warning}")
        for error in self._errors:
            logger.error(f"{prefix}{error}")


# Type aliases for common result types
OperationResult = Result[
    bool
]  # For operations that succeed/fail without returning data
ValidationResult = Result[bool]  # For validation operations
ProcessingResult = Result[dict[str, Any]]  # For processing operations that return data


def combine_results(*results: Result[T]) -> Result[list[T]]:
    """Combine multiple results into a single result.

    If all results are successful, returns Success with list of values.
    If any result is a failure, returns Failure with all errors combined.
    """
    collector = ErrorCollector()
    values = []

    for result in results:
        if result.is_success():
            values.append(cast(T, result.get_value()))
        else:
            for error in result.get_errors():
                if error.severity == ErrorSeverity.WARNING:
                    collector.add_warning(
                        error.code, error.message, error.context, error.source
                    )
                else:
                    collector.add_error(
                        error.code, error.message, error.context, error.source
                    )

    if collector.has_errors():
        return Failure(errors=collector.get_all())
    return Success(value=values)



================================================================================
File: fin_statement_model/statements/utilities/retry_handler.py
================================================================================

"""Retry handler for managing transient failures in statement operations.

This module provides a flexible retry mechanism that can be used throughout
the statements package to handle transient failures gracefully. It supports
configurable retry strategies, backoff algorithms, and error classification.
"""

import logging
import random
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Generic, Optional, TypeVar, cast
from collections.abc import Callable

from fin_statement_model.statements.utilities.result_types import (
    Result,
    Failure,
    ErrorDetail,
    ErrorCollector,
)

logger = logging.getLogger(__name__)

__all__ = [
    "BackoffStrategy",
    "ConstantBackoff",
    "ExponentialBackoff",
    "LinearBackoff",
    "RetryConfig",
    "RetryHandler",
    "RetryResult",
    "RetryStrategy",
    "is_retryable_error",
    "retry_on_specific_errors",
    "retry_with_exponential_backoff",
]

T = TypeVar("T")


class RetryStrategy(Enum):
    """Strategy for determining when to retry."""

    IMMEDIATE = "immediate"  # Retry immediately on failure
    BACKOFF = "backoff"  # Use backoff strategy between retries
    CONDITIONAL = "conditional"  # Retry only for specific error types


@dataclass
class RetryConfig:
    """Configuration for retry behavior.

    Attributes:
        max_attempts: Maximum number of attempts (including initial).
                     If not provided, uses config default from api.api_retry_count
        strategy: Retry strategy to use
        backoff: Optional backoff strategy for delays
        retryable_errors: Set of error codes that are retryable
        log_retries: Whether to log retry attempts
        collect_all_errors: Whether to collect errors from all attempts
    """

    max_attempts: Optional[int] = None
    strategy: RetryStrategy = RetryStrategy.BACKOFF
    backoff: Optional["BackoffStrategy"] = None
    retryable_errors: Optional[set[str]] = None
    log_retries: bool = True
    collect_all_errors: bool = False

    def __post_init__(self) -> None:
        """Validate configuration and set defaults."""
        # Use config default if not provided
        if self.max_attempts is None:
            from fin_statement_model.config.helpers import cfg_or_param

            self.max_attempts = cfg_or_param("api.api_retry_count", None)

        if self.max_attempts < 1:
            raise ValueError("max_attempts must be at least 1")

        if self.strategy == RetryStrategy.BACKOFF and not self.backoff:
            # Default to exponential backoff
            self.backoff = ExponentialBackoff()

        if self.retryable_errors is None:
            # Default retryable errors
            self.retryable_errors = {
                "timeout",
                "connection_error",
                "rate_limit",
                "temporary_failure",
                "calculation_error",  # For graph calculations
                "node_not_ready",  # For dependency issues
            }


class BackoffStrategy(ABC):
    """Abstract base class for backoff strategies."""

    @abstractmethod
    def get_delay(self, attempt: int) -> float:
        """Get delay in seconds for the given attempt number.

        Args:
            attempt: The attempt number (1-based)

        Returns:
            Delay in seconds before the next retry
        """


class ExponentialBackoff(BackoffStrategy):
    """Exponential backoff strategy with optional jitter."""

    def __init__(
        self,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        multiplier: float = 2.0,
        jitter: bool = True,
    ):
        """Initialize exponential backoff.

        Args:
            base_delay: Initial delay in seconds
            max_delay: Maximum delay in seconds
            multiplier: Multiplier for each retry
            jitter: Whether to add random jitter
        """
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.multiplier = multiplier
        self.jitter = jitter

    def get_delay(self, attempt: int) -> float:
        """Calculate exponential backoff delay."""
        delay = min(
            self.base_delay * (self.multiplier ** (attempt - 1)), self.max_delay
        )

        if self.jitter:
            # Add up to 20% jitter
            jitter_amount = delay * 0.2 * random.random()  # noqa: S311
            delay += jitter_amount

        return delay


class LinearBackoff(BackoffStrategy):
    """Linear backoff strategy."""

    def __init__(self, base_delay: float = 1.0, increment: float = 1.0):
        """Initialize linear backoff.

        Args:
            base_delay: Initial delay in seconds
            increment: Increment for each retry
        """
        self.base_delay = base_delay
        self.increment = increment

    def get_delay(self, attempt: int) -> float:
        """Calculate linear backoff delay."""
        return self.base_delay + (attempt - 1) * self.increment


class ConstantBackoff(BackoffStrategy):
    """Constant delay backoff strategy."""

    def __init__(self, delay: float = 1.0):
        """Initialize constant backoff.

        Args:
            delay: Constant delay in seconds
        """
        self.delay = delay

    def get_delay(self, attempt: int) -> float:
        """Return constant delay."""
        return self.delay


@dataclass
class RetryResult(Generic[T]):
    """Result of a retry operation.

    Attributes:
        result: The final result (success or failure)
        attempts: Number of attempts made
        total_delay: Total delay time in seconds
        all_errors: All errors collected if configured
    """

    result: Result[T]
    attempts: int
    total_delay: float
    all_errors: Optional[list[ErrorDetail]] = None

    @property
    def success(self) -> bool:
        """Check if the operation eventually succeeded."""
        return self.result.is_success()

    def unwrap(self) -> T:
        """Get the value or raise an exception."""
        return self.result.unwrap()

    def unwrap_or(self, default: T) -> T:
        """Get the value or return default."""
        return self.result.unwrap_or(default)


def is_retryable_error(error: ErrorDetail, retryable_codes: set[str]) -> bool:
    """Check if an error is retryable based on its code.

    Args:
        error: The error to check
        retryable_codes: Set of error codes that are retryable

    Returns:
        True if the error should be retried
    """
    return error.code in retryable_codes


class RetryHandler:
    """Handles retry logic for operations that may fail transiently."""

    def __init__(self, config: Optional[RetryConfig] = None):
        """Initialize retry handler.

        Args:
            config: Retry configuration, uses defaults if not provided
        """
        self.config = config or RetryConfig()

    def retry(
        self,
        operation: Callable[[], Result[T]],
        operation_name: Optional[str] = None,
    ) -> RetryResult[T]:
        """Execute an operation with retry logic.

        Args:
            operation: Callable that returns a Result
            operation_name: Optional name for logging

        Returns:
            RetryResult containing the final result and metadata
        """
        error_collector = ErrorCollector() if self.config.collect_all_errors else None
        total_delay = 0.0
        # Ensure max_attempts is int
        max_attempts = cast(int, self.config.max_attempts)
        op_name = operation_name or operation.__name__

        for attempt in range(1, max_attempts + 1):
            if attempt > 1 and self.config.log_retries:
                logger.info(f"Retrying {op_name} (attempt {attempt}/{max_attempts})")

            # Execute the operation
            try:
                result = operation()
            except Exception as e:
                # Convert exception to Result
                result = Failure.from_exception(e)

            # Check if successful
            if result.is_success():
                return RetryResult(
                    result=result,
                    attempts=attempt,
                    total_delay=total_delay,
                    all_errors=error_collector.get_all() if error_collector else None,
                )

            # Handle failure
            errors = result.get_errors()

            # Collect errors if configured
            if error_collector:
                for error in errors:
                    error_collector.add_error(
                        code=error.code,
                        message=f"Attempt {attempt}: {error.message}",
                        context=error.context,
                        source=error.source,
                    )

            # Check if we should retry
            if attempt >= max_attempts:
                # No more retries
                if self.config.log_retries:
                    logger.warning(
                        f"{op_name} failed after {attempt} attempts. "
                        f"Final error: {errors[0].message if errors else 'Unknown'}"
                    )
                break

            # Check if errors are retryable
            if self.config.strategy == RetryStrategy.CONDITIONAL:
                # Safe cast retryable_errors to non-nullable set
                retryable_errors = cast(set[str], self.config.retryable_errors)
                retryable = any(
                    is_retryable_error(error, retryable_errors) for error in errors
                )
                if not retryable:
                    if self.config.log_retries:
                        logger.debug(
                            f"{op_name} failed with non-retryable error: "
                            f"{errors[0].code if errors else 'Unknown'}"
                        )
                    break

            # Calculate delay
            if self.config.strategy == RetryStrategy.BACKOFF and self.config.backoff:
                delay = self.config.backoff.get_delay(attempt)
                if self.config.log_retries:
                    logger.debug(f"Waiting {delay:.2f}s before retry")
                time.sleep(delay)
                total_delay += delay

        # Return final result
        return RetryResult(
            result=result,
            attempts=attempt,
            total_delay=total_delay,
            all_errors=error_collector.get_all() if error_collector else None,
        )

    def retry_async(
        self,
        operation: Callable[[], Result[T]],
        operation_name: Optional[str] = None,
    ) -> RetryResult[T]:
        """Execute an async operation with retry logic.

        Note: This is a placeholder for future async support.
        Currently just delegates to sync retry.

        Args:
            operation: Callable that returns a Result
            operation_name: Optional name for logging

        Returns:
            RetryResult containing the final result and metadata
        """
        # TODO: Implement proper async support when needed
        return self.retry(operation, operation_name)


# Convenience functions for common retry patterns


def retry_with_exponential_backoff(
    operation: Callable[[], Result[T]],
    max_attempts: Optional[int] = None,
    base_delay: float = 1.0,
    operation_name: Optional[str] = None,
) -> RetryResult[T]:
    """Retry an operation with exponential backoff.

    Args:
        operation: The operation to retry
        max_attempts: Maximum number of attempts. If not provided, uses config default from api.api_retry_count
        base_delay: Initial delay in seconds
        operation_name: Optional name for logging

    Returns:
        RetryResult with the final outcome
    """
    # Use config default if not provided
    from fin_statement_model.config.helpers import cfg_or_param

    max_attempts = cfg_or_param("api.api_retry_count", max_attempts)

    config = RetryConfig(
        max_attempts=max_attempts,
        strategy=RetryStrategy.BACKOFF,
        backoff=ExponentialBackoff(base_delay=base_delay),
    )
    handler = RetryHandler(config)
    return handler.retry(operation, operation_name)


def retry_on_specific_errors(
    operation: Callable[[], Result[T]],
    retryable_errors: set[str],
    max_attempts: Optional[int] = None,
    operation_name: Optional[str] = None,
) -> RetryResult[T]:
    """Retry an operation only for specific error codes.

    Args:
        operation: The operation to retry
        retryable_errors: Set of error codes that trigger retry
        max_attempts: Maximum number of attempts. If not provided, uses config default from api.api_retry_count
        operation_name: Optional name for logging

    Returns:
        RetryResult with the final outcome
    """
    # Use config default if not provided
    from fin_statement_model.config.helpers import cfg_or_param

    max_attempts = cfg_or_param("api.api_retry_count", max_attempts)

    config = RetryConfig(
        max_attempts=max_attempts,
        strategy=RetryStrategy.CONDITIONAL,
        retryable_errors=retryable_errors,
        backoff=ExponentialBackoff(),
    )
    handler = RetryHandler(config)
    return handler.retry(operation, operation_name)



================================================================================
File: tests/smoke/test_adjustments_api.py
================================================================================

from fin_statement_model.core.graph.graph import Graph
from fin_statement_model.core.adjustments.models import AdjustmentType


def test_adjustment_application() -> None:
    """Ensure a simple additive adjustment is applied to the base node value."""
    graph = Graph(periods=["2023"])
    graph.add_financial_statement_item("Revenue", {"2023": 100.0})

    graph.add_adjustment(
        "Revenue",
        "2023",
        value=10.0,
        reason="Manual adjustment",
        scenario="default",
        adj_type=AdjustmentType.ADDITIVE,
    )

    adjusted_value = graph.get_adjusted_value("Revenue", "2023")
    assert adjusted_value == 110.0



================================================================================
File: tests/smoke/test_build_and_export.py
================================================================================

import pytest
from pathlib import Path

from fin_statement_model.core.graph.graph import Graph
from fin_statement_model.io.formats.markdown.writer import MarkdownWriter
from fin_statement_model.io.formats.excel.writer import ExcelWriter
from fin_statement_model.io.config.models import ExcelWriterConfig
from fin_statement_model.statements.structure import (
    StatementStructure,
    Section,
    LineItem,
)


def _build_sample_graph() -> Graph:
    """Create a minimal Graph with a single data node."""
    graph = Graph(periods=["2023"])
    graph.add_financial_statement_item("Revenue", {"2023": 1000.0})
    return graph


def _build_sample_structure() -> StatementStructure:
    """Construct a bare-bones StatementStructure that references the sample Graph."""
    statement = StatementStructure(
        id="IS",
        name="Income Statement",
        display_scale_factor=1.0,
    )
    section = Section(
        id="revenue_section",
        name="Revenue Section",
        display_scale_factor=1.0,
    )
    section.add_item(
        LineItem(
            id="rev",
            name="Revenue",
            node_id="Revenue",
            display_scale_factor=1.0,
        )
    )
    statement.add_section(section)
    return statement


@pytest.mark.parametrize("sheet_name", ["Sheet1"])
def test_markdown_and_excel_export(tmp_path: Path, sheet_name: str) -> None:
    """Ensure Markdown and Excel writers produce non-empty outputs."""
    graph = _build_sample_graph()
    structure = _build_sample_structure()

    # Markdown export (returns string)
    md_writer = MarkdownWriter()
    markdown_out = md_writer.write(graph, statement_structure=structure)
    assert isinstance(markdown_out, str) and markdown_out.strip(), (
        "Markdown output is empty"
    )

    # Excel export (writes file)
    target_file = tmp_path / "export.xlsx"
    cfg = ExcelWriterConfig(
        format_type="excel",
        target=str(target_file),
        sheet_name=sheet_name,
    )
    excel_writer = ExcelWriter(cfg)
    excel_writer.write(graph, target=str(target_file))

    assert target_file.exists() and target_file.stat().st_size > 0, (
        "Excel file not created or empty"
    )



================================================================================
File: tests/smoke/test_forecasting_api.py
================================================================================

from fin_statement_model.core.graph.graph import Graph
from fin_statement_model.forecasting import StatementForecaster


def test_forecasting_api_simple() -> None:
    """Verify StatementForecaster.forecast_value returns expected length and dtype."""
    graph = Graph(periods=["2023"])
    graph.add_financial_statement_item("revenue", {"2023": 100.0})

    forecaster = StatementForecaster(graph)
    forecast_periods = ["2024", "2025", "2026", "2027"]

    results = forecaster.forecast_value(
        "revenue",
        forecast_periods=forecast_periods,
        forecast_config={"method": "simple", "config": 0.05},
    )

    assert list(results.keys()) == forecast_periods, "Forecast periods mismatch"
    for value in results.values():
        assert isinstance(value, float), "Forecast value is not a float"
        assert value > 0, "Forecast value should be positive"



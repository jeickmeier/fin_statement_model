# --- START FILE: fin_statement_model/__init__.py ---
"""finlib - A Python library for financial statement analysis and forecasting."""

__all__ = [
    "CalculationNode",
    "CurveGrowthForecastNode",
    "CustomGrowthForecastNode",
    "FinancialStatementGraph",
    "FinancialStatementItemNode",
    "FixedGrowthForecastNode",
    "ForecastNode",
    "Graph",
    "LLMClient",
    "LLMConfig",
    "MetricCalculationNode",
    "MultiPeriodStatNode",
    "Node",
    "StatisticalGrowthForecastNode",
    "YoYGrowthNode",
]

from .extensions.llm.llm_client import LLMClient, LLMConfig
from .core.graph import Graph
from .core.nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    MetricCalculationNode,
    YoYGrowthNode,
    MultiPeriodStatNode,
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    CustomGrowthForecastNode,
)
from .statements import FinancialStatementGraph

# ensure our library-wide logging policy is applied immediately
from . import logging_config  # noqa: F401

# --------------------------------------------------------------------------
# Optional Extensions (via entry points / importlib.metadata)
# --------------------------------------------------------------------------
# Extensions are optional modules that add functionality without modifying
# the core library. They might depend on heavy libraries (e.g., LLMs,
# ML frameworks) and should be lazy-loaded.
# Example entry point group: 'fin_statement_model.extensions.reporting'
# Expected interface: TBD (e.g., a class with specific methods)
# Note: Avoid hard imports from extensions into core/statements/io.
# Goal: Keep core library lean, allow users to install extras like:
# pip install fin-statement-model[openai]
# pip install fin-statement-model[reporting-tools]
# --------------------------------------------------------------------------


# Core API Exports (ensure essential classes/functions are accessible)
# Example:
# from .core.graph import Graph
# from .core.nodes import Node, FinancialStatementItemNode
# from .core.calculation_engine import CalculationEngine
# from .statements.manager import StatementManager

# Placeholder: Explicitly list key public API components later.
# For now, just rely on sub-package __init__ files if they exist.

__version__ = "0.1.0"  # Central version definition

# --- END FILE: fin_statement_model/__init__.py ---

# --- START FILE: fin_statement_model/core/__init__.py ---
"""Core components for the Financial Statement Model.

This package forms the foundation of the library, providing the core infrastructure
for building, calculating, and managing financial models. It includes:

- Graph engine (`core.graph`): For representing financial relationships.
- Base node hierarchy (`core.nodes`): Abstract and concrete node types.
- Calculation engine (`calculation_engine.py`): For evaluating the graph.
- Metric registry and definitions (`core.metrics`): For managing financial metrics.
- Data management (`data_manager.py`): For handling financial data.
- Calculation strategies (`core.strategies`): Reusable calculation logic.
- Core utilities and exceptions (`errors.py`, `node_factory.py`).

This `core` package is designed to be self-contained and does not depend on
other higher-level packages like `statements`, `io`, or `forecasting`.
"""

from .node_factory import NodeFactory
from .graph import Graph
from .nodes import (
    Node,
    FinancialStatementItemNode,
    MetricCalculationNode,
    CalculationNode,
    YoYGrowthNode,
    MultiPeriodStatNode,
    FormulaCalculationNode,
    CustomCalculationNode,
    TwoPeriodAverageNode,
)
from .calculations import (
    AdditionCalculation,
    SubtractionCalculation,
    MultiplicationCalculation,
    DivisionCalculation,
)
from .errors import (
    FinancialModelError,
    ConfigurationError,
    CalculationError,
    NodeError,
    GraphError,
    DataValidationError,
    CircularDependencyError,
    PeriodError,
    StatementError,
    StrategyError,
    TransformationError,
)

__all__ = [
    "AdditionCalculation",
    "CalculationError",
    "CalculationNode",
    "CircularDependencyError",
    "ConfigurationError",
    "CustomCalculationNode",
    "DataValidationError",
    "DivisionCalculation",
    "FinancialModelError",
    "FinancialStatementItemNode",
    "FormulaCalculationNode",
    "Graph",
    "GraphError",
    "MetricCalculationNode",
    "MultiPeriodStatNode",
    "MultiplicationCalculation",
    "Node",
    "NodeError",
    "NodeFactory",
    "PeriodError",
    "StatementError",
    "StrategyError",
    "SubtractionCalculation",
    "TransformationError",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]

# --- END FILE: fin_statement_model/core/__init__.py ---

# --- START FILE: fin_statement_model/core/calculations/__init__.py ---
"""Calculations module for the Financial Statement Model.

This module provides classes for implementing the Calculation Pattern for calculations
in the Financial Statement Model. It allows different calculation algorithms to be
defined, registered, and applied to financial data.
"""

from .calculation import (
    Calculation,
    AdditionCalculation,
    SubtractionCalculation,
    MultiplicationCalculation,
    DivisionCalculation,
    WeightedAverageCalculation,
    CustomFormulaCalculation,
)
from .registry import Registry

# Register calculations
Registry.register(AdditionCalculation)
Registry.register(SubtractionCalculation)
Registry.register(MultiplicationCalculation)
Registry.register(DivisionCalculation)
Registry.register(WeightedAverageCalculation)
Registry.register(CustomFormulaCalculation)

__all__ = [
    "AdditionCalculation",
    "Calculation",
    "CustomFormulaCalculation",
    "DivisionCalculation",
    "MultiplicationCalculation",
    "Registry",
    "SubtractionCalculation",
    "WeightedAverageCalculation",
]

# --- END FILE: fin_statement_model/core/calculations/__init__.py ---

# --- START FILE: fin_statement_model/core/calculations/calculation.py ---
"""Calculation for the Financial Statement Model.

This module provides the Calculation Pattern implementation for calculations,
allowing different calculation types to be encapsulated in calculation classes.
"""

from abc import ABC, abstractmethod
import logging
from typing import Callable, Optional

from fin_statement_model.core.nodes.base import Node  # Absolute

# Configure logging
logger = logging.getLogger(__name__)


class Calculation(ABC):
    """Abstract base class for all calculations.

    This class defines the interface that all concrete calculation classes must
    implement. It employs a calculation pattern, allowing the algorithm
    used by a CalculationNode to be selected at runtime.

    Each concrete calculation encapsulates a specific method for computing a
    financial value based on a list of input nodes and a given time period.
    """

    @abstractmethod
    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculate a value based on input nodes for a specific period.

        This abstract method must be implemented by all concrete calculation classes.
        It defines the core logic for the calculation.

        Args:
            inputs: A list of input Node objects whose values will be used in
                the calculation.
            period: The time period string (e.g., "2023Q1") for which the
                calculation should be performed.

        Returns:
            The calculated numerical value as a float.

        Raises:
            NotImplementedError: If the method is not implemented by a subclass.
            ValueError: If the inputs are invalid for the specific calculation
                (e.g., wrong number of inputs, incompatible types).
            ZeroDivisionError: If the calculation involves division and a divisor
                is zero.
            Exception: Other exceptions depending on the calculation logic.
        """
        # pragma: no cover

    @property
    def description(self) -> str:
        """Provides a human-readable description of the calculation.

        This is useful for documentation, debugging, and for user interfaces
        that need to explain how a value is derived.

        Returns:
            A string describing the calculation.
        """
        # Default implementation returns the class name. Subclasses should override
        # for more specific descriptions.
        class_name = self.__class__.__name__  # pragma: no cover
        return class_name


class AdditionCalculation(Calculation):
    """Implements an addition calculation, summing values from multiple input nodes.

    This calculation sums the values obtained from calling
    the `calculate` method on each of the provided input nodes for a given period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Sums the calculated values from all input nodes for the specified period.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023Q4") for the calculation.

        Returns:
            The total sum of the values calculated from the input nodes. Returns
            0.0 if the input list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = AdditionCalculation()
            >>> nodes = [MockNode(10), MockNode(20), MockNode(5)]
            >>> strategy.calculate(nodes, "2023")
            35.0
            >>> strategy.calculate([], "2023")
            0.0
        """
        logger.debug(f"Applying addition calculation for period {period}")
        # Using a generator expression for potentially better memory efficiency
        return sum(input_node.calculate(period) for input_node in inputs)

    @property
    def description(self) -> str:
        """Returns a description of the addition calculation."""
        return "Addition (sum of all inputs)"


class SubtractionCalculation(Calculation):
    """Implements a subtraction calculation: first input minus the sum of the rest.

    This calculation takes the calculated value of the first node in the input list
    and subtracts the sum of the calculated values of all subsequent nodes for
    a specific period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the difference: value of the first input minus the sum of others.

        Args:
            inputs: A list of Node objects. Must contain at least one node.
            period: The time period string (e.g., "2024Q1") for the calculation.

        Returns:
            The result of the subtraction. If only one input node is provided,
            its value is returned.

        Raises:
            ValueError: If the `inputs` list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = SubtractionCalculation()
            >>> nodes = [MockNode(100), MockNode(20), MockNode(30)]
            >>> strategy.calculate(nodes, "2023")
            50.0
            >>> nodes_single = [MockNode(100)]
            >>> strategy.calculate(nodes_single, "2023")
            100.0
        """
        if not inputs:
            raise ValueError("Subtraction calculation requires at least one input node")

        logger.debug(f"Applying subtraction calculation for period {period}")
        # Calculate values first to avoid multiple calls if nodes are complex
        values = [node.calculate(period) for node in inputs]
        return values[0] - sum(values[1:])

    @property
    def description(self) -> str:
        """Returns a description of the subtraction calculation."""
        return "Subtraction (first input minus sum of subsequent inputs)"


class MultiplicationCalculation(Calculation):
    """Implements a multiplication calculation, calculating the product of input values.

    This calculation multiplies the calculated values of all provided input nodes
    for a given period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the product of the values from all input nodes.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023FY") for the calculation.

        Returns:
            The product of all input values. Returns 1.0 (multiplicative identity)
            if the input list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = MultiplicationCalculation()
            >>> nodes = [MockNode(2), MockNode(3), MockNode(4)]
            >>> strategy.calculate(nodes, "2023")
            24.0
            >>> strategy.calculate([], "2023")
            1.0
        """
        # Multiplication calculation should ideally return 1.0 for empty inputs.
        # Raising error if empty seems less conventional for multiplication.
        if not inputs:
            logger.warning("Multiplication calculation called with empty inputs, returning 1.0")
            return 1.0

        logger.debug(f"Applying multiplication calculation for period {period}")
        result = 1.0
        for input_node in inputs:
            result *= input_node.calculate(period)
        return result

    @property
    def description(self) -> str:
        """Returns a description of the multiplication calculation."""
        return "Multiplication (product of all inputs)"


class DivisionCalculation(Calculation):
    """Implements a division calculation: first input divided by the product of the rest.

    This calculation takes the calculated value of the first node (numerator) and
    divides it by the product of the calculated values of all subsequent nodes
    (denominator) for a specific period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the division: first input / (product of subsequent inputs).

        Args:
            inputs: A list of Node objects. Must contain at least two nodes.
            period: The time period string (e.g., "2024Q2") for the calculation.

        Returns:
            The result of the division.

        Raises:
            ValueError: If `inputs` list contains fewer than two nodes.
            ZeroDivisionError: If the calculated product of the subsequent nodes
                (denominator) is zero.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = DivisionCalculation()
            >>> nodes = [MockNode(100), MockNode(5), MockNode(2)]
            >>> strategy.calculate(nodes, "2023")
            10.0
            >>> nodes_zero_denom = [MockNode(100), MockNode(5), MockNode(0)]
            >>> try:
            ...     strategy.calculate(nodes_zero_denom, "2023")
            ... except ZeroDivisionError as e:
            ...     print(e)
            Division by zero: Denominator product is zero
        """
        if len(inputs) < 2:
            raise ValueError("Division calculation requires at least two input nodes")

        logger.debug(f"Applying division calculation for period {period}")

        values = [node.calculate(period) for node in inputs]
        numerator = values[0]

        denominator = 1.0
        for val in values[1:]:
            denominator *= val

        if denominator == 0.0:
            raise ZeroDivisionError("Division by zero: Denominator product is zero")

        return numerator / denominator

    @property
    def description(self) -> str:
        """Returns a description of the division calculation."""
        return "Division (first input / product of subsequent inputs)"


class WeightedAverageCalculation(Calculation):
    """Calculates the weighted average of input node values.

    This calculation computes the average of the values from input nodes, where each
    node's contribution is weighted. If no weights are provided during
    initialization, it defaults to an equal weighting (simple average).
    """

    def __init__(self, weights: Optional[list[float]] = None):
        """Initializes the WeightedAverageCalculation.

        Args:
            weights: An optional list of floats representing the weight for each
                corresponding input node. The length of this list must match the
                number of input nodes provided to the `calculate` method. If None,
                equal weights are assumed.
        """
        # Validate weights if provided immediately? No, validation happens in calculate
        # as the number of inputs isn't known here.
        self.weights = weights
        logger.info(f"Initialized WeightedAverageCalculation with weights: {weights}")

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Computes the weighted average of the input node values for the period.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023H1") for the calculation.

        Returns:
            The calculated weighted average as a float.

        Raises:
            ValueError: If the `inputs` list is empty.
            ValueError: If `weights` were provided during initialization and their
                count does not match the number of `inputs`.
            ValueError: If the sum of weights is zero (to prevent division by zero
                if normalization were implemented differently).

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> # Equal weights (simple average)
            >>> strategy_equal = WeightedAverageCalculation()
            >>> nodes = [MockNode(10), MockNode(20), MockNode(30)]
            >>> strategy_equal.calculate(nodes, "2023")
            20.0
            >>> # Custom weights
            >>> strategy_custom = WeightedAverageCalculation(weights=[0.5, 0.3, 0.2])
            >>> strategy_custom.calculate(nodes, "2023")
            17.0
            >>> # Mismatched weights
            >>> strategy_mismatch = WeightedAverageCalculation(weights=[0.5, 0.5])
            >>> try:
            ...     strategy_mismatch.calculate(nodes, "2023")
            ... except ValueError as e:
            ...     print(e)
            Number of weights (2) must match number of inputs (3)
        """
        if not inputs:
            raise ValueError("Weighted average calculation requires at least one input node")

        num_inputs = len(inputs)
        effective_weights: list[float]

        if self.weights is None:
            # Use equal weights if none provided
            if num_inputs == 0:  # Should be caught by the check above, but defensive
                return 0.0
            equal_weight = 1.0 / num_inputs
            effective_weights = [equal_weight] * num_inputs
            logger.debug("Using equal weights for weighted average.")
        elif len(self.weights) == num_inputs:
            effective_weights = self.weights
            logger.debug(f"Using provided weights: {effective_weights}")
        else:
            raise ValueError(
                f"Number of weights ({len(self.weights)}) must match "
                f"number of inputs ({num_inputs})"
            )

        logger.debug(f"Applying weighted average calculation for period {period}")
        weighted_sum = 0.0
        total_weight = sum(effective_weights)
        input_values = [node.calculate(period) for node in inputs]

        if total_weight == 0.0:
            # Avoid division by zero. If weights are all zero, the concept is ill-defined.
            # Returning 0 might be a reasonable default, or raising an error.
            # Let's raise ValueError for clarity.
            raise ValueError("Total weight for weighted average cannot be zero.")

        for value, weight in zip(input_values, effective_weights):
            weighted_sum += value * weight

        # If weights don't sum to 1, this isn't a standard weighted average.
        # Decide whether to normalize or return the weighted sum directly.
        # Normalize by total weight for a true weighted average.
        return weighted_sum / total_weight

    @property
    def description(self) -> str:
        """Returns a description of the weighted average calculation."""
        if self.weights:
            return f"Weighted Average (using provided weights: {self.weights})"
        else:
            return "Weighted Average (using equal weights)"


# Type alias for the custom formula function
FormulaFunc = Callable[[dict[str, float]], float]


class CustomFormulaCalculation(Calculation):
    """Executes a user-defined Python function to calculate a value.

    This calculation provides maximum flexibility by allowing any custom Python
    function to be used for calculation. The function receives a dictionary
    mapping input node names (or fallback names) to their calculated values
    for the period and should return a single float result.
    """

    def __init__(self, formula_function: FormulaFunc):
        """Initializes the CustomFormulaCalculation with a calculation function.

        Args:
            formula_function: A callable (function, lambda, etc.) that accepts
                a single argument: a dictionary mapping string keys (input node
                names or `input_<i>`) to their float values for the period.
                It must return a float.

        Raises:
            TypeError: If `formula_function` is not callable.
        """
        if not callable(formula_function):
            raise TypeError("formula_function must be callable")
        self.formula_function = formula_function
        logger.info(f"Initialized CustomFormulaCalculation with function: {formula_function.__name__}")

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Applies the custom formula function to the calculated input values.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2025M1") for the calculation.

        Returns:
            The float result returned by the `formula_function`.

        Raises:
            ValueError: If the `formula_function` encounters an error during execution
                (e.g., incorrect input keys, calculation errors). Wraps the original
                exception.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, name, value): self.name = name; self._value = value
            ...     def calculate(self, period): return self._value
            >>> def my_formula(data):
            ...     # Example: Gross Profit Margin
            ...     return (data['revenue'] - data['cogs']) / data['revenue'] * 100
            >>> strategy = CustomFormulaCalculation(my_formula)
            >>> nodes = [MockNode('revenue', 1000), MockNode('cogs', 600)]
            >>> strategy.calculate(nodes, "2023")
            40.0
            >>> # Example with unnamed nodes
            >>> def simple_sum(data):
            ...     return data['input_0'] + data['input_1']
            >>> strategy_unnamed = CustomFormulaCalculation(simple_sum)
            >>> nodes_unnamed = [MockNode(None, 10), MockNode(None, 20)] # No names
            >>> strategy_unnamed.calculate(nodes_unnamed, "2023")
            30.0
        """
        # Prepare input values dictionary, using names if available
        input_values: dict[str, float] = {}
        for i, node in enumerate(inputs):
            # Prefer node.name if it exists and is a non-empty string
            key = getattr(node, "name", None)
            if not isinstance(key, str) or not key:
                key = f"input_{i}"
            input_values[key] = node.calculate(period)

        logger.debug(
            f"Applying custom formula calculation for period {period} with inputs: {input_values}"
        )
        try:
            # Execute the user-provided function
            result = self.formula_function(input_values)
            if not isinstance(result, (int, float)):
                logger.warning(
                    f"Custom formula function {self.formula_function.__name__} "
                    f"returned non-numeric type: {type(result)}. Attempting cast."
                )
                # Attempt conversion, but be aware this might fail or be lossy
                try:
                    return float(result)
                except (ValueError, TypeError) as cast_err:
                    raise ValueError(
                        f"Custom formula {self.formula_function.__name__} result "
                        f"({result!r}) could not be cast to float."
                    ) from cast_err
            return float(result)  # Ensure result is float
        except Exception as e:
            # Catch any exception from the custom function and wrap it
            logger.error(
                f"Error executing custom formula '{self.formula_function.__name__}': {e}",
                exc_info=True,
            )
            raise ValueError(
                f"Error in custom formula '{self.formula_function.__name__}': {e}"
            ) from e

    @property
    def description(self) -> str:
        """Returns a description of the custom formula calculation."""
        func_name = getattr(self.formula_function, "__name__", "[anonymous function]")
        return f"Custom Formula (using function: {func_name})"

# --- END FILE: fin_statement_model/core/calculations/calculation.py ---

# --- START FILE: fin_statement_model/core/calculations/registry.py ---
"""Registry for calculation classes in the Financial Statement Model.

This module provides a central registry for discovering and accessing different
calculation classes. Calculations can be registered using their class
object and later retrieved by their class name.
"""

# Use lowercase built-in types
from typing import ClassVar  # Keep Type for now
import logging

from .calculation import Calculation

# Configure logging
logger = logging.getLogger(__name__)


class Registry:
    """A central registry for managing and accessing calculation classes.

    This class uses class methods to provide a global registry. Calculations
    are stored in a dictionary mapping their class name (string) to the
    calculation class itself.

    Attributes:
        _strategies: A dictionary holding the registered calculation classes.
                     Keys are calculation class names (str), values are calculation
                     types (Type[Calculation]).
    """

    _strategies: ClassVar[dict[str, type[Calculation]]] = {}  # Use dict, type

    @classmethod
    def register(cls, calculation: type[Calculation]) -> None:
        """Register a calculation class with the registry.

        If a calculation with the same name is already registered, it will be
        overwritten.

        Args:
            calculation: The calculation class (Type[Calculation]) to register.
                         The class's __name__ attribute will be used as the key.
        """
        if not issubclass(calculation, Calculation):
            raise TypeError(f"Can only register subclasses of Calculation, not {calculation}")
        cls._strategies[calculation.__name__] = calculation
        logger.debug(f"Registered calculation: {calculation.__name__}")

    @classmethod
    def get(cls, name: str) -> type[Calculation]:
        """Retrieve a calculation class from the registry by its name.

        Args:
            name: The string name of the calculation class to retrieve.

        Returns:
            The calculation class (Type[Calculation]) associated with the given name.

        Raises:
            KeyError: If no calculation with the specified name is found in the
                      registry.
        """
        # Debug print including id of the dictionary
        if name not in cls._strategies:
            logger.error(f"Attempted to access unregistered calculation: {name}")
            raise KeyError(f"Calculation '{name}' not found in registry.")
        return cls._strategies[name]

    @classmethod
    def list(cls) -> dict[str, type[Calculation]]:  # Use dict, type
        """List all registered calculation classes.

        Returns:
            A dictionary containing all registered calculation names (str) and their
            corresponding calculation classes (Type[Calculation]). Returns a copy
            to prevent modification of the internal registry.
        """
        return cls._strategies.copy()

# --- END FILE: fin_statement_model/core/calculations/registry.py ---

# --- START FILE: fin_statement_model/core/errors.py ---
"""Define custom exceptions for the Financial Statement Model.

This module defines exception classes for specific error cases in the
Financial Statement Model, allowing for more precise error handling
and better error messages.
"""

from typing import Optional, Any


class FinancialModelError(Exception):
    """Define the base exception class for all Financial Statement Model errors.

    All custom exceptions raised within the library should inherit from this class.

    Args:
        message: A human-readable description of the error.
    """

    def __init__(self, message: str):
        """Initializes the FinancialModelError."""
        self.message = message
        super().__init__(self.message)


class ConfigurationError(FinancialModelError):
    """Raise an error for invalid configuration files or objects.

    This typically occurs when parsing or validating configuration data,
    such as YAML files defining metrics or statement structures.

    Args:
        message: The base error message.
        config_path: Optional path to the configuration file where the error occurred.
        errors: Optional list of specific validation errors found.

    Examples:
        >>> raise ConfigurationError("Invalid syntax", config_path="config.yaml")
        >>> raise ConfigurationError(
        ...     "Missing required fields",
        ...     config_path="metrics.yaml",
        ...     errors=["Missing 'formula' for 'revenue'"]
        ... )
    """

    def __init__(
        self,
        message: str,
        config_path: Optional[str] = None,
        errors: Optional[list[str]] = None,
    ):
        """Initializes the ConfigurationError."""
        self.config_path = config_path
        self.errors = errors or []

        if config_path and errors:
            full_message = f"{message} in {config_path}: {'; '.join(errors)}"
        elif config_path:
            full_message = f"{message} in {config_path}"
        elif errors:
            full_message = f"{message}: {'; '.join(errors)}"
        else:
            full_message = message

        super().__init__(full_message)


class CalculationError(FinancialModelError):
    """Raise an error during calculation operations.

    This indicates a problem while computing the value of a node, often due
    to issues with the calculation logic, input data, or strategy used.

    Args:
        message: The base error message.
        node_id: Optional ID of the node where the calculation failed.
        period: Optional period for which the calculation failed.
        details: Optional dictionary containing additional context about the error.

    Examples:
        >>> raise CalculationError("Division by zero", node_id="profit_margin", period="2023-Q1")
        >>> raise CalculationError(
        ...     "Incompatible input types",
        ...     node_id="total_assets",
        ...     details={"input_a_type": "str", "input_b_type": "int"}
        ... )
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str] = None,
        period: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
    ):
        """Initializes the CalculationError."""
        self.node_id = node_id
        self.period = period
        self.details = details or {}

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if period:
            context.append(f"period '{period}'")

        full_message = f"{message} for {' and '.join(context)}" if context else message

        # Append details to the message for better context
        if self.details:
            details_str = ", ".join(f'{k}="{v}"' for k, v in self.details.items())
            # Prioritize showing the original underlying error if captured
            original_error_str = self.details.get("original_error")
            if original_error_str:
                full_message = f"{full_message}: {original_error_str}"
            else:
                full_message = f"{full_message} (Details: {details_str})"

        super().__init__(full_message)


class NodeError(FinancialModelError):
    """Raise an error for issues related to graph nodes.

    This covers issues like trying to access a non-existent node,
    invalid node configurations, or type mismatches related to nodes.

    Args:
        message: The base error message.
        node_id: Optional ID of the node related to the error.

    Examples:
        >>> raise NodeError("Node not found", node_id="non_existent_node")
        >>> raise NodeError("Invalid node type for operation", node_id="revenue")
    """

    def __init__(self, message: str, node_id: Optional[str] = None):
        """Initializes the NodeError."""
        self.node_id = node_id

        full_message = f"{message} for node '{node_id}'" if node_id else message

        super().__init__(full_message)


class MissingInputError(FinancialModelError):
    """Raise an error when a required calculation input is missing.

    This occurs when a calculation node needs data from another node for a
    specific period, but that data is unavailable.

    Args:
        message: The base error message.
        node_id: Optional ID of the node requiring the input.
        input_name: Optional name or ID of the missing input node.
        period: Optional period for which the input was missing.

    Examples:
        >>> raise MissingInputError(
        ...     "Required input data unavailable",
        ...     node_id="cogs",
        ...     input_name="inventory",
        ...     period="2023-12-31"
        ... )
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str] = None,
        input_name: Optional[str] = None,
        period: Optional[str] = None,
    ):
        """Initializes the MissingInputError."""
        self.node_id = node_id
        self.input_name = input_name
        self.period = period

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if input_name:
            context.append(f"input '{input_name}'")
        if period:
            context.append(f"period '{period}'")

        full_message = f"{message} for {' in '.join(context)}" if context else message

        super().__init__(full_message)


class GraphError(FinancialModelError):
    """Raise an error for invalid graph structure or operations.

    This covers issues like inconsistencies in the graph (e.g., orphaned nodes),
    problems during graph traversal, or invalid modifications to the graph.

    Args:
        message: The base error message.
        nodes: Optional list of node IDs involved in the graph error.

    Examples:
        >>> raise GraphError("Orphaned node detected", nodes=["unconnected_node"])
        >>> raise GraphError("Failed to add edge due to type mismatch")
    """

    def __init__(self, message: str, nodes: Optional[list[str]] = None):
        """Initializes the GraphError."""
        self.nodes = nodes or []

        full_message = f"{message} involving nodes: {', '.join(nodes)}" if nodes else message

        super().__init__(full_message)


class DataValidationError(FinancialModelError):
    """Raise an error for data validation failures.

    This typically occurs during data import or preprocessing when data
    does not conform to expected formats, types, or constraints.

    Args:
        message: The base error message.
        validation_errors: Optional list of specific validation failures.

    Examples:
        >>> raise DataValidationError(
        ...     "Input data failed validation",
        ...     validation_errors=["Column 'Date' has invalid format", "Value '-100' is not allowed for 'Revenue'"]
        ... )
    """

    def __init__(self, message: str, validation_errors: Optional[list[str]] = None):
        """Initializes the DataValidationError."""
        self.validation_errors = validation_errors or []

        if validation_errors:
            full_message = f"{message}: {'; '.join(validation_errors)}"
        else:
            full_message = message

        super().__init__(full_message)


class CircularDependencyError(FinancialModelError):
    """Raise an error when a circular dependency is detected in calculations.

    This occurs if the calculation graph contains cycles, meaning a node
    directly or indirectly depends on itself.

    Args:
        message: The base error message. Defaults to "Circular dependency detected".
        cycle: Optional list of node IDs forming the detected cycle.

    Examples:
        >>> raise CircularDependencyError(cycle=["node_a", "node_b", "node_c", "node_a"])
    """

    def __init__(
        self,
        message: str = "Circular dependency detected",
        cycle: Optional[list[str]] = None,
    ):
        """Initializes the CircularDependencyError."""
        self.cycle = cycle or []

        if cycle:
            cycle_str = " -> ".join(cycle)
            full_message = f"{message}: {cycle_str}"
        else:
            full_message = message

        super().__init__(full_message)


class PeriodError(FinancialModelError):
    """Raise an error for invalid or missing periods.

    This covers issues like requesting data for a non-existent period or
    using invalid period formats.

    Args:
        message: The base error message.
        period: Optional specific period involved in the error.
        available_periods: Optional list of valid periods.

    Examples:
        >>> raise PeriodError("Invalid period format", period="2023Q5")
        >>> raise PeriodError("Period not found", period="2024-01-01", available_periods=["2023-12-31"])
    """

    def __init__(
        self,
        message: str,
        period: Optional[str] = None,
        available_periods: Optional[list[str]] = None,
    ):
        """Initializes the PeriodError."""
        self.period = period
        self.available_periods = available_periods or []

        if period and available_periods:
            full_message = f"{message} for period '{period}'. Available periods: {', '.join(available_periods)}"
        elif period:
            full_message = f"{message} for period '{period}'"
        else:
            full_message = message

        super().__init__(full_message)


class StatementError(FinancialModelError):
    """Raise an error for issues related to financial statements.

    This is used for errors specific to the structure, definition, or
    processing of financial statements (e.g., Balance Sheet, P&L).

    Args:
        message: The base error message.
        statement_id: Optional ID or name of the statement involved.

    Examples:
        >>> raise StatementError("Balance sheet does not balance", statement_id="BS_2023")
        >>> raise StatementError("Required account missing from P&L", statement_id="PnL_Q1")
    """

    def __init__(self, message: str, statement_id: Optional[str] = None):
        """Initializes the StatementError."""
        self.statement_id = statement_id

        full_message = f"{message} for statement '{statement_id}'" if statement_id else message

        super().__init__(full_message)


class StrategyError(FinancialModelError):
    """Raise an error for issues related to calculation strategies.

    This indicates a problem with the configuration or execution of a
    specific calculation strategy (e.g., Summation, GrowthRate).

    Args:
        message: The base error message.
        strategy_type: Optional name or type of the strategy involved.
        node_id: Optional ID of the node using the strategy.

    Examples:
        >>> raise StrategyError("Invalid parameter for GrowthRate strategy", strategy_type="GrowthRate", node_id="revenue_forecast")
        >>> raise StrategyError("Strategy not applicable to node type", strategy_type="Summation", node_id="text_description")
    """

    def __init__(
        self,
        message: str,
        strategy_type: Optional[str] = None,
        node_id: Optional[str] = None,
    ):
        """Initializes the StrategyError."""
        self.strategy_type = strategy_type
        self.node_id = node_id

        context = []
        if strategy_type:
            context.append(f"strategy type '{strategy_type}'")
        if node_id:
            context.append(f"node '{node_id}'")

        full_message = f"{message} for {' in '.join(context)}" if context else message

        super().__init__(full_message)


class TransformationError(FinancialModelError):
    """Raise an error during data transformation.

    This occurs during preprocessing steps when a specific transformation
    (e.g., normalization, scaling) fails.

    Args:
        message: The base error message.
        transformer_type: Optional name or type of the transformer involved.
        parameters: Optional dictionary of parameters used by the transformer.

    Examples:
        >>> raise TransformationError("Log transform requires positive values", transformer_type="LogTransformer")
        >>> raise TransformationError(
        ...     "Incompatible data type for scaling",
        ...     transformer_type="MinMaxScaler",
        ...     parameters={"feature_range": (0, 1)}
        ... )
    """

    def __init__(
        self,
        message: str,
        transformer_type: Optional[str] = None,
        parameters: Optional[dict[str, Any]] = None,
    ):
        """Initializes the TransformationError."""
        self.transformer_type = transformer_type
        self.parameters = parameters or {}

        if transformer_type:
            full_message = f"{message} in transformer '{transformer_type}'"
            if parameters:
                params_str = ", ".join(f"{k}={v}" for k, v in parameters.items())
                full_message = f"{full_message} with parameters: {params_str}"
        else:
            full_message = message

        super().__init__(full_message)


class MetricError(FinancialModelError):
    """Raise an error for issues related to metric definitions or registry.

    This covers issues with loading, validating, or accessing financial metrics,
    whether defined in YAML or Python code.

    Args:
        message: The base error message.
        metric_name: Optional name of the metric involved in the error.
        details: Optional dictionary containing additional context about the error.

    Examples:
        >>> raise MetricError("Metric definition not found", metric_name="unknown_ratio")
        >>> raise MetricError(
        ...     "Invalid formula syntax in metric definition",
        ...     metric_name="profitability_index",
        ...     details={"formula": "NPV / Initial Investment)"} # Missing parenthesis
        ... )
    """

    def __init__(
        self,
        message: str,
        metric_name: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
    ):
        """Initializes the MetricError."""
        self.metric_name = metric_name
        self.details = details or {}

        full_message = f"{message} related to metric '{metric_name}'" if metric_name else message

        super().__init__(full_message)

# --- END FILE: fin_statement_model/core/errors.py ---

# --- START FILE: fin_statement_model/core/graph/__init__.py ---
"""Graph module for the financial statement model.

This module provides the core graph functionality for building and evaluating
financial statement models.
"""

from fin_statement_model.core.graph.graph import Graph

__all__ = ["Graph"]

# --- END FILE: fin_statement_model/core/graph/__init__.py ---

# --- START FILE: fin_statement_model/core/graph/graph.py ---
"""Provide graph operations for the financial statement model.

This module provides the `Graph` class that combines manipulation, traversal,
forecasting, and calculation capabilities for building and evaluating
financial statement models.
"""

import logging
from typing import Any, Callable, Optional

from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.core.nodes import Node, FinancialStatementItemNode, MetricCalculationNode, CalculationNode
from fin_statement_model.core.errors import NodeError, ConfigurationError, CalculationError
from fin_statement_model.core.metrics import metric_registry
from fin_statement_model.core.calculations import Registry
from fin_statement_model.core.graph.manipulator import GraphManipulator
from fin_statement_model.core.graph.traverser import GraphTraverser


# Configure logging
logger = logging.getLogger(__name__)

__all__ = ["Graph"]

class Graph:
    """Represent the financial statement model as a directed graph.

    This class integrates graph manipulation, traversal, forecasting, and
    calculation capabilities. It serves as the central orchestrator for nodes,
    periods, caching, and calculation workflows.

    Attributes:
        _nodes: A dict mapping node names (str) to Node objects.
        _periods: A list of time period identifiers (str) managed by the graph.
        _cache: A nested dict caching calculated values per node per period.
        _metric_names: A set of node names (str) corresponding to metric nodes.
        _node_factory: An instance of NodeFactory for creating new nodes.
    """

    def __init__(self, periods: Optional[list[str]] = None):
        """Initialize the Graph instance.

        Set up core components: node registry, `DataManager`, and `CalculationEngine`.
        Optionally initialize the graph with a list of time periods.

        Args:
            periods: An optional list of strings representing the initial time
                     periods for the financial model (e.g., ["2023", "2024"]).
                     The `DataManager` will handle sorting and ensuring uniqueness.

        Raises:
            TypeError: If `periods` is provided but is not a list.

        Examples:
            >>> graph_no_periods = Graph()
            >>> print(graph_no_periods.periods)
            []
            >>> graph_with_periods = Graph(periods=["2023", "2022"])
            >>> print(graph_with_periods.periods) # Periods are sorted
            ['2022', '2023']
            >>> try:
            ...     Graph(periods="2023") # Invalid type
            ... except TypeError as e:
            ...     print(e)
            Initial periods must be a list
        """
        # No super().__init__() needed as mixins don't have __init__
        # and GraphCore is removed.

        self._nodes: dict[str, Node] = {}

        # Initialize core attributes for periods, cache, metrics, and node factory
        self._periods: list[str] = []
        self._cache: dict[str, dict[str, float]] = {}
        self._metric_names: set[str] = set()
        self._node_factory: NodeFactory = NodeFactory()

        # Handle initial periods directly
        if periods:
            if not isinstance(periods, list):
                raise TypeError("Initial periods must be a list")
            self.add_periods(periods)

        self.manipulator = GraphManipulator(self)
        self.traverser = GraphTraverser(self)

    @property
    def nodes(self) -> dict[str, Node]:
        """Provide access to the dictionary of all nodes in the graph.

        Returns:
            A dictionary where keys are node names (str) and values are
            `Node` objects. This dictionary represents the shared node registry.

        Examples:
            >>> graph = Graph()
            >>> item_node = graph.add_financial_statement_item("Revenue", {"2023": 100})
            >>> print(list(graph.nodes.keys()))
            ['Revenue']
            >>> print(graph.nodes["Revenue"] == item_node)
            True
        """
        return self._nodes

    @property
    def periods(self) -> list[str]:
        """Retrieve the list of time periods currently managed by the graph.

        Returns:
            A sorted list of unique time period strings managed by the graph.

        Examples:
            >>> graph = Graph(periods=["2024", "2023"])
            >>> print(graph.periods)
            ['2023', '2024']
            >>> graph.add_periods(["2025"])
            >>> print(graph.periods)
            ['2023', '2024', '2025']
        """
        return self._periods

    def add_periods(self, periods: list[str]) -> None:
        """Add new time periods to the graph.

        Update the internal period list, ensuring uniqueness and sorting.

        Args:
            periods: A list of strings representing the time periods to add.

        Raises:
            TypeError: If `periods` is not a list.
        """
        if not isinstance(periods, list):
            raise TypeError("Periods must be provided as a list.")
        # Ensure unique and sorted periods
        combined = set(self._periods).union(periods)
        self._periods = sorted(combined)
        logger.debug(f"Added periods {periods}; current periods: {self._periods}")

    def add_calculation(
        self,
        name: str,
        input_names: list[str],
        operation_type: str,
        **kwargs: dict[str, Any],
    ) -> Node:
        """Add a new calculation node to the graph using the node factory.

        Resolve input node names to Node objects, create a CalculationNode,
        register it in the graph, and return it.

        Args:
            name: Unique name for the calculation node.
            input_names: List of node names to use as inputs.
            operation_type: Calculation type key (e.g., 'addition').
            **kwargs: Additional parameters for the calculation.

        Returns:
            The created calculation node.

        Raises:
            NodeError: If any input node name does not exist.
            ValueError: If the name is invalid or creation fails.
            TypeError: If inputs are invalid.
        """
        # Validate name
        if not name or not isinstance(name, str):
            raise ValueError("Calculation node name must be a non-empty string.")
        # Resolve input node names
        if not isinstance(input_names, list):
            raise TypeError("input_names must be a list of node names.")
        resolved_inputs: list[Node] = []
        missing = []
        for nm in input_names:
            nd = self._nodes.get(nm)
            if nd is None:
                missing.append(nm)
            else:
                resolved_inputs.append(nd)
        if missing:
            raise NodeError(
                f"Cannot create calculation node '{name}': missing input nodes {missing}",
                node_id=name,
            )
        # Create the node via factory
        try:
            node = self._node_factory.create_calculation_node(
                name=name,
                inputs=resolved_inputs,
                calculation_type=operation_type,
                **kwargs,
            )
        except (ValueError, TypeError):
            logger.exception(
                f"Failed to create calculation node '{name}' with type '{operation_type}'"
            )
            raise
        # Register node in graph
        if name in self._nodes:
            logger.warning(f"Overwriting existing node '{name}' in graph.")
        self._nodes[name] = node
        logger.info(
            f"Added calculation node '{name}' of type '{operation_type}' with inputs {input_names}"
        )
        return node

    def add_metric(
        self, metric_name: str, node_name: Optional[str] = None
    ) -> MetricCalculationNode:
        """Add a metric calculation node based on a metric definition.

        If `node_name` is None, uses `metric_name` as the node name.

        Use the metric registry to load inputs and formula, create a MetricCalculationNode,
        register it, and track it as a metric.

        Args:
            metric_name: Key of the metric definition to add.
            node_name: Optional name for the metric node; defaults to metric_name.

        Returns:
            The created MetricCalculationNode.

        Raises:
            TypeError: If node_name is invalid.
            ValueError: If node_name already exists.
            ConfigurationError: If metric definition is missing or invalid.
            NodeError: If required input nodes are missing.
        """
        # Default node_name to metric_name if not provided
        if node_name is None:
            node_name = metric_name
        if not node_name or not isinstance(node_name, str):
            raise TypeError("Metric node name must be a non-empty string.")
        # Check for name conflict
        if node_name in self._nodes:
            raise ValueError(f"A node with name '{node_name}' already exists in the graph.")
        # Load metric definition
        try:
            metric_def = metric_registry.get(metric_name)
        except KeyError as e:
            raise ConfigurationError(f"Unknown metric definition: '{metric_name}'") from e
        # Validate definition fields
        required_inputs = metric_def.get("inputs")
        if not isinstance(required_inputs, list):
            raise ConfigurationError(
                f"Metric definition for '{metric_name}' is invalid: 'inputs' must be a list."
            )
        if "formula" not in metric_def:
            raise ConfigurationError(
                f"Metric definition for '{metric_name}' is invalid: missing 'formula'."
            )
        # Resolve input nodes
        resolved_inputs: dict[str, Node] = {}
        missing = []
        for req in required_inputs:
            nd = self._nodes.get(req)
            if nd is None:
                missing.append(req)
            else:
                resolved_inputs[req] = nd
        if missing:
            raise NodeError(
                f"Cannot create metric '{metric_name}': missing required nodes {missing}",
                node_id=node_name,
            )
        # Create metric node via factory
        try:
            metric_node = self._node_factory.create_metric_node(
                name=node_name,
                metric_name=metric_name,
                input_nodes=resolved_inputs,
            )
        except (ValueError, TypeError, ConfigurationError):
            logger.exception(
                f"Failed to create metric node '{node_name}' for metric '{metric_name}'"
            )
            raise
        # Register metric node
        self._nodes[node_name] = metric_node
        self._metric_names.add(node_name)
        logger.info(
            f"Added metric '{metric_name}' as node '{node_name}' with inputs {list(resolved_inputs)}"
        )
        return metric_node

    def add_custom_calculation(
        self,
        name: str,
        calculation_func: Callable[..., float],
        inputs: Optional[list[str]] = None,
        description: str = "",
    ) -> Node:
        """Add a custom calculation node using a Python callable.

        Args:
            name: Unique name for the custom calculation node.
            calculation_func: A callable that accepts (period, **inputs) and returns float.
            inputs: Optional list of node names to use as inputs.
            description: Optional description of the calculation.

        Returns:
            The created custom calculation node.

        Raises:
            NodeError: If any specified input nodes are missing.
            TypeError: If calculation_func is not callable.
        """
        # Validate inputs list
        resolved_inputs: list[Node] = []
        if inputs is not None:
            if not isinstance(inputs, list):
                raise TypeError("inputs must be a list of node names.")
            missing = []
            for nm in inputs:
                nd = self._nodes.get(nm)
                if nd is None:
                    missing.append(nm)
                else:
                    resolved_inputs.append(nd)
            if missing:
                raise NodeError(
                    f"Cannot create custom calculation '{name}': missing input nodes {missing}",
                    node_id=name,
                )
        # Validate callable
        if not callable(calculation_func):
            raise TypeError("calculation_func must be callable.")
        # Create custom node via factory
        try:
            custom_node = self._node_factory._create_custom_node_from_callable(
                name=name,
                inputs=resolved_inputs,
                formula=calculation_func,
                description=description,
            )
        except (ValueError, TypeError):
            logger.exception(f"Failed to create custom calculation node '{name}'")
            raise
        # Register custom node
        if name in self._nodes:
            logger.warning(f"Overwriting existing node '{name}' in graph.")
        self._nodes[name] = custom_node
        logger.info(f"Added custom calculation node '{name}' with inputs {inputs}")
        return custom_node

    def change_calculation_method(
        self,
        node_name: str,
        new_method_key: str,
        **kwargs: dict[str, Any],
    ) -> None:
        """Change the calculation method for an existing calculation-based node.

        Args:
            node_name: Name of the existing calculation node.
            new_method_key: Key of the new calculation method to apply.
            **kwargs: Additional parameters required by the new calculation.

        Returns:
            None

        Raises:
            NodeError: If the target node does not exist or is not a CalculationNode.
            ValueError: If `new_method_key` is not a recognized calculation key.
            TypeError: If the new calculation cannot be instantiated with the provided arguments.

        Examples:
            >>> graph.change_calculation_method("GrossProfit", "addition")
        """
        node = self.manipulator.get_node(node_name)
        if node is None:
            raise NodeError("Node not found for calculation change", node_id=node_name)
        if not isinstance(node, CalculationNode):
            raise NodeError(
                f"Node '{node_name}' is not a CalculationNode", node_id=node_name
            )
        # Map method key to registry name
        if new_method_key not in self._node_factory._calculation_methods:
            raise ValueError(
                f"Calculation '{new_method_key}' is not recognized."
            )
        calculation_class_name = self._node_factory._calculation_methods[new_method_key]
        try:
            calculation_cls = Registry.get(calculation_class_name)
        except KeyError as e:
            raise ValueError(
                f"Calculation class '{calculation_class_name}' not found in registry."
            ) from e
        try:
            calculation_instance = calculation_cls(**kwargs)
        except TypeError as e:
            raise TypeError(
                f"Failed to instantiate calculation '{new_method_key}': {e}"
            )
        # Apply new calculation
        node.set_calculation(calculation_instance)
        # Clear cached calculations for this node
        if node_name in self._cache:
            del self._cache[node_name]
        logger.info(
            f"Changed calculation for node '{node_name}' to '{new_method_key}'"
        )

    def get_metric(self, metric_id: str) -> Optional[Node]:
        """Return the metric node for a given metric ID, if present.

        Args:
            metric_id: Identifier of the metric node to retrieve.

        Returns:
            The MetricCalculationNode corresponding to `metric_id`, or None if not found.

        Examples:
            >>> m = graph.get_metric("current_ratio")
            >>> if m:
            ...     print(m.name)
        """
        node = self._nodes.get(metric_id)
        return node if node and metric_id in self._metric_names else None

    def get_available_metrics(self) -> list[str]:
        """Return a sorted list of all metric node IDs currently in the graph.

        Returns:
            A sorted list of metric node names.

        Examples:
            >>> graph.get_available_metrics()
            ['current_ratio', 'debt_equity_ratio']
        """
        return sorted(self._metric_names)

    def get_metric_info(self, metric_id: str) -> dict:
        """Return detailed information for a specific metric node.

        Args:
            metric_id: Identifier of the metric node to inspect.

        Returns:
            A dict containing 'id', 'name', 'description', and 'inputs' for the metric.

        Raises:
            ValueError: If `metric_id` does not correspond to a metric node.

        Examples:
            >>> info = graph.get_metric_info("current_ratio")
            >>> print(info['inputs'])
        """
        metric_node = self.get_metric(metric_id)
        if metric_node is None:
            if metric_id in self._nodes:
                raise ValueError(f"Node '{metric_id}' exists but is not a metric.")
            raise ValueError(f"Metric '{metric_id}' not found in graph.")
        # Extract info from the MetricCalculationNode
        try:
            description = metric_node.definition.get("description")  # type: ignore
            name = metric_node.definition.get("name")  # type: ignore
            inputs = metric_node.get_dependencies()
        except Exception as e:
            raise ValueError(
                f"Failed to retrieve metric info for '{metric_id}': {e}"
            )
        return {"id": metric_id, "name": name, "description": description, "inputs": inputs}

    def calculate(self, node_name: str, period: str) -> float:
        """Calculate and return the value of a specific node for a given period.

        This method uses internal caching to speed repeated calls, and wraps
        underlying errors in CalculationError for clarity.

        Args:
            node_name: Name of the node to calculate.
            period: Time period identifier for the calculation.

        Returns:
            The calculated float value for the node and period.

        Raises:
            NodeError: If the specified node does not exist.
            TypeError: If the node has no callable `calculate` method.
            CalculationError: If an error occurs during the node's calculation.

        Examples:
            >>> value = graph.calculate("Revenue", "2023")
        """
        # Return cached value if present
        if node_name in self._cache and period in self._cache[node_name]:
            logger.debug(f"Cache hit for node '{node_name}', period '{period}'")
            return self._cache[node_name][period]
        # Resolve node
        node = self.manipulator.get_node(node_name)
        if node is None:
            raise NodeError(f"Node '{node_name}' not found", node_id=node_name)
        # Validate calculate method
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise TypeError(f"Node '{node_name}' has no callable calculate method.")
        # Perform calculation with error handling
        try:
            value = node.calculate(period)
        except (NodeError, ConfigurationError, CalculationError, ValueError, KeyError, ZeroDivisionError) as e:
            logger.error(f"Error calculating node '{node_name}' for period '{period}': {e}", exc_info=True)
            raise CalculationError(
                message=f"Failed to calculate node '{node_name}'",
                node_id=node_name,
                period=period,
                details={"original_error": str(e)},
            ) from e
        # Cache and return
        self._cache.setdefault(node_name, {})[period] = value
        logger.debug(f"Cached value for node '{node_name}', period '{period}': {value}")
        return value

    def recalculate_all(self, periods: Optional[list[str]] = None) -> None:
        """Recalculate all nodes for given periods, clearing all caches first.

        Args:
            periods: List of period strings, a single string, or None to use all periods.

        Returns:
            None

        Raises:
            TypeError: If `periods` is not a list, string, or None.

        Examples:
            >>> graph.recalculate_all(["2023", "2024"])
        """
        # Normalize periods input
        if periods is None:
            periods_to_use = self.periods
        elif isinstance(periods, str):
            periods_to_use = [periods]
        elif isinstance(periods, list):
            periods_to_use = periods
        else:
            raise TypeError("Periods must be a list of strings, a single string, or None.")
        # Clear all caches (node-level and central) to force full recalculation
        self.clear_all_caches()
        if not periods_to_use:
            return
        # Recalculate each node for each period
        for node_name in list(self._nodes.keys()):
            for period in periods_to_use:
                try:
                    self.calculate(node_name, period)
                except Exception as e:
                    logger.warning(f"Error recalculating node '{node_name}' for period '{period}': {e}")

    def clear_all_caches(self) -> None:
        """Clear all node-level and central calculation caches.

        Returns:
            None

        Examples:
            >>> graph.clear_all_caches()
        """
        logger.debug(f"Clearing node-level caches for {len(self.nodes)} nodes.")
        for node in self.nodes.values():
            if hasattr(node, "clear_cache"):
                try:
                    node.clear_cache()
                except Exception as e:
                    logger.warning(f"Failed to clear cache for node '{node.name}': {e}")
        # Clear central calculation cache
        self.clear_calculation_cache()
        logger.debug("Cleared central calculation cache.")

    def clear_calculation_cache(self) -> None:
        """Clear the graph's internal calculation cache.

        Returns:
            None

        Examples:
            >>> graph.clear_calculation_cache()
        """
        self._cache.clear()
        logger.debug("Cleared graph calculation cache.")

    def clear(self) -> None:
        """Reset the graph by clearing nodes, periods, metrics, and caches.

        Returns:
            None

        Examples:
            >>> graph.clear()
        """
        self._nodes = {}
        self._periods = []
        self._cache = {}
        self._metric_names = set()
        logger.info("Graph cleared: nodes, periods, metrics, and caches reset.")

    def add_financial_statement_item(self, name: str, values: dict[str, float]) -> FinancialStatementItemNode:
        """Add a basic financial statement item (data node) to the graph.

        Args:
            name: Unique name for the financial statement item node.
            values: Mapping of period strings to float values for this item.

        Returns:
            The newly created `FinancialStatementItemNode`.

        Raises:
            NodeError: If a node with the same name already exists.
            TypeError: If `values` is not a dict or contains invalid types.

        Examples:
            >>> item_node = graph.add_financial_statement_item("SG&A", {"2023": 50.0})
            >>> item_node.get_value("2023")
            50.0
        """
        if name in self._nodes:
            raise NodeError("Node with name already exists", node_id=name)
        if not isinstance(values, dict):
            raise TypeError("Values must be provided as a dict[str, float]")
        # Create a new financial statement item node
        new_node = self._node_factory.create_financial_statement_item(
            name=name, values=values.copy()
        )
        self._nodes[name] = new_node
        # Update periods from node values
        self.add_periods(list(values.keys()))
        logger.info(f"Added FinancialStatementItemNode '{name}' with periods {list(values.keys())}")
        return new_node

    def update_financial_statement_item(
        self, name: str, values: dict[str, float], replace_existing: bool = False
    ) -> FinancialStatementItemNode:
        """Update values for an existing financial statement item node.

        Args:
            name: Name of the existing financial statement item node.
            values: Mapping of new period strings to float values.
            replace_existing: If True, replace existing values entirely; otherwise merge.

        Returns:
            The updated `FinancialStatementItemNode`.

        Raises:
            NodeError: If the node does not exist.
            TypeError: If the node is not a `FinancialStatementItemNode` or `values` is not a dict.

        Examples:
            >>> graph.update_financial_statement_item("SG&A", {"2024": 60.0})
        """
        node = self.manipulator.get_node(name)
        if node is None:
            raise NodeError("Node not found", node_id=name)
        if not isinstance(node, FinancialStatementItemNode):
            raise TypeError(f"Node '{name}' is not a FinancialStatementItemNode")
        if not isinstance(values, dict):
            raise TypeError("Values must be provided as a dict[str, float]")
        if replace_existing:
            node.values = values.copy()
        else:
            node.values.update(values)
        self.add_periods(list(values.keys()))
        logger.info(f"Updated FinancialStatementItemNode '{name}' with periods {list(values.keys())}; replace_existing={replace_existing}")
        return node

    def get_financial_statement_items(self) -> list[Node]:
        """Retrieve all financial statement item nodes from the graph.

        Returns:
            A list of `FinancialStatementItemNode` objects currently in the graph.

        Examples:
            >>> items = graph.get_financial_statement_items()
        """
        from fin_statement_model.core.nodes import (
            FinancialStatementItemNode,
        )  # Keep import local as it's specific

        return [
            node
            for node in self.nodes.values()
            if isinstance(node, FinancialStatementItemNode)
        ]

    def __repr__(self) -> str:
        """Provide a concise, developer-friendly string representation of the graph.

        Summarize total nodes, FS items, calculations, dependencies, and periods.

        Returns:
            A string summarizing the graph's structure and contents.

        Examples:
            >>> repr(graph)
        """
        from fin_statement_model.core.nodes import (
            FinancialStatementItemNode,
        )  # Keep import local

        num_nodes = len(self.nodes)
        periods_str = ", ".join(map(repr, self.periods)) if self.periods else "None"

        fs_item_count = 0
        calc_node_count = 0
        other_node_count = 0
        dependencies_count = 0

        for node in self.nodes.values():
            if isinstance(node, FinancialStatementItemNode):
                fs_item_count += 1
            elif node.has_calculation():
                calc_node_count += 1
                # Prioritize get_dependencies if available, otherwise check inputs
                if hasattr(node, "get_dependencies"):
                    try:
                        dependencies_count += len(node.get_dependencies())
                    except Exception as e:
                        logger.warning(
                            f"Error calling get_dependencies for node '{node.name}': {e}"
                        )
                elif hasattr(node, "inputs"):
                    try:
                        if isinstance(node.inputs, list):
                            # Ensure inputs are nodes with names
                            dep_names = [inp.name for inp in node.inputs if hasattr(inp, "name")]
                            dependencies_count += len(dep_names)
                        elif isinstance(node.inputs, dict):
                            # Assume keys are dependency names for dict inputs
                            dependencies_count += len(node.inputs)
                    except Exception as e:
                        logger.warning(f"Error processing inputs for node '{node.name}': {e}")
            else:
                other_node_count += 1

        repr_parts = [
            f"Total Nodes: {num_nodes}",
            f"FS Items: {fs_item_count}",
            f"Calculations: {calc_node_count}",
        ]
        if other_node_count > 0:
            repr_parts.append(f"Other: {other_node_count}")
        repr_parts.append(f"Dependencies: {dependencies_count}")
        repr_parts.append(f"Periods: [{periods_str}]")

        return f"<{type(self).__name__}({', '.join(repr_parts)})>"

    def has_cycle(self, source_node: Node, target_node: Node) -> bool:
        """Check if a cycle exists from a source node to a target node.

        This method delegates to GraphTraverser.breadth_first_search to determine
        if `target_node` is reachable from `source_node` via dependencies, indicating
        that adding an edge from `target_node` to `source_node` would create a cycle.

        Args:
            source_node: The starting node for cycle detection.
            target_node: The node to detect return path to.

        Returns:
            True if a cycle exists, False otherwise.
        """
        if source_node.name not in self._nodes or target_node.name not in self._nodes:
            return False

        # Use BFS to check reachability via predecessors (dependencies)
        bfs_levels = self.traverser.breadth_first_search(source_node.name, direction="predecessors")
        reachable_nodes = {n for level in bfs_levels for n in level}
        return target_node.name in reachable_nodes

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its name.

        Args:
            name: The unique name of the node to retrieve.

        Returns:
            The `Node` instance if found, else None.

        Examples:
            >>> node = graph.get_node("Revenue")
        """
        return self.manipulator.get_node(name)

    def add_node(self, node: Node) -> None:
        """Add a node to the graph, replacing any existing node with the same name.

        Args:
            node: The `Node` instance to add.

        Returns:
            None

        Examples:
            >>> graph.add_node(custom_node)
        """
        return self.manipulator.add_node(node)

    def remove_node(self, node_name: str) -> None:
        """Remove a node from the graph by name, updating dependencies.

        Args:
            node_name: The name of the node to remove.

        Returns:
            None

        Examples:
            >>> graph.remove_node("OldItem")
        """
        return self.manipulator.remove_node(node_name)

    def replace_node(self, node_name: str, new_node: Node) -> None:
        """Replace an existing node with a new node instance.

        Args:
            node_name: Name of the node to replace.
            new_node: The new `Node` instance to substitute.

        Returns:
            None

        Examples:
            >>> graph.replace_node("Item", updated_node)
        """
        return self.manipulator.replace_node(node_name, new_node)

    def has_node(self, node_id: str) -> bool:
        """Check if a node with the given ID exists in the graph.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> graph.has_node("Revenue")
        """
        return self.manipulator.has_node(node_id)

    def set_value(self, node_id: str, period: str, value: float) -> None:
        """Set or update the value for a node in a specific period.

        Args:
            node_id: The name of the node.
            period: The period identifier to set the value for.
            value: The float value to assign.

        Returns:
            None

        Raises:
            ValueError: If the period is not recognized by the graph.
            NodeError: If the node does not exist.
            TypeError: If the node does not support setting a value.

        Examples:
            >>> graph.set_value("SG&A", "2024", 55.0)
        """
        return self.manipulator.set_value(node_id, period, value)

    def topological_sort(self) -> list[str]:
        """Perform a topological sort of all graph nodes.

        Returns:
            A list of node IDs in topological order.

        Raises:
            ValueError: If a cycle is detected in the graph.

        Examples:
            >>> order = graph.topological_sort()
        """
        return self.traverser.topological_sort()

    def get_calculation_nodes(self) -> list[str]:
        """Get all calculation node IDs in the graph.

        Returns:
            A list of node names that have associated calculations.

        Examples:
            >>> graph.get_calculation_nodes()
        """
        return self.traverser.get_calculation_nodes()

    def get_dependencies(self, node_id: str) -> list[str]:
        """Get the direct predecessor node IDs (dependencies) for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node depends on.

        Examples:
            >>> graph.get_dependencies("GrossProfit")
        """
        return self.traverser.get_dependencies(node_id)

    def get_dependency_graph(self) -> dict[str, list[str]]:
        """Get the full dependency graph mapping of node IDs to their inputs.

        Returns:
            A dict mapping each node ID to a list of its dependency node IDs.

        Examples:
            >>> graph.get_dependency_graph()
        """
        return self.traverser.get_dependency_graph()

    def detect_cycles(self) -> list[list[str]]:
        """Detect all cycles in the graph's dependency structure.

        Returns:
            A list of cycles, each represented as a list of node IDs.

        Examples:
            >>> graph.detect_cycles()
        """
        return self.traverser.detect_cycles()

    def validate(self) -> list[str]:
        """Validate the graph structure for errors such as cycles or missing nodes.

        Returns:
            A list of validation error messages, empty if valid.

        Examples:
            >>> graph.validate()
        """
        return self.traverser.validate()

    def breadth_first_search(self, start_node: str, direction: str = "successors") -> list[str]:
        """Perform a breadth-first search (BFS) traversal of the graph.

        Args:
            start_node: The starting node ID for BFS.
            direction: Either 'successors' or 'predecessors' to traverse.

        Returns:
            A nested list of node IDs per BFS level.

        Raises:
            ValueError: If `direction` is not 'successors' or 'predecessors'.

        Examples:
            >>> graph.breadth_first_search("Revenue", "successors")
        """
        return self.traverser.breadth_first_search(start_node, direction)

    def get_direct_successors(self, node_id: str) -> list[str]:
        """Get immediate successor node IDs for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that directly follow the given node.

        Examples:
            >>> graph.get_direct_successors("Revenue")
        """
        return self.traverser.get_direct_successors(node_id)

    def get_direct_predecessors(self, node_id: str) -> list[str]:
        """Get immediate predecessor node IDs (inputs) for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node directly depends on.

        Examples:
            >>> graph.get_direct_predecessors("GrossProfit")
        """
        return self.traverser.get_direct_predecessors(node_id)

# --- END FILE: fin_statement_model/core/graph/graph.py ---

# --- START FILE: fin_statement_model/core/graph/manipulator.py ---
"""Provide graph manipulation utilities.

This module defines the GraphManipulator class, encapsulating node-level mutation helpers for Graph.
"""

import logging
from typing import Optional, Any
from fin_statement_model.core.errors import NodeError
from fin_statement_model.core.nodes import Node

logger = logging.getLogger(__name__)

class GraphManipulator:
    """Encapsulate node-level mutation helpers for Graph.

    Attributes:
        graph: The Graph instance this manipulator operates on.
    """

    def __init__(self, graph: Any) -> None:
        """Initialize the GraphManipulator with a Graph reference.

        Args:
            graph: The Graph instance to manipulate.
        """
        self.graph = graph

    def add_node(self, node: Node) -> None:
        """Add a node to the graph, replacing any existing node with the same name.

        Args:
            node: The Node instance to add.

        Returns:
            None

        Raises:
            TypeError: If the provided object is not a Node instance.

        Examples:
            >>> manipulator.add_node(node)
        """
        if not isinstance(node, Node):
            raise TypeError(f"Object {node} is not a valid Node instance.")
        if self.has_node(node.name):
            self.remove_node(node.name)
        self.graph._nodes[node.name] = node

    def _update_calculation_nodes(self) -> None:
        """Refresh input references for all calculation nodes after structure changes.

        This method re-resolves `input_names` to current Node objects and clears
        individual node caches.

        Returns:
            None
        """
        for nd in self.graph._nodes.values():
            if nd.has_calculation() and hasattr(nd, "input_names") and nd.input_names:
                try:
                    resolved_inputs: list[Node] = []
                    for name in nd.input_names:
                        input_node = self.get_node(name)
                        if input_node is None:
                            raise NodeError(
                                f"Input node '{name}' not found for calculation node '{nd.name}'"
                            )
                        resolved_inputs.append(input_node)
                    nd.inputs = resolved_inputs
                    if hasattr(nd, "clear_cache"):
                        nd.clear_cache()
                except NodeError:
                    logger.exception(f"Error updating inputs for node '{nd.name}'")
                except AttributeError:
                    logger.warning(
                        f"Node '{nd.name}' has input_names but no 'inputs' attribute to update."
                    )

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its unique name.

        Args:
            name: The unique node name to retrieve.

        Returns:
            The Node instance if found, else None.

        Examples:
            >>> manipulator.get_node("Revenue")
        """
        return self.graph._nodes.get(name)

    def replace_node(self, node_name: str, new_node: Node) -> None:
        """Replace an existing node with a new one, ensuring consistency.

        Args:
            node_name: Name of the node to replace.
            new_node: The new Node instance; its name must match `node_name`.

        Returns:
            None

        Raises:
            NodeError: If `node_name` does not exist.
            ValueError: If `new_node.name` does not match `node_name`.

        Examples:
            >>> manipulator.replace_node("Revenue", updated_node)
        """
        if not self.has_node(node_name):
            raise NodeError(f"Node '{node_name}' not found, cannot replace.")
        if node_name != new_node.name:
            raise ValueError("New node name must match the name of the node being replaced.")
        self.remove_node(node_name)
        self.add_node(new_node)

    def has_node(self, node_id: str) -> bool:
        """Check if a node with the given ID exists.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> manipulator.has_node("Revenue")
        """
        return node_id in self.graph._nodes

    def remove_node(self, node_name: str) -> None:
        """Remove a node from the graph and update calculation nodes.

        Args:
            node_name: The name of the node to remove.

        Returns:
            None

        Examples:
            >>> manipulator.remove_node("OldItem")
        """
        if not self.has_node(node_name):
            return
        self.graph._nodes.pop(node_name, None)
        self._update_calculation_nodes()

    def set_value(self, node_id: str, period: str, value: float) -> None:
        """Set the value for a specific node and period, clearing all caches.

        Args:
            node_id: The name of the node.
            period: The time period identifier.
            value: The numeric value to assign.

        Returns:
            None

        Raises:
            ValueError: If `period` is not recognized by the graph.
            NodeError: If the node does not exist.
            TypeError: If the node does not support setting a value.

        Examples:
            >>> manipulator.set_value("Revenue", "2023", 1100.0)
        """
        if period not in self.graph._periods:
            raise ValueError(f"Period '{period}' not in graph periods")
        nd = self.get_node(node_id)
        if not nd:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if not hasattr(nd, "set_value"):
            raise TypeError(
                f"Node '{node_id}' of type {type(nd).__name__} does not support set_value."
            )
        nd.set_value(period, value)
        self.graph.clear_all_caches()

    def clear_all_caches(self) -> None:
        """Clear caches associated with individual nodes in the graph.

        Returns:
            None

        Examples:
            >>> manipulator.clear_all_caches()
        """
        for nd in self.graph._nodes.values():
            if hasattr(nd, "clear_cache"):
                nd.clear_cache()

# --- END FILE: fin_statement_model/core/graph/manipulator.py ---

# --- START FILE: fin_statement_model/core/graph/traverser.py ---
"""Provide graph traversal and validation utilities.

This module defines the GraphTraverser class, encapsulating read-only graph traversal helpers.
"""

import logging
from typing import Optional, Any
from collections import deque

from fin_statement_model.core.errors import NodeError
from fin_statement_model.core.nodes import Node

logger = logging.getLogger(__name__)

class GraphTraverser:
    """Encapsulate traversal and validation helpers for Graph.

    Attributes:
        graph: The Graph instance this traverser operates on.
    """

    def __init__(self, graph: Any) -> None:
        """Initialize the GraphTraverser with a Graph reference.

        Args:
            graph: The Graph instance to traverse.
        """
        self.graph = graph

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its name.

        Args:
            name: The unique name of the node to retrieve.

        Returns:
            The Node instance if found, else None.

        Examples:
            >>> traverser.get_node("Revenue")
        """
        return self.graph.manipulator.get_node(name)

    def has_node(self, node_id: str) -> bool:
        """Check if a node exists in the graph.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> traverser.has_node("Revenue")
        """
        return self.graph.manipulator.has_node(node_id)

    @property
    def nodes(self) -> dict[str, Node]:
        """Access the full node registry dictionary.

        Returns:
            A dict mapping node names to Node instances.

        Examples:
            >>> list(traverser.nodes.keys())
        """
        return self.graph.nodes

    def get_direct_successors(self, node_id: str) -> list[str]:
        """Get immediate successor node IDs for a given node.

        Args:
            node_id: The name of the node whose successors to retrieve.

        Returns:
            A list of node IDs that directly follow the given node.

        Examples:
            >>> traverser.get_direct_successors("Revenue")
        """
        successors: list[str] = []
        for other_id, node in self.nodes.items():
            if hasattr(node, "inputs") and any(inp.name == node_id for inp in node.inputs):
                successors.append(other_id)
        return successors

    def get_direct_predecessors(self, node_id: str) -> list[str]:
        """Get immediate predecessor node IDs (dependencies) for a given node.

        Args:
            node_id: The name of the node whose dependencies to retrieve.

        Returns:
            A list of node IDs that the given node depends on.

        Raises:
            NodeError: If the node does not exist.

        Examples:
            >>> traverser.get_direct_predecessors("GrossProfit")
        """
        node = self.get_node(node_id)
        if not node:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if hasattr(node, "inputs"):
            return [inp.name for inp in node.inputs]
        return []

    def topological_sort(self) -> list[str]:
        """Perform a topological sort of nodes based on dependencies.

        Returns:
            A list of node IDs in topological order.

        Raises:
            ValueError: If a cycle is detected in the graph.

        Examples:
            >>> traverser.topological_sort()
        """
        in_degree: dict[str, int] = {n: 0 for n in self.nodes}
        adjacency: dict[str, list[str]] = {n: [] for n in self.nodes}
        for name, node in self.nodes.items():
            if hasattr(node, "inputs"):
                for inp in node.inputs:
                    adjacency[inp.name].append(name)
                    in_degree[name] += 1
        queue: list[str] = [n for n, d in in_degree.items() if d == 0]
        topo_order: list[str] = []
        while queue:
            current = queue.pop()
            topo_order.append(current)
            for nbr in adjacency[current]:
                in_degree[nbr] -= 1
                if in_degree[nbr] == 0:
                    queue.append(nbr)
        if len(topo_order) != len(self.nodes):
            raise ValueError("Cycle detected in graph, can't do a valid topological sort.")
        return topo_order

    def get_calculation_nodes(self) -> list[str]:
        """Identify all nodes in the graph that represent calculations.

        Returns:
            A list of node IDs for nodes with calculations.

        Examples:
            >>> traverser.get_calculation_nodes()
        """
        return [node_id for node_id, node in self.nodes.items() if node.has_calculation()]

    def get_dependencies(self, node_id: str) -> list[str]:
        """Retrieve the direct dependencies (inputs) of a specific node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node depends on.

        Raises:
            NodeError: If the node does not exist.

        Examples:
            >>> traverser.get_dependencies("GrossProfit")
        """
        node = self.get_node(node_id)
        if not node:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if hasattr(node, "inputs"):
            return [inp.name for inp in node.inputs]
        return []

    def get_dependency_graph(self) -> dict[str, list[str]]:
        """Construct a representation of the full dependency graph.

        Returns:
            A dict mapping each node ID to its list of dependency node IDs.

        Examples:
            >>> traverser.get_dependency_graph()
        """
        dependencies: dict[str, list[str]] = {}
        for node_id, node in self.nodes.items():
            try:
                if hasattr(node, "inputs"):
                    dependencies[node_id] = [inp.name for inp in node.inputs]
                else:
                    dependencies[node_id] = []
            except NodeError:
                dependencies[node_id] = []
        return dependencies

    def detect_cycles(self) -> list[list[str]]:
        """Detect all cycles present in the graph's dependency structure.

        Returns:
            A list of cycles, each cycle is a list of node IDs forming the cycle.

        Examples:
            >>> traverser.detect_cycles()
        """
        dependency_graph = self.get_dependency_graph()
        visited: set[str] = set()
        rec_stack: set[str] = set()
        cycles: list[list[str]] = []

        def dfs_detect_cycles(n_id: str, path: Optional[list[str]] = None) -> None:
            if path is None:
                path = []
            if n_id in rec_stack:
                cycle_start = path.index(n_id)
                cycle = path[cycle_start:] + [n_id]
                if cycle not in cycles:
                    cycles.append(cycle)
                return
            if n_id in visited:
                return
            visited.add(n_id)
            rec_stack.add(n_id)
            path.append(n_id)
            for dep in dependency_graph.get(n_id, []):
                dfs_detect_cycles(dep, path[:])
            rec_stack.remove(n_id)

        for node_id in self.nodes:
            if node_id not in visited:
                dfs_detect_cycles(node_id)
        return cycles

    def validate(self) -> list[str]:
        """Perform validation checks on the graph structure.

        Returns:
            A list of validation error messages; empty list if graph is valid.

        Examples:
            >>> traverser.validate()
        """
        errors: list[str] = [
            f"Circular dependency detected: {' -> '.join(cycle)}"
            for cycle in self.detect_cycles()
        ]
        errors.extend(
            f"Node '{node_id}' depends on non-existent node '{inp.name}'"
            for node_id, node in self.nodes.items()
            if hasattr(node, "inputs")
            for inp in node.inputs
            if not self.has_node(inp.name)
        )
        return errors

    def breadth_first_search(self, start_node: str, direction: str = "successors") -> list[str]:
        """Perform a breadth-first search (BFS) traversal of the graph.

        Args:
            start_node: The starting node ID for the traversal.
            direction: The traversal direction, either 'successors' or 'predecessors'.

        Returns:
            A list of levels, each level is a list of node IDs visited at that depth.

        Raises:
            ValueError: If `direction` is not 'successors' or 'predecessors'.

        Examples:
            >>> traverser.breadth_first_search("Revenue", "successors")
        """
        if direction not in ["successors", "predecessors"]:
            raise ValueError("Invalid direction. Use 'successors' or 'predecessors'.")

        visited = set()
        queue = deque([start_node])
        visited.add(start_node)
        traversal_order = []

        while queue:
            level_size = len(queue)
            current_level = []

            for _ in range(level_size):
                n_id = queue.popleft()
                current_level.append(n_id)

                if direction == "successors":
                    for successor in self.get_direct_successors(n_id):
                        if successor not in visited:
                            visited.add(successor)
                            queue.append(successor)
                elif direction == "predecessors":
                    for predecessor in self.get_direct_predecessors(n_id):
                        if predecessor not in visited:
                            visited.add(predecessor)
                            queue.append(predecessor)

            traversal_order.append(current_level)

        return traversal_order

# --- END FILE: fin_statement_model/core/graph/traverser.py ---

# --- START FILE: fin_statement_model/core/metrics/__init__.py ---
"""Metrics Subpackage.

Handles definition, loading, and access for financial metrics.
"""

import logging
from pathlib import Path

from .registry import MetricRegistry, HAS_YAML

logger = logging.getLogger(__name__)

# --- Singleton Registry Instance ---
# Create a single instance of the registry for the application lifetime.
metric_registry = MetricRegistry()

# --- Auto-load Built-in Metrics ---
# Determine the path to the built-in metrics directory relative to this file.
_current_dir = Path(__file__).parent
_builtin_dir = _current_dir / "builtin"

# Attempt to load metrics only if PyYAML is installed and the directory exists.
if HAS_YAML:
    if _builtin_dir.is_dir():
        try:
            loaded_count = metric_registry.load_metrics_from_directory(_builtin_dir)
            logger.info(f"Auto-loaded {loaded_count} built-in metrics from {_builtin_dir}")
        except Exception as e:
            # Log error but don't prevent library import if built-ins fail to load
            logger.error(
                f"Failed to auto-load built-in metrics from {_builtin_dir}: {e}",
                exc_info=True,
            )
    else:
        logger.warning(
            f"Built-in metric directory not found: {_builtin_dir}. No built-in metrics loaded."
        )
else:
    logger.warning("PyYAML not installed. Cannot load YAML metrics. Skipping auto-load.")


# --- Public API ---
__all__ = [
    "MetricRegistry",
    "metric_registry",  # Expose the singleton instance
]

# --- END FILE: fin_statement_model/core/metrics/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/registry.py ---
"""Manage loading and accessing metric definitions from YAML files.

This module provides a registry to discover, validate, and retrieve
metric definitions from YAML files and associate them with calculation classes.
"""


import logging
from pathlib import Path
from typing import Any, Union, Callable

# Use a try-except block for the YAML import
try:
    import yaml

    HAS_YAML = True
except ImportError:
    HAS_YAML = False

from fin_statement_model.core.errors import ConfigurationError
from fin_statement_model.core.nodes.metric_node import MetricCalculation

logger = logging.getLogger(__name__)

# Registry mapping metric type strings to MetricCalculation classes
_registry: dict[str, type[MetricCalculation]] = {}


class MetricRegistry:
    """Manage loading and accessing metric definitions from YAML files.

    This includes discovering YAML definitions, validating their structure,
    and providing retrieval methods by metric ID.
    """

    _REQUIRED_FIELDS: list[str] = ["inputs", "formula", "description", "name"]

    def __init__(self):
        """Initialize the MetricRegistry with an empty metrics store.

        Examples:
            >>> registry = MetricRegistry()
            >>> len(registry)
            0
        """
        self._metrics: dict[str, dict[str, Any]] = {}
        logger.info("MetricRegistry initialized.")

    def load_metrics_from_directory(self, directory_path: Union[str, Path]) -> int:
        """Load all metric definitions from a directory.

        This method searches for '*.yaml' files, validates their content,
        and stores them in the registry.

        Args:
            directory_path: Path to the directory containing metric YAML files.

        Returns:
            The number of metrics successfully loaded.

        Raises:
            ImportError: If PyYAML is not installed.
            FileNotFoundError: If the directory_path does not exist.
            ConfigurationError: If a YAML file is invalid or missing required fields.

        Examples:
            >>> registry = MetricRegistry()
            >>> count = registry.load_metrics_from_directory("./metrics")
            >>> print(f"Loaded {count} metrics.")
        """
        if not HAS_YAML:
            logger.error("PyYAML is required to load metrics from YAML files. Please install it.")
            raise ImportError("PyYAML is required to load metrics from YAML files.")

        dir_path = Path(directory_path)
        if not dir_path.is_dir():
            logger.error(f"Metric directory not found: {dir_path}")
            raise FileNotFoundError(f"Metric directory not found: {dir_path}")

        logger.info(f"Loading metrics from directory: {dir_path}")
        loaded_count = 0
        for filepath in dir_path.glob("*.yaml"):
            metric_id = filepath.stem  # Use filename without extension as ID (e.g., "gross_profit")
            logger.debug(f"Attempting to load metric '{metric_id}' from {filepath}")
            try:
                with open(filepath, encoding="utf-8") as f:
                    data = yaml.safe_load(f)

                if not isinstance(data, dict):
                    raise TypeError("YAML content must be a dictionary.")

                # Validate required fields
                missing_fields = [field for field in self._REQUIRED_FIELDS if field not in data]
                if missing_fields:
                    raise ValueError(f"Missing required fields: {missing_fields}")

                # Basic type validation for key fields
                if not isinstance(data["name"], str):
                    raise TypeError("'name' field must be a string.")
                if not isinstance(data["description"], str):
                    raise TypeError("'description' field must be a string.")
                if not isinstance(data["inputs"], list):
                    raise TypeError("'inputs' field must be a list.")
                if not isinstance(data["formula"], str):
                    raise TypeError("'formula' field must be a string.")

                # Store the validated metric definition
                if metric_id in self._metrics:
                    logger.warning(
                        f"Overwriting existing metric definition for '{metric_id}' from {filepath}"
                    )
                self._metrics[metric_id] = data
                logger.debug(f"Successfully loaded and validated metric '{metric_id}'")
                loaded_count += 1

            except yaml.YAMLError as e:
                logger.exception(f"Error parsing YAML file {filepath}")
                raise ConfigurationError(
                    f"Invalid YAML syntax in {filepath}", config_path=str(filepath)
                ) from e
            except (TypeError, ValueError) as e:
                logger.exception(f"Invalid metric definition structure in {filepath}")
                raise ConfigurationError(
                    f"Invalid metric structure in {filepath}: {e}",
                    config_path=str(filepath),
                ) from e
            except Exception as e:
                logger.error(
                    f"Unexpected error loading metric from {filepath}",
                    exc_info=True,
                )
                raise ConfigurationError(
                    f"Failed to load metric from {filepath} due to: {e}",
                    config_path=str(filepath),
                ) from e

        logger.info(f"Successfully loaded {loaded_count} metrics from {dir_path}.")
        return loaded_count

    def get(self, metric_id: str) -> dict[str, Any]:
        """Retrieve a loaded metric definition by its ID.

        Args:
            metric_id: Identifier of the metric (filename stem).

        Returns:
            The dictionary containing the metric definition.

        Raises:
            KeyError: If the metric_id is not found in the registry.

        Examples:
            >>> definition = registry.get("gross_profit")
            >>> print(definition["formula"])
        """
        try:
            return self._metrics[metric_id]
        except KeyError:
            logger.warning(f"Metric ID '{metric_id}' not found in registry.")
            raise KeyError(f"Metric ID '{metric_id}' not found. Available: {self.list_metrics()}")

    def list_metrics(self) -> list[str]:
        """Get a sorted list of all loaded metric IDs.

        Returns:
            A sorted list of available metric IDs.

        Examples:
            >>> registry.list_metrics()
            ['current_ratio', 'debt_equity_ratio']
        """
        return sorted(self._metrics.keys())

    def __len__(self) -> int:
        """Return the number of loaded metrics.

        Returns:
            The count of metrics loaded into the registry.

        Examples:
            >>> len(registry)
            5
        """
        return len(self._metrics)

    def __contains__(self, metric_id: str) -> bool:
        """Check if a metric ID exists in the registry.

        Args:
            metric_id: The metric identifier to check.

        Returns:
            True if the metric is present, False otherwise.

        Examples:
            >>> 'current_ratio' in registry
        """
        return metric_id in self._metrics

    @classmethod
    def register(cls, name: str) -> Callable[[type[MetricCalculation]], type[MetricCalculation]]:
        """Register a new metric calculation class under a given name.

        Args:
            name: The identifier for the metric calculation type.

        Returns:
            A decorator that registers the decorated class under `name`.

        Examples:
            >>> @MetricRegistry.register("custom_metric")
            ... class CustomMetric(MetricCalculation):
            ...     pass
        """

        def decorator(metric_class: type[MetricCalculation]) -> type[MetricCalculation]:
            _registry[name] = metric_class
            return metric_class

        return decorator

# --- END FILE: fin_statement_model/core/metrics/registry.py ---

# --- START FILE: fin_statement_model/core/node_factory.py ---
"""Provide a factory for creating nodes in the financial statement model.

This module centralizes node creation logic and ensures consistent initialization
for different types of nodes used in the financial statement model.
"""

import logging
from typing import Callable, Any, Union, Optional, ClassVar

# Force import of strategies package to ensure registration happens

from .nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    MetricCalculationNode,
    CustomCalculationNode,
)
from .nodes.forecast_nodes import (
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
)

# Force import of calculations package to ensure registration happens
from fin_statement_model.core.calculations import Registry, Calculation

# Configure logging
logger = logging.getLogger(__name__)


class NodeFactory:
    """Provide a factory for creating nodes in the financial statement model.

    This class centralizes node creation for financial statement items,
    calculations, metrics, forecasts, and custom logic.

    Attributes:
        _calculation_methods: Maps simple string keys (e.g., 'addition') to
            the class names of Calculation implementations registered in the
            `Registry`. This allows creating CalculationNodes without
            directly importing Calculation classes.
    """

    # Mapping of calculation type strings to Calculation class names (keys in the Registry)
    _calculation_methods: ClassVar[dict[str, str]] = {
        "addition": "AdditionCalculation",
        "subtraction": "SubtractionCalculation",
        "multiplication": "MultiplicationCalculation",
        "division": "DivisionCalculation",
        "weighted_average": "WeightedAverageCalculation",
        "custom_formula": "CustomFormulaCalculation",
    }

    # Mapping from node type names to Node classes
    _node_types: ClassVar[dict[str, type[Node]]] = {
        "financial_statement_item": FinancialStatementItemNode,
        "metric_calculation": MetricCalculationNode,
    }

    @classmethod
    def create_financial_statement_item(
        cls, name: str, values: dict[str, float]
    ) -> FinancialStatementItemNode:
        """Create a FinancialStatementItemNode representing a base financial item.

        This node holds historical or projected values for a specific
        line item (e.g., Revenue, COGS) over different periods.

        Args:
            name: Identifier for the node (e.g., "Revenue").
            values: Mapping of period identifiers to numerical values.

        Returns:
            A FinancialStatementItemNode initialized with the provided values.

        Raises:
            ValueError: If the provided name is empty or not a string.

        Examples:
            >>> revenue_node = NodeFactory.create_financial_statement_item(
            ...     name="Revenue",
            ...     values={"2023": 1000.0, "2024": 1100.0}
            ... )
            >>> revenue_node.get_value("2023")
            1000.0
        """
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        logger.debug(f"Creating financial statement item node: {name}")
        return FinancialStatementItemNode(name, values)

    @classmethod
    def create_calculation_node(
        cls,
        name: str,
        inputs: list[Node],
        calculation_type: str,
        **calculation_kwargs: Any,
    ) -> CalculationNode:
        """Create a CalculationNode using a pre-defined calculation.

        This method resolves a calculation class from a calculation_type key,
        instantiates it with optional parameters, and wraps it in
        a CalculationNode.

        Args:
            name: Identifier for the calculation node instance.
            inputs: List of Node instances serving as inputs to the calculation.
            calculation_type: Key for the desired calculation in the registry.
            **calculation_kwargs: Additional parameters for the calculation constructor.

        Returns:
            A CalculationNode configured with the selected calculation.

        Raises:
            ValueError: If name is invalid, inputs list is empty, or the
                calculation_type is unrecognized.
            TypeError: If the calculation cannot be instantiated with given kwargs.

        Examples:
            >>> gross_profit = NodeFactory.create_calculation_node(
            ...     name="GrossProfit",
            ...     inputs=[revenue, cogs],
            ...     calculation_type="subtraction"
            ... )
        """
        # Validate inputs
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        if not inputs:
            raise ValueError("Calculation node must have at least one input")

        # Check if the calculation type maps to a known calculation name
        if calculation_type not in cls._calculation_methods:
            valid_types = list(cls._calculation_methods.keys())
            raise ValueError(
                f"Invalid calculation type: '{calculation_type}'. Valid types are: {valid_types}"
            )

        # Get the calculation name and resolve the calculation class from the registry
        calculation_name = cls._calculation_methods[calculation_type]
        try:
            calculation_cls: type[Calculation] = Registry.get(calculation_name)
        except KeyError:
            raise ValueError(
                f"Calculation '{calculation_name}' not found in Registry for type '{calculation_type}'"
            )

        # Instantiate the calculation, passing any extra kwargs
        try:
            calculation_instance = calculation_cls(**calculation_kwargs)
        except TypeError as e:
            logger.exception(
                f"Failed to instantiate calculation '{calculation_name}' with kwargs {calculation_kwargs}"
            )
            raise TypeError(
                f"Could not instantiate calculation '{calculation_name}' for node '{name}'. "
                f"Check required arguments for {calculation_cls.__name__}. Provided kwargs: {calculation_kwargs}"
            ) from e

        # Create and return a CalculationNode with the instantiated calculation
        logger.debug(
            f"Creating calculation node '{name}' with '{calculation_name}' calculation."
        )
        return CalculationNode(name, inputs, calculation_instance)

    @classmethod
    def create_metric_node(
        cls, name: str, metric_name: str, input_nodes: dict[str, Node]
    ) -> MetricCalculationNode:
        """Create a MetricCalculationNode based on a pre-defined metric definition.

        This node encapsulates a standard financial metric lookup and
        calculation defined externally (e.g., in metrics YAML files).

        Args:
            name: Identifier for this metric node instance.
            metric_name: Key of the metric definition to apply.
            input_nodes: Mapping of required input names to Node instances.

        Returns:
            A MetricCalculationNode configured with the metric definition.

        Raises:
            ValueError: If name, metric_name, or inputs are invalid.
            TypeError: If input_nodes is not a dict of Node instances.

        Examples:
            >>> current_ratio = NodeFactory.create_metric_node(
            ...     name="CurrentRatio",
            ...     metric_name="current_ratio",
            ...     input_nodes={
            ...         "current_assets": assets,
            ...         "current_liabilities": liabilities
            ...     }
            ... )
        """
        logger.debug(f"Attempting to create metric node '{name}' for metric '{metric_name}'")

        # Basic input validation
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")
        if not isinstance(input_nodes, dict):
            raise TypeError("input_nodes must be a dictionary of Node objects.")

        # The MetricCalculationNode constructor now handles definition loading,
        # validation of the definition, and validation of input_nodes.
        try:
            node = MetricCalculationNode(
                name=name, metric_name=metric_name, input_nodes=input_nodes
            )
            logger.info(
                f"Successfully created MetricCalculationNode '{name}' for metric '{metric_name}'"
            )
            return node
        except (ValueError, TypeError):
            logger.exception(
                f"Failed to create MetricCalculationNode '{name}' for metric '{metric_name}'"
            )
            # Re-raise the specific error from the constructor
            raise
        else:
            return node

    @classmethod
    def create_forecast_node(
        cls,
        name: str,
        base_node: Node,
        base_period: str,
        forecast_periods: list[str],
        forecast_type: str,
        growth_params: Union[float, list[float], Callable[[], float]],
    ) -> Node:
        """Create a forecast node of the specified type using core forecast classes.

        Args:
            name: Custom name for the forecast node.
            base_node: The Node instance to base projections on.
            base_period: Period identifier providing the base value.
            forecast_periods: List of periods for which to forecast.
            forecast_type: Forecast method ('fixed', 'curve', 'statistical',
                'average', 'historical_growth').
            growth_params: Parameters controlling forecast behavior (float,
                list of floats, or callable). Ignored for 'average' and 'historical_growth'.

        Returns:
            A Node instance implementing the chosen forecast.

        Raises:
            ValueError: If an unsupported forecast_type is provided.

        Examples:
            >>> forecast = NodeFactory.create_forecast_node(
            ...     name="RevForecast",
            ...     base_node=revenue,
            ...     base_period="2023",
            ...     forecast_periods=["2024", "2025"],
            ...     forecast_type="fixed",
            ...     growth_params=0.05
            ... )
        """
        # Instantiate the appropriate forecast node
        if forecast_type == "fixed":
            node = FixedGrowthForecastNode(base_node, base_period, forecast_periods, growth_params)
        elif forecast_type == "curve":
            node = CurveGrowthForecastNode(base_node, base_period, forecast_periods, growth_params)
        elif forecast_type == "statistical":
            node = StatisticalGrowthForecastNode(base_node, base_period, forecast_periods, growth_params)
        elif forecast_type == "average":
            node = AverageValueForecastNode(base_node, base_period, forecast_periods)
        elif forecast_type == "historical_growth":
            node = AverageHistoricalGrowthForecastNode(base_node, base_period, forecast_periods)
        else:
            raise ValueError(f"Invalid forecast type: {forecast_type}")

        # Override forecast node's name to match factory 'name' argument
        node.name = name
        logger.debug(f"Forecast node created with custom name: {name} (original: {base_node.name})")
        return node

    @classmethod
    def _create_custom_node_from_callable(
        cls,
        name: str,
        inputs: list[Node],
        formula: Callable,
        description: Optional[str] = None,
    ) -> CustomCalculationNode:
        """Create a CustomCalculationNode using a Python callable for the calculation logic.

        This supports ad-hoc or complex calculations not covered by standard
        strategies or metrics. The `formula` callable will be invoked with
        input node values at calculation time.

        Note:
            Renamed from `create_metric_node` to avoid confusion with metric-based nodes.

        Args:
            name: Identifier for the custom calculation node.
            inputs: List of Node instances providing values to the formula.
            formula: Callable that computes a value from input node values.
            description: Optional description of the calculation logic.

        Returns:
            A CustomCalculationNode configured with the provided formula.

        Raises:
            ValueError: If name is empty or not a string.
            TypeError: If formula is not callable or inputs contain non-Node items.

        Examples:
            >>> def complex_tax_logic(revenue, expenses, tax_rate_node):
            ...     profit = revenue - expenses
            ...     if profit <= 0:
            ...         return 0.0
            ...     tax_rate = tax_rate_node
            ...     return profit * tax_rate
            ...
            >>> tax_node = NodeFactory._create_custom_node_from_callable(
            ...     name="CalculatedTaxes",
            ...     inputs=[revenue_node, expenses_node, tax_rate_schedule_node],
            ...     formula=complex_tax_logic,
            ...     description="Calculates income tax based on profit and a variable rate."
            ... )

            Using a lambda for a simple ratio:
            >>> quick_ratio_node = NodeFactory._create_custom_node_from_callable(
            ...    name="QuickRatioCustom",
            ...    inputs=[cash_node, receivables_node, current_liabilities_node],
            ...    formula=lambda cash, rec, liab: (cash + rec) / liab if liab else 0
            ... )
        """
        # Validate inputs
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        if not inputs:
            # Allowing no inputs might be valid for some custom functions (e.g., constants)
            # Reconsider if this check is always needed here.
            logger.warning(f"Creating CustomCalculationNode '{name}' with no inputs.")
            # raise ValueError("Custom node must have at least one input")

        if not callable(formula):
            raise TypeError("Formula must be a callable function")
        if not all(isinstance(i, Node) for i in inputs):
            raise TypeError("All items in inputs must be Node instances.")

        # Use the imported CustomCalculationNode
        logger.debug(f"Creating CustomCalculationNode: {name} using provided callable.")
        return CustomCalculationNode(name, inputs, formula_func=formula, description=description)

    # Consider adding a method for creating FormulaCalculationNode if needed directly
    # @classmethod
    # def create_formula_node(cls, name: str, inputs: Dict[str, Node], formula: str) -> FormulaCalculationNode:
    #     ...

# --- END FILE: fin_statement_model/core/node_factory.py ---

# --- START FILE: fin_statement_model/core/nodes/__init__.py ---
"""Core Node Implementations for the Financial Statement Model.

This package exports the base `Node` class and various concrete node types
used to build the financial model graph. These include:

- Data Nodes:
    - `FinancialStatementItemNode`: Stores raw numerical data for specific periods.

- Calculation Nodes:
    - `FormulaCalculationNode`: Calculates based on mathematical string formulas.
    - `CalculationNode`: Uses a calculation object for calculation logic.
    - `MetricCalculationNode`: Calculates based on registered metric definitions.
    - `CustomCalculationNode`: Uses arbitrary Python functions for calculation.

- Statistical Nodes:
    - `YoYGrowthNode`: Calculates year-over-year percentage growth.
    - `MultiPeriodStatNode`: Computes statistics (mean, stddev) over multiple periods.
    - `TwoPeriodAverageNode`: Calculates the average over two specific periods.

The `__all__` list defines the public API of this package.
"""

# Core node package public interface: re-export submodules
from .base import Node
from .item_node import FinancialStatementItemNode

# Imports from consolidated files
from .calculation_nodes import (
    FormulaCalculationNode,
    CalculationNode,
    MetricCalculationNode,
    CustomCalculationNode,
)
from .stats_nodes import (
    YoYGrowthNode,
    MultiPeriodStatNode,
    TwoPeriodAverageNode,
)
from .forecast_nodes import (
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    CustomGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
)

__all__ = [
    "AverageHistoricalGrowthForecastNode",
    "AverageValueForecastNode",
    "CalculationNode",
    "CurveGrowthForecastNode",
    "CustomCalculationNode",
    "CustomGrowthForecastNode",
    "FinancialStatementItemNode",
    "FixedGrowthForecastNode",
    "ForecastNode",
    "FormulaCalculationNode",
    "MetricCalculationNode",
    "MultiPeriodStatNode",
    "Node",
    "StatisticalGrowthForecastNode",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]

# --- END FILE: fin_statement_model/core/nodes/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/base.py ---
"""Define the abstract base class for all nodes in the graph.

This module provides the Node base class with interfaces for calculation,
attribute access, and optional caching behavior.
"""

from abc import ABC, abstractmethod


class Node(ABC):
    """Define the abstract base class for graph nodes.

    Provide the essential interface for all nodes in the financial statement
    model graph, including calculation, caching, and attribute access.

    Attributes:
    name (str): Unique identifier for the node instance.
    """

    name: str

    def __init__(self, name: str):
        """Initialize the Node instance with a unique name.

        Args:
            name: Unique identifier for the node. Must be a non-empty string.

        Raises:
            ValueError: If `name` is empty or not a string.

        Examples:
            >>> class Dummy(Node):
            ...     def calculate(self, period): return 0.0
            >>> dn = Dummy("Revenue")
            >>> dn.name
            'Revenue'
        """
        if not isinstance(name, str) or not name:
            raise ValueError("Node name must be a non-empty string.")
        self.name = name

    @abstractmethod
    def calculate(self, period: str) -> float:
        """Calculate the node's value for a specific period.

        This abstract method must be implemented by subclasses to define how to
        determine the node's value for a given time period.

        Args:
            period: The time period identifier for the calculation.

        Returns:
            The calculated float value for the specified period.

        Raises:
            NotImplementedError: If the subclass does not implement this method.

        Examples:
            >>> class Dummy(Node):
            ...     def calculate(self, period): return 100.0
            >>> d = Dummy("Test")
            >>> d.calculate("2023")
            100.0
        """

    def clear_cache(self):
        """Clear cached calculation results for this node.

        Subclasses with caching should override this method to clear their internal cache.

        Returns:
            None

        Examples:
            >>> node.clear_cache()
        """
        # Default: no cache to clear

    def has_attribute(self, attr_name: str) -> bool:
        """Check if the node has a specific attribute.

        Args:
            attr_name: The name of the attribute to check.

        Returns:
            True if the attribute exists, otherwise False.

        Examples:
            >>> node.has_attribute("name")
            True
        """
        return hasattr(self, attr_name)

    def get_attribute(self, attribute_name: str) -> object:
        """Get a named attribute from the node.

        Args:
            attribute_name: The name of the attribute to retrieve.

        Returns:
            The value of the specified attribute.

        Raises:
            AttributeError: If the attribute does not exist.

        Examples:
            >>> node.get_attribute("name")
            'Revenue'
        """
        try:
            return getattr(self, attribute_name)
        except AttributeError:
            raise AttributeError(f"Node '{self.name}' has no attribute '{attribute_name}'")

    def has_value(self, period: str) -> bool:
        """Indicate whether the node stores a direct value for a period.

        Primarily for data-bearing nodes; calculation nodes override has_calculation.

        Args:
            period: The time period to check for a stored value.

        Returns:
            True if a direct value is stored, otherwise False.

        Examples:
            >>> node.has_value("2023")
            False
        """
        return False

    def get_value(self, period: str) -> float:
        """Retrieve the node's directly stored value for a period.

        This method must be overridden by data-bearing nodes to return stored values.

        Args:
            period: The time period string for which to retrieve the value.

        Returns:
            The float value stored for the given period.

        Raises:
            NotImplementedError: If the node does not store direct values.

        Examples:
            >>> node.get_value("2023")
        """
        raise NotImplementedError(f"Node {self.name} does not implement get_value")

    def has_calculation(self) -> bool:
        """Indicate whether this node performs calculation.

        Distinguish calculation nodes from data-holding nodes.

        Returns:
            True if the node performs calculations, otherwise False.

        Examples:
            >>> node.has_calculation()
            False
        """
        return False

# --- END FILE: fin_statement_model/core/nodes/base.py ---

# --- START FILE: fin_statement_model/core/nodes/calculation_nodes.py ---
"""Provide node implementations for performing calculations in the financial statement model.

This module defines the different types of calculation nodes available in the system:
- FormulaCalculationNode: Evaluates a formula expression string (e.g., "a + b / 2")
- CalculationNode: Uses a calculation object for calculation logic
- MetricCalculationNode: Calculates a registered financial metric
- CustomCalculationNode: Calculates using a Python callable/function
"""

import ast
import operator
from typing import Callable, Optional, ClassVar

from fin_statement_model.core.calculations.calculation import Calculation
from fin_statement_model.core.errors import (
    CalculationError,
    ConfigurationError,
    MetricError,
)
from fin_statement_model.core.metrics import metric_registry
from fin_statement_model.core.nodes.base import Node

# === FormulaCalculationNode ===


class FormulaCalculationNode(Node):
    """Calculate a value based on a mathematical formula string.

    Parses and evaluates simple mathematical expressions involving input nodes.
    Supports basic arithmetic operators (+, -, *, /) and unary negation.

    Attributes:
        name (str): Identifier for this node.
        inputs (Dict[str, Node]): Mapping of variable names used in the formula
            to their corresponding input Node instances.
        formula (str): The mathematical expression string to evaluate (e.g., "a + b").
        _ast (ast.Expression): The parsed Abstract Syntax Tree of the formula.

    Examples:
        >>> # Assume revenue and cogs are Node instances
        >>> revenue = FinancialStatementItemNode("revenue", {"2023": 100})
        >>> cogs = FinancialStatementItemNode("cogs", {"2023": 60})
        >>> gross_profit = FormulaCalculationNode(
        ...     "gross_profit",
        ...     inputs={"rev": revenue, "cost": cogs},
        ...     formula="rev - cost"
        ... )
        >>> print(gross_profit.calculate("2023"))
        40.0
    """

    # Supported AST operators mapping to Python operator functions
    OPERATORS: ClassVar[dict[type, Callable]] = {
        ast.Add: operator.add,
        ast.Sub: operator.sub,
        ast.Mult: operator.mul,
        ast.Div: operator.truediv,
        ast.USub: operator.neg,
    }

    def __init__(self, name: str, inputs: dict[str, Node], formula: str):
        """Initialize the FormulaCalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (Dict[str, Node]): Dictionary mapping variable names in the
                formula to the corresponding input nodes.
            formula (str): The mathematical formula string.

        Raises:
            ValueError: If the formula string has invalid syntax.
            TypeError: If any value in `inputs` is not a Node instance.
        """
        super().__init__(name)
        if not isinstance(inputs, dict) or not all(isinstance(n, Node) for n in inputs.values()):
            raise TypeError("FormulaCalculationNode inputs must be a dict of Node instances.")
        self.inputs = inputs
        self.formula = formula
        try:
            # Parse the formula string into an AST expression
            self._ast = ast.parse(formula, mode="eval").body
        except SyntaxError as e:
            raise ValueError(f"Invalid formula syntax for node '{name}': {formula}") from e

    def calculate(self, period: str) -> float:
        """Calculate the node's value for a period by evaluating the formula.

        Args:
            period (str): The time period for which to perform the calculation.

        Returns:
            float: The result of the formula evaluation.

        Raises:
            CalculationError: If an error occurs during evaluation, such as
                an unknown variable, unsupported operator, or if an input node
                fails to provide a numeric value for the period.
        """
        try:
            return self._evaluate(self._ast, period)
        except (ValueError, TypeError, KeyError, ZeroDivisionError) as e:
            raise CalculationError(
                message=f"Error evaluating formula for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={"formula": self.formula, "error": str(e)},
            ) from e

    def _evaluate(self, node: ast.AST, period: str) -> float:
        """Recursively evaluate the parsed AST node for the formula.

        Args:
            node (ast.AST): The current AST node to evaluate.
            period (str): The time period context for the evaluation.

        Returns:
            float: The result of evaluating the AST node.

        Raises:
            TypeError: If a non-numeric constant or input node value is encountered.
            ValueError: If an unknown variable or unsupported operator/syntax is found.
            ZeroDivisionError: If division by zero occurs.
        """
        # Numeric literal (Constant in Python 3.8+, Num in earlier versions)
        if isinstance(node, ast.Constant):
            if isinstance(node.value, (int, float)):
                return float(node.value)
            else:
                raise TypeError(
                    f"Unsupported constant type '{type(node.value).__name__}' in formula for node '{self.name}'"
                )

        # Variable reference
        elif isinstance(node, ast.Name):
            var_name = node.id
            if var_name not in self.inputs:
                raise ValueError(
                    f"Unknown variable '{var_name}' in formula for node '{self.name}'. Available: {list(self.inputs.keys())}"
                )
            input_node = self.inputs[var_name]
            # Recursively calculate the value of the input node
            value = input_node.calculate(period)
            if not isinstance(value, (int, float)):
                raise TypeError(
                    f"Input node '{input_node.name}' (variable '{var_name}') did not return a numeric value for period '{period}'"
                )
            return float(value)

        # Binary operation (e.g., a + b)
        elif isinstance(node, ast.BinOp):
            left_val = self._evaluate(node.left, period)
            right_val = self._evaluate(node.right, period)
            op_type = type(node.op)
            if op_type not in self.OPERATORS:
                raise ValueError(
                    f"Unsupported binary operator '{op_type.__name__}' in formula for node '{self.name}'"
                )
            # Perform the operation
            return float(self.OPERATORS[op_type](left_val, right_val))

        # Unary operation (e.g., -a)
        elif isinstance(node, ast.UnaryOp):
            operand_val = self._evaluate(node.operand, period)
            op_type = type(node.op)
            if op_type not in self.OPERATORS:
                raise ValueError(
                    f"Unsupported unary operator '{op_type.__name__}' in formula for node '{self.name}'"
                )
            # Perform the operation
            return float(self.OPERATORS[op_type](operand_val))

        # If the node type is unsupported
        else:
            raise TypeError(
                f"Unsupported syntax node type '{type(node).__name__}' in formula for node '{self.name}': {ast.dump(node)}"
            )

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used in the formula.

        Returns:
            A list of variable names corresponding to the formula inputs.
        """
        return [node.name for node in self.inputs.values()]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as FormulaCalculationNode performs calculations.
        """
        return True


# === CalculationNode ===


class CalculationNode(Node):
    """Delegate calculation logic to a separate calculation object.

    Uses a calculation object. The actual calculation algorithm is
    encapsulated in a `calculation` object provided during initialization.
    This allows for flexible and interchangeable calculation logic.

    Attributes:
        name (str): Identifier for this node.
        inputs (List[Node]): A list of input nodes required by the calculation.
        calculation (Any): An object possessing a `calculate(inputs: List[Node], period: str) -> float` method.
        _values (Dict[str, float]): Internal cache for calculated results.

    Examples:
        >>> class SumCalculation:
        ...     def calculate(self, inputs: List[Node], period: str) -> float:
        ...         return sum(node.calculate(period) for node in inputs)
        >>> node_a = FinancialStatementItemNode("a", {"2023": 10})
        >>> node_b = FinancialStatementItemNode("b", {"2023": 20})
        >>> sum_node = CalculationNode(
        ...     "sum_ab",
        ...     inputs=[node_a, node_b],
        ...     calculation=SumCalculation()
        ... )
        >>> print(sum_node.calculate("2023"))
        30.0
    """

    def __init__(self, name: str, inputs: list[Node], calculation: Calculation):
        """Initialize the CalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (List[Node]): List of input nodes needed by the calculation.
            calculation (Any): The calculation object implementing the calculation.
                Must have a `calculate` method.

        Raises:
            TypeError: If `inputs` is not a list of Nodes, or if `calculation`
                does not have a callable `calculate` method.
        """
        super().__init__(name)
        if not isinstance(inputs, list) or not all(isinstance(n, Node) for n in inputs):
            raise TypeError("CalculationNode inputs must be a list of Node instances.")
        if not hasattr(calculation, "calculate") or not callable(getattr(calculation, "calculate")):
            raise TypeError("Calculation object must have a callable 'calculate' method.")

        self.inputs = inputs
        self.calculation = calculation
        self._values: dict[str, float] = {}  # Cache for calculated values

    def calculate(self, period: str) -> float:
        """Calculate the value for a period using the assigned calculation.

        Checks the cache first. If not found, delegates to the calculation's
        `calculate` method and stores the result.

        Args:
            period (str): The time period for the calculation.

        Returns:
            float: The calculated value from the calculation.

        Raises:
            CalculationError: If the calculation fails or returns
                a non-numeric value.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Delegate to the calculation object's calculate method
            result = self.calculation.calculate(self.inputs, period)
            if not isinstance(result, (int, float)):
                raise TypeError(
                    f"Calculation for node '{self.name}' did not return a numeric value (got {type(self.calculation).__name__})."
                )
            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the calculation
            raise CalculationError(
                message=f"Error during calculation for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={"calculation": type(self.calculation).__name__, "error": str(e)},
            ) from e

    def set_calculation(self, calculation: Calculation) -> None:
        """Change the calculation object for the node.

        Args:
            calculation (Any): The new calculation object. Must have a callable
                `calculate` method.

        Raises:
            TypeError: If the new calculation is invalid.
        """
        if not hasattr(calculation, "calculate") or not callable(getattr(calculation, "calculate")):
            raise TypeError("New calculation object must have a callable 'calculate' method.")
        self.calculation = calculation
        self.clear_cache()  # Clear cache as logic has changed

    def clear_cache(self) -> None:
        """Clear the internal cache of calculated values.

        Returns:
            None
        """
        self._values.clear()

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used by the calculation.

        Returns:
            A list of input node names.
        """
        return [node.name for node in self.inputs]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as CalculationNode performs calculations.
        """
        return True


# === MetricCalculationNode (Already documented in metric_node.py, refined here) ===
# Note: This class seems to duplicate metric_node.py. Assuming this is the consolidated version.


class MetricCalculationNode(Node):
    """Calculate a value based on a predefined metric definition from the registry.

    Looks up a metric in `metric_registry`, resolves input nodes, and uses an
    appropriate calculation method based on the metric definition.

    Attributes:
        name (str): Identifier for this node.
        metric_name (str): The identifier for the metric in the registry.
        input_nodes (Dict[str, Node]): Mapping of input variable names to Node instances.
        _values (Dict[str, float]): Internal cache for calculated results.

    Examples:
        >>> # Assuming gross_margin metric is registered with formula "revenue - cogs"
        >>> revenue = FinancialStatementItemNode("revenue", {"2023": 100})
        >>> cogs = FinancialStatementItemNode("cogs", {"2023": 60})
        >>> gross_margin = MetricCalculationNode(
        ...     "gross_margin_calc",
        ...     metric_name="gross_margin",
        ...     input_nodes={"revenue": revenue, "cogs": cogs}
        ... )
        >>> print(gross_margin.calculate("2023"))
        40.0
    """

    def __init__(self, name: str, metric_name: str, input_nodes: dict[str, Node]):
        """Initialize the MetricCalculationNode.

        Args:
            name (str): The unique identifier for this node.
            metric_name (str): The identifier for the metric in the registry.
            input_nodes (Dict[str, Node]): Dictionary mapping variable names in the
                metric definition to the corresponding input nodes.

        Raises:
            ConfigurationError: If the metric_name is not registered in `metric_registry`.
            MetricError: If the metric definition is invalid or there's a mismatch between
                input_nodes and required inputs from the metric definition.
            TypeError: If any value in `input_nodes` is not a Node instance.
        """
        super().__init__(name)

        # Validate input_nodes type
        if not isinstance(input_nodes, dict):
            raise TypeError("MetricCalculationNode input_nodes must be a dict of Node instances")

        if not all(isinstance(node, Node) for node in input_nodes.values()):
            raise TypeError("MetricCalculationNode input_nodes must be a dict of Node instances")

        self.metric_name = metric_name
        self.input_nodes = input_nodes
        self._values: dict[str, float] = {}  # Cache for calculated results

        # Try to get the metric definition from the registry
        try:
            self.definition = metric_registry.get(metric_name)
            if self.definition is None:
                raise KeyError(f"Metric '{metric_name}' not found")
        except KeyError:
            raise ConfigurationError(f"Metric definition '{metric_name}' not found")

        # Validate the metric definition
        required_fields = ["inputs", "formula"]
        missing_fields = [field for field in required_fields if field not in self.definition]
        if missing_fields:
            raise MetricError(
                f"Metric definition '{metric_name}' is invalid. Missing fields: {missing_fields}"
            )

        # Validate the input_nodes match what's required in the definition
        required_inputs = set(self.definition["inputs"])
        provided_inputs = set(input_nodes.keys())

        missing_inputs = required_inputs - provided_inputs
        if missing_inputs:
            raise MetricError(
                f"Input nodes mismatch for metric '{metric_name}': missing required inputs: {missing_inputs}"
            )

        extra_inputs = provided_inputs - required_inputs
        if extra_inputs:
            raise MetricError(
                f"Input nodes mismatch for metric '{metric_name}': unexpected inputs provided: {extra_inputs}"
            )

        # Create internal formula calculation node
        self.calc_node = FormulaCalculationNode(
            name=f"_{name}_formula_calc",
            inputs=input_nodes,
            formula=self.definition["formula"],
        )

    def calculate(self, period: str) -> float:
        """Calculate the node's value for a period using the metric definition.

        Looks up the metric in `metric_registry`, resolves input nodes, and uses an
        appropriate calculation method based on the metric definition.

        Args:
            period (str): The time period for which to perform the calculation.

        Returns:
            float: The calculated value from the metric.

        Raises:
            CalculationError: If an error occurs during calculation, such as
                an unknown variable, unsupported operator, or if an input node
                fails to provide a numeric value for the period.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Delegate calculation to the internal formula node
            result = self.calc_node.calculate(period)

            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the formula node
            raise CalculationError(
                message=f"Error calculating metric '{self.metric_name}' for node '{self.name}' and period '{period}'",
                node_id=self.name,
                period=period,
                details={"metric_name": self.metric_name, "original_error": str(e)},
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used in the metric definition.

        Returns:
            A list of metric input node names.
        """
        return list(self.input_nodes.keys())

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as MetricCalculationNode performs calculations.
        """
        return True


# === CustomCalculationNode ===


class CustomCalculationNode(Node):
    """Calculate a value using a Python callable/function.

    Uses a Python callable/function to calculate the value for a node.
    The function is provided during initialization.

    Attributes:
        name (str): Identifier for this node.
        inputs (List[Node]): List of input nodes needed for calculation.
        formula_func (Callable): The Python callable function to use for calculation.
        description (str, optional): Description of what this calculation does.
        _values (Dict[str, float]): Internal cache for calculated results.

    Examples:
        >>> def custom_calculation(a, b):
        ...     return a + b
        >>> node_a = FinancialStatementItemNode("NodeA", values={"2023": 10.0})
        >>> node_b = FinancialStatementItemNode("NodeB", values={"2023": 5.0})
        >>> node = CustomCalculationNode(
        ...     "custom_calc",
        ...     inputs=[node_a, node_b],
        ...     formula_func=custom_calculation
        ... )
        >>> print(node.calculate("2023"))
        15.0
    """

    def __init__(
        self,
        name: str,
        inputs: list[Node],
        formula_func: Callable,
        description: Optional[str] = None,
    ):
        """Initialize the CustomCalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (List[Node]): The input nodes whose values will be passed to formula_func.
            formula_func (Callable): The Python callable function to use for calculation.
            description (str, optional): Description of what this calculation does.

        Raises:
            TypeError: If `inputs` is not a list of Nodes or `formula_func` is not a callable.
        """
        super().__init__(name)
        if not isinstance(inputs, list) or not all(isinstance(n, Node) for n in inputs):
            raise TypeError("CustomCalculationNode inputs must be a list of Node instances")
        if not callable(formula_func):
            raise TypeError("CustomCalculationNode formula_func must be a callable function")

        self.inputs = inputs
        self.formula_func = formula_func
        self.description = description
        self._values: dict[str, float] = {}  # Cache for calculated results

    def calculate(self, period: str) -> float:
        """Calculate the node's value for a period using the provided function.

        Args:
            period (str): The time period for which to perform the calculation.

        Returns:
            float: The calculated value from the function.

        Raises:
            CalculationError: If an error occurs during calculation, such as
                if an input node fails to provide a numeric value for the period.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Get input values
            input_values = []
            for node in self.inputs:
                value = node.calculate(period)
                if not isinstance(value, (int, float)):
                    raise TypeError(
                        f"Input node '{node.name}' did not return a numeric value for period '{period}'. Got {type(value).__name__}."
                    )
                input_values.append(value)

            # Calculate the value using the provided function
            result = self.formula_func(*input_values)
            if not isinstance(result, (int, float)):
                raise TypeError(
                    f"Formula did not return a numeric value. Got {type(result).__name__}."
                )

            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the function
            raise CalculationError(
                message=f"Error during custom calculation for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={"function": self.formula_func.__name__, "error": str(e)},
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used in the function.

        Returns:
            A list of input node names.
        """
        return [node.name for node in self.inputs]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as CustomCalculationNode performs calculations.
        """
        return True

# --- END FILE: fin_statement_model/core/nodes/calculation_nodes.py ---

# --- START FILE: fin_statement_model/core/nodes/forecast_nodes.py ---
"""Provide forecast nodes to project future values from historical data.

This module defines the base `ForecastNode` class and its subclasses,
implementing various forecasting strategies (fixed, curve, statistical,
custom, average, and historical growth).
"""

import logging
from typing import Callable

# Use absolute imports
from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class ForecastNode(Node):
    """Define base class for forecast nodes to project future values from historical data.

    A forecast node takes an input node (typically a financial statement item) and projects its
    future values using various growth methods. The node caches calculated values to avoid
    redundant computations.

    Attributes:
        name (str): Identifier for the forecast node (derived from input_node.name)
        input_node (Node): Source node containing historical values to forecast from
        base_period (str): Last historical period to use as basis for forecasting
        forecast_periods (List[str]): List of future periods to generate forecasts for
        _cache (dict): Internal cache of calculated values
        values (dict): Dictionary mapping periods to values (including historical)

    Methods:
        calculate(period): Get value for a specific period (historical or forecast)
        _calculate_value(period): Core calculation logic for a period
        _get_previous_period(period): Helper to get chronologically previous period
        _get_growth_factor_for_period(): Abstract method for growth rate calculation

    Examples:
        # Create 5% fixed growth forecast for revenue
        base = "FY2022"
        forecasts = ["FY2023", "FY2024", "FY2025"]
        node = FixedGrowthForecastNode(revenue_node, base, forecasts, 0.05)

        # Get forecasted value
        fy2024_revenue = node.calculate("FY2024")
    """

    def __init__(self, input_node: Node, base_period: str, forecast_periods: list[str]):
        """Initialize ForecastNode with input node and forecast periods.

        Args:
            input_node: Source node containing historical values.
            base_period: The last historical period serving as the forecast base.
            forecast_periods: List of future periods for which forecasts will be generated.
        """
        self.name = input_node.name
        self.input_node = input_node
        self.base_period = base_period
        self.forecast_periods = forecast_periods
        self._cache = {}

        # Copy historical values from input node
        if hasattr(input_node, "values"):
            self.values = input_node.values.copy()
        else:
            self.values = {}

    def calculate(self, period: str) -> float:
        """Calculate the value for a specific period, using cached results if available.

        This method returns historical values for periods up to the base period, and
        calculates forecasted values for future periods. Results are cached to avoid
        redundant calculations.

        Args:
            period (str): The period to calculate the value for (e.g. "FY2023")

        Returns:
            float: The calculated value for the specified period

        Raises:
            ValueError: If the requested period is not in base_period or forecast_periods

        Examples:
            # Get historical value
            base_value = node.calculate("FY2022")  # Returns actual historical value

            # Get forecasted value
            forecast_value = node.calculate("FY2024")  # Returns projected value
        """
        if period not in self._cache:
            self._cache[period] = self._calculate_value(period)
        return self._cache[period]

    def clear_cache(self):
        """Clear the calculation cache.

        This method clears any cached calculation results, forcing future calls to
        calculate() to recompute values rather than using cached results.

        Examples:
            # Clear cached calculations
            node.clear_cache()  # Future calculate() calls will recompute values
        """
        self._cache.clear()

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True

    def _calculate_value(self, period: str) -> float:
        """Calculate the value for a specific period.

        For historical periods (up to base_period), returns the actual value.
        For forecast periods, calculates the value using the growth rate.

        Args:
            period: The period to calculate the value for

        Returns:
            float: The calculated value for the period

        Raises:
            ValueError: If the period is not in base_period or forecast_periods
        """
        # For historical periods, return the actual value
        if period <= self.base_period:
            return self.values.get(period, 0.0)

        # For forecast periods, calculate using growth rate
        if period not in self.forecast_periods:
            raise ValueError(f"Period '{period}' not in forecast periods for {self.name}")

        # Get the previous period's value
        prev_period = self._get_previous_period(period)
        prev_value = self.calculate(prev_period)

        # Get the growth rate for this period
        growth_factor = self._get_growth_factor_for_period(period, prev_period, prev_value)

        # Calculate the new value
        return prev_value * (1 + growth_factor)

    def _get_previous_period(self, current_period: str) -> str:
        all_periods = sorted([self.base_period, *self.forecast_periods])
        idx = all_periods.index(current_period)
        return all_periods[idx - 1]

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        raise NotImplementedError("Implement in subclass.")


class FixedGrowthForecastNode(ForecastNode):
    """A forecast node that applies a fixed growth rate to project future values.

    This node takes a constant growth rate and applies it to each forecast period,
    compounding from the base period value. It's useful for simple forecasting scenarios
    where steady growth is expected.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_rate (float): The fixed growth rate to apply (e.g. 0.05 for 5% growth)

    Examples:
        # Create node forecasting 5% annual revenue growth
        forecast = FixedGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            0.05
        )

        # Get forecasted value
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1.05)^2
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_rate: float,
    ):
        """Initialize FixedGrowthForecastNode with a constant growth rate.

        Args:
            input_node: Node containing historical values to base the forecast on.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_rate: Fixed growth rate (e.g., 0.05 for 5% growth).
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.growth_rate = float(growth_rate)  # Ensure it's a float
        logger.debug(f"Created FixedGrowthForecastNode with growth rate: {self.growth_rate}")

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        logger.debug(
            f"FixedGrowthForecastNode: Using growth rate {self.growth_rate} for period {period}"
        )
        return self.growth_rate


class CurveGrowthForecastNode(ForecastNode):
    """A forecast node that applies different growth rates for each forecast period.

    This node takes a list of growth rates corresponding to each forecast period,
    allowing for varying growth assumptions over time. This is useful when you expect
    growth patterns to change, such as high initial growth followed by moderation.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_rates (List[float]): List of growth rates for each period (e.g. [0.08, 0.06, 0.04])
                                   Must match length of forecast_periods.

    Raises:
        ValueError: If length of growth_rates doesn't match forecast_periods

    Examples:
        # Create node with declining growth rates
        forecast = CurveGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            [0.08, 0.06, 0.04]  # 8% then 6% then 4% growth
        )

        # Get forecasted value
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1.08) * (1.06)
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_rates: list[float],
    ):
        """Initialize CurveGrowthForecastNode with variable growth rates per period.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_rates: List of growth rates matching each forecast period.
        """
        super().__init__(input_node, base_period, forecast_periods)
        if len(growth_rates) != len(forecast_periods):
            raise ValueError("Number of growth rates must match forecast periods.")
        self.growth_rates = [float(rate) for rate in growth_rates]  # Ensure all are floats
        logger.debug(f"Created CurveGrowthForecastNode with growth rates: {self.growth_rates}")
        logger.debug(f"  Base period: {base_period}")
        logger.debug(f"  Forecast periods: {forecast_periods}")
        logger.debug(f"  Base value: {input_node.calculate(base_period)}")

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Get the growth factor for a specific period."""
        idx = self.forecast_periods.index(period)
        growth_rate = self.growth_rates[idx]
        logger.debug(
            f"CurveGrowthForecastNode: Using growth rate {growth_rate} for period {period}"
        )
        logger.debug(f"  Previous period: {prev_period}")
        logger.debug(f"  Previous value: {prev_value}")
        return growth_rate


class StatisticalGrowthForecastNode(ForecastNode):
    """A forecast node that generates growth rates from a statistical distribution.

    This node uses a provided statistical distribution function to randomly generate
    growth rates for each forecast period. This is useful for modeling uncertainty
    and running Monte Carlo simulations of different growth scenarios.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        distribution_callable (Callable[[], float]): Function that returns random growth rates
                                                   from a statistical distribution

    Examples:
        # Create node with normally distributed growth rates
        from numpy.random import normal
        forecast = StatisticalGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            lambda: normal(0.05, 0.02)  # Mean 5% growth, 2% std dev
        )

        # Get forecasted value (will vary due to randomness)
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1 + r1) * (1 + r2) where r1,r2 are random
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        distribution_callable: Callable[[], float],
    ):
        """Initialize StatisticalGrowthForecastNode with a distribution function.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            distribution_callable: Function that returns a random growth rate.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.distribution_callable = distribution_callable

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.distribution_callable()


class CustomGrowthForecastNode(ForecastNode):
    """A forecast node that uses a custom function to determine growth rates.

    This node allows complete flexibility in how growth rates are calculated by accepting
    a custom function that can incorporate any logic or external data to determine the
    growth rate for each period.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_function (Callable[[str, str, float], float]): Function that returns growth rate
            given current period, previous period, and previous value

    The growth_function should accept three parameters:
        - current_period (str): The period being forecasted
        - prev_period (str): The previous period
        - prev_value (float): The value from the previous period
    And return a float representing the growth rate for that period.

    Examples:
        def custom_growth(period, prev_period, prev_value):
            # Growth rate increases by 1% each year, starting at 5%
            year_diff = int(period[-4:]) - int(prev_period[-4:])
            return 0.05 + (0.01 * year_diff)

        forecast = CustomGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            custom_growth
        )
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_function: Callable[[str, str, float], float],
    ):
        """Initialize CustomGrowthForecastNode with a custom growth function.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_function: Callable(period, prev_period, prev_value) -> growth rate.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.growth_function = growth_function

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.growth_function(period, prev_period, prev_value)


class AverageValueForecastNode(ForecastNode):
    """A forecast node that uses the average of historical values for all forecast periods.

    This node calculates the average of historical values and returns that constant value
    for all forecast periods. It's useful when you want to project future values based
    on the historical average, without any growth.
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
    ):
        """Initialize AverageValueForecastNode by computing historical average.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.

        """
        super().__init__(input_node, base_period, forecast_periods)
        self.average_value = self._calculate_average_value()
        logger.debug(
            f"Created AverageValueForecastNode with average value: {self.average_value}"
        )

    def _calculate_average_value(self) -> float:
        """Calculate the average historical value up to the base period.

        Returns:
            float: The average of historical values or 0.0 if none.
        """
        values = [value for period, value in self.values.items() if period <= self.base_period]
        if not values:
            logger.warning(
                f"No historical values found for {self.name}, using 0.0 as average"
            )
            return 0.0
        return sum(values) / len(values)

    def _calculate_value(self, period: str) -> float:
        """Calculate the value for a specific period using the computed average value."""
        # For historical periods, return the actual value
        if period <= self.base_period:
            return self.values.get(period, 0.0)

        # For forecast periods, return the constant average value
        if period not in self.forecast_periods:
            raise ValueError(f"Period '{period}' not in forecast periods for {self.name}")

        return self.average_value

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Not used for average value forecasts."""
        return 0.0


class AverageHistoricalGrowthForecastNode(ForecastNode):
    """A forecast node that uses the average historical growth rate for forecasting.

    This node calculates the average growth rate from historical values and applies
    that same growth rate consistently to all forecast periods. It's useful when you
    want to project future values based on the historical growth pattern.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
    ):
        """Initialize AverageHistoricalGrowthForecastNode by computing average growth.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.avg_growth_rate = self._calculate_average_growth_rate()
        logger.debug(
            f"Created AverageHistoricalGrowthForecastNode with growth rate: {self.avg_growth_rate}"
        )

    def _calculate_average_growth_rate(self) -> float:
        """Calculate the average historical growth rate from input node values.

        Returns:
            float: The average growth rate across historical periods
        """
        if not self.values:
            logger.warning(f"No historical values found for {self.name}, using 0% growth")
            return 0.0

        # Get sorted historical periods up to base period
        historical_periods = sorted([p for p in self.values if p <= self.base_period])
        if len(historical_periods) < 2:
            logger.warning(f"Insufficient historical data for {self.name}, using 0% growth")
            return 0.0

        # Calculate growth rates between consecutive periods
        growth_rates = []
        for i in range(1, len(historical_periods)):
            prev_period = historical_periods[i - 1]
            curr_period = historical_periods[i]
            prev_value = self.values[prev_period]
            curr_value = self.values[curr_period]

            if prev_value == 0:
                logger.warning(
                    f"Zero value found for {self.name} in period {prev_period}, skipping growth rate"
                )
                continue

            growth_rate = (curr_value - prev_value) / prev_value
            growth_rates.append(growth_rate)

        if not growth_rates:
            logger.warning(f"No valid growth rates calculated for {self.name}, using 0% growth")
            return 0.0

        # Calculate and return average growth rate
        avg_growth = sum(growth_rates) / len(growth_rates)
        logger.debug(f"Calculated average growth rate for {self.name}: {avg_growth}")
        return avg_growth

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Get the growth factor for a specific period.

        Args:
            period (str): The current period
            prev_period (str): The previous period
            prev_value (float): The value from the previous period

        Returns:
            float: The growth rate to apply
        """
        logger.debug(
            f"AverageHistoricalGrowthForecastNode: Using growth rate {self.avg_growth_rate} for period {period}"
        )
        return self.avg_growth_rate

# --- END FILE: fin_statement_model/core/nodes/forecast_nodes.py ---

# --- START FILE: fin_statement_model/core/nodes/item_node.py ---
"""Define a node representing a basic financial statement item."""

import logging

# Use absolute imports
from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class FinancialStatementItemNode(Node):
    """Define a leaf node containing raw financial statement data.

    This node type typically stores actual reported values (e.g., Revenue,
    COGS) for different time periods.

    Attributes:
        name (str): The unique identifier for the financial item (e.g., "Revenue").
        values (Dict[str, float]): A dictionary mapping time periods (str)
            to their corresponding numerical values (float).

    Examples:
        >>> revenue_data = {"2022": 1000.0, "2023": 1200.0}
        >>> revenue_node = FinancialStatementItemNode("Revenue", revenue_data)
        >>> print(revenue_node.name)
        Revenue
        >>> print(revenue_node.get_value("2023"))
        1200.0
        >>> print(revenue_node.calculate("2022")) # Calculate retrieves the value
        1000.0
        >>> revenue_node.set_value("2024", 1500.0)
        >>> print(revenue_node.get_value("2024"))
        1500.0
        >>> print(revenue_node.has_value("2021"))
        False
    """

    def __init__(self, name: str, values: dict[str, float]):
        """Initialize the financial statement item node.

        Args:
            name (str): The name of the financial statement item.
            values (Dict[str, float]): Dictionary of period-value pairs.
        """
        # Call base Node init if it requires name
        # super().__init__(name)  # Assuming base Node init takes name
        self.name = name
        self.values = values

    def calculate(self, period: str) -> float:
        """Retrieve the value for the specified period.

        For this node type, calculation simply means retrieving the stored value.

        Args:
            period (str): The time period for which to retrieve the value.

        Returns:
            float: The value for the given period, or 0.0 if the period is not found.
        """
        return self.get_value(period)

    def set_value(self, period: str, value: float) -> None:
        """Update or add a value for a specific period.

        Modifies the stored data for the given period.

        Args:
            period (str): The time period to set the value for.
            value (float): The numerical value to store for the period.
        """
        self.values[period] = value

    def has_value(self, period: str) -> bool:
        """Check if a value exists for the specified period.

        Args:
            period (str): The time period to check.

        Returns:
            bool: True if a value is stored for the period, False otherwise.
        """
        return period in self.values

    def get_value(self, period: str) -> float:
        """Retrieve the stored value for a specific period.

        Args:
            period (str): The time period for which to get the value.

        Returns:
            float: The stored value, defaulting to 0.0 if the period is not found.
        """
        return self.values.get(period, 0.0)

# --- END FILE: fin_statement_model/core/nodes/item_node.py ---

# --- START FILE: fin_statement_model/core/nodes/metric_node.py ---
"""Define the MetricCalculation class for financial metrics.

This module provides the base class for all metric calculations in the financial statement model.
"""

from typing import Optional

from fin_statement_model.core.nodes.base import Node


class MetricCalculation(Node):
    """Base class for all metric calculations in the financial statement model.

    This class provides the foundation for implementing specific financial metrics.
    Subclasses should implement the calculate method to provide the specific
    calculation logic for the metric.

    Attributes:
        name (str): The name of the metric.
        inputs (Dict[str, Node]): Mapping of input names to their corresponding nodes.
        description (Optional[str]): Optional description of the metric.
    """

    def __init__(
        self,
        name: str,
        inputs: dict[str, Node],
        description: Optional[str] = None,
    ):
        """Initialize a new metric calculation.

        Args:
            name: The name of the metric.
            inputs: Mapping of input names to their corresponding nodes.
            description: Optional description of the metric.
        """
        super().__init__(name)
        self.inputs = inputs
        self.description = description

    def calculate(self, period: str) -> float:
        """Calculate the metric value for the given period.

        This method must be implemented by subclasses to provide the specific
        calculation logic for the metric.

        Args:
            period: The period for which to calculate the metric.

        Returns:
            The calculated metric value.

        Raises:
            MetricError: If the calculation fails.
        """
        raise NotImplementedError("Subclasses must implement calculate()")

    def get_dependencies(self) -> list[str]:
        """Get the names of all input nodes this metric depends on.

        Returns:
            A list of input node names.
        """
        return list(self.inputs.keys())

    def has_calculation(self) -> bool:
        """Check if this node has a calculation defined.

        Returns:
            True, as metric nodes always have calculations.
        """
        return True

# --- END FILE: fin_statement_model/core/nodes/metric_node.py ---

# --- START FILE: fin_statement_model/core/nodes/stats_nodes.py ---
"""Provide nodes for statistical calculations on financial data across periods.

This module provides nodes for common time-series statistical analyses:
- `YoYGrowthNode`: Calculates year-over-year percentage growth.
- `MultiPeriodStatNode`: Computes statistics (mean, stddev, etc.) over a range of periods.
- `TwoPeriodAverageNode`: Calculates the simple average over two specific periods.
"""

import logging
import math
import statistics

# Use lowercase built-in types for annotations
from typing import Optional, Callable, Union
from collections.abc import Sequence

# Use absolute imports
from fin_statement_model.core.nodes.base import Node
from fin_statement_model.core.errors import CalculationError

# Added logger instance
logger = logging.getLogger(__name__)

Numeric = Union[int, float]
StatFunc = Callable[[Sequence[Numeric]], Numeric]


class YoYGrowthNode(Node):
    """Calculate year-over-year (YoY) percentage growth.

    Compares the value of an input node between two specified periods
    (prior and current) and calculates the relative change.

    Growth = (Current Value - Prior Value) / Prior Value

    Attributes:
        name (str): The node's identifier.
        input_node (Node): The node providing the values for comparison.
        prior_period (str): Identifier for the earlier time period.
        current_period (str): Identifier for the later time period.

    Examples:
        >>> # Assume revenue_node holds {"2022": 100, "2023": 120}
        >>> revenue_node = FinancialStatementItemNode("revenue", {"2022": 100.0, "2023": 120.0})
        >>> yoy_growth = YoYGrowthNode(
        ...     "revenue_yoy",
        ...     input_node=revenue_node,
        ...     prior_period="2022",
        ...     current_period="2023"
        ... )
        >>> print(yoy_growth.calculate("any_period")) # Period arg is ignored
        0.2
    """

    def __init__(self, name: str, input_node: Node, prior_period: str, current_period: str):
        """Initialize the YoY Growth node.

        Args:
            name (str): The identifier for this growth node.
            input_node (Node): The node whose values will be compared.
            prior_period (str): The identifier for the earlier period.
            current_period (str): The identifier for the later period.

        Raises:
            TypeError: If `input_node` is not a Node instance or periods are not strings.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError("YoYGrowthNode input_node must be a Node instance.")
        if not isinstance(prior_period, str) or not isinstance(current_period, str):
            raise TypeError("YoYGrowthNode prior_period and current_period must be strings.")

        self.input_node = input_node
        self.prior_period = prior_period
        self.current_period = current_period

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the year-over-year growth rate.

        Retrieves values for the prior and current periods from the input node
        and computes the percentage growth. The `period` argument is ignored
        as the calculation periods are fixed during initialization.

        Args:
            period (Optional[str]): Ignored. The calculation uses the periods
                defined during initialization.

        Returns:
            float: The calculated growth rate (e.g., 0.2 for 20% growth).
                   Returns `float('nan')` if the prior period value is zero
                   or non-numeric.

        Raises:
            CalculationError: If the input node fails to provide numeric values
                for the required periods.
        """
        try:
            prior_value = self.input_node.calculate(self.prior_period)
            current_value = self.input_node.calculate(self.current_period)

            # Validate input types
            if not isinstance(prior_value, (int, float)):
                raise TypeError(f"Prior period ('{self.prior_period}') value is non-numeric.")
            if not isinstance(current_value, (int, float)):
                raise TypeError(f"Current period ('{self.current_period}') value is non-numeric.")

            # Handle division by zero or non-finite prior value
            if prior_value == 0 or not math.isfinite(prior_value):
                logger.warning(
                    f"YoYGrowthNode '{self.name}': Prior period '{self.prior_period}' value is zero or non-finite ({prior_value}). Returning NaN."
                )
                return float("nan")

            # Calculate growth
            growth = (float(current_value) - float(prior_value)) / float(prior_value)
            return growth

        except Exception as e:
            # Wrap any exception during calculation
            raise CalculationError(
                message=f"Failed to calculate YoY growth for node '{self.name}'",
                node_id=self.name,
                period=f"{self.prior_period}_to_{self.current_period}",  # Indicate period span
                details={
                    "input_node": self.input_node.name,
                    "prior_period": self.prior_period,
                    "current_period": self.current_period,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True


class MultiPeriodStatNode(Node):
    """Calculate a statistical measure across multiple periods.

    Applies a specified statistical function (e.g., mean, standard deviation)
    to the values of an input node over a list of periods.

    Attributes:
        name (str): The node's identifier.
        input_node (Node): The node providing the values for analysis.
        periods (List[str]): The list of period identifiers to include.
        stat_func (StatFunc): The statistical function to apply (e.g.,
            `statistics.mean`, `statistics.stdev`). Must accept a sequence
            of numbers and return a single number.

    Examples:
        >>> # Assume sales_node holds {"Q1": 10, "Q2": 12, "Q3": 11, "Q4": 13}
        >>> sales_node = FinancialStatementItemNode("sales", {"Q1": 10, "Q2": 12, "Q3": 11, "Q4": 13})
        >>> mean_sales = MultiPeriodStatNode(
        ...     "avg_quarterly_sales",
        ...     input_node=sales_node,
        ...     periods=["Q1", "Q2", "Q3", "Q4"],
        ...     stat_func=statistics.mean
        ... )
        >>> print(mean_sales.calculate()) # Period arg is ignored
        11.5
        >>> stddev_sales = MultiPeriodStatNode(
        ...     "sales_volatility",
        ...     input_node=sales_node,
        ...     periods=["Q1", "Q2", "Q3", "Q4"],
        ...     stat_func=statistics.stdev # Default
        ... )
        >>> print(round(stddev_sales.calculate(), 2))
        1.29
    """

    def __init__(
        self,
        name: str,
        input_node: Node,
        periods: list[str],
        stat_func: StatFunc = statistics.stdev,  # Default to standard deviation
    ):
        """Initialize the multi-period statistics node.

        Args:
            name (str): The identifier for this statistical node.
            input_node (Node): The node providing the source values.
            periods (List[str]): A list of period identifiers to analyze.
            stat_func (StatFunc): The statistical function to apply. Defaults to
                `statistics.stdev`. It must accept a sequence of numerics and
                return a numeric value.

        Raises:
            ValueError: If `periods` is not a list or is empty.
            TypeError: If `input_node` is not a Node, `periods` contains non-strings,
                or `stat_func` is not callable.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError("MultiPeriodStatNode input_node must be a Node instance.")
        if not isinstance(periods, list) or not periods:
            raise ValueError("MultiPeriodStatNode periods must be a non-empty list.")
        if not all(isinstance(p, str) for p in periods):
            raise TypeError("MultiPeriodStatNode periods must contain only strings.")
        if not callable(stat_func):
            raise TypeError("MultiPeriodStatNode stat_func must be a callable function.")

        self.input_node = input_node
        self.periods = periods
        self.stat_func = stat_func

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the statistical measure across the specified periods.

        Retrieves values from the input node for each period in the configured list,
        then applies the `stat_func`. The `period` argument is ignored.

        Args:
            period (Optional[str]): Ignored. Calculation uses the periods defined
                during initialization.

        Returns:
            float: The result of the statistical function. Returns `float('nan')`
                   if the statistical function requires more data points than
                   available (e.g., standard deviation with < 2 values) or if
                   no valid numeric data is found.

        Raises:
            CalculationError: If retrieving input node values fails or if the
                statistical function itself raises an unexpected error.
        """
        values: list[Numeric] = []
        retrieval_errors = []
        try:
            for p in self.periods:
                try:
                    value = self.input_node.calculate(p)
                    if isinstance(value, (int, float)) and math.isfinite(value):
                        values.append(float(value))
                    else:
                        # Log non-numeric/non-finite values but continue if possible
                        logger.warning(
                            f"MultiPeriodStatNode '{self.name}': Input '{self.input_node.name}' gave non-numeric/non-finite value ({value}) for period '{p}'. Skipping."
                        )
                except Exception as node_err:
                    # Log error fetching data for a specific period but continue
                    logger.error(
                        f"MultiPeriodStatNode '{self.name}': Error getting value for period '{p}' from '{self.input_node.name}': {node_err}",
                        exc_info=True,
                    )
                    retrieval_errors.append(p)

            # If no valid numeric values were collected
            if not values:
                logger.warning(
                    f"MultiPeriodStatNode '{self.name}': No valid numeric data points found across periods {self.periods}. Returning NaN."
                )
                return float("nan")

            # Attempt the statistical calculation
            try:
                result = self.stat_func(values)
                # Ensure result is float, handle potential NaN from stat_func
                return float(result) if math.isfinite(result) else float("nan")
            except (statistics.StatisticsError, ValueError, TypeError) as stat_err:
                # Handle errors specific to statistical functions (e.g., stdev needs >= 2 points)
                logger.warning(
                    f"MultiPeriodStatNode '{self.name}': Stat function '{self.stat_func.__name__}' failed ({stat_err}). Values: {values}. Returning NaN."
                )
                return float("nan")

        except Exception as e:
            # Catch any other unexpected errors during the process
            raise CalculationError(
                message=f"Failed to calculate multi-period stat for node '{self.name}'",
                node_id=self.name,
                period="multi-period",  # Indicate calculation context
                details={
                    "input_node": self.input_node.name,
                    "periods": self.periods,
                    "stat_func": self.stat_func.__name__,
                    "collected_values_count": len(values),
                    "retrieval_errors_periods": retrieval_errors,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True


class TwoPeriodAverageNode(Node):
    """Compute the simple average of an input node's value over two periods.

    Calculates (Value at Period 1 + Value at Period 2) / 2.

    Attributes:
        name (str): Identifier for this node.
        input_node (Node): Node providing the values to be averaged.
        period1 (str): Identifier for the first period.
        period2 (str): Identifier for the second period.

    Examples:
        >>> # Assume price_node holds {"Jan": 10.0, "Feb": 11.0}
        >>> price_node = FinancialStatementItemNode("price", {"Jan": 10.0, "Feb": 11.0})
        >>> avg_price = TwoPeriodAverageNode(
        ...     "jan_feb_avg_price",
        ...     input_node=price_node,
        ...     period1="Jan",
        ...     period2="Feb"
        ... )
        >>> print(avg_price.calculate()) # Period arg is ignored
        10.5
    """

    def __init__(self, name: str, input_node: Node, period1: str, period2: str):
        """Initialize the two-period average node.

        Args:
            name (str): The identifier for this node.
            input_node (Node): The node providing values.
            period1 (str): The identifier for the first period.
            period2 (str): The identifier for the second period.

        Raises:
            TypeError: If `input_node` is not a Node, or periods are not strings.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError(
                f"TwoPeriodAverageNode input_node must be a Node instance, got {type(input_node).__name__}"
            )
        if not isinstance(period1, str) or not isinstance(period2, str):
            raise TypeError("TwoPeriodAverageNode period1 and period2 must be strings.")

        self.input_node = input_node
        self.period1 = period1
        self.period2 = period2

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the average of the input node for the two fixed periods.

        Ignores the `period` argument, using `period1` and `period2` defined
        during initialization.

        Args:
            period (Optional[str]): Ignored.

        Returns:
            float: The average of the input node's values for `period1` and `period2`.
                   Returns `float('nan')` if either input value is non-numeric.

        Raises:
            CalculationError: If retrieving values from the input node fails.
        """
        try:
            val1 = self.input_node.calculate(self.period1)
            val2 = self.input_node.calculate(self.period2)

            # Ensure values are numeric and finite
            if not isinstance(val1, (int, float)) or not math.isfinite(val1):
                logger.warning(
                    f"TwoPeriodAverageNode '{self.name}': Value for period '{self.period1}' is non-numeric/non-finite ({val1}). Returning NaN."
                )
                return float("nan")
            if not isinstance(val2, (int, float)) or not math.isfinite(val2):
                logger.warning(
                    f"TwoPeriodAverageNode '{self.name}': Value for period '{self.period2}' is non-numeric/non-finite ({val2}). Returning NaN."
                )
                return float("nan")

            # Calculate the average
            return (float(val1) + float(val2)) / 2.0

        except Exception as e:
            # Wrap potential errors during input node calculation
            raise CalculationError(
                message=f"Failed to calculate two-period average for node '{self.name}'",
                node_id=self.name,
                period=f"{self.period1}_and_{self.period2}",  # Indicate context
                details={
                    "input_node": self.input_node.name,
                    "period1": self.period1,
                    "period2": self.period2,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True


__all__ = [
    "MultiPeriodStatNode",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]

# --- END FILE: fin_statement_model/core/nodes/stats_nodes.py ---

# --- START FILE: fin_statement_model/extensions/__init__.py ---
"""Extensions package for fin_statement_model.

This package hosts optional in-repo extensions and third-party plugins discovered via entry-points under
`fin_statement_model.extensions`.
"""

# --- END FILE: fin_statement_model/extensions/__init__.py ---

# --- START FILE: fin_statement_model/extensions/llm/__init__.py ---
"""LLM extension subpackage for fin_statement_model.

Provides built-in OpenAI-based LLM client extension for generating and injecting content.
"""

# --- END FILE: fin_statement_model/extensions/llm/__init__.py ---

# --- START FILE: fin_statement_model/extensions/llm/llm_client.py ---
"""LLM client module for OpenAI and backoff integration.

This module provides `LLMConfig` for client configuration and `LLMClient` for
asynchronous interactions with OpenAI's ChatCompletion API, including retry logic.
"""

import logging
import openai
import backoff
from dataclasses import dataclass
from typing import Optional, Any
from types import TracebackType

logger = logging.getLogger(__name__)


@dataclass
class LLMConfig:
    """Configuration data for LLMClient.

    Attributes:
        api_key: API key for OpenAI authentication.
        model_name: Model to use (e.g., 'gpt-4o').
        temperature: Sampling temperature setting.
        max_tokens: Maximum tokens to generate.
        timeout: Request timeout in seconds.
        max_retries: Number of retries on failure.
    """

    api_key: str
    model_name: str = "gpt-4o"
    temperature: float = 0.7
    max_tokens: int = 1500
    timeout: int = 30
    max_retries: int = 3
    # base_url is no longer needed as the openai library handles the endpoint configuration.


class LLMClientError(Exception):
    """Base exception for LLM client errors."""


class LLMTimeoutError(LLMClientError):
    """Exception for timeout errors."""


class LLMClient:
    """Asynchronous client for OpenAI ChatCompletion API with retry logic.

    Utilizes `LLMConfig` and supports retries on rate limits and timeouts.

    Methods:
        _make_api_call: Internal method for performing the API call with retry logic.
        get_completion: High-level method to obtain chat completions.
    """

    def __init__(self, config: Optional[LLMConfig] = None):
        """Initialize the async LLM client with configuration."""
        self.config = config or LLMConfig(api_key="")
        openai.api_key = self.config.api_key

    @backoff.on_exception(
        backoff.expo,
        (Exception, LLMTimeoutError),
        max_tries=3,
        giveup=lambda e: isinstance(e, LLMTimeoutError),
    )
    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=8)
    async def _make_api_call(self, messages: list[dict[str, str]]) -> dict[str, Any]:
        """Make the async API call to OpenAI with retry logic.

        Args:
            messages: List of message dicts for the ChatCompletion API

        Returns:
            Dict containing the API response

        Raises:
            LLMClientError: For any client-related errors, including timeout if applicable
        """
        try:
            logger.debug(f"Sending async request to OpenAI API with model {self.config.model_name}")
            response = await openai.ChatCompletion.acreate(
                model=self.config.model_name,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                timeout=self.config.timeout,
            )

            if not response.get("choices"):
                logger.error("No suggestions received from OpenAI API")
                raise LLMClientError("No suggestions received from API")

            return response
        except Exception as e:
            if "timeout" in str(e).lower():
                logger.exception("Async request timed out")
                raise LLMTimeoutError("Request timed out") from e
            logger.exception("OpenAI async API request failed")
            raise LLMClientError(f"API request failed: {e!s}") from e

    async def get_completion(self, messages: list[dict[str, str]]) -> str:
        """Get a completion result from the LLM using async API.

        Args:
            messages: List of message dictionaries for the ChatCompletion API

        Returns:
            str: The suggested completion from the LLM

        Raises:
            LLMClientError: For any client-related errors
        """
        try:
            logger.info("Requesting completion from OpenAI async API")
            response = await self._make_api_call(messages)
            completion = response["choices"][0]["message"]["content"].strip()
            logger.info("Successfully received completion")
            return completion
        except Exception as e:
            logger.exception("Error getting completion")
            raise LLMClientError(f"Failed to get completion: {e!s}") from e

    async def __aenter__(self) -> "LLMClient":
        """Enter the asynchronous context manager, returning the client."""
        return self

    async def __aexit__(
        self,
        exc_type: Optional[type[BaseException]],
        exc: Optional[BaseException],
        tb: Optional[TracebackType],
    ) -> None:
        """Exit the asynchronous context manager, performing cleanup."""
        # Ensure method has a body

# --- END FILE: fin_statement_model/extensions/llm/llm_client.py ---

# --- START FILE: fin_statement_model/io/__init__.py ---
"""Input/Output components for the Financial Statement Model.

This package provides a unified interface for reading and writing financial model
data from/to various formats using a registry-based approach.
"""

import logging
from typing import Union

from fin_statement_model.core.graph import Graph

from .base import DataReader, DataWriter
from .registry import get_reader, get_writer, list_readers, list_writers
from .exceptions import IOError, ReadError, WriteError, FormatNotSupportedError

# Configure logging for the io package
logger = logging.getLogger(__name__)

# --- Trigger Registration ---
# Import reader/writer modules to ensure their @register decorators run.
# This makes them available in the registry when the io package is imported.
try:
    from . import readers  # noqa: F401
    from . import writers  # noqa: F401
except ImportError:
    # This might happen during setup or if directories are missing
    logger.warning("Could not automatically import readers/writers")


# --- Facade Functions ---

# Define known keyword arguments for reader/writer initialization
# This helps separate config args from read/write specific args
_READER_INIT_KWARGS = {"api_key", "mapping_config"}
_WRITER_INIT_KWARGS = set()  # Currently no common writer init kwargs identified


def read_data(
    format_type: str, source: str, **kwargs: dict[str, Union[str, int, float, bool]]
) -> Graph:
    """Reads data from a source using the specified format.

    Keyword arguments passed to this function are divided between reader initialization
    (e.g., 'api_key', 'mapping_config') and the reader's `read()` method
    (e.g., 'sheet_name', 'statement_type'), based on predefined keys. Consult the
    specific reader's documentation for the exact parameters handled by each.

    Args:
        format_type (str): The format identifier (e.g., 'excel', 'csv', 'fmp', 'dict').
        source (str): The data source (e.g., file path, ticker symbol, dict, DataFrame).
        **kwargs: Additional keyword arguments. Arguments like 'api_key' or
                  'mapping_config' might be used to initialize the reader,
                  while others (e.g., 'sheet_name', 'statement_type') are passed
                  to the reader's `read()` method.

    Returns:
        Graph: A new Graph object populated with the read data.

    Raises:
        ReadError: If reading fails.
        FormatNotSupportedError: If the format_type is not registered.
        Exception: Other errors during reader initialization or reading.
    """
    logger.info(
        f"Attempting to read data using format '{format_type}' from source type '{type(source).__name__}'"
    )

    # Separate kwargs for init vs read
    init_kwargs = {k: v for k, v in kwargs.items() if k in _READER_INIT_KWARGS}
    read_kwargs = {k: v for k, v in kwargs.items() if k not in _READER_INIT_KWARGS}

    try:
        reader = get_reader(format_type, **init_kwargs)  # Pass only init kwargs
        return reader.read(source, **read_kwargs)  # Pass remaining kwargs to read
    except (IOError, FormatNotSupportedError):
        logger.exception("IO Error reading data")
        raise  # Re-raise specific IO errors
    except Exception as e:
        logger.exception(f"Unexpected error reading data with format '{format_type}'")
        # Wrap unexpected errors in ReadError for consistency?
        raise ReadError(
            "Unexpected error during read",
            source=str(source),
            reader_type=format_type,
            original_error=e,
        ) from e


def write_data(
    format_type: str,
    graph: Graph,
    target: object,
    **kwargs: dict[str, Union[str, int, float, bool]],
) -> object:
    """Writes graph data to a target using the specified format.

    Keyword arguments passed to this function are divided between writer initialization
    options and the writer's `write()` method, based on predefined keys. Consult the
    specific writer's documentation for the exact parameters handled by each.

    Args:
        format_type (str): The format identifier (e.g., 'excel', 'dataframe', 'dict').
        graph (Graph): The graph object containing data to write.
        target (object): The destination target (e.g., file path). Specific writers
                          might ignore this if they return data (like DataFrameWriter).
        **kwargs: Additional keyword arguments passed to the specific writer's
                  `write()` method (e.g., `sheet_name` for excel).

    Returns:
        object: The result of the write operation. For writers like DataFrameWriter
                or DictWriter, this is the created object. For file writers, it's None.

    Raises:
        WriteError: If writing fails.
        FormatNotSupportedError: If the format_type is not registered.
        Exception: Other errors during writer initialization or writing.
    """
    logger.info(
        f"Attempting to write graph data using format '{format_type}' to target type '{type(target).__name__}'"
    )

    # Separate kwargs for init vs write
    init_kwargs = {k: v for k, v in kwargs.items() if k in _WRITER_INIT_KWARGS}
    write_kwargs = {k: v for k, v in kwargs.items() if k not in _WRITER_INIT_KWARGS}

    try:
        writer = get_writer(format_type, **init_kwargs)  # Pass only init kwargs
        # Pass remaining kwargs to the write method
        return writer.write(graph, target, **write_kwargs)
    except (IOError, FormatNotSupportedError):
        logger.exception("IO Error writing data")
        raise  # Re-raise specific IO errors
    except Exception as e:
        logger.exception(f"Unexpected error writing data with format '{format_type}'")
        # Wrap unexpected errors
        raise WriteError(
            "Unexpected error during write",
            target=str(target),
            writer_type=format_type,
            original_error=e,
        ) from e


# --- Public API ---

__all__ = [
    # "get_reader", # Probably don't expose getters directly
    # "get_writer",
    # Base classes (optional exposure)
    "DataReader",
    "DataWriter",
    "FormatNotSupportedError",
    # Exceptions
    "IOError",
    "ReadError",
    "WriteError",
    # Registry functions (optional exposure)
    "list_readers",
    "list_writers",
    # Facade functions
    "read_data",
    "write_data",
]

# --- END FILE: fin_statement_model/io/__init__.py ---

# --- START FILE: fin_statement_model/io/base.py ---
"""Base classes for data readers and writers."""

from abc import ABC, abstractmethod
from typing import Any

# Use absolute import based on project structure
from fin_statement_model.core.graph import Graph


class DataReader(ABC):
    """Abstract base class for all data readers.

    Defines the interface for classes that read data from various sources
    and typically populate or return a Graph object.
    """

    @abstractmethod
    def read(self, source: str, **kwargs: dict[str, Any]) -> Graph:
        """Read data from the specified source and return a Graph.

        Args:
            source: The data source (e.g., file path, API endpoint, dict).
            **kwargs: Additional format-specific options for reading.

        Returns:
            A Graph object populated with the data from the source.

        Raises:
            ReadError: If an error occurs during the reading process.
            NotImplementedError: If the method is not implemented by a subclass.
        """
        raise NotImplementedError


class DataWriter(ABC):
    """Abstract base class for all data writers.

    Defines the interface for classes that write graph data to various targets.
    """

    @abstractmethod
    def write(self, graph: Graph, target: object, **kwargs: dict[str, Any]) -> object:
        """Write data from the Graph object to the specified target.

        Args:
            graph: The Graph object containing the data to write.
            target: The destination target (e.g., file path, database connection).
            **kwargs: Additional format-specific options for writing.

        Raises:
            WriteError: If an error occurs during the writing process.
            NotImplementedError: If the method is not implemented by a subclass.
        """
        raise NotImplementedError

# --- END FILE: fin_statement_model/io/base.py ---

# --- START FILE: fin_statement_model/io/exceptions.py ---
"""IO specific exceptions."""

from typing import Optional

# Use absolute import based on project structure
from fin_statement_model.core.errors import FinancialModelError


class IOError(FinancialModelError):
    """Base exception for all Input/Output errors in the IO package."""

    def __init__(
        self,
        message: str,
        source_or_target: Optional[str] = None,
        format_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the IOError.

        Args:
            message: The base error message.
            source_or_target: Optional identifier for the source (read) or target (write).
            format_type: Optional name of the format or handler involved.
            original_error: Optional underlying exception that caused the failure.
        """
        self.source_or_target = source_or_target
        self.format_type = format_type
        self.original_error = original_error

        context = []
        if source_or_target:
            context.append(f"source/target '{source_or_target}'")
        if format_type:
            context.append(f"format '{format_type}'")

        full_message = f"{message} involving {' and '.join(context)}" if context else message

        if original_error:
            full_message = f"{full_message}: {original_error!s}"

        super().__init__(full_message)


class ReadError(IOError):
    """Exception raised specifically for errors during data read/import operations."""

    def __init__(
        self,
        message: str,
        source: Optional[str] = None,
        reader_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the ReadError.

        Args:
            message: The base error message.
            source: Optional identifier for the data source (e.g., file path, URL).
            reader_type: Optional name of the reader class used for importing.
            original_error: Optional underlying exception that caused the import failure.
        """
        super().__init__(
            message=message,
            source_or_target=source,
            format_type=reader_type,
            original_error=original_error,
        )


class WriteError(IOError):
    """Exception raised specifically for errors during data write/export operations."""

    def __init__(
        self,
        message: str,
        target: Optional[str] = None,
        writer_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the WriteError.

        Args:
            message: The base error message.
            target: Optional identifier for the export destination (e.g., file path).
            writer_type: Optional name of the writer class being used.
            original_error: Optional underlying exception that caused the export failure.
        """
        super().__init__(
            message=message,
            source_or_target=target,
            format_type=writer_type,
            original_error=original_error,
        )


class FormatNotSupportedError(IOError):
    """Exception raised when a requested IO format is not registered or supported."""

    def __init__(self, format_type: str, operation: str = "read/write"):
        """Initializes the FormatNotSupportedError.

        Args:
            format_type: The requested format identifier (e.g., 'excel', 'json').
            operation: The operation being attempted ('read' or 'write').
        """
        message = f"Format '{format_type}' is not supported for {operation} operations."
        super().__init__(message=message, format_type=format_type)

# --- END FILE: fin_statement_model/io/exceptions.py ---

# --- START FILE: fin_statement_model/io/readers/__init__.py ---
"""Data readers for various formats."""

# Import specific readers to ensure they are registered
from . import dict  # noqa: F401
from . import excel  # noqa: F401
from . import csv  # noqa: F401
from . import dataframe  # noqa: F401
from . import fmp  # noqa: F401

__all__ = [
    # Expose reader classes if needed directly, though using the facade is preferred
    # "DictReader",
    # "ExcelReader",
    # "CsvReader",
    # "DataFrameReader",
    # "FmpReader",
]

# --- END FILE: fin_statement_model/io/readers/__init__.py ---

# --- START FILE: fin_statement_model/io/readers/base.py ---
"""Shared utilities for IO readers."""

from typing import Optional, Union

MappingConfig = Union[
    dict[str, str],
    dict[Optional[str], dict[str, str]]
]

def normalize_mapping(
    mapping_config: MappingConfig = None,
    context_key: Optional[str] = None
) -> dict[str, str]:
    """Turn a scoped MappingConfig into a unified flat dict with a required default mapping under None.

    Args:
        mapping_config: MappingConfig object defining name mappings.
        context_key: Optional key (e.g., sheet name or statement type) to select
            a scoped mapping within a scoped config.

    Returns:
        A flat dict mapping original names to canonical names.

    Raises:
        TypeError: If the provided mapping_config is not of a supported structure.
    """
    if mapping_config is None:
        return {}
    if not isinstance(mapping_config, dict):
        raise TypeError(
            f"mapping_config must be a dict, got {type(mapping_config).__name__}"
        )
    if None not in mapping_config:
        raise TypeError("mapping_config must include a default mapping under None")
    default_map = mapping_config[None]

    if not isinstance(default_map, dict):
        raise TypeError(
            "Default mapping (None key) must be a dict[str, str] if present"
        )
    result: dict[str, str] = dict(default_map)
    if context_key and context_key in mapping_config:
        scoped = mapping_config[context_key]
        if not isinstance(scoped, dict):
            raise TypeError(
                f"Scoped mapping for key '{context_key}' must be a dict[str, str]"
            )
        result.update(scoped)
    return result

# --- END FILE: fin_statement_model/io/readers/base.py ---

# --- START FILE: fin_statement_model/io/readers/config/__init__.py ---
"""Package for IO reader default mappings.

This package supports resource loading via importlib.resources.
"""

# --- END FILE: fin_statement_model/io/readers/config/__init__.py ---

# --- START FILE: fin_statement_model/io/readers/csv.py ---
"""Data reader for CSV files."""

import logging
import os
from typing import Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataReader
from fin_statement_model.io.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.readers.base import MappingConfig, normalize_mapping

logger = logging.getLogger(__name__)


@register_reader("csv")
class CsvReader(DataReader):
    """Reads financial statement data from a CSV file into a Graph.

    Assumes a 'long' format where each row represents a single data point
    (item, period, value).
    Requires specifying the columns containing item names, period identifiers,
    and values.

    Supports a `mapping_config` constructor parameter for name mapping,
    accepting either a flat mapping or a statement-type scoped mapping.

    Note:
        When using the `read_data` facade, pass `mapping_config` via init,
        and reader-specific options (`item_col`, `period_col`, `value_col`,
        `pandas_read_csv_kwargs`) to `read()`. Direct instantiation is also supported.
    """

    def __init__(self, mapping_config: MappingConfig = None, **kwargs: Any) -> None:
        """Initialize the CsvReader.

        Args:
            mapping_config (MappingConfig): Optional mapping configuration to
                map CSV item names to canonical node names. Can be either:
                  - Dict[str, str] for a flat mapping.
                  - Dict[Optional[str], Dict[str, str]] for scoped mappings
                    keyed by statement type (or None for default).
            **kwargs: Not used by CsvReader init; reserved for API consistency.
        """
        # Store a normalized flat mapping for default use
        self.mapping = normalize_mapping(mapping_config)

    def read(self, source: str, **kwargs: dict[str, Any]) -> Graph:
        """Read data from a CSV file into a new Graph.

        Args:
            source (str): Path to the CSV file.
            **kwargs: Keyword arguments supported by this reader:
                item_col (str): Name of the column containing item identifiers.
                period_col (str): Name of the column containing period identifiers.
                value_col (str): Name of the column containing numeric values.
                mapping_config (MappingConfig): Overrides the mapping config
                    provided at initialization.
                pandas_read_csv_kwargs (dict): Additional arguments passed
                    directly to `pandas.read_csv()`.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the file cannot be read or required columns are missing.
        """
        file_path = source
        logger.info(f"Starting import from CSV file: {file_path}")

        # --- Validate Inputs ---
        if not os.path.exists(file_path):
            raise ReadError(
                f"File not found: {file_path}",
                source=file_path,
                reader_type="CsvReader",
            )

        item_col = kwargs.get("item_col")
        period_col = kwargs.get("period_col")
        value_col = kwargs.get("value_col")
        read_csv_options = kwargs.get("pandas_read_csv_kwargs", {})

        if not item_col or not period_col or not value_col:
            raise ReadError(
                "Missing required arguments: 'item_col', 'period_col', 'value_col' must be provided.",
                source=file_path,
                reader_type="CsvReader",
            )

        # Normalize mapping config for this read operation
        current_mapping_config = kwargs.get("mapping_config", self.mapping)
        try:
            mapping = normalize_mapping(current_mapping_config)
        except TypeError as te:
            raise ReadError(
                "Invalid mapping_config provided.",
                source=file_path,
                reader_type="CsvReader",
                original_error=te,
            )
        logger.debug(f"Using mapping: {mapping}")

        # --- Read CSV Data ---
        try:
            df = pd.read_csv(file_path, **read_csv_options)

            # Validate required columns exist
            required_cols = {item_col, period_col, value_col}
            missing_cols = required_cols - set(df.columns)
            if missing_cols:
                raise ReadError(
                    f"Missing required columns in CSV: {missing_cols}",
                    source=file_path,
                    reader_type="CsvReader",
                )

            # Convert period column to string
            df[period_col] = df[period_col].astype(str)
            all_periods = sorted(df[period_col].unique().tolist())
            if not all_periods:
                raise ReadError(
                    "No periods found in the specified period column.",
                    source=file_path,
                    reader_type="CsvReader",
                )

            logger.info(f"Identified periods: {all_periods}")
            graph = Graph(periods=all_periods)

            # --- Populate Graph ---
            # Group data by item name
            grouped = df.groupby(item_col)
            validation_errors = []
            nodes_added = 0

            for item_name_csv, group in grouped:
                if pd.isna(item_name_csv) or not item_name_csv:
                    logger.debug("Skipping group with empty item name.")
                    continue

                item_name_csv_str = str(item_name_csv).strip()
                node_name = mapping.get(item_name_csv_str, item_name_csv_str)

                period_values: dict[str, float] = {}
                for _, row in group.iterrows():
                    period = row[period_col]
                    value = row[value_col]

                    if pd.isna(value):
                        continue  # Skip missing values

                    if not isinstance(value, (int, float)):
                        try:
                            value = float(value)
                            logger.warning(
                                f"Converted non-numeric value '{row[value_col]}' to float for node '{node_name}' period '{period}'"
                            )
                        except (ValueError, TypeError):
                            validation_errors.append(
                                f"Item '{item_name_csv_str}': Non-numeric value '{value}' for period '{period}'"
                            )
                            continue  # Skip this invalid value

                    if period in period_values:
                        logger.warning(
                            f"Duplicate value found for node '{node_name}' (from CSV item '{item_name_csv_str}') period '{period}'. Using the last one found."
                        )

                    period_values[period] = float(value)

                if period_values:
                    if graph.has_node(node_name):
                        logger.warning(
                            f"Node '{node_name}' (from CSV item '{item_name_csv_str}') already exists. Overwriting data is not standard for readers."
                        )
                        # Potentially update existing node? For now, log.
                    else:
                        new_node = FinancialStatementItemNode(name=node_name, values=period_values)
                        graph.add_node(new_node)
                        nodes_added += 1

            if validation_errors:
                raise ReadError(
                    f"Validation errors occurred while reading {file_path}: {'; '.join(validation_errors)}",
                    source=file_path,
                    reader_type="CsvReader",
                )

            logger.info(f"Successfully created graph with {nodes_added} nodes from {file_path}.")
            return graph

        except FileNotFoundError:
            raise ReadError(
                f"File not found: {file_path}",
                source=file_path,
                reader_type="CsvReader",
            )
        except ValueError as ve:
            raise ReadError(
                f"Error reading CSV file: {ve}",
                source=file_path,
                reader_type="CsvReader",
                original_error=ve,
            )
        except KeyError as ke:
            raise ReadError(
                f"Column not found error (check item/period/value_col names): {ke}",
                source=file_path,
                reader_type="CsvReader",
                original_error=ke,
            )
        except Exception as e:
            logger.error(f"Failed to read CSV file {file_path}: {e}", exc_info=True)
            raise ReadError(
                message=f"Failed to process CSV file: {e}",
                source=file_path,
                reader_type="CsvReader",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/io/readers/csv.py ---

# --- START FILE: fin_statement_model/io/readers/dataframe.py ---
"""Data reader for pandas DataFrames."""

import logging
import pandas as pd
import numpy as np
from typing import Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataReader
from fin_statement_model.io.registry import register_reader
from fin_statement_model.io.exceptions import ReadError

logger = logging.getLogger(__name__)


@register_reader("dataframe")
class DataFrameReader(DataReader):
    """Reads data from a pandas DataFrame into a Graph.

    Assumes the DataFrame index contains node names and columns contain periods.
    Values should be numeric.
    """

    def __init__(self) -> None:
        """Initialize the DataFrameReader."""
        # No mapping configuration is supported.

    def read(self, source: pd.DataFrame, periods: Optional[list[str]] = None) -> Graph:
        """Read data from a pandas DataFrame into a new Graph.

        Assumes DataFrame index = node names, columns = periods.

        Args:
            source (pd.DataFrame): The DataFrame to read data from.
            periods (list[str], optional): Explicit list of periods (columns) to include.
                If None, all columns are assumed to be periods.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the source is not a DataFrame or has invalid structure.
        """
        df = source
        logger.info("Starting import from DataFrame.")

        # --- Validate Inputs ---
        if not isinstance(df, pd.DataFrame):
            raise ReadError(
                "Source is not a pandas DataFrame.",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        if df.index.name is None and df.index.empty:
            logger.warning(
                "DataFrame index is unnamed and empty, assuming columns are nodes if periods kwarg is provided."
            )
            # Handle case where DF might be oriented differently if periods kwarg is present?
            # For now, stick to index=nodes assumption.

        # Determine periods: use explicit list or infer from columns
        graph_periods_arg = periods
        if graph_periods_arg:
            if not isinstance(graph_periods_arg, list):
                raise ReadError("'periods' argument must be a list of column names.")
            missing_cols = [p for p in graph_periods_arg if p not in df.columns]
            if missing_cols:
                raise ReadError(
                    f"Specified periods (columns) not found in DataFrame: {missing_cols}"
                )
            graph_periods = sorted(graph_periods_arg)
            df_subset = df[graph_periods]  # Select only specified period columns
        else:
            # Assume all columns are periods
            graph_periods = sorted(df.columns.astype(str).tolist())
            df_subset = df

        if not graph_periods:
            raise ReadError(
                "No periods identified in DataFrame columns.",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        logger.info(f"Using periods (columns): {graph_periods}")
        graph = Graph(periods=graph_periods)

        # --- Populate Graph ---
        validation_errors = []
        nodes_added = 0
        for node_name_df, row in df_subset.iterrows():
            if pd.isna(node_name_df) or not node_name_df:
                logger.debug("Skipping row with empty index name.")
                continue

            node_name = str(node_name_df).strip()
            period_values: dict[str, float] = {}
            for period in graph_periods:
                value = row[period]
                if pd.isna(value):
                    continue  # Skip NaN values

                if not isinstance(value, (int, float, np.number)):
                    try:
                        value = float(value)
                        logger.warning(
                            f"Converted non-numeric value '{row[period]}' to float for node '{node_name}' period '{period}'"
                        )
                    except (ValueError, TypeError):
                        validation_errors.append(
                            f"Node '{node_name}': Non-numeric value '{value}' for period '{period}'"
                        )
                        continue  # Skip invalid value

                period_values[period] = float(value)

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' already exists. Overwriting data is not standard for readers."
                    )
                    # Update existing? Log for now.
                else:
                    new_node = FinancialStatementItemNode(name=node_name, values=period_values)
                    graph.add_node(new_node)
                    nodes_added += 1

        if validation_errors:
            raise ReadError(
                f"Validation errors occurred while reading DataFrame: {'; '.join(validation_errors)}",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        logger.info(f"Successfully created graph with {nodes_added} nodes from DataFrame.")
        return graph

        # No specific file operations, so less need for broad Exception catch
        # Specific errors handled above (TypeError, ValueError from float conversion)

# --- END FILE: fin_statement_model/io/readers/dataframe.py ---

# --- START FILE: fin_statement_model/io/readers/dict.py ---
"""Data reader for Python dictionaries."""

import logging
from typing import Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataReader
from fin_statement_model.io.registry import register_reader
from fin_statement_model.io.exceptions import ReadError

logger = logging.getLogger(__name__)


@register_reader("dict")
class DictReader(DataReader):
    """Reads data from a Python dictionary to create a new Graph.

    Expects a dictionary format: {node_name: {period: value, ...}, ...}
    Creates FinancialStatementItemNode instances for each entry.

    Note:
        When using the `read_data` facade, pass dictionary data via `source` and
        reader-specific options (`periods`) to `read()`.
    """

    def read(self, source: dict[str, dict[str, float]], periods: Optional[list[str]] = None) -> Graph:
        """Create a new Graph from a dictionary.

        Args:
            source: Dictionary mapping node names to period-value dictionaries.
                    Format: {node_name: {period: value, ...}, ...}
            periods (list[str], optional): Explicit list of periods for the new graph.
                If None, inferred from data keys.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the source data format is invalid or processing fails.
            # DataValidationError: If data values are not numeric.
        """
        logger.info("Starting import from dictionary to create a new graph.")

        if not isinstance(source, dict):
            raise ReadError(
                message="Invalid source type for DictReader. Expected dict.",
                source="dict_input",
                reader_type="DictReader",
            )

        # Validate data structure and collect all periods
        all_periods = set()
        validation_errors = []
        try:
            for node_name, period_values in source.items():
                if not isinstance(period_values, dict):
                    validation_errors.append(
                        f"Node '{node_name}': Invalid format - expected dict, got {type(period_values).__name__}"
                    )
                    continue  # Skip further checks for this node
                for period, value in period_values.items():
                    # Basic type checks - can be expanded
                    if not isinstance(period, str):
                        validation_errors.append(
                            f"Node '{node_name}': Invalid period format '{period}' - expected string."
                        )
                    if not isinstance(value, (int, float)):
                        validation_errors.append(
                            f"Node '{node_name}' period '{period}': Invalid value type {type(value).__name__} - expected number."
                        )
                    all_periods.add(str(period))

            if validation_errors:
                # Use core DataValidationError if it exists and is suitable
                # Otherwise, stick to ReadError or a specific IOValidationError
                # raise DataValidationError(
                #     message="Input dictionary failed validation",
                #     validation_errors=validation_errors
                # )
                raise ReadError(
                    f"Input dictionary failed validation: {'; '.join(validation_errors)}",
                    source="dict_input",
                    reader_type="DictReader",
                )

        except Exception as e:
            # Catch unexpected validation errors
            raise ReadError(
                message=f"Error validating input dictionary: {e}",
                source="dict_input",
                reader_type="DictReader",
                original_error=e,
            ) from e

        # Determine graph periods
        graph_periods = periods
        if graph_periods is None:
            graph_periods = sorted(list(all_periods))
            logger.debug(f"Inferred graph periods from data: {graph_periods}")
        # Optional: Validate if all data periods are within the provided list
        elif not all_periods.issubset(set(graph_periods)):
            missing = all_periods - set(graph_periods)
            logger.warning(f"Data contains periods not in specified graph periods: {missing}")
            # Decide whether to error or just ignore extra data

        # Create graph and add nodes
        try:
            graph = Graph(periods=graph_periods)
            for node_name, period_values in source.items():
                # Filter values to only include those matching graph_periods
                filtered_values = {p: v for p, v in period_values.items() if p in graph_periods}
                if filtered_values:
                    # Create FinancialStatementItemNode directly
                    # Assumes FinancialStatementItemNode takes name and values dict
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=filtered_values.copy()
                    )
                    graph.add_node(new_node)
                else:
                    logger.debug(
                        f"Node '{node_name}' has no data for specified graph periods. Skipping."
                    )

            logger.info(
                f"Successfully created graph with {len(graph.nodes)} nodes from dictionary."
            )
            return graph

        except Exception as e:
            # Catch errors during graph/node creation
            logger.error(f"Failed to create graph from dictionary: {e}", exc_info=True)
            raise ReadError(
                message="Failed to build graph from dictionary data",
                source="dict_input",
                reader_type="DictReader",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/io/readers/dict.py ---

# --- START FILE: fin_statement_model/io/readers/excel.py ---
"""Data reader for Excel files."""

import logging
import os
from typing import Optional, ClassVar, Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataReader
from fin_statement_model.io.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.readers.base import MappingConfig, normalize_mapping

logger = logging.getLogger(__name__)


@register_reader("excel")
class ExcelReader(DataReader):
    """Reads financial statement data from an Excel file into a Graph.

    Expects data in a tabular format where rows typically represent items
    and columns represent periods, or vice-versa.
    Requires specifying sheet name, period identification, and item identification.

    Note:
        When using the `read_data` facade, pass `mapping_config` via init,
        and reader-specific options (`sheet_name`, `periods_row`, `items_col`,
        `statement_type`, `header_row`, `nrows`, `skiprows`) to the `read()` method.
        Direct instantiation and use of `ExcelReader` is also supported.
    """

    # Default field mappings (can be overridden in __init__)
    DEFAULT_INCOME_STATEMENT_MAPPING: ClassVar[dict[str, str]] = {
        "Revenue": "revenue",
        "Cost of Revenue": "cost_of_goods_sold",
        "Gross Profit": "gross_profit",
        # ... (Add other defaults as needed)
    }
    DEFAULT_BALANCE_SHEET_MAPPING: ClassVar[dict[str, str]] = {
        "Cash & Cash Equivalents": "cash_and_cash_equivalents",
        # ... (Add other defaults as needed)
    }
    DEFAULT_CASH_FLOW_MAPPING: ClassVar[dict[str, str]] = {
        "Net Income": "net_income",
        # ... (Add other defaults as needed)
    }

    # Default mappings from expected columns to potential Excel column names
    _REQUIRED_COLUMNS: ClassVar[dict[str, list[str]]] = {
        "item": ["Item", "Metric", "Account", "Financial Statement Line Item"],
    }
    # Optional config: mapping periods in file to standard internal names
    _OPTIONAL_COLUMNS: ClassVar[dict[str, list[str]]] = {
        "description": ["Description"],
    }
    # Default item name mapping
    _DEFAULT_MAPPING: ClassVar[dict[str, str]] = {}

    def __init__(self, mapping_config: MappingConfig = None, **kwargs: Any) -> None:
        """Initialize the ExcelReader.

        Args:
            mapping_config (MappingConfig): Optional mapping configuration to
                map Excel item names to canonical node names. Can be either:
                  - Dict[str, str] for a flat mapping applied to all statement types.
                  - Dict[Optional[str], Dict[str, str]] for scoped mappings
                    keyed by statement type (or None for default).
            **kwargs: Not used by ExcelReader init; reserved for API consistency.
        """
        super().__init__(**kwargs)
        # Store raw mapping_config for later resolution
        self.mapping_config = mapping_config or {}

    def _get_mapping(
        self,
        statement_type: Optional[str],
        mapping_config: MappingConfig = None
    ) -> dict[str, str]:
        """Get the appropriate mapping based on statement type and optional override config."""
        # Start with defaults based on statement type
        if statement_type == "income_statement":
            mapping = dict(self.DEFAULT_INCOME_STATEMENT_MAPPING)
        elif statement_type == "balance_sheet":
            mapping = dict(self.DEFAULT_BALANCE_SHEET_MAPPING)
        elif statement_type == "cash_flow":
            mapping = dict(self.DEFAULT_CASH_FLOW_MAPPING)
        else:
            mapping = {}

        # Determine which config to use: override if provided, else instance config
        config = mapping_config if mapping_config is not None else self.mapping_config
        # Normalize and overlay user-provided mappings
        user_map = normalize_mapping(config, context_key=statement_type)
        mapping.update(user_map)
        return mapping

    def read(self, source: str, **kwargs: dict[str, Any]) -> Graph:
        """Read data from an Excel file sheet into a new Graph.

        Args:
            source (str): Path to the Excel file.
            **kwargs: Required keyword arguments:
                sheet_name (str): Name of the sheet containing the data.
                periods_row (int): 1-based index of the row containing period headers.
                items_col (int): 1-based index of the column containing item names.
            Optional keyword arguments:
                statement_type (str): Type of statement ('income_statement', 'balance_sheet', 'cash_flow'),
                    used to select default mappings if the provided mapping config is scoped.
                header_row (int): 1-based index for pandas header reading (defaults to periods_row).
                    Use if data headers differ from period headers.
                nrows (int): Number of rows to read from the sheet.
                skiprows (int): Number of rows to skip at the beginning.
                mapping_config (MappingConfig): Overrides the mapping configuration provided at initialization.
                    Accepts either a flat `Dict[str, str]` or a scoped `Dict[Optional[str], Dict[str, str]]`.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the file cannot be read, sheet/row/col are invalid, or required kwargs missing.
        """
        file_path = source
        logger.info(f"Starting import from Excel file: {file_path}")

        # --- Validate Inputs ---
        if not os.path.exists(file_path):
            raise ReadError(
                f"File not found: {file_path}",
                source=file_path,
                reader_type="ExcelReader",
            )
        if not file_path.endswith((".xls", ".xlsx", ".xlsm")):
            raise ReadError(
                f"Not a valid Excel file extension: {file_path}",
                source=file_path,
                reader_type="ExcelReader",
            )

        sheet_name = kwargs.get("sheet_name")
        periods_row_0idx = (
            kwargs.get("periods_row") - 1 if kwargs.get("periods_row") else None
        )  # 0-based for pandas
        items_col_0idx = (
            kwargs.get("items_col") - 1 if kwargs.get("items_col") else None
        )  # 0-based for pandas
        header_row_0idx = (
            kwargs.get("header_row") - 1 if kwargs.get("header_row") else periods_row_0idx
        )

        if sheet_name is None or periods_row_0idx is None or items_col_0idx is None:
            raise ReadError(
                "Missing required arguments: 'sheet_name', 'periods_row', 'items_col' must be provided.",
                source=file_path,
                reader_type="ExcelReader",
            )

        statement_type = kwargs.get("statement_type")
        nrows = kwargs.get("nrows")
        skiprows = kwargs.get("skiprows")

        # Determine mapping for this operation, allowing override via kwargs
        current_mapping_config = kwargs.get("mapping_config")
        try:
            mapping = self._get_mapping(statement_type, mapping_config=current_mapping_config)
        except TypeError as te:
            raise ReadError(
                "Invalid mapping_config provided.",
                source=file_path,
                reader_type="ExcelReader",
                original_error=te,
            )
        logger.debug(f"Using mapping for statement type '{statement_type}': {mapping}")

        # --- Read Excel Data ---
        try:
            # Read the sheet, potentially skipping rows and limiting rows read
            # Use header_row_0idx to correctly identify column headers
            df = pd.read_excel(
                file_path,
                sheet_name=sheet_name,
                header=header_row_0idx,
                skiprows=skiprows,  # skiprows is applied *before* header selection
                nrows=nrows,
            )

            # Identify the actual period columns based on the periods_row
            # Read the periods row separately if header is different
            if header_row_0idx != periods_row_0idx:
                periods_df = pd.read_excel(
                    file_path,
                    sheet_name=sheet_name,
                    header=None,
                    skiprows=periods_row_0idx,
                    nrows=1,
                )
                period_headers = periods_df.iloc[0].astype(str).tolist()
            else:
                # Periods are in the main header row read by pandas
                period_headers = df.columns.astype(str).tolist()

            # Find the item column name from the initial read
            if items_col_0idx >= len(df.columns):
                raise ReadError(
                    f"items_col index ({items_col_0idx + 1}) is out of bounds for sheet '{sheet_name}'. Found columns: {df.columns.tolist()}",
                    source=file_path,
                    reader_type="ExcelReader",
                )
            # item_column_name = df.columns[items_col_0idx] # This variable is assigned but never used.

            # Filter period headers: exclude the item column header itself
            # Assuming periods start *after* the item column typically
            graph_periods = [p for i, p in enumerate(period_headers) if i > items_col_0idx and p]
            if not graph_periods:
                raise ReadError(
                    f"Could not identify period columns in row {periods_row_0idx + 1} after column {items_col_0idx + 1} in sheet '{sheet_name}'. Headers found: {period_headers}",
                    source=file_path,
                    reader_type="ExcelReader",
                )
            logger.info(f"Identified periods: {graph_periods}")

            # Create graph
            graph = Graph(periods=graph_periods)

            # --- Populate Graph ---
            validation_errors = []
            nodes_added = 0
            for index, row in df.iterrows():
                item_name_excel = row.iloc[
                    items_col_0idx
                ]  # Get item name using the identified column
                if pd.isna(item_name_excel) or not item_name_excel:
                    # logger.debug(f"Skipping row {index + (skiprows or 0) + (header_row_0idx or 0) + 1}: Empty item name.")
                    continue

                item_name_excel = str(item_name_excel).strip()
                node_name = mapping.get(
                    item_name_excel, item_name_excel
                )  # Use mapping or fallback to original name

                # Get values for the identified periods
                period_values: dict[str, float] = {}
                for period in graph_periods:
                    try:
                        # Find the corresponding column in the DataFrame using the period header
                        # This assumes the period headers read initially match the df columns
                        if period in df.columns:
                            value = row[period]
                            if pd.isna(value):
                                # Keep NaN or skip? For now, skip. Could represent as None or NaN later.
                                # logger.debug(f"NaN value for {node_name} period {period}")
                                continue
                            elif isinstance(value, (int, float)):
                                period_values[period] = float(value)
                            else:
                                # Attempt conversion, log warning if fails
                                try:
                                    period_values[period] = float(value)
                                    logger.warning(
                                        f"Converted non-numeric value '{value}' to float for node '{node_name}' period '{period}'"
                                    )
                                except (ValueError, TypeError):
                                    validation_errors.append(
                                        f"Row {index}: Non-numeric value '{value}' for node '{node_name}' (from '{item_name_excel}') period '{period}'"
                                    )
                        else:
                            # This shouldn't happen if period_headers came from df.columns
                            logger.warning(
                                f"Period header '{period}' not found in DataFrame columns for row {index}."
                            )
                    except KeyError:
                        validation_errors.append(
                            f"Row {index}: Column for period '{period}' not found for node '{node_name}'"
                        )
                    except Exception as e:
                        validation_errors.append(
                            f"Row {index}: Error processing value for node '{node_name}' period '{period}': {e}"
                        )

                if period_values:
                    if graph.has_node(node_name):
                        logger.warning(
                            f"Node '{node_name}' (from Excel item '{item_name_excel}') already exists. Overwriting data is not standard for readers. Consider unique names or merging logic."
                        )
                        # Get existing node and update? Or raise error? For now, log warning.
                        # existing_node = graph.get_node(node_name)
                        # if isinstance(existing_node, FinancialStatementItemNode):
                        #     existing_node.values.update(period_values)
                    else:
                        # Create and add new node
                        new_node = FinancialStatementItemNode(name=node_name, values=period_values)
                        graph.add_node(new_node)
                        nodes_added += 1
                # else: No valid values for this item in the specified periods

            if validation_errors:
                raise ReadError(
                    f"Validation errors occurred while reading {file_path} sheet '{sheet_name}': {'; '.join(validation_errors)}",
                    source=file_path,
                    reader_type="ExcelReader",
                )

            logger.info(
                f"Successfully created graph with {nodes_added} nodes from {file_path} sheet '{sheet_name}'."
            )
            return graph

        except FileNotFoundError:
            raise ReadError(
                f"File not found: {file_path}",
                source=file_path,
                reader_type="ExcelReader",
            )
        except ValueError as ve:
            # Pandas raises ValueError for bad sheet names etc.
            raise ReadError(
                f"Error reading Excel file: {ve}",
                source=file_path,
                reader_type="ExcelReader",
                original_error=ve,
            )
        except KeyError as ke:
            # Raised if essential columns (like item column after mapping) are missing
            raise ReadError(
                f"Missing expected column/item: {ke}. Check items_col ({items_col_0idx + 1}) and sheet structure.",
                source=file_path,
                reader_type="ExcelReader",
                original_error=ke,
            )
        except Exception as e:
            logger.error(
                f"Failed to read Excel file {file_path} sheet '{sheet_name}': {e}",
                exc_info=True,
            )
            raise ReadError(
                message=f"Failed to process Excel file: {e}",
                source=file_path,
                reader_type="ExcelReader",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/io/readers/excel.py ---

# --- START FILE: fin_statement_model/io/readers/fmp.py ---
"""Data reader for the Financial Modeling Prep (FMP) API."""

import logging
import os
import requests
from typing import Optional, ClassVar, Any
import numpy as np
import yaml
from pathlib import Path

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataReader
from fin_statement_model.io.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.readers.base import MappingConfig, normalize_mapping

logger = logging.getLogger(__name__)


@register_reader("fmp")
class FmpReader(DataReader):
    """Reads financial statement data from the FMP API into a Graph.

    Fetches data for a specific ticker and statement type.
    Requires an API key, either passed directly or via the FMP_API_KEY env var.

    Supports a `mapping_config` constructor parameter for mapping API field names to canonical node names,
    accepting either a flat mapping or a statement-type keyed mapping.

    Note:
        When using the `read_data` facade, pass `api_key` and `mapping_config` via init,
        and reader-specific options (`statement_type`, `period_type`, `limit`) to the `read()` method.
        Direct instantiation of `FmpReader` is also supported.

    Stateful Use:
        For advanced use cases involving repeated API calls, consider instantiating
        and reusing a single `FmpReader` instance to avoid redundant API key
        validations and improve performance.
    """

    BASE_URL = "https://financialmodelingprep.com/api/v3"

    # Load default mappings from YAML configuration
    DEFAULT_MAPPINGS: ClassVar[dict[str, dict[str, str]]] = {}

    @classmethod
    def _load_default_mappings(cls) -> None:
        """Load default mapping configurations from YAML file into DEFAULT_MAPPINGS."""
        # Load mapping YAML from config directory relative to this file
        config_path = Path(__file__).parent / "config" / "fmp_default_mappings.yaml"
        with config_path.open("r", encoding="utf-8") as f:
            cls.DEFAULT_MAPPINGS = yaml.safe_load(f)

    # Default statement types requested from FMP API
    _STATEMENT_TYPES: ClassVar[dict[str, str]] = {
        "income_statement": "income-statement",
        "balance_sheet": "balance-sheet-statement",
        "cash_flow": "cash-flow-statement",
    }
    # Mapping from API field names to standard node names
    _API_ENDPOINTS: ClassVar[dict[str, str]] = {
        "income_statement": "income-statement",
        "balance_sheet": "balance-sheet-statement",
        "cash_flow": "cash-flow-statement",
    }
    # Default required parameters for the API
    _REQUIRED_PARAMS: ClassVar[list[str]] = ["apikey"]

    def __init__(
        self,
        api_key: Optional[str] = None,
        mapping_config: MappingConfig = None,
    ) -> None:
        """Initialize the FmpReader.

        Args:
            api_key: FMP API key. If None, attempts to use FMP_API_KEY env var.
            mapping_config (MappingConfig): Optional mapping configuration to
                override default API field mappings. Can be either:
                  - Dict[str, str] for a flat mapping.
                  - Dict[Optional[str], Dict[str, str]] for scoped mappings
                    keyed by statement type (or None for default).
        """
        self.api_key = api_key or os.environ.get("FMP_API_KEY")
        if not self.api_key:
            logger.warning(
                "FMP API key not provided via init or FMP_API_KEY env var."
            )

        # Store raw mapping_config for later resolution
        self.mapping_config = mapping_config

    def _get_mapping(
        self,
        statement_type: Optional[str],
        mapping_config: MappingConfig = None,
    ) -> dict[str, str]:
        """Get the appropriate mapping based on statement type and optional override config."""
        # Start with defaults based on statement type loaded from config
        mapping = dict(self.DEFAULT_MAPPINGS.get(statement_type, {}))

        # Choose override config if provided, else use instance config
        config = mapping_config if mapping_config is not None else self.mapping_config
        # Normalize and overlay user-provided mappings
        user_map = normalize_mapping(config, context_key=statement_type)
        mapping.update(user_map)
        return mapping

    def _validate_api_key(self):
        """Perform a simple check if the API key seems valid."""
        if not self.api_key:
            raise ReadError(
                "FMP API key is required for reading.",
                source="FMP API",
                reader_type="FmpReader",
            )
        try:
            # Use a cheap endpoint for validation
            test_url = f"{self.BASE_URL}/profile/AAPL?apikey={self.api_key}"  # Example
            response = requests.get(test_url, timeout=10)
            response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
            # Basic check on response content if needed
            if not response.json():
                raise ReadError(
                    "API key validation returned empty response.",
                    source="FMP API",
                    reader_type="FmpReader",
                )
            logger.debug("FMP API key validated successfully.")
        except requests.exceptions.RequestException as e:
            logger.error(f"FMP API key validation failed: {e}", exc_info=True)
            raise ReadError(
                f"FMP API key validation failed: {e}",
                source="FMP API",
                reader_type="FmpReader",
                original_error=e,
            )

    def read(self, source: str, **kwargs: dict[str, Any]) -> Graph:
        """Fetch data from FMP API and return a Graph.

        Args:
            source (str): The stock ticker symbol (e.g., "AAPL").
            **kwargs: Required keyword arguments:
                statement_type (str): Type of statement ('income_statement', 'balance_sheet', 'cash_flow').
            Optional keyword arguments:
                period_type (str): 'FY' for annual (default) or 'QTR' for quarterly.
                limit (int): Number of past periods to fetch (default: 50).
                mapping_config (MappingConfig): Overrides the mapping configuration provided at init.
                    Accepts either a flat `Dict[str, str]` or a scoped `Dict[Optional[str], Dict[str, str]]`.
                api_key (str): Overrides the API key provided at init.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If API key is missing/invalid, API request fails, or data format is unexpected.
        """
        ticker = source
        statement_type = kwargs.get("statement_type")
        period_type_arg = kwargs.get("period_type", "FY")
        limit = kwargs.get("limit", 50)
        self.api_key = kwargs.get("api_key", self.api_key)

        # --- Validate Inputs ---
        if not ticker or not isinstance(ticker, str):
            raise ReadError(
                "Invalid source (ticker) provided. Expected a non-empty string.",
                source=ticker,
                reader_type="FmpReader",
            )
        if statement_type not in ["income_statement", "balance_sheet", "cash_flow"]:
            raise ReadError(
                "Missing or invalid required argument: 'statement_type'. Must be one of: income_statement, balance_sheet, cash_flow.",
                source=ticker,
                reader_type="FmpReader",
            )
        if period_type_arg not in ["FY", "QTR"]:
            raise ReadError(
                "Invalid 'period_type'. Must be 'FY' or 'QTR'.",
                source=ticker,
                reader_type="FmpReader",
            )

        self._validate_api_key()  # Ensure API key is usable

        # Determine mapping for this operation, allowing override via kwargs
        current_mapping_config = kwargs.get("mapping_config")
        try:
            mapping = self._get_mapping(
                statement_type,
                mapping_config=current_mapping_config,
            )
        except TypeError as te:
            raise ReadError(
                "Invalid mapping_config provided.",
                source=ticker,
                reader_type="FmpReader",
                original_error=te,
            )
        logger.debug(f"Using mapping for {ticker} {statement_type}: {mapping}")

        # --- Fetch API Data ---
        # Correct endpoint construction based on FMP v3 docs
        # e.g., /income-statement/AAPL, not /income_statement-statement/AAPL
        endpoint_path = statement_type.replace("_", "-")
        endpoint = f"{self.BASE_URL}/{endpoint_path}/{ticker}"
        params = {"apikey": self.api_key, "limit": limit}
        if period_type_arg == "QTR":
            params["period"] = "quarter"

        try:
            logger.info(
                f"Fetching {period_type_arg} {statement_type} for {ticker} from FMP API (limit={limit})."
            )
            response = requests.get(endpoint, params=params, timeout=30)  # Increased timeout
            response.raise_for_status()  # Check for HTTP errors
            api_data = response.json()

            if not isinstance(api_data, list):
                raise ReadError(
                    f"Unexpected API response format. Expected list, got {type(api_data)}. Response: {str(api_data)[:100]}...",
                    source=f"FMP API ({ticker})",
                    reader_type="FmpReader",
                )
            if not api_data:
                logger.warning(f"FMP API returned empty list for {ticker} {statement_type}.")
                # Return empty graph or raise? Returning empty for now.
                return Graph(periods=[])

        except requests.exceptions.RequestException as e:
            logger.error(
                f"FMP API request failed for {ticker} {statement_type}: {e}",
                exc_info=True,
            )
            raise ReadError(
                f"FMP API request failed: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            )
        except Exception as e:
            logger.error(f"Failed to process FMP API response: {e}", exc_info=True)
            raise ReadError(
                f"Failed to process FMP API response: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            )

        # --- Process Data and Populate Graph ---
        try:
            # FMP data is usually newest first, reverse to process chronologically
            api_data.reverse()

            # Extract periods (e.g., 'date' or 'fillingDate')
            # Using 'date' as it usually represents the period end date
            periods = [item.get("date") for item in api_data if item.get("date")]
            if not periods:
                raise ReadError(
                    "Could not extract periods ('date' field) from FMP API response.",
                    source=f"FMP API ({ticker})",
                    reader_type="FmpReader",
                )

            graph = Graph(periods=periods)
            all_item_data: dict[str, dict[str, float]] = {}

            # Collect data for all items across all periods
            for period_data in api_data:
                period = period_data.get("date")
                if not period:
                    continue  # Skip records without a date

                for api_field, value in period_data.items():
                    node_name = mapping.get(api_field, api_field)  # Use mapping or fallback

                    # Initialize node data dict if first time seeing this node
                    if node_name not in all_item_data:
                        all_item_data[node_name] = {p: np.nan for p in periods}  # Pre-fill with NaN

                    # Store value for this period
                    if isinstance(value, (int, float)):
                        all_item_data[node_name][period] = float(value)

            # Create nodes from collected data
            nodes_added = 0
            for node_name, period_values in all_item_data.items():
                # Filter out periods that only have NaN
                valid_period_values = {p: v for p, v in period_values.items() if not np.isnan(v)}
                if valid_period_values:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=valid_period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

            logger.info(
                f"Successfully created graph with {nodes_added} nodes from FMP API for {ticker} {statement_type}."
            )
            return graph

        except Exception as e:
            logger.error(f"Failed to parse FMP data and build graph: {e}", exc_info=True)
            raise ReadError(
                message=f"Failed to parse FMP data: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            ) from e

# After class definition, load default mappings
FmpReader._load_default_mappings()

# --- END FILE: fin_statement_model/io/readers/fmp.py ---

# --- START FILE: fin_statement_model/io/registry.py ---
"""Registry for readers and writers."""

import logging
from typing import TypeVar, Callable, Any

from .base import DataReader, DataWriter
from .exceptions import FormatNotSupportedError

logger = logging.getLogger(__name__)

# Type variables for generic functions
R = TypeVar("R", bound=DataReader)
W = TypeVar("W", bound=DataWriter)

# Internal registries
_readers: dict[str, type[DataReader]] = {}
_writers: dict[str, type[DataWriter]] = {}


def register_reader(format_type: str) -> Callable[[type[R]], type[R]]:
    """Decorator to register a DataReader class for a specific format type.

    Args:
        format_type: The string identifier for the format (e.g., 'excel', 'csv').

    Returns:
        A decorator function that registers the class and returns it unmodified.

    Raises:
        ValueError: If the format_type is already registered for a reader.
    """

    def decorator(cls: type[R]) -> type[R]:
        if format_type in _readers:
            # Allow re-registration if it's the exact same class (e.g., during reload)
            if _readers[format_type] is not cls:
                raise ValueError(
                    f"Reader format type '{format_type}' already registered to {_readers[format_type]}."
                )
            # If same class, just log and allow (idempotent registration)
            logger.debug(f"Re-registering reader format type '{format_type}' to {cls.__name__}")
        else:
            logger.debug(f"Registering reader format type '{format_type}' to {cls.__name__}")
        _readers[format_type] = cls
        return cls

    return decorator


def register_writer(format_type: str) -> Callable[[type[W]], type[W]]:
    """Decorator to register a DataWriter class for a specific format type.

    Args:
        format_type: The string identifier for the format (e.g., 'excel', 'json').

    Returns:
        A decorator function that registers the class and returns it unmodified.

    Raises:
        ValueError: If the format_type is already registered for a writer.
    """

    def decorator(cls: type[W]) -> type[W]:
        if format_type in _writers:
            # Allow re-registration if it's the exact same class
            if _writers[format_type] is not cls:
                raise ValueError(
                    f"Writer format type '{format_type}' already registered to {_writers[format_type]}."
                )
            logger.debug(f"Re-registering writer format type '{format_type}' to {cls.__name__}")
        else:
            logger.debug(f"Registering writer format type '{format_type}' to {cls.__name__}")
        _writers[format_type] = cls
        return cls

    return decorator


def get_reader(format_type: str, **kwargs: Any) -> DataReader:
    """Get an instance of the registered DataReader for the given format type.

    Args:
        format_type: The string identifier for the format.
        **kwargs: Keyword arguments to pass to the reader's constructor.

    Returns:
        An initialized DataReader instance.

    Raises:
        FormatNotSupportedError: If no reader is registered for the format type.
        Exception: Any exception raised during the reader's __init__.
    """
    if format_type not in _readers:
        raise FormatNotSupportedError(format_type=format_type, operation="read")

    reader_class = _readers[format_type]
    try:
        return reader_class(**kwargs)
    except Exception as e:
        logger.error(
            f"Failed to instantiate reader for format '{format_type}' ({reader_class.__name__}): {e}",
            exc_info=True,
        )
        # Re-raise to allow specific handling upstream, but provide context
        raise RuntimeError(f"Failed to initialize reader {reader_class.__name__}: {e}") from e


def get_writer(format_type: str, **kwargs: Any) -> DataWriter:
    """Get an instance of the registered DataWriter for the given format type.

    Args:
        format_type: The string identifier for the format.
        **kwargs: Keyword arguments to pass to the writer's constructor.

    Returns:
        An initialized DataWriter instance.

    Raises:
        FormatNotSupportedError: If no writer is registered for the format type.
        Exception: Any exception raised during the writer's __init__.
    """
    if format_type not in _writers:
        raise FormatNotSupportedError(format_type=format_type, operation="write")

    writer_class = _writers[format_type]
    try:
        return writer_class(**kwargs)
    except Exception as e:
        logger.error(
            f"Failed to instantiate writer for format '{format_type}' ({writer_class.__name__}): {e}",
            exc_info=True,
        )
        # Re-raise
        raise RuntimeError(f"Failed to initialize writer {writer_class.__name__}: {e}") from e


def list_readers() -> dict[str, type[DataReader]]:
    """Return a copy of the registered reader classes."""
    return _readers.copy()


def list_writers() -> dict[str, type[DataWriter]]:
    """Return a copy of the registered writer classes."""
    return _writers.copy()

# --- END FILE: fin_statement_model/io/registry.py ---

# --- START FILE: fin_statement_model/io/writers/__init__.py ---
"""Data writers for various formats."""

# Import specific writers to ensure they are registered
from . import dict  # noqa: F401
from . import excel  # noqa: F401
from . import dataframe  # noqa: F401

# Note: No CsvWriter was identified/created

__all__ = [
    # Expose writer classes if needed directly
    # "DictWriter",
    # "ExcelWriter",
    # "DataFrameWriter",
]

# --- END FILE: fin_statement_model/io/writers/__init__.py ---

# --- START FILE: fin_statement_model/io/writers/dataframe.py ---
"""Data writer for pandas DataFrames."""

import logging

import pandas as pd
import numpy as np

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.base import DataWriter
from fin_statement_model.io.registry import register_writer
from fin_statement_model.io.exceptions import WriteError

logger = logging.getLogger(__name__)


@register_writer("dataframe")
class DataFrameWriter(DataWriter):
    """Writes graph data to a pandas DataFrame.

    Converts the graph to a DataFrame with node names as index and periods as columns.

    Note:
        When using the `write_data` facade, writer initialization has no specific kwargs,
        and writer-specific options (`recalculate`, `include_nodes`) should be passed to `write()`.
        Direct instantiation of `DataFrameWriter` is also supported.
    """

    def write(
        self, graph: Graph, target: object = None, **kwargs: dict[str, object]
    ) -> pd.DataFrame:
        """Convert the graph data to a pandas DataFrame.

        Args:
            graph (Graph): The Graph instance to export.
            target (object): Ignored for DataFrameWriter; returns the DataFrame.
            **kwargs: Writer-specific keyword arguments:
                recalculate (bool): Recalculate graph before export (default: True).
                include_nodes (list[str]): Optional list of node names to include.

        Returns:
            pd.DataFrame: DataFrame with node names as index and periods as columns.

        Raises:
            WriteError: If an error occurs during conversion.
        """
        recalculate = kwargs.get("recalculate", True)
        include_nodes = kwargs.get("include_nodes")
        logger.info("Exporting graph to DataFrame format.")

        try:
            if recalculate:
                try:
                    if graph.periods:
                        graph.recalculate_all(periods=graph.periods)
                        logger.info("Recalculated graph before exporting to DataFrame.")
                    else:
                        logger.warning("Graph has no periods defined, skipping recalculation.")
                except Exception as e:
                    logger.error(
                        f"Error during recalculation for DataFrame export: {e}",
                        exc_info=True,
                    )
                    logger.warning(
                        "Proceeding to export DataFrame without successful recalculation."
                    )

            periods = sorted(graph.periods) if graph.periods else []
            data: dict[str, dict[str, float]] = {}

            nodes_to_process = include_nodes if include_nodes else graph.nodes.keys()
            if include_nodes:
                missing_nodes = [n for n in include_nodes if n not in graph.nodes]
                if missing_nodes:
                    logger.warning(
                        f"Nodes specified in include_nodes not found in graph: {missing_nodes}"
                    )
                nodes_to_process = [n for n in include_nodes if n in graph.nodes]

            for node_id in nodes_to_process:
                node = graph.nodes[node_id]
                row: dict[str, float] = {}
                for period in periods:
                    value = np.nan
                    try:
                        if hasattr(node, "calculate") and callable(node.calculate):
                            value = node.calculate(period)
                        elif (
                            hasattr(node, "values")
                            and isinstance(node.values, dict)
                            and period in node.values
                        ):
                            value = node.values.get(period, np.nan)

                        if not isinstance(value, (int, float, np.number)) or not np.isfinite(value):
                            value = np.nan
                    except Exception as e:
                        logger.debug(
                            f"Could not get value for node '{node_id}' period '{period}' for DataFrame export: {e}"
                        )
                        value = np.nan
                    row[period] = float(value)
                data[node_id] = row

            df = pd.DataFrame.from_dict(data, orient="index", columns=periods)
            df.index.name = "node_name"

            logger.info(f"Successfully exported {len(df)} nodes to DataFrame.")
        except Exception as e:
            logger.error(f"Failed to export graph to DataFrame: {e}", exc_info=True)
            raise WriteError(
                message=f"Failed to export graph to DataFrame: {e}",
                target="DataFrame",
                writer_type="DataFrameWriter",
                original_error=e,
            ) from e
        else:
            return df

# --- END FILE: fin_statement_model/io/writers/dataframe.py ---

# --- START FILE: fin_statement_model/io/writers/dict.py ---
"""Data writer for Python dictionaries."""

import logging
from typing import Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import (
    FinancialStatementItemNode,
)  # Import specific node if needed
from fin_statement_model.io.base import DataWriter
from fin_statement_model.io.registry import register_writer
from fin_statement_model.io.exceptions import WriteError

logger = logging.getLogger(__name__)


@register_writer("dict")
class DictWriter(DataWriter):
    """Writes graph data to a Python dictionary.

    Specifically extracts data from FinancialStatementItemNode instances.

    Note:
        When using the `write_data` facade, writer initialization has no specific kwargs,
        and no writer-specific options are supported for DictWriter. Direct instantiation
        of `DictWriter` is also supported.
    """

    def write(
        self, graph: Graph, target: object = None, **kwargs: dict[str, Any]
    ) -> dict[str, dict[str, float]]:
        """Export data from graph nodes with values to a dictionary.

        Args:
            graph (Graph): The Graph instance to export data from.
            target (object): Ignored for DictWriter; returns the dictionary directly.
            **kwargs: Writer-specific keyword arguments (none supported by DictWriter).

        Returns:
            Dict[str, Dict[str, float]]: Mapping node names to period-value dicts.
                                         Only includes FinancialStatementItemNode instances
                                         with a 'values' attribute.

        Raises:
            WriteError: If an unexpected error occurs during export.
        """
        logger.info(f"Starting export of graph '{graph}' to dictionary format.")
        result: dict[str, dict[str, float]] = {}
        try:
            for node_id, node in graph.nodes.items():
                # Check if the node is a FinancialStatementItemNode and has 'values'
                # This makes the export specific to data-holding nodes.
                if (
                    isinstance(node, FinancialStatementItemNode)
                    and hasattr(node, "values")
                    and isinstance(node.values, dict)
                ):
                    # Validate and copy values
                    # Ensure values are {str: float | int}
                    validated_values = {
                        str(k): float(v)
                        for k, v in node.values.items()
                        if isinstance(k, str) and isinstance(v, (int, float))
                    }
                    if validated_values:
                        result[node_id] = validated_values
                    else:
                        logger.debug(
                            f"Node '{node_id}' has no valid period-value data to export. Skipping."
                        )
                # else: Not an FSI node or no valid values, skip

            logger.info(f"Successfully exported {len(result)} nodes to dictionary.")
        except Exception as e:
            logger.error(f"Failed to export graph to dictionary: {e}", exc_info=True)
            raise WriteError(
                message="Failed to export graph to dictionary",
                target="dict",
                writer_type="DictWriter",
                original_error=e,
            ) from e
        else:
            return result

# --- END FILE: fin_statement_model/io/writers/dict.py ---

# --- START FILE: fin_statement_model/io/writers/excel.py ---
"""Data writer for Excel files."""

import logging
from pathlib import Path
from typing import Any


from fin_statement_model.core.graph import Graph
from fin_statement_model.io.base import DataWriter
from fin_statement_model.io.registry import register_writer
from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.io.writers.dataframe import DataFrameWriter

logger = logging.getLogger(__name__)


@register_writer("excel")
class ExcelWriter(DataWriter):
    """Writes graph data to an Excel file.

    Converts the graph data to a pandas DataFrame first, then writes to an Excel file.

    Note:
        When using the `write_data` facade, writer initialization has no specific kwargs,
        and writer-specific options (`sheet_name`, `recalculate`, `include_nodes`,
        `excel_writer_kwargs`) should be passed to the `write()` method. Direct
        instantiation of `ExcelWriter` is also supported.
    """

    def write(self, graph: Graph, target: str, **kwargs: dict[str, Any]) -> None:
        """Write data from the Graph object to an Excel file.

        Args:
            graph (Graph): The Graph object containing the data to write.
            target (str): Path to the target Excel file.
            **kwargs: Writer-specific keyword arguments:
                sheet_name (str): Name of the sheet to write to (default: "Sheet1").
                recalculate (bool): Recalculate graph before export (default: True).
                include_nodes (list[str]): Optional list of node names to include.
                excel_writer_kwargs (dict): Additional args passed directly to
                    `pandas.DataFrame.to_excel()`.

        Raises:
            WriteError: If an error occurs during the writing process.
        """
        file_path = target
        sheet_name = kwargs.get("sheet_name", "Sheet1")
        recalculate = kwargs.get("recalculate", True)
        include_nodes = kwargs.get("include_nodes")
        excel_writer_options = kwargs.get("excel_writer_kwargs", {})

        logger.info(f"Exporting graph to Excel file: {file_path}, sheet: {sheet_name}")

        try:
            # 1. Convert graph to DataFrame using DataFrameWriter
            df_writer = DataFrameWriter()
            df = df_writer.write(graph=graph, target=None, recalculate=recalculate, include_nodes=include_nodes)

            # 2. Write DataFrame to Excel
            output_path = Path(file_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            df.to_excel(
                output_path,
                sheet_name=sheet_name,
                index=True,  # Keep node names as index column
                **excel_writer_options,
            )
            logger.info(f"Successfully exported graph to {file_path}, sheet '{sheet_name}'")

        except Exception as e:
            logger.error(
                f"Failed to export graph to Excel file '{file_path}': {e}",
                exc_info=True,
            )
            raise WriteError(
                message=f"Failed to export graph to Excel: {e}",
                target=file_path,
                writer_type="ExcelWriter",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/io/writers/excel.py ---

# --- START FILE: fin_statement_model/logging_config.py ---
"""Centralized logging configuration for the fin_statement_model library."""

import logging

# Attach a NullHandler to the base fin_statement_model logger so that
# all child loggers inherit it and avoid 'No handler' warnings by default.
logging.getLogger("fin_statement_model").addHandler(logging.NullHandler())

# --- END FILE: fin_statement_model/logging_config.py ---

# --- START FILE: fin_statement_model/preprocessing/__init__.py ---
"""Export DataTransformer, CompositeTransformer, and TransformerFactory for preprocessing.

This module exposes core transformer interfaces and factory for the preprocessing layer.
"""

from .base_transformer import DataTransformer, CompositeTransformer
from .transformer_factory import TransformerFactory
from .transformation_service import TransformationService

## Trigger transformer discovery on package import
TransformerFactory.discover_transformers("fin_statement_model.preprocessing.transformers")

__all__ = [
    "CompositeTransformer",
    "DataTransformer",
    "TransformationService",
    "TransformerFactory",
]

# --- END FILE: fin_statement_model/preprocessing/__init__.py ---

# --- START FILE: fin_statement_model/preprocessing/base_transformer.py ---
"""Define base DataTransformer interface for preprocessing layer.

This module provides the DataTransformer abstract base class and CompositeTransformer.
"""

from abc import ABC, abstractmethod
import pandas as pd
from typing import Optional
import logging

logger = logging.getLogger(__name__)


class DataTransformer(ABC):
    """Define base class for data transformers.

    Data transformers convert data between formats and apply business rules.

    This separation follows the Single Responsibility Principle for maintainability.
    """

    def __init__(self, config: Optional[dict[str, object]] = None):
        """Initialize the transformer with optional configuration.

        Args:
            config: Optional configuration dictionary for the transformer
        """
        self.config = config or {}
        logger.debug(f"Initialized {self.__class__.__name__} with config: {self.config}")

    @abstractmethod
    def transform(self, data: object) -> object:
        """Transform the input data.

        Args:
            data: The input data to transform

        Returns:
            Transformed data

        Raises:
            ValueError: If the data cannot be transformed
        """

    def validate_input(self, data: object) -> bool:
        """Validate that the input data is a pandas DataFrame by default.

        This performs a basic DataFrame type check and can be overridden by subclasses with more specific validation logic.

        Args:
            data (object): The input data to validate.

        Returns:
            bool: True if data is a pandas.DataFrame, False otherwise.
        """
        return isinstance(data, pd.DataFrame)

    def _pre_transform_hook(self, data: object) -> object:
        """Hook method called before transformation.

        Args:
            data: The input data

        Returns:
            Processed data to be passed to the transform method

        This method can be overridden by subclasses to add pre-processing steps.
        """
        return data

    def _post_transform_hook(self, data: object) -> object:
        """Hook method called after transformation.

        Args:
            data: The transformed data

        Returns:
            Final processed data

        This method can be overridden by subclasses to add post-processing steps.
        """
        return data

    def execute(self, data: object) -> object:
        """Execute the complete transformation pipeline.

        Args:
            data: The input data to transform

        Returns:
            Transformed data

        Raises:
            ValueError: If the data is invalid or cannot be transformed
        """
        if not self.validate_input(data):
            raise ValueError(f"Invalid input data for {self.__class__.__name__}")

        try:
            # Apply pre-transform hook
            processed_data = self._pre_transform_hook(data)

            # Perform transformation
            result = self.transform(processed_data)
            result = self._post_transform_hook(result)
            logger.debug(f"Successfully transformed data with {self.__class__.__name__}")
        except Exception as e:
            logger.exception(f"Error transforming data with {self.__class__.__name__}")
            raise ValueError("Error transforming data") from e
        else:
            return result


class CompositeTransformer(DataTransformer):
    """Compose multiple transformers into a pipeline.

    This allows building complex transformation chains from simple steps.
    """

    def __init__(self, transformers: list[DataTransformer], config: Optional[dict] = None):
        """Initialize with a list of transformers.

        Args:
            transformers: List of transformers to apply in sequence
            config: Optional configuration dictionary
        """
        super().__init__(config)
        self.transformers = transformers

    def transform(self, data: object) -> object:
        """Apply each transformer in sequence.

        Args:
            data: The input data to transform

        Returns:
            Data transformed by the pipeline
        """
        result = data
        for transformer in self.transformers:
            result = transformer.execute(result)
        return result

    def add_transformer(self, transformer: DataTransformer) -> None:
        """Add a transformer to the pipeline.

        Args:
            transformer: The transformer to add
        """
        self.transformers.append(transformer)

    def remove_transformer(self, index: int) -> Optional[DataTransformer]:
        """Remove a transformer from the pipeline.

        Args:
            index: Index of the transformer to remove

        Returns:
            The removed transformer or None if index is invalid
        """
        if 0 <= index < len(self.transformers):
            return self.transformers.pop(index)
        return None

    def validate_input(self, data: object) -> bool:
        """Validate input for the composite transformer.

        If the pipeline is empty, accepts any data; otherwise, delegates validation to the first transformer.

        Args:
            data (object): Input data to validate.

        Returns:
            bool: True if input is valid for the pipeline.
        """
        if not hasattr(self, "transformers") or not self.transformers:
            return True
        return self.transformers[0].validate_input(data)

# --- END FILE: fin_statement_model/preprocessing/base_transformer.py ---

# --- START FILE: fin_statement_model/preprocessing/config/__init__.py ---
"""Package for preprocessing configuration files."""

# Package for preprocessing configuration files

# --- END FILE: fin_statement_model/preprocessing/config/__init__.py ---

# --- START FILE: fin_statement_model/preprocessing/enums.py ---
"""Define Enum classes for preprocessing transformer types.

Centralize transformer type constants as Enums for clarity.
"""

from enum import Enum


class NormalizationType(Enum):
    """Available normalization types for NormalizationTransformer."""

    PERCENT_OF = "percent_of"
    MINMAX = "minmax"
    STANDARD = "standard"
    SCALE_BY = "scale_by"


class TransformationType(Enum):
    """Available transformation types for TimeSeriesTransformer."""

    GROWTH_RATE = "growth_rate"
    MOVING_AVG = "moving_avg"
    CAGR = "cagr"
    YOY = "yoy"
    QOQ = "qoq"


class ConversionType(Enum):
    """Available conversion types for PeriodConversionTransformer."""

    QUARTERLY_TO_ANNUAL = "quarterly_to_annual"
    MONTHLY_TO_QUARTERLY = "monthly_to_quarterly"
    MONTHLY_TO_ANNUAL = "monthly_to_annual"
    ANNUAL_TO_TTM = "annual_to_ttm"


class StatementType(Enum):
    """Available statement types for StatementFormattingTransformer."""

    INCOME_STATEMENT = "income_statement"
    BALANCE_SHEET = "balance_sheet"
    CASH_FLOW = "cash_flow"

# --- END FILE: fin_statement_model/preprocessing/enums.py ---

# --- START FILE: fin_statement_model/preprocessing/transformation_service.py ---
"""Transformation Service for the Financial Statement Model.

This module provides a high-level service for managing and applying data transformations.
"""

from typing import Optional, Union

import pandas as pd
import logging

from .base_transformer import DataTransformer, CompositeTransformer
from .transformer_factory import TransformerFactory

logger = logging.getLogger(__name__)


class TransformationService:
    """Service for managing and applying data transformations.

    This service separates data transformation logic from data processing,
    making it easier to maintain, test, and extend the codebase.

    It provides methods for common financial data transformations and allows
    for composing multiple transformations into pipelines.
    """

    def __init__(self):
        """Initialize the transformation service."""
        logger.info("TransformationService initialized")

    def normalize_data(
        self,
        data: Union[pd.DataFrame, dict],
        normalization_type: str = "percent_of",
        reference: Optional[str] = None,
        scale_factor: Optional[float] = None,
    ) -> Union[pd.DataFrame, dict]:
        """Normalize financial data.

        Args:
            data: The data to normalize (DataFrame or Dict)
            normalization_type: Type of normalization
            reference: Reference field for percent_of normalization
            scale_factor: Scale factor for scale_by normalization

        Returns:
            Normalized data
        """
        transformer = TransformerFactory.create_transformer(
            "normalization",
            normalization_type=normalization_type,
            reference=reference,
            scale_factor=scale_factor,
        )

        return transformer.execute(data)

    def transform_time_series(
        self,
        data: Union[pd.DataFrame, dict],
        transformation_type: str = "growth_rate",
        periods: int = 1,
        window_size: int = 3,
    ) -> Union[pd.DataFrame, dict]:
        """Apply time series transformations to financial data.

        Args:
            data: The time series data to transform
            transformation_type: Type of transformation
            periods: Number of periods for calculations
            window_size: Window size for moving averages

        Returns:
            Transformed data
        """
        transformer = TransformerFactory.create_transformer(
            "time_series",
            transformation_type=transformation_type,
            periods=periods,
            window_size=window_size,
        )

        return transformer.execute(data)

    def convert_periods(
        self, data: pd.DataFrame, conversion_type: str, aggregation: str = "sum"
    ) -> pd.DataFrame:
        """Convert data between different period types.

        Args:
            data: DataFrame with time periods
            conversion_type: Type of period conversion
            aggregation: Aggregation method

        Returns:
            Transformed DataFrame with converted periods
        """
        transformer = TransformerFactory.create_transformer(
            "period_conversion",
            conversion_type=conversion_type,
            aggregation=aggregation,
        )

        return transformer.execute(data)

    def format_statement(
        self,
        data: pd.DataFrame,
        statement_type: str = "income_statement",
        add_subtotals: bool = True,
        apply_sign_convention: bool = True,
    ) -> pd.DataFrame:
        """Format a financial statement DataFrame.

        Args:
            data: Financial statement data
            statement_type: Type of statement
            add_subtotals: Whether to add standard subtotals
            apply_sign_convention: Whether to apply sign conventions

        Returns:
            Formatted financial statement
        """
        transformer = TransformerFactory.create_transformer(
            "statement_formatting",
            statement_type=statement_type,
            add_subtotals=add_subtotals,
            apply_sign_convention=apply_sign_convention,
        )

        return transformer.execute(data)

    def create_transformation_pipeline(
        self, transformers_config: list[dict[str, object]]
    ) -> DataTransformer:
        """Create a composite transformer from a list of transformer configurations.

        Args:
            transformers_config: List of dicts with transformer configurations
                Each dict should have:
                - 'name': Name of the transformer
                - Additional configuration parameters for that transformer

        Returns:
            A composite transformer with the configured pipeline

        Example:
            config = [
                {'name': 'period_conversion', 'conversion_type': 'quarterly_to_annual'},
                {'name': 'normalization', 'normalization_type': 'percent_of', 'reference': 'revenue'}
            ]
            pipeline = service.create_transformation_pipeline(config)
            transformed_data = pipeline.execute(data)
        """
        transformers = []

        for config in transformers_config:
            if "name" not in config:
                raise ValueError("Each transformer configuration must have a 'name' field")

            name = config.pop("name")
            transformer = TransformerFactory.create_transformer(name, **config)
            transformers.append(transformer)

        return CompositeTransformer(transformers)

    def apply_transformation_pipeline(
        self, data: object, transformers_config: list[dict[str, object]]
    ) -> object:
        """Apply a transformation pipeline to data.

        Args:
            data: The data to transform
            transformers_config: List of transformer configurations

        Returns:
            Transformed data
        """
        pipeline = self.create_transformation_pipeline(transformers_config)
        return pipeline.execute(data)

    def register_custom_transformer(
        self, name: str, transformer_class: type[DataTransformer]
    ) -> None:
        """Register a custom transformer with the factory.

        Args:
            name: Name for the transformer
            transformer_class: The transformer class to register
        """
        TransformerFactory.register_transformer(name, transformer_class)
        logger.info(f"Registered custom transformer: {name}")

    def list_available_transformers(self) -> list[str]:
        """List all available transformer types.

        Returns:
            List of transformer names
        """
        return TransformerFactory.list_transformers()

# --- END FILE: fin_statement_model/preprocessing/transformation_service.py ---

# --- START FILE: fin_statement_model/preprocessing/transformer_factory.py ---
"""Provide TransformerFactory to create and manage data transformers.

This module implements a factory for registering and instantiating transformers.
"""

import importlib
import re
import inspect
import pkgutil
import logging
from typing import ClassVar, Any

from .base_transformer import DataTransformer

logger = logging.getLogger(__name__)


class TransformerFactory:
    """Create and manage transformer instances.

    Centralizes transformer registration, discovery, and instantiation.
    """

    # Registry of transformer types
    _transformers: ClassVar[dict[str, type[DataTransformer]]] = {}

    @classmethod
    def register_transformer(cls, name: str, transformer_class: type[DataTransformer]) -> None:
        """Register a transformer class with the factory.

        Args:
            name: Name to register the transformer under
            transformer_class: The transformer class to register

        Raises:
            ValueError: If the name is already registered
            TypeError: If transformer_class is not a subclass of DataTransformer
        """
        if name in cls._transformers:
            raise ValueError(f"Transformer name '{name}' is already registered")

        if not issubclass(transformer_class, DataTransformer):
            raise TypeError("Transformer class must be a subclass of DataTransformer")

        cls._transformers[name] = transformer_class
        logger.info(f"Registered transformer '{name}'")

    @classmethod
    def create_transformer(cls, name: str, **kwargs: dict[str, Any]) -> DataTransformer:
        """Create a transformer instance by name.

        Args:
            name: Name of the registered transformer
            **kwargs: Arguments to pass to the transformer constructor

        Returns:
            DataTransformer: An instance of the requested transformer

        Raises:
            ValueError: If no transformer is registered with the given name
        """
        if name not in cls._transformers:
            raise ValueError(f"No transformer registered with name '{name}'")

        transformer_class = cls._transformers[name]
        transformer = transformer_class(**kwargs)
        logger.debug(f"Created transformer '{name}'")
        return transformer

    @classmethod
    def list_transformers(cls) -> list[str]:
        """List all registered transformer names.

        Returns:
            List[str]: List of registered transformer names
        """
        return list(cls._transformers.keys())

    @classmethod
    def get_transformer_class(cls, name: str) -> type[DataTransformer]:
        """Get a transformer class by name.

        Args:
            name: Name of the registered transformer

        Returns:
            Type[DataTransformer]: The requested transformer class

        Raises:
            ValueError: If no transformer is registered with the given name
        """
        if name not in cls._transformers:
            raise ValueError(f"No transformer registered with name '{name}'")

        return cls._transformers[name]

    @classmethod
    def discover_transformers(cls, package_name: str) -> None:
        """Discover and register all transformers in a package.

        This method imports all modules in the specified package and
        registers any DataTransformer subclasses found.

        Args:
            package_name: Name of the package to search
        """
        try:
            package = importlib.import_module(package_name)
            package_path = package.__path__

            # Import all modules in the package
            for _, module_name, _ in pkgutil.iter_modules(package_path):
                full_module_name = f"{package_name}.{module_name}"
                module = importlib.import_module(full_module_name)

                # Find all DataTransformer subclasses in the module
                for name, obj in inspect.getmembers(module):
                    if (
                        inspect.isclass(obj)
                        and issubclass(obj, DataTransformer)
                        and obj != DataTransformer
                    ):
                        # Register the transformer with its class name
                        cls.register_transformer(name, obj)
                        # Register snake_case alias without '_transformer'
                        snake = re.sub(r"(.)([A-Z][a-z]+)", r"\1_\2", name)
                        snake = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", snake).lower()
                        alias = snake.replace("_transformer", "")
                        if alias not in cls._transformers:
                            cls.register_transformer(alias, obj)

            logger.info(f"Discovered transformers from package '{package_name}'")

        except ImportError:
            logger.exception(f"Error discovering transformers from package '{package_name}'")

    @classmethod
    def create_composite_transformer(
        cls, transformer_names: list[str], **kwargs: dict[str, Any]
    ) -> DataTransformer:
        """Create a composite transformer from a list of transformer names.

        Args:
            transformer_names: List of registered transformer names to include in the pipeline
            **kwargs: Additional arguments to pass to individual transformers

        Returns:
            DataTransformer: A composite transformer containing the specified transformers

        Raises:
            ValueError: If any transformer name is not registered
        """
        from .base_transformer import CompositeTransformer

        # Use list comprehension for PERF401
        transformers = [cls.create_transformer(name, **kwargs) for name in transformer_names]

        return CompositeTransformer(transformers)

# --- END FILE: fin_statement_model/preprocessing/transformer_factory.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/__init__.py ---
"""Package for preprocessing transformers.

This package exports built-in data transformer classes for the preprocessing layer.
"""

from .normalization import NormalizationTransformer
from .time_series import TimeSeriesTransformer
from .period_conversion import PeriodConversionTransformer
from .statement_formatting import StatementFormattingTransformer

__all__ = [
    "NormalizationTransformer",
    "PeriodConversionTransformer",
    "StatementFormattingTransformer",
    "TimeSeriesTransformer",
]

# --- END FILE: fin_statement_model/preprocessing/transformers/__init__.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/normalization.py ---
"""Provide a NormalizationTransformer to normalize financial data.

Transforms data by percent_of, minmax, standard, or scale_by methods.

This module implements the NormalizationTransformer for the preprocessing layer.
"""

from typing import Optional, Union, ClassVar

import pandas as pd

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.enums import NormalizationType
from fin_statement_model.preprocessing.types import NormalizationConfig


class NormalizationTransformer(DataTransformer):
    """Transformer that normalizes financial data.

    This transformer can normalize values by:
    - Dividing by a reference value (e.g. convert to percentages of revenue)
    - Scaling to a specific range (e.g. 0-1)
    - Applying standard normalization ((x - mean) / std)

    It can operate on DataFrames or dictionary data structures.
    """

    NORMALIZATION_TYPES: ClassVar[list[str]] = [t.value for t in NormalizationType]

    def __init__(
        self,
        normalization_type: Union[str, NormalizationType] = NormalizationType.PERCENT_OF,
        reference: Optional[str] = None,
        scale_factor: Optional[float] = None,
        config: Optional[NormalizationConfig] = None,
    ):
        """Initialize the normalizer.

        Args:
            normalization_type: Type of normalization to apply
                - 'percent_of': Divides by a reference value
                - 'minmax': Scales to range [0,1]
                - 'standard': Applies (x - mean) / std
                - 'scale_by': Multiplies by a scale factor
            reference: Reference field for percent_of normalization
            scale_factor: Factor to scale by for scale_by normalization
            config: Additional configuration options
        """
        super().__init__(config)
        # Normalize enum to string
        if isinstance(normalization_type, NormalizationType):
            norm_type = normalization_type.value
        else:
            norm_type = normalization_type
        if norm_type not in self.NORMALIZATION_TYPES:
            raise ValueError(
                f"Invalid normalization type: {norm_type}. "
                f"Must be one of {self.NORMALIZATION_TYPES}"
            )
        self.normalization_type = norm_type

        self.reference = reference
        self.scale_factor = scale_factor

        # Validation
        if self.normalization_type == NormalizationType.PERCENT_OF.value and not reference:
            raise ValueError("Reference field must be provided for percent_of normalization")

        if self.normalization_type == NormalizationType.SCALE_BY.value and scale_factor is None:
            raise ValueError("Scale factor must be provided for scale_by normalization")

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Normalize the data based on the configured normalization type.

        Args:
            data: pd.DataFrame containing financial data

        Returns:
            pd.DataFrame: Normalized DataFrame
        """
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f"Unsupported data type: {type(data)}. Expected pandas.DataFrame")
        return self._transform_dataframe(data)

    def _transform_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform a DataFrame."""
        result = df.copy()

        if self.normalization_type == NormalizationType.PERCENT_OF.value:
            if self.reference not in df.columns:
                raise ValueError(f"Reference column '{self.reference}' not found in DataFrame")

            for col in df.columns:
                if col != self.reference:
                    result[col] = df[col] / df[self.reference] * 100

        elif self.normalization_type == NormalizationType.MINMAX.value:  # pragma: no cover
            for col in df.columns:
                min_val = df[col].min()
                max_val = df[col].max()

                if max_val > min_val:
                    result[col] = (df[col] - min_val) / (max_val - min_val)  # pragma: no cover

        elif self.normalization_type == NormalizationType.STANDARD.value:
            for col in df.columns:
                mean = df[col].mean()
                std = df[col].std()

                if std > 0:
                    result[col] = (df[col] - mean) / std

        elif self.normalization_type == NormalizationType.SCALE_BY.value:
            for col in df.columns:
                result[col] = df[col] * self.scale_factor

        return result

# --- END FILE: fin_statement_model/preprocessing/transformers/normalization.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/period_conversion.py ---
"""Financial data transformers for the Financial Statement Model.

This module provides the PeriodConversionTransformer for converting between period types:
quarterly_to_annual, monthly_to_quarterly, monthly_to_annual, and annual_to_ttm.
"""

import pandas as pd
from typing import Optional, Union, ClassVar

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.types import PeriodConversionConfig
from fin_statement_model.preprocessing.enums import ConversionType

# Configure logging


class PeriodConversionTransformer(DataTransformer):
    """Transformer for converting between different period types.

    This transformer can convert:
    - Quarterly data to annual
    - Monthly data to quarterly or annual
    - Annual data to trailing twelve months (TTM)
    """

    # All valid conversion types
    CONVERSION_TYPES: ClassVar[list[str]] = [t.value for t in ConversionType]

    def __init__(
        self,
        conversion_type: Union[str, ConversionType] = ConversionType.QUARTERLY_TO_ANNUAL,
        aggregation: str = "sum",
        config: Optional[PeriodConversionConfig] = None,
    ):
        """Initialize the period conversion transformer.

        Args:
            conversion_type: Type of period conversion to apply (enum or string)
            aggregation: How to aggregate data (sum, mean, last, etc.)
            config: Optional transformer configuration
        """
        super().__init__(config)
        # Normalize enum to string
        if isinstance(conversion_type, ConversionType):
            ctype = conversion_type.value
        else:
            ctype = conversion_type
        if ctype not in self.CONVERSION_TYPES:
            raise ValueError(
                f"Invalid conversion type: {ctype}. Must be one of {self.CONVERSION_TYPES}"
            )
        self.conversion_type = ctype
        self.aggregation = aggregation

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform data by converting between period types.

        Args:
            data: DataFrame with DatetimeIndex or period labels in the index

        Returns:
            DataFrame with transformed periods
        """
        # Ensure we have a DataFrame
        if not isinstance(data, pd.DataFrame):
            raise TypeError("Period conversion requires a pandas DataFrame")

        # Try to convert index to datetime if it's not already
        if not isinstance(data.index, pd.DatetimeIndex):
            try:
                data = data.copy()
                data.index = pd.to_datetime(data.index, format="%Y-%m-%d")
            except Exception:
                raise ValueError("Index must be convertible to datetime for period conversion")

        if self.conversion_type == ConversionType.QUARTERLY_TO_ANNUAL.value:
            # Group by year and aggregate
            return data.groupby(data.index.year).agg(self.aggregation)

        elif self.conversion_type == ConversionType.MONTHLY_TO_QUARTERLY.value:
            # Group by year and quarter
            return data.groupby([data.index.year, data.index.quarter]).agg(self.aggregation)

        elif self.conversion_type == ConversionType.MONTHLY_TO_ANNUAL.value:
            # Group by year
            return data.groupby(data.index.year).agg(self.aggregation)

        elif self.conversion_type == ConversionType.ANNUAL_TO_TTM.value:
            # Implement TTM as rolling sum with window=4 for quarterly data
            if self.aggregation == "sum":
                return data.rolling(window=4).sum()
            else:
                # For other aggregation methods, we need custom logic
                raise ValueError("annual_to_ttm conversion only supports 'sum' aggregation")

        return data  # pragma: no cover

# --- END FILE: fin_statement_model/preprocessing/transformers/period_conversion.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/statement_formatting.py ---
"""Financial data transformer for the Financial Statement Model.

This module provides the StatementFormattingTransformer for formatting financial statements:
adding subtotals, applying sign conventions, and reordering line items.
"""

import pandas as pd
import yaml
from pathlib import Path
from typing import Optional, Union, ClassVar

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.types import StatementFormattingConfig
from fin_statement_model.preprocessing.enums import StatementType

# Configure logging


class StatementFormattingTransformer(DataTransformer):
    """Transformer for formatting financial statements.

    This transformer can:
    - Add subtotals and totals
    - Reorder line items according to standard formats
    - Apply sign conventions (negative expenses, etc.)
    """

    # Load standard orderings from YAML configuration
    DEFAULT_ORDERS: ClassVar[dict[str, list[str]]] = {}

    @classmethod
    def _load_standard_orders(cls) -> None:
        """Load standard order configurations from YAML file into DEFAULT_ORDERS."""
        # Load the YAML from the preprocessing config directory
        config_path = Path(__file__).parent.parent / "config" / "statement_standard_orders.yaml"
        with config_path.open("r", encoding="utf-8") as f:
            cls.DEFAULT_ORDERS = yaml.safe_load(f)

    def __init__(
        self,
        statement_type: Union[str, StatementType] = StatementType.INCOME_STATEMENT,
        add_subtotals: bool = True,
        apply_sign_convention: bool = True,
        config: Optional[StatementFormattingConfig] = None,
    ):
        """Initialize the statement formatting transformer.

        Args:
            statement_type: Type of statement ('income_statement', 'balance_sheet', 'cash_flow')
            add_subtotals: Whether to add standard subtotals
            apply_sign_convention: Whether to apply standard sign conventions
            config: Additional configuration options
        """
        super().__init__(config)
        # Normalize enum to string
        if isinstance(statement_type, StatementType):
            stype = statement_type.value
        else:
            stype = statement_type
        if stype not in [t.value for t in StatementType]:
            raise ValueError(
                f"Invalid statement type: {stype}. Must be one of {[t.value for t in StatementType]}"
            )
        self.statement_type = stype
        self.add_subtotals = add_subtotals
        self.apply_sign_convention = apply_sign_convention

        # Define standard orderings for different statement types
        self.item_order = self._get_standard_order()

    def _get_standard_order(self) -> list[str]:
        """Get the standard ordering of items for the current statement type."""
        return list(self.DEFAULT_ORDERS.get(self.statement_type, []))

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Format a financial statement DataFrame.

        Args:
            data: DataFrame containing financial statement data

        Returns:
            Formatted DataFrame
        """
        result = data.copy()

        if self.apply_sign_convention:
            result = self._apply_sign_convention(result)

        if self.add_subtotals:
            result = self._add_subtotals(result)

        result = self._reorder_items(result)

        return result

    def _apply_sign_convention(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply standard sign conventions to line items."""
        result = df.copy()
        negative_items: list[str] = []

        if self.statement_type == "income_statement":
            negative_items = [
                "cost_of_goods_sold",
                "operating_expenses",
                "interest_expense",
                "income_tax",
            ]

        elif self.statement_type == "cash_flow":
            negative_items = [
                "capital_expenditures",
                "investments",
                "debt_repayment",
                "dividends",
                "share_repurchases",
            ]

        for item in negative_items:
            if item in result.index:
                result.loc[item] = (
                    result.loc[item] * -1 if result.loc[item].mean() > 0 else result.loc[item]
                )

        return result

    def _add_subtotals(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add standard subtotals to the statement."""
        result = df.copy()

        if self.statement_type == "income_statement":
            if (
                "revenue" in result.index
                and "cost_of_goods_sold" in result.index
                and "gross_profit" not in result.index
            ):
                result.loc["gross_profit"] = (
                    result.loc["revenue"] + result.loc["cost_of_goods_sold"]
                )

            if (
                "gross_profit" in result.index
                and "operating_expenses" in result.index
                and "operating_income" not in result.index
            ):
                result.loc["operating_income"] = (
                    result.loc["gross_profit"] + result.loc["operating_expenses"]
                )

            if (
                "operating_income" in result.index
                and "interest_expense" in result.index
                and "income_before_taxes" not in result.index
            ):
                result.loc["income_before_taxes"] = (
                    result.loc["operating_income"] + result.loc["interest_expense"]
                )

            if (
                "income_before_taxes" in result.index
                and "income_tax" in result.index
                and "net_income" not in result.index
            ):
                result.loc["net_income"] = (
                    result.loc["income_before_taxes"] + result.loc["income_tax"]
                )

        elif self.statement_type == "balance_sheet":
            current_assets = [
                "cash_and_equivalents",
                "short_term_investments",
                "accounts_receivable",
                "inventory",
            ]
            if (
                any(item in result.index for item in current_assets)
                and "current_assets" not in result.index
            ):
                result.loc["current_assets"] = sum(
                    result.loc[item] for item in current_assets if item in result.index
                )

            if (
                "current_assets" in result.index
                and "property_plant_equipment" in result.index
                and "total_assets" not in result.index
            ):
                result.loc["total_assets"] = (
                    result.loc["current_assets"] + result.loc["property_plant_equipment"]
                )

            current_liabilities = ["accounts_payable", "short_term_debt"]
            if (
                any(item in result.index for item in current_liabilities)
                and "current_liabilities" not in result.index
            ):
                result.loc["current_liabilities"] = sum(
                    result.loc[item] for item in current_liabilities if item in result.index
                )

            if (
                "current_liabilities" in result.index
                and "long_term_debt" in result.index
                and "total_liabilities" not in result.index
            ):
                result.loc["total_liabilities"] = (
                    result.loc["current_liabilities"] + result.loc["long_term_debt"]
                )

            equity_items = ["common_stock", "retained_earnings"]
            if (
                any(item in result.index for item in equity_items)
                and "total_equity" not in result.index
            ):
                result.loc["total_equity"] = sum(
                    result.loc[item] for item in equity_items if item in result.index
                )  # pragma: no cover

            if (
                "total_liabilities" in result.index
                and "total_equity" in result.index
                and "total_liabilities_and_equity" not in result.index
            ):
                result.loc["total_liabilities_and_equity"] = (
                    result.loc["total_liabilities"] + result.loc["total_equity"]
                )  # pragma: no cover

        elif self.statement_type == "cash_flow":
            operating_items = [
                "net_income",
                "depreciation_amortization",
                "changes_in_working_capital",
            ]
            if (
                any(item in result.index for item in operating_items)
                and "cash_from_operating_activities" not in result.index
            ):
                result.loc["cash_from_operating_activities"] = sum(
                    result.loc[item] for item in operating_items if item in result.index
                )

            investing_items = ["capital_expenditures", "investments"]
            if (
                any(item in result.index for item in investing_items)
                and "cash_from_investing_activities" not in result.index
            ):
                result.loc["cash_from_investing_activities"] = sum(
                    result.loc[item] for item in investing_items if item in result.index
                )

            financing_items = [
                "debt_issuance",
                "debt_repayment",
                "dividends",
                "share_repurchases",
            ]
            if (
                any(item in result.index for item in financing_items)
                and "cash_from_financing_activities" not in result.index
            ):
                result.loc["cash_from_financing_activities"] = sum(
                    result.loc[item] for item in financing_items if item in result.index
                )

            cash_flow_categories = [
                "cash_from_operating_activities",
                "cash_from_investing_activities",
                "cash_from_financing_activities",
            ]
            if (
                any(item in result.index for item in cash_flow_categories)
                and "net_change_in_cash" not in result.index
            ):
                result.loc["net_change_in_cash"] = sum(
                    result.loc[item] for item in cash_flow_categories if item in result.index
                )

        return result

    def _reorder_items(self, df: pd.DataFrame) -> pd.DataFrame:
        """Reorder the DataFrame according to standard financial statement ordering."""
        ordered_items = [item for item in self.item_order if item in df.index]
        ordered_items.extend([item for item in df.index if item not in self.item_order])
        return df.loc[ordered_items]

# After class definition, load standard orders
StatementFormattingTransformer._load_standard_orders()

# --- END FILE: fin_statement_model/preprocessing/transformers/statement_formatting.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/time_series.py ---
"""Financial data transformers for the Financial Statement Model.

This module provides the TimeSeriesTransformer which applies growth rates,
moving averages, CAGR, year-over-year, and quarter-over-quarter conversions.
"""

import pandas as pd
from typing import Union, Optional, ClassVar

from fin_statement_model.preprocessing.types import TimeSeriesConfig
from fin_statement_model.preprocessing.enums import TransformationType
from fin_statement_model.preprocessing.base_transformer import DataTransformer


class TimeSeriesTransformer(DataTransformer):
    """Transformer for time series financial data.

    This transformer can apply common time series transformations like:
    - Calculating growth rates
    - Calculating moving averages
    - Computing compound annual growth rate (CAGR)
    - Converting to year-over-year or quarter-over-quarter comparisons
    """

    TRANSFORMATION_TYPES: ClassVar[list[str]] = [t.value for t in TransformationType]

    def __init__(
        self,
        transformation_type: Union[str, TransformationType] = TransformationType.GROWTH_RATE,
        periods: int = 1,
        window_size: int = 3,
        config: Optional[TimeSeriesConfig] = None,
    ):
        """Initialize the time series transformer.

        Args:
            transformation_type: Type of transformation to apply
                - 'growth_rate': Calculate period-to-period growth rates
                - 'moving_avg': Calculate moving average
                - 'cagr': Calculate compound annual growth rate
                - 'yoy': Year-over-year comparison
                - 'qoq': Quarter-over-quarter comparison
            periods: Number of periods to use in calculations
            window_size: Size of the moving average window
            config: Additional configuration options
        """
        super().__init__(config)
        # Normalize to string
        if isinstance(transformation_type, TransformationType):
            ttype = transformation_type.value
        else:
            ttype = transformation_type
        if ttype not in self.TRANSFORMATION_TYPES:
            raise ValueError(
                f"Invalid transformation type: {ttype}. Must be one of {self.TRANSFORMATION_TYPES}"
            )
        self.transformation_type = ttype

        self.periods = periods
        self.window_size = window_size

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform time series data based on the configured transformation type.

        Args:
            data: pd.DataFrame containing time series financial data

        Returns:
            pd.DataFrame: Transformed DataFrame
        """
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f"Unsupported data type: {type(data)}. Expected pandas.DataFrame")
        return self._transform_dataframe(data)

    def _transform_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform a DataFrame with time series data."""
        result = df.copy()

        if self.transformation_type == "growth_rate":
            for col in df.columns:
                result[f"{col}_growth"] = df[col].pct_change(periods=self.periods) * 100

        elif self.transformation_type == "moving_avg":
            for col in df.columns:
                result[f"{col}_ma{self.window_size}"] = (
                    df[col].rolling(window=self.window_size).mean()
                )

        elif self.transformation_type == "cagr":
            # Assuming the index represents time periods
            n_periods = len(df) - 1
            for col in df.columns:
                start = df[col].iloc[0]
                end = df[col].iloc[-1]
                if start > 0:
                    result[f"{col}_cagr"] = ((end / start) ** (1 / n_periods) - 1) * 100

        elif self.transformation_type == "yoy":
            for col in df.columns:
                result[f"{col}_yoy"] = df[col].pct_change(periods=12) * 100

        elif self.transformation_type == "qoq":
            for col in df.columns:
                result[f"{col}_qoq"] = df[col].pct_change(periods=3) * 100

        return result

# --- END FILE: fin_statement_model/preprocessing/transformers/time_series.py ---

# --- START FILE: fin_statement_model/preprocessing/types.py ---
"""Define types and TypedDicts for preprocessing transformers.

This module provides a TabularData alias (pd.DataFrame only) and configuration TypedDicts.
"""

from typing import TypedDict
import pandas as pd

# Alias for tabular data inputs (DataFrame-only) accepted by transformers
TabularData = pd.DataFrame


class NormalizationConfig(TypedDict, total=False):
    """Configuration for normalization transformations.

    Attributes:
        normalization_type: 'percent_of', 'minmax', 'standard', or 'scale_by'
        reference: reference field name for 'percent_of' normalization
        scale_factor: factor to apply for 'scale_by' normalization
    """

    normalization_type: str  # 'percent_of', 'minmax', 'standard', 'scale_by'
    reference: str  # reference field name for percent_of
    scale_factor: float  # factor for scale_by normalization


class TimeSeriesConfig(TypedDict, total=False):
    """Configuration for time series transformations.

    Attributes:
        transformation_type: 'growth_rate', 'moving_avg', 'cagr', 'yoy', or 'qoq'
        periods: number of periods for percentage change or other transformations
        window_size: window size for rolling calculations
    """

    transformation_type: str  # 'growth_rate', 'moving_avg', 'cagr', 'yoy', 'qoq'
    periods: int  # periods for pct_change or other
    window_size: int  # window size for rolling calculations


class PeriodConversionConfig(TypedDict, total=False):
    """Configuration for period conversion transformations.

    Attributes:
        conversion_type: 'quarterly_to_annual', 'monthly_to_quarterly', etc.
        aggregation: aggregation method: 'sum', 'mean', 'last', etc.
    """

    conversion_type: str  # 'quarterly_to_annual', 'monthly_to_quarterly', etc.
    aggregation: str  # aggregation method: sum, mean, last, etc.


class StatementFormattingConfig(TypedDict, total=False):
    """Configuration for formatting statement output.

    Attributes:
        statement_type: 'income_statement', 'balance_sheet', 'cash_flow'
        add_subtotals: whether to insert computed subtotals
        apply_sign_convention: whether to apply sign rules to values
    """

    statement_type: str  # 'income_statement', 'balance_sheet', 'cash_flow'
    add_subtotals: bool  # whether to insert subtotals
    apply_sign_convention: bool  # whether to apply sign rules

# --- END FILE: fin_statement_model/preprocessing/types.py ---

# --- START FILE: fin_statement_model/statements/__init__.py ---
"""Financial statements module for the Financial Statement Model.

This module defines configurable financial statement structures,
including various types of financial statements (Income Statement,
Balance Sheet, Cash Flow) with hierarchical sections and line items
defined through configuration.
"""

from .structure import (
    StatementStructure,
    Section,
    LineItem,
    CalculatedLineItem,
    SubtotalLineItem,
    StatementItemType,
)
from .config.config import StatementConfig
from .config.loader import load_statement_config
from .formatter import StatementFormatter
from .manager import StatementManager
from .factory import StatementFactory
from .graph.financial_graph import FinancialStatementGraph

__all__ = [
    "CalculatedLineItem",
    "FinancialStatementGraph",
    "LineItem",
    "Section",
    "StatementConfig",
    "StatementFactory",
    "StatementFormatter",
    "StatementItemType",
    "StatementManager",
    "StatementStructure",
    "SubtotalLineItem",
    "load_statement_config",
]

# --- END FILE: fin_statement_model/statements/__init__.py ---

# --- START FILE: fin_statement_model/statements/config/config.py ---
"""Statement configuration handling for Financial Statement Model.

This module provides utilities for loading and parsing statement configuration files,
which define the structure of financial statements including sections and line items.
"""

import json
import yaml
import logging
from typing import Any, Union, Optional
from pathlib import Path

# Use absolute imports
from fin_statement_model.statements.errors import ConfigurationError
from fin_statement_model.statements.structure import (
    StatementStructure,
    Section,
    LineItem,
    CalculatedLineItem,
    SubtotalLineItem,
)

# Configure logging
logger = logging.getLogger(__name__)


class StatementConfig:
    """Manages configuration for financial statement structures.

    This class handles loading, parsing, and validating statement configuration files,
    and building StatementStructure objects from these configurations.
    """

    def __init__(
        self,
        config_data: Optional[dict[str, Any]] = None,
        config_path: Optional[str] = None,
    ):
        """Initialize a statement configuration.

        Args:
            config_data: Optional dictionary containing the configuration data
            config_path: Optional path to a configuration file (JSON or YAML)

        If both config_data and config_path are provided, config_data takes precedence.

        Raises:
            ConfigurationError: If the configuration file is invalid or cannot be loaded
        """
        self.config_data = {}
        self.config_path = config_path

        if config_data:
            self.config_data = config_data
        elif config_path:
            self.load_config(config_path)

    def load_config(self, config_path: str) -> None:
        """Load configuration from a file.

        Args:
            config_path: Path to the configuration file (JSON or YAML)

        Raises:
            ConfigurationError: If the file cannot be loaded or parsed
        """
        self.config_path = config_path
        path = Path(config_path)

        if not path.exists():
            raise ConfigurationError(
                message="Configuration file not found", config_path=config_path
            )

        extension = path.suffix.lower()

        try:
            if extension == ".json":
                with open(path) as f:
                    self.config_data = json.load(f)
            elif extension in [".yaml", ".yml"]:
                with open(path) as f:
                    self.config_data = yaml.safe_load(f)
            else:
                raise ConfigurationError(
                    message="Unsupported file extension",
                    config_path=config_path,
                    errors=[f"Use .json, .yaml, or .yml instead of {extension}"],
                )

            logger.info(f"Loaded statement configuration from {config_path}")
        except json.JSONDecodeError as e:
            logger.exception(f"Error parsing JSON configuration file {config_path}")
            raise ConfigurationError(
                message="Invalid JSON format",
                config_path=config_path,
                errors=[f"JSON decode error at line {e.lineno}, column {e.colno}: {e.msg}"],
            ) from e
        except yaml.YAMLError as e:
            logger.exception(f"Error parsing YAML configuration file {config_path}")
            if hasattr(e, "problem_mark"):
                mark = e.problem_mark
                error_detail = (
                    f"YAML parse error at line {mark.line + 1}, column {mark.column + 1}: "
                    f"{e.problem}"
                )
            else:
                error_detail = str(e)
            raise ConfigurationError(
                message="Invalid YAML format",
                config_path=config_path,
                errors=[error_detail],
            ) from e
        except Exception as e:
            logger.exception(f"Unexpected error loading configuration from {config_path}")
            raise ConfigurationError(
                message="Failed to load configuration",
                config_path=config_path,
                errors=[str(e)],
            ) from e

    def validate_config(self) -> list[str]:
        """Validate the configuration data.

        Returns:
            List[str]: List of validation errors, or empty list if valid
        """
        errors = []

        # Check for required top-level fields
        required_fields = ["id", "name", "sections"]
        errors.extend(
            [
                f"Missing required field: {field}"
                for field in required_fields
                if field not in self.config_data
            ]
        )

        # Check for sections
        if "sections" in self.config_data:
            # Validate each section
            sections = self.config_data.get("sections", [])
            if not isinstance(sections, list):
                errors.append("'sections' must be a list")
            else:
                for i, section in enumerate(sections):
                    section_errors = self._validate_section(section, i)
                    errors.extend(section_errors)

        return errors

    def _validate_section(self, section: dict[str, Any], index: int) -> list[str]:
        """Validate a section configuration.

        Args:
            section: Section configuration dictionary
            index: Index of the section in the sections list

        Returns:
            List[str]: List of validation errors, or empty list if valid
        """
        errors = []

        # Check if section is a dictionary
        if not isinstance(section, dict):
            return [f"Section[{index}]: Must be a dictionary"]

        # Check for required section fields
        required_fields = ["id", "name"]
        errors.extend(
            [
                f"Section[{index}]: Missing required field: {field}"
                for field in required_fields
                if field not in section
            ]
        )

        # Validate section ID format
        section_id = section.get("id", "")
        if not section_id:
            errors.append(f"Section[{index}]: ID cannot be empty")
        elif not isinstance(section_id, str):
            errors.append(f"Section[{index}]: ID must be a string")
        elif " " in section_id:
            errors.append(f"Section[{index}]: ID '{section_id}' should not contain spaces")

        # Validate items
        if "items" in section:
            # Validate each item
            items = section.get("items", [])
            if not isinstance(items, list):
                errors.append(f"Section[{index}]: 'items' must be a list")
            else:
                for j, item in enumerate(items):
                    item_errors = self._validate_item(item, index, j)
                    errors.extend(item_errors)

        # Validate subsections if present
        if "subsections" in section:
            subsections = section.get("subsections", [])
            if not isinstance(subsections, list):
                errors.append(f"Section[{index}]: 'subsections' must be a list")
            else:
                for j, subsection in enumerate(subsections):
                    subsection_errors = self._validate_section(subsection, f"{index}.{j}")
                    errors.extend(subsection_errors)

        return errors

    def _validate_item(self, item: dict[str, Any], section_idx: int, item_idx: int) -> list[str]:
        """Validate an item configuration.

        Args:
            item: Item configuration dictionary
            section_idx: Index of the parent section
            item_idx: Index of the item within the section

        Returns:
            List[str]: List of validation errors, or empty list if valid
        """
        errors = []

        # Check if item is a dictionary
        if not isinstance(item, dict):
            return [f"Section[{section_idx}].Item[{item_idx}]: Must be a dictionary"]

        # Check for required item fields
        required_fields = ["id", "name"]
        errors.extend(
            [
                f"Section[{section_idx}].Item[{item_idx}]: Missing required field: {field}"
                for field in required_fields
                if field not in item
            ]
        )

        # Validate item ID format
        item_id = item.get("id", "")
        if not item_id:
            errors.append(f"Section[{section_idx}].Item[{item_idx}]: ID cannot be empty")
        elif not isinstance(item_id, str):
            errors.append(f"Section[{section_idx}].Item[{item_idx}]: ID must be a string")
        elif " " in item_id:
            errors.append(
                f"Section[{section_idx}].Item[{item_idx}]: ID '{item_id}' should not contain spaces"
            )

        # Validate based on item type
        item_type = item.get("type", "line_item")

        if item_type == "section":
            # This is a nested section
            if "items" not in item:
                errors.append(
                    f"Section[{section_idx}].Item[{item_idx}]: Nested section missing 'items' field"
                )
            else:
                nested_items = item.get("items", [])
                if not isinstance(nested_items, list):
                    errors.append(
                        f"Section[{section_idx}].Item[{item_idx}]: 'items' must be a list"
                    )
                else:
                    for k, nested_item in enumerate(nested_items):
                        nested_errors = self._validate_item(
                            nested_item, f"{section_idx}.{item_idx}", k
                        )
                        errors.extend(nested_errors)

        elif item_type == "line_item":
            # Basic line item should have a node_id
            if "node_id" not in item:
                errors.append(
                    f"Section[{section_idx}].Item[{item_idx}]: Line item missing 'node_id' field"
                )

        elif item_type == "calculated":
            if "calculation" not in item:
                errors.append(
                    f"Section[{section_idx}].Item[{item_idx}]: Calculated item missing 'calculation' field"
                )
            else:
                calculation = item.get("calculation", {})
                if not isinstance(calculation, dict):
                    errors.append(
                        f"Section[{section_idx}].Item[{item_idx}]: 'calculation' must be a dictionary"
                    )
                else:
                    # Validate calculation
                    if "type" not in calculation:
                        errors.append(
                            f"Section[{section_idx}].Item[{item_idx}]: Calculation missing 'type' field"
                        )
                    if "inputs" not in calculation:
                        errors.append(
                            f"Section[{section_idx}].Item[{item_idx}]: Calculation missing 'inputs' field"
                        )
                    elif not isinstance(calculation.get("inputs", []), list):
                        errors.append(
                            f"Section[{section_idx}].Item[{item_idx}]: 'inputs' must be a list"
                        )

        elif item_type == "subtotal":
            # Check for calculation with addition type
            if "calculation" in item:
                calculation = item.get("calculation", {})
                if calculation.get("type") != "addition":
                    errors.append(
                        f"Section[{section_idx}].Item[{item_idx}]: Subtotal calculation type must be 'addition'"
                    )
            elif "items_to_sum" not in item:
                errors.append(
                    f"Section[{section_idx}].Item[{item_idx}]: Subtotal item missing 'items_to_sum' field"
                )
            elif not isinstance(item.get("items_to_sum", []), list):
                errors.append(
                    f"Section[{section_idx}].Item[{item_idx}]: 'items_to_sum' must be a list"
                )

        else:
            errors.append(
                f"Section[{section_idx}].Item[{item_idx}]: Unknown item type: {item_type}"
            )

        return errors

    def build_statement_structure(self) -> StatementStructure:
        """Build a StatementStructure object from the configuration.

        Returns:
            StatementStructure: The constructed statement structure

        Raises:
            ConfigurationError: If the configuration is invalid
        """
        # Validate the configuration
        errors = self.validate_config()
        if errors:
            error_msg = "Configuration validation failed"
            logger.error(f"{error_msg}: {'; '.join(errors)}")
            raise ConfigurationError(message=error_msg, config_path=self.config_path, errors=errors)

        try:
            # Create the statement structure
            statement = StatementStructure(
                id=self.config_data["id"],
                name=self.config_data["name"],
                description=self.config_data.get("description", ""),
                metadata=self.config_data.get("metadata", {}),
            )

            # Build sections
            sections = self.config_data.get("sections", [])
            for section_config in sections:
                section = self._build_section(section_config)
                statement.add_section(section)

            return statement
        except Exception as e:
            logger.exception("Error building statement structure")
            raise ConfigurationError(
                message="Failed to build statement structure",
                config_path=self.config_path,
                errors=[str(e)],
            ) from e
        else:
            return statement

    def _build_section(self, section_config: dict[str, Any]) -> Section:
        """Build a Section object from a section configuration.

        Args:
            section_config: Section configuration dictionary

        Returns:
            Section: The constructed section

        Raises:
            ConfigurationError: If the section configuration is invalid
        """
        try:
            # Create the section
            section = Section(
                id=section_config["id"],
                name=section_config["name"],
                description=section_config.get("description", ""),
                metadata=section_config.get("metadata", {}),
            )

            # Add items
            items = section_config.get("items", [])
            for item_config in items:
                item = self._build_item(item_config)
                section.add_item(item)

            # Add subsections
            subsections = section_config.get("subsections", [])
            for subsection_config in subsections:
                subsection = self._build_section(subsection_config)
                section.add_item(subsection)

            # Add subtotal if specified
            subtotal_config = section_config.get("subtotal")
            if subtotal_config:
                section.subtotal = self._build_subtotal(subtotal_config)

            return section
        except KeyError as e:
            section_id = section_config.get("id", "unknown")
            raise ConfigurationError(
                message=f"Missing required field '{e.args[0]}'",
                errors=[f"Section '{section_id}' is missing required field: {e.args[0]}"],
            ) from e
        except Exception as e:
            section_id = section_config.get("id", "unknown")
            raise ConfigurationError(
                message="Failed to build section",
                errors=[f"Error in section '{section_id}': {e!s}"],
            ) from e
        else:
            return section

    def _build_item(
        self, item_config: dict[str, Any]
    ) -> Union[LineItem, CalculatedLineItem, SubtotalLineItem, Section]:
        """Build a LineItem object from an item configuration.

        Args:
            item_config: Item configuration dictionary

        Returns:
            Union[LineItem, CalculatedLineItem, SubtotalLineItem, Section]:
                The constructed item

        Raises:
            ConfigurationError: If the item configuration is invalid
        """
        try:
            item_type = item_config.get("type", "line_item")

            if item_type == "section":
                # Handle nested section
                return self._build_section(item_config)

            elif item_type == "line_item":
                return LineItem(
                    id=item_config["id"],
                    name=item_config["name"],
                    node_id=item_config.get("node_id"),
                    description=item_config.get("description", ""),
                    sign_convention=item_config.get("sign_convention", 1),
                    metadata=item_config.get("metadata", {}),
                )

            elif item_type == "calculated":
                return CalculatedLineItem(
                    id=item_config["id"],
                    name=item_config["name"],
                    calculation=item_config["calculation"],
                    description=item_config.get("description", ""),
                    sign_convention=item_config.get("sign_convention", 1),
                    metadata=item_config.get("metadata", {}),
                )

            elif item_type == "subtotal":
                return self._build_subtotal(item_config)

            else:
                raise ConfigurationError(
                    message=f"Unknown item type: {item_type}",
                    errors=[
                        f"Item '{item_config.get('id', 'unknown')}' has invalid type: {item_type}"
                    ],
                )
        except KeyError as e:
            item_id = item_config.get("id", "unknown")
            raise ConfigurationError(
                message=f"Missing required field '{e.args[0]}'",
                errors=[f"Item '{item_id}' is missing required field: {e.args[0]}"],
            ) from e
        except Exception as e:
            item_id = item_config.get("id", "unknown")
            raise ConfigurationError(
                message="Failed to build item",
                errors=[f"Error in item '{item_id}': {e!s}"],
            ) from e

    def _build_subtotal(self, subtotal_config: dict[str, Any]) -> SubtotalLineItem:
        """Build a SubtotalLineItem object from a subtotal configuration.

        Args:
            subtotal_config: Subtotal configuration dictionary

        Returns:
            SubtotalLineItem: The constructed subtotal line item

        Raises:
            ConfigurationError: If the subtotal configuration is invalid
        """
        try:
            # Get items to sum
            items_to_sum = []

            if "calculation" in subtotal_config:
                # If using calculation format
                calculation = subtotal_config.get("calculation", {})
                items_to_sum = calculation.get("inputs", [])
            elif "item_ids" in subtotal_config:
                # If using item_ids format
                items_to_sum = subtotal_config.get("item_ids", [])
            else:
                # If using items_to_sum format
                items_to_sum = subtotal_config.get("items_to_sum", [])

            # Create subtotal
            return SubtotalLineItem(
                id=subtotal_config["id"],
                name=subtotal_config["name"],
                item_ids=items_to_sum,
                description=subtotal_config.get("description", ""),
                sign_convention=subtotal_config.get("sign_convention", 1),
                metadata=subtotal_config.get("metadata", {}),
            )
        except KeyError as e:
            subtotal_id = subtotal_config.get("id", "unknown")
            raise ConfigurationError(
                message=f"Missing required field '{e.args[0]}'",
                errors=[f"Subtotal '{subtotal_id}' is missing required field: {e.args[0]}"],
            ) from e
        except Exception as e:
            subtotal_id = subtotal_config.get("id", "unknown")
            raise ConfigurationError(
                message="Failed to build subtotal",
                errors=[f"Error in subtotal '{subtotal_id}': {e!s}"],
            ) from e

# --- END FILE: fin_statement_model/statements/config/config.py ---

# --- START FILE: fin_statement_model/statements/config/loader.py ---
"""Loader for statement configuration files and built-in mappings.

Provides:
  - load_statement_config: load any JSON/YAML config via StatementConfig
  - list_built_in_statements: list mapping file names in config/mappings/
  - load_built_in_statement: load a named mapping from config/mappings/
"""

import logging
import os
from pathlib import Path

from fin_statement_model.statements.errors import ConfigurationError
from fin_statement_model.statements.config.config import StatementConfig
from fin_statement_model.statements.structure import StatementStructure

logger = logging.getLogger(__name__)

# Directory where built-in mapping files live (allow override via env var)
this_dir = Path(__file__).parent


def _get_mapping_dir() -> Path:
    """Return path to mapping directory, using FIN_STATEMENTS_MAPPING_DIR if set."""
    mapping_env = os.getenv("FIN_STATEMENTS_MAPPING_DIR")
    if mapping_env:
        return Path(mapping_env)
    return this_dir / "mappings"


__all__ = [
    "list_built_in_statements",
    "load_built_in_statement",
    "load_statement_config",
]


def load_statement_config(config_path: str) -> StatementStructure:
    """Load a statement structure from a JSON or YAML file at any path.

    Args:
        config_path: Path to .json, .yaml, or .yml file defining the statement

    Returns:
        StatementStructure: Parsed statement structure

    Raises:
        ConfigurationError: If the file cannot be parsed or is invalid
    """
    try:
        cfg = StatementConfig(config_path=config_path)
        return cfg.build_statement_structure()
    except Exception as e:
        if isinstance(e, ConfigurationError):
            raise
        logger.exception(f"Error loading statement config from {config_path}")
        raise ConfigurationError(
            message="Failed to load statement configuration",
            config_path=config_path,
            errors=[str(e)],
        ) from e


def list_built_in_statements() -> list[str]:
    """List the names of all built-in statement mappings available.

    Returns:
        List[str]: List of mapping names (filename without extension)
    """
    mapping_dir = _get_mapping_dir()
    if not mapping_dir.exists():
        return []
    names = [
        p.stem
        for p in mapping_dir.iterdir()
        if p.is_file() and p.suffix.lower() in (".yaml", ".yml", ".json")
    ]
    return sorted(names)


def load_built_in_statement(name: str) -> StatementStructure:
    """Load a built-in statement by name from the config/mappings directory.

    Args:
        name: Mapping name (filename without extension)

    Returns:
        StatementStructure: Parsed statement structure

    Raises:
        ConfigurationError: If no such mapping exists or parsing fails
    """
    mapping_dir = _get_mapping_dir()
    for ext in (".yaml", ".yml", ".json"):
        path = mapping_dir / f"{name}{ext}"
        if path.exists():
            return load_statement_config(str(path))
    raise ConfigurationError(
        message=f"Built-in statement '{name}' not found",
        config_path=str(mapping_dir),
        errors=[f"No file for '{name}' with .yaml, .yml, or .json extension"],
    )

# --- END FILE: fin_statement_model/statements/config/loader.py ---

# --- START FILE: fin_statement_model/statements/config/mappings/__init__.py ---
"""Configuration mappings for statement importers and extensions."""

# --- END FILE: fin_statement_model/statements/config/mappings/__init__.py ---

# --- START FILE: fin_statement_model/statements/errors.py ---
"""Exceptions for the statements package."""

from typing import Optional
from fin_statement_model.core.errors import FinancialModelError


class StatementError(FinancialModelError):
    """Base exception for statement-related errors."""


class ConfigurationError(StatementError):
    """Exception raised for statement configuration errors."""

    def __init__(
        self,
        message: str,
        config_path: Optional[str] = None,
        errors: Optional[list[str]] = None,
    ):
        """Initialize a configuration error.

        Args:
            message: Main error message
            config_path: Path to the configuration file that caused the error
            errors: List of specific validation errors
        """
        self.message = message
        self.config_path = config_path
        self.errors = errors or []

        # Build detailed error message
        details = []
        if config_path:
            details.append(f"Config file: {config_path}")
        if errors:
            details.append("Validation errors:")
            details.extend([f"  - {error}" for error in errors])

        full_message = message
        if details:
            full_message = f"{message}\n" + "\n".join(details)

        super().__init__(full_message)

# --- END FILE: fin_statement_model/statements/errors.py ---

# --- START FILE: fin_statement_model/statements/factory.py ---
"""Statement factory module for building, formatting, and exporting financial statements.

Provides `StatementFactory` with convenience methods to build managers, format DataFrame/JSON outputs,
and export statements to Excel or JSON files.
"""

from typing import Any
import pandas as pd
from fin_statement_model.core.graph import Graph
from .manager import StatementManager
import os
import json


class StatementFactory:
    """Factory for building and formatting financial statements.

    Provides convenience methods to create a StatementManager and produce
    formatted outputs based on configuration files.
    """

    @staticmethod
    def build_manager(graph: Graph, config_path: str) -> StatementManager:
        """Create and return a StatementManager loaded with the given configuration.

        Args:
            graph: An instance of the core Graph to use for calculations.
            config_path: Path to the statement configuration file (JSON/YAML).

        Returns:
            StatementManager: Manager with the statement loaded and registered.
        """
        manager = StatementManager(graph)
        # Support loading a single config file or all configs in a directory
        if os.path.isdir(config_path):
            manager.load_statements_from_directory(config_path)
        else:
            manager.load_statement(config_path)
        return manager

    @staticmethod
    def create_statement_dataframe(
        graph: Graph, config_path: str, format_type: str = "dataframe", **kwargs: Any
    ) -> pd.DataFrame:
        """Load, calculate, and format the statement as a pandas DataFrame.

        Args:
            graph: An instance of the core Graph to use for calculations.
            config_path: Path to the statement configuration file.
            format_type: Formatting type to use (default 'dataframe').
            **kwargs: Additional parameters for the formatter (e.g., subtotals).

        Returns:
            pd.DataFrame: The formatted statement data.
        """
        manager = StatementFactory.build_manager(graph, config_path)
        stmt_ids = manager.get_all_statement_ids()
        if not stmt_ids:
            raise ValueError("No statements registered in manager.")
        # Generate formatted output for each statement
        outputs: dict[str, pd.DataFrame] = {}
        for stmt_id in stmt_ids:
            manager.create_calculations(stmt_id)
            outputs[stmt_id] = manager.format_statement(stmt_id, format_type, **kwargs)
        # If only one statement, return its DataFrame directly
        if len(outputs) == 1:
            return next(iter(outputs.values()))
        return outputs

    @staticmethod
    def create_statement_json(
        graph: Graph,
        config_path: str,
    ) -> dict[str, object]:
        """Load statements, calculate all, and return JSON-serializable dict of statement data."""
        manager = StatementFactory.build_manager(graph, config_path)
        stmt_ids = manager.get_all_statement_ids()
        if not stmt_ids:
            raise ValueError("No statements registered in manager.")
        json_outputs: dict[str, Any] = {}
        for stmt_id in stmt_ids:
            manager.create_calculations(stmt_id)
            json_outputs[stmt_id] = manager.build_data_dictionary(stmt_id)
        return json_outputs

    @staticmethod
    def export_statements_to_excel(
        graph: Graph, config_path: str, output_dir: str, **kwargs: Any
    ) -> None:
        """Export all registered statements to individual Excel files in output_dir."""
        manager = StatementFactory.build_manager(graph, config_path)
        os.makedirs(output_dir, exist_ok=True)
        for stmt_id in manager.get_all_statement_ids():
            manager.create_calculations(stmt_id)
            file_path = os.path.join(output_dir, f"{stmt_id}.xlsx")
            manager.export_to_excel(stmt_id, file_path, **kwargs)

    @staticmethod
    def export_statements_to_json(
        graph: Graph,
        config_path: str,
        output_dir: str,
    ) -> None:
        """Export all registered statements to individual JSON files in output_dir."""
        os.makedirs(output_dir, exist_ok=True)
        json_outputs = StatementFactory.create_statement_json(graph, config_path)
        for stmt_id, data in json_outputs.items():
            file_path = os.path.join(output_dir, f"{stmt_id}.json")
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=4)

# --- END FILE: fin_statement_model/statements/factory.py ---

# --- START FILE: fin_statement_model/statements/forecasting/__init__.py ---
"""Forecasting subpackage for financial statement graphs."""

__all__ = ["StatementForecaster"]

from fin_statement_model.statements.forecasting.forecaster import StatementForecaster

# --- END FILE: fin_statement_model/statements/forecasting/__init__.py ---

# --- START FILE: fin_statement_model/statements/forecasting/forecaster.py ---
"""Forecasting operations dedicated to statement-level financial graphs."""

import logging
from typing import Any, Optional, Union, Callable
import numpy as np

# Core imports
from fin_statement_model.core.nodes import Node
from fin_statement_model.core.node_factory import NodeFactory

logger = logging.getLogger(__name__)

class StatementForecaster:
    """Handles forecasting operations specifically for a FinancialStatementGraph."""

    def __init__(self, fsg: Any) -> None:
        """Initialize the forecaster.

        Args:
            fsg: The FinancialStatementGraph instance this forecaster will operate on.
        """
        self.fsg = fsg

    def create_forecast(
        self,
        forecast_periods: list[str],
        node_configs: Optional[dict[str, dict[str, Any]]] = None,
        historical_periods: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> None:
        """Create forecasts for financial statement items on the graph.

        Args:
            forecast_periods: List of future periods to forecast.
            node_configs: Mapping of node names to their forecast configurations.
            historical_periods: Optional list of historical periods to use as base.
            **kwargs: Additional arguments passed to the forecasting logic.

        Returns:
            None
        """
        logger.info(f"StatementForecaster: Creating forecast for periods {forecast_periods}")
        try:
            if historical_periods is None:
                if not self.fsg.periods or not forecast_periods:
                    raise ValueError("Cannot infer historical periods: missing graph or forecast periods.")
                first_forecast = forecast_periods[0]
                try:
                    idx = self.fsg.periods.index(first_forecast)
                    historical_periods = self.fsg.periods[:idx]
                    logger.debug(f"Inferred historical periods: {historical_periods}")
                except ValueError:
                    historical_periods = list(self.fsg.periods)
                    logger.warning(f"First forecast period {first_forecast} not found; using all periods.")
            else:
                logger.debug(f"Using explicitly provided historical periods: {historical_periods}")

            if not historical_periods:
                raise ValueError("No historical periods found for forecasting")
            if not forecast_periods:
                raise ValueError("No forecast periods provided")

            for period in forecast_periods:
                if period not in self.fsg.periods:
                    self.fsg.manipulator.add_periods([period])
                    logger.debug(f"Added forecast period to graph: {period}")

            if node_configs is None:
                node_configs = {}

            for node_name, config in node_configs.items():
                node = self.fsg.get_node(node_name)
                if node is None:
                    raise ValueError(f"Node {node_name} not found in graph")
                if not hasattr(node, "values") or not isinstance(node.values, dict):
                    logger.warning(f"Skipping forecast for node {node_name}: not a value-storing node")
                    continue

                method = config.get("method", "simple")
                growth_config = config.get("config")
                if method == "simple":
                    if isinstance(growth_config, list):
                        growth_config = growth_config[0]
                elif method == "curve":
                    if not isinstance(growth_config, list):
                        growth_config = [growth_config] * len(forecast_periods)
                elif method == "statistical":
                    if not isinstance(growth_config, dict) or "distribution" not in growth_config:
                        raise ValueError(f"Statistical method requires distribution parameters for {node_name}")
                elif method in {"average", "historical_growth"}:
                    growth_config = 0.0
                else:
                    raise ValueError(f"Invalid forecasting method: {method}")

                self._forecast_node(node, historical_periods, forecast_periods, growth_config, method)

            self.fsg.recalculate_all()
            logger.info(f"Created forecast for {len(forecast_periods)} periods and {len(node_configs)} nodes")
        except Exception as e:
            logger.error(f"Error creating forecast: {e}", exc_info=True)
            raise

    def _forecast_node(
        self,
        node: Node,
        historical_periods: list[str],
        forecast_periods: list[str],
        growth_config: Union[float, list[float], Callable[[], float]],
        method: str,
        **kwargs: dict[str, Any],
    ) -> None:
        """Calculate forecast values and update the original node.

        Args:
            node: The graph node to forecast.
            historical_periods: List of historical periods for base values.
            forecast_periods: List of periods for which to calculate forecasts.
            growth_config: Growth parameter(s) (rate, list, or generator).
            method: Forecasting method name.
            **kwargs: Additional arguments passed to growth logic.

        Returns:
            None
        """
        logger.debug(f"StatementForecaster: Forecasting node {node.name} using method {method}")
        if not historical_periods:
            raise ValueError(f"No historical periods for node {node.name}")

        base_period = historical_periods[-1]
        if not hasattr(node, "values") or not isinstance(node.values, dict):
            logger.error(f"Cannot forecast node {node.name}: no 'values' dict")
            return

        if base_period not in node.values:
            available = sorted([p for p in node.values if p in historical_periods], reverse=True)
            if available:
                base_period = available[0]
                logger.info(f"Using {base_period} as base for {node.name}")
            else:
                raise ValueError(f"No valid historical base period for node {node.name}")

        method_map = {
            "simple": "fixed",
            "curve": "curve",
            "statistical": "statistical",
            "average": "average",
            "historical_growth": "historical_growth",
        }
        forecast_type = method_map.get(method)
        if forecast_type is None:
            raise ValueError(f"Invalid forecasting method: {method}")

        if method == "simple":
            growth_params = float(growth_config)
        elif method == "curve":
            if not isinstance(growth_config, list) or len(growth_config) != len(forecast_periods):
                raise ValueError("Curve growth list length mismatch")
            growth_params = [float(x) for x in growth_config]
        elif method == "statistical":
            distr = growth_config["distribution"]
            params = growth_config["params"]
            def gen() -> float:
                if distr == "normal":
                    return np.random.normal(params["mean"], params["std"])
                elif distr == "uniform":
                    return np.random.uniform(params["low"], params["high"])
                else:
                    raise ValueError(f"Unsupported distribution: {distr}")
            growth_params = gen
        elif method == "average":
            hist_vals = [node.calculate(p) for p in historical_periods if p in node.values]
            valid = [v for v in hist_vals if v is not None and not np.isnan(v)]
            if not valid:
                raise ValueError(f"No data to compute average for {node.name}")
            growth_params = sum(valid) / len(valid)
        else:
            growth_params = None

        tmp_node = NodeFactory.create_forecast_node(
            name=f"{node.name}_forecast_temp",
            base_node=node,
            base_period=base_period,
            forecast_periods=forecast_periods,
            forecast_type=forecast_type,
            growth_params=growth_params,
        )
        for period in forecast_periods:
            try:
                val = tmp_node.calculate(period)
                if np.isnan(val) or np.isinf(val):
                    logger.warning(f"Bad forecast {val} for {node.name}@{period}; defaulting to 0.0")
                    val = 0.0
                node.values[period] = val
            except Exception as e:
                logger.error(f"Error forecasting {node.name}@{period}: {e}", exc_info=True)
                node.values[period] = 0.0

        if hasattr(node, "clear_cache"):
            node.clear_cache()

    def forecast_value(
        self,
        node_name: str,
        forecast_periods: list[str],
        base_period: Optional[str] = None,
        forecast_config: Optional[dict[str, Any]] = None,
        **kwargs: Any,
    ) -> dict[str, float]:
        """Forecast and return values for a node without mutating the graph.

        Args:
            node_name: Name of the node to forecast.
            forecast_periods: List of future periods to forecast.
            base_period: Optional base period to use for forecasting. If omitted, inferred from graph.
            forecast_config: Forecast configuration dict mapping 'method' and 'config'.
            **kwargs: Additional arguments passed to the internal forecasting logic.

        Returns:
            A mapping from forecast period to its forecasted float value.
        """
        # Locate the node
        node = self.fsg.get_node(node_name)
        if node is None:
            raise ValueError(f"Node {node_name} not found in graph")

        # Ensure the node can store values
        if not hasattr(node, "values") or not isinstance(node.values, dict):
            logger.warning(
                f"Skipping forecast_value for node {node_name}: no 'values' dictionary"
            )
            return {}

        # Determine historical periods list
        if base_period:
            historical_periods = [base_period]
        # Try custom getter, otherwise infer
        elif hasattr(self.fsg, "get_historical_periods") and callable(
            getattr(self.fsg, "get_historical_periods")
        ):
            historical_periods = self.fsg.get_historical_periods()
        else:
            if not self.fsg.periods or not forecast_periods:
                raise ValueError(
                    "Cannot infer historical periods: graph or forecast periods missing"
                )
            first = forecast_periods[0]
            try:
                idx = self.fsg.periods.index(first)
                historical_periods = self.fsg.periods[:idx]
            except ValueError:
                historical_periods = list(self.fsg.periods)

        if not historical_periods:
            raise ValueError("No historical periods found for forecasting")
        if not forecast_periods:
            raise ValueError("No forecast periods provided")

        # Add missing forecast periods to graph
        for period in forecast_periods:
            if period not in self.fsg.periods:
                self.fsg.manipulator.add_periods([period])

        config = forecast_config or {}
        method = config.get("method", "simple")
        growth_cfg = config.get("config")
        # Normalize configuration parameters
        if method == "simple" and isinstance(growth_cfg, list):
            growth_cfg = growth_cfg[0]
        elif method == "curve" and not isinstance(growth_cfg, list):
            growth_cfg = [growth_cfg] * len(forecast_periods)
        elif method == "statistical":
            if not isinstance(growth_cfg, dict) or "distribution" not in growth_cfg:
                raise ValueError(f"Statistical forecast requires distribution for {node_name}")
        elif method in {"average", "historical_growth"}:
            growth_cfg = 0.0
        else:
            # Leave other methods as-is
            pass

        # Snapshot original values
        original_vals = dict(node.values)

        # Perform forecasting, mutating node.values temporarily
        self._forecast_node(node, historical_periods, forecast_periods, growth_cfg, method, **kwargs)

        # Collect forecast results
        results: dict[str, float] = {}
        for period in forecast_periods:
            if period in node.values:
                results[period] = node.values[period]

        # Restore original node values
        node.values = original_vals
        return results

# --- END FILE: fin_statement_model/statements/forecasting/forecaster.py ---

# --- START FILE: fin_statement_model/statements/formatter/__init__.py ---
"""Formatter package for financial statements.

This package provides tools for formatting financial statements for display
and reporting, with controls for formatting, styling, and presentation.
"""

from .formatter import StatementFormatter

__all__ = ["StatementFormatter"]

# --- END FILE: fin_statement_model/statements/formatter/__init__.py ---

# --- START FILE: fin_statement_model/statements/formatter/formatter.py ---
"""Formatter for financial statements.

This module provides functionality for formatting financial statements
for display or reporting, including applying formatting rules, adding subtotals,
and applying sign conventions.
"""

import pandas as pd
from typing import Optional, Any
from pandas.api.types import is_numeric_dtype
import logging

from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.statements.structure import (
    Section,
    CalculatedLineItem,
    SubtotalLineItem,
    StatementItem,
)

# Configure logging
logger = logging.getLogger(__name__)


class StatementFormatter:
    """Formats financial statements for display or reporting.

    This class provides methods to transform raw financial data into
    formatted financial statements with proper headers, indentation,
    subtotals, and sign conventions.
    """

    def __init__(self, statement: StatementStructure):
        """Initialize a statement formatter.

        Args:
            statement: The statement structure to format
        """
        self.statement = statement
        self.config = {}
        self.default_formats = {
            "precision": 2,
            "use_thousands_separator": True,
            "show_zero_values": True,
            "show_negative_sign": True,
            "indent_character": "  ",
            "subtotal_style": "bold",
            "total_style": "bold",
            "header_style": "bold",
        }

    def generate_dataframe(
        self,
        data: dict[str, dict[str, float]],
        apply_sign_convention: bool = True,
        include_empty_items: bool = False,
        number_format: Optional[str] = None,
    ) -> pd.DataFrame:
        """Generate a formatted DataFrame of the statement.

        Combines the statement structure with period data.

        Args:
            data: Mapping of node IDs to period-value dicts.
            apply_sign_convention: Whether to apply sign conventions.
            include_empty_items: Whether to include items with no data.
            number_format: Optional format string for numbers

        Returns:
            pd.DataFrame: Formatted statement DataFrame
        """
        # Convert structure to base DataFrame
        # This needs modification to incorporate period data
        # Placeholder: Creates structure, but needs data merge
        rows = []
        all_periods = sorted(list(set(p for node_data in data.values() for p in node_data))) if data else []

        # Helper to recursively build rows including data
        def _process_item_with_data(
            item: StatementItem, depth: int, rows_list: list[dict[str, Any]]
        ) -> None:
            item_data = data.get(getattr(item, "node_id", None), {})
            row = {
                "Line Item": "  " * depth + item.name, # Indentation
                "ID": item.id, # Added ID for clarity
                # Add periods as columns
                **{period: item_data.get(period) for period in all_periods},
                # Metadata (could be added later or made optional)
                "line_type": self._get_item_type(item),
                "node_id": getattr(item, "node_id", None),
                "sign_convention": getattr(item, "sign_convention", 1),
                "is_subtotal": isinstance(item, SubtotalLineItem),
                "is_calculated": isinstance(item, CalculatedLineItem),
            }
            if include_empty_items or any(row[p] is not None for p in all_periods):
                 rows_list.append(row)

            if hasattr(item, "children"):
                for child in item.children:
                    _process_item_with_data(child, depth + 1, rows_list)
            elif isinstance(item, Section):
                for child_item in item.items:
                    _process_item_with_data(child_item, depth + 1, rows_list)

        # Process sections and their items recursively
        for section in self.statement.sections:
            # Section Header (optional, depends on desired format)
            # rows.append({"Line Item": section.name, "ID": section.id, **{p: None for p in all_periods}})
             _process_item_with_data(section, 0, rows)


        if not rows:
            return pd.DataFrame(columns=["Line Item", "ID", *all_periods])

        df = pd.DataFrame(rows)
        # Reorder columns: Line Item, ID, then sorted periods
        cols = ["Line Item", "ID", *all_periods]
        df = df[cols + [c for c in df.columns if c not in cols]] # Keep metadata at end

        # Apply formatting
        if apply_sign_convention:
            df = self._apply_sign_convention_to_data(df, all_periods)

        # Format numbers (apply to period columns only)
        df = self._format_numbers(df, number_format, period_columns=all_periods)

        # TODO: Add subtotals (requires rework for multi-period data)
        # df = self._add_subtotals(df)

        # Remove metadata if not requested - keeping for now
        # if not include_metadata:
        #     metadata_cols = [col for col in df.columns if col.startswith("meta_")]
        #     if metadata_cols:
        #         df = df.drop(columns=metadata_cols)

        return df

    def _process_item(self, item: StatementItem, depth: int, rows: list[dict[str, Any]]) -> None:
        # This method seems less relevant now with _process_item_with_data
        # Keep it for now or remove if fully replaced
        pass

    def _get_item_type(self, item: StatementItem) -> str:
        """Get the type of a statement item.

        Args:
            item: Statement item to get type for

        Returns:
            str: Item type identifier
        """
        if isinstance(item, Section):
            return "section"
        elif isinstance(item, SubtotalLineItem):
            return "subtotal"
        elif isinstance(item, CalculatedLineItem):
            return "calculated"
        else:
            return "item"

    def _apply_sign_convention(self, df: pd.DataFrame) -> pd.DataFrame:
        # This method needs adjustment for multi-period data
        # The logic assumed a single 'value' column
        # Keep for reference, replace with _apply_sign_convention_to_data
        pass

    def _apply_sign_convention_to_data(self, df: pd.DataFrame, period_columns: list[str]) -> pd.DataFrame:
        """Apply sign conventions to the statement values across periods."""
        result = df.copy()
        if "sign_convention" in result.columns:
            for col in period_columns:
                if col in result.columns and is_numeric_dtype(result[col]):
                    mask = result[col].notna()
                    # Ensure sign_convention is treated as numeric if needed
                    sign_col = pd.to_numeric(result.loc[mask, "sign_convention"], errors="coerce").fillna(1)
                    result.loc[mask, col] = (
                        result.loc[mask, col] * sign_col
                    )
        return result

    def _add_subtotals(self, df: pd.DataFrame) -> pd.DataFrame:
        # This needs significant rework for multi-period data
        # Placeholder - current logic is single-value based
        logger.warning("Subtotal calculation needs rework for multi-period data.")
        return df

    def _calculate_section_subtotal(self, items: list[dict[str, Any]]) -> float:
        # This needs rework for multi-period data
        pass

    def _format_numbers(
        self, df: pd.DataFrame, number_format: Optional[str] = None, period_columns: Optional[list[str]] = None
    ) -> pd.DataFrame:
        """Format numeric values in the statement.

        Args:
            df: DataFrame to format numbers in
            number_format: Optional format string
            period_columns: List of columns containing period data to format.
                            If None, attempts to format all numeric columns
                            except metadata/indicators.

        Returns:
            pd.DataFrame: DataFrame with formatted numbers
        """
        result = df.copy()

        if period_columns:
            numeric_cols = [col for col in period_columns if col in result.columns and is_numeric_dtype(result[col])]
        else:
             # Original logic if period_columns not specified
            numeric_cols = [
                col
                for col in result.columns
                if is_numeric_dtype(result[col])
                and col not in ("sign_convention", "depth", "ID") # Added ID
                and not col.startswith("meta_")
                and col != "Line Item" # Ensure Line Item name is not formatted
            ]

        # Format to specified precision
        precision = self.config.get("precision", self.default_formats["precision"])

        if number_format:
            # Use provided format string
            for col in numeric_cols:
                # Check if column exists before applying format
                if col in result.columns:
                    result[col] = result[col].apply(
                        lambda x: f"{x:{number_format}}" if pd.notna(x) else ""
                    )
        else:
            # Use default formatting
            for col in numeric_cols:
                 # Check if column exists before applying format
                if col in result.columns:
                    result[col] = result[col].apply(
                        lambda x: (
                            (f"{x:,.{precision}f}" if pd.notna(x) else "")
                            if self.default_formats["use_thousands_separator"]
                            else (f"{x:.{precision}f}" if pd.notna(x) else "")
                        )
                    )

        return result

    def format_html(
        self,
        data: dict[str, dict[str, float]],
        apply_sign_convention: bool = True,
        include_empty_items: bool = False,
        css_styles: Optional[dict[str, str]] = None,
    ) -> str:
        """Format the statement data as HTML.

        Args:
            data: Mapping of node IDs to period-value dicts.
            apply_sign_convention: Whether to apply sign conventions.
            include_empty_items: Whether to include items with no data.
            css_styles: Optional dict of CSS styles for the HTML.

        Returns:
            str: HTML string representing the statement.
        """
        df = self.generate_dataframe(data, apply_sign_convention, include_empty_items)

        # Convert DataFrame to HTML
        html = df.to_html(index=False)

        # Add CSS styles if provided
        if css_styles:
            style_str = "<style>\n"
            for selector, style in css_styles.items():
                style_str += f"{selector} {{ {style} }}\n"
            style_str += "</style>\n"
            html = style_str + html

        return html

# --- END FILE: fin_statement_model/statements/formatter/formatter.py ---

# --- START FILE: fin_statement_model/statements/graph/financial_graph.py ---
"""FinancialStatementGraph module for statement-specific graph operations.

This module defines `FinancialStatementGraph`, which extends the core `Graph`
with mixins for statement analysis, merging, metrics, forecasting, and conversion to DataFrame.
"""

from typing import Optional
from fin_statement_model.core.graph import Graph
from fin_statement_model.preprocessing.transformation_service import (
    TransformationService,
)
from fin_statement_model.statements.forecasting.forecaster import StatementForecaster
from fin_statement_model.statements.merging.merger import StatementMerger
from fin_statement_model.io import write_data
import pandas as pd


class FinancialStatementGraph(Graph):
    """Main class for managing financial statement data and calculations.

    This class combines functionality from multiple mixins to provide a complete
    financial statement modeling solution.
    """

    def __init__(self, periods: Optional[list[str]] = None):
        """Initialize a new FinancialStatementGraph.

        Args:
            periods: Optional list of time periods to initialize the graph with.
        """
        super().__init__(periods=periods)
        # Facade services for statement-specific operations
        self.transformer: TransformationService = TransformationService()
        self.forecaster: StatementForecaster = StatementForecaster(self)
        self.merger: StatementMerger = StatementMerger(self)

    def to_dataframe(self) -> pd.DataFrame:
        """Exports the financial statement graph data to a pandas DataFrame.

        Returns:
            pd.DataFrame: A DataFrame representation of the graph's node values over periods.
        """
        return write_data(format_type="dataframe", graph=self, target=None)

# --- END FILE: fin_statement_model/statements/graph/financial_graph.py ---

# --- START FILE: fin_statement_model/statements/importer/__init__.py ---
"""Importer package for financial statement data.

Provides functions to import raw data (e.g., cell dicts) into the graph.
"""

from .cell_importer import import_from_cells

__all__ = ["import_from_cells"]

# --- END FILE: fin_statement_model/statements/importer/__init__.py ---

# --- START FILE: fin_statement_model/statements/importer/cell_importer.py ---
"""Importer module for reading cell-based financial statement data into a FinancialStatementGraph."""

from typing import Any
from fin_statement_model.statements.graph.financial_graph import FinancialStatementGraph

__all__ = ["import_from_cells"]


def import_from_cells(cells_info: list[dict[str, Any]]) -> FinancialStatementGraph:
    """Import a list of cell dictionaries into a FinancialStatementGraph.

    Each cell dict should include at minimum:
    - 'row_name': identifier for the line item
    - 'column_name': the period label
    - 'value': the numeric value

    Args:
        cells_info: List of cell metadata dictionaries.

    Returns:
        A FinancialStatementGraph populated with detected periods and items.
    """
    # Group cells by row_name to aggregate values per financial statement item
    items: dict[str, dict[str, Any]] = {}
    unique_periods: set = set()

    for cell in cells_info:
        # Clean the item name and period
        item_name = cell.get("row_name", "").strip()
        period = cell.get("column_name", "").strip()
        value = cell.get("value")

        if not item_name or not period:
            continue

        unique_periods.add(period)
        if item_name not in items:
            items[item_name] = {}
        items[item_name][period] = value

    # Sort periods and create the graph
    sorted_periods = sorted(unique_periods)
    fsg = FinancialStatementGraph(periods=sorted_periods)

    # Add each financial statement item to the graph
    for name, values in items.items():
        fsg.add_financial_statement_item(name, values)

    return fsg

# --- END FILE: fin_statement_model/statements/importer/cell_importer.py ---

# --- START FILE: fin_statement_model/statements/manager.py ---
"""Statement manager for Financial Statement Model.

This module provides a manager for handling statement structures and formatting
in the Financial Statement Model.
"""

import os
import logging
from typing import Any, Optional, Union
import pandas as pd
from pathlib import Path

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import StatementError, ConfigurationError
from .structure import (
    StatementStructure,
    LineItem,
)
from .config.loader import load_statement_config
from .services.calculation_service import CalculationService
from .services.export_service import ExportService
from fin_statement_model.statements.formatter import StatementFormatter

# Configure logging
logger = logging.getLogger(__name__)


class StatementManager:
    """Manages financial statement structures and their integration with the graph.

    This class handles loading statement configurations, creating the necessary
    nodes in the graph, calculating values, and formatting the results.
    """

    def __init__(self, graph: Graph):
        """Initialize a statement manager.

        Args:
            graph: The graph to use for calculations
        """
        self.graph = graph
        self.statements: dict[str, StatementStructure] = {}
        self.calculation_service = CalculationService(self.graph)
        self.export_service = ExportService(self)

    def load_statement(self, config_path: str) -> StatementStructure:
        """Load a statement configuration and register it with the manager.

        Args:
            config_path: Path to the statement configuration file

        Returns:
            StatementStructure: The loaded statement structure

        Raises:
            ConfigurationError: If the configuration file is invalid or cannot be loaded
            StatementError: If the statement cannot be registered
        """
        logger.info(f"Loading statement configuration from {config_path}")
        try:
            statement = load_statement_config(config_path)
            self.register_statement(statement)
        except ConfigurationError:
            # Re-raise ConfigurationError directly
            raise
        except Exception as e:
            logger.exception(f"Unexpected error loading statement configuration from {config_path}")
            raise StatementError(
                message="Failed to load statement configuration",
                statement_id=os.path.basename(config_path),
            ) from e
        else:
            return statement

    def register_statement(self, statement: StatementStructure) -> None:
        """Register a statement structure with the manager.

        Args:
            statement: The statement structure to register

        Raises:
            StatementError: If a statement with the same ID is already registered
                or if the statement is invalid
        """
        try:
            if statement.id in self.statements:
                raise StatementError(
                    message="Statement with this ID is already registered",
                    statement_id=statement.id,
                )

            self.statements[statement.id] = statement
            logger.info(f"Registered statement '{statement.name}' with ID '{statement.id}'")
        except Exception as e:
            if isinstance(e, StatementError):
                # Re-raise StatementError
                raise
            else:
                # Wrap other exceptions
                statement_id = getattr(statement, "id", "unknown")
                logger.exception(f"Error registering statement '{statement_id}'")
                raise StatementError(
                    message="Failed to register statement", statement_id=statement_id
                ) from e

    def get_statement(self, statement_id: str) -> Optional[StatementStructure]:
        """Get a registered statement by ID.

        Args:
            statement_id: The ID of the statement to get

        Returns:
            Optional[StatementStructure]: The statement structure, or None if not found
        """
        return self.statements.get(statement_id)

    def create_calculations(self, statement_id: str) -> list[str]:
        """Delegate calculation creation to the CalculationService.

        Args:
            statement_id: The ID of the statement to create calculations for

        Returns:
            List[str]: List of created calculation node IDs
        """
        statement = self.get_statement(statement_id)
        if statement is None:
            raise StatementError(message="Statement not found", statement_id=statement_id)
        return self.calculation_service.create_calculations(statement)

    def build_data_dictionary(self, statement_id: str) -> dict[str, dict[str, float]]:
        """Build a data dictionary for a statement from the graph.

        Args:
            statement_id: The ID of the statement to build data for

        Returns:
            Dict[str, Dict[str, float]]: Dictionary mapping node IDs to period values

        Raises:
            StatementError: If the statement ID is not registered
        """
        statement = self.get_statement(statement_id)
        if statement is None:
            raise StatementError(message="Statement not found", statement_id=statement_id)

        data = {}

        # Get all items that need data
        all_items = statement.get_all_items()

        # Build data dictionary
        for item in all_items:
            if isinstance(item, LineItem):
                node_id = item.node_id
                if self.graph.get_node(node_id) is not None:
                    # Get the values for all periods
                    values = {}
                    for period in self.graph.periods:
                        try:
                            value = self.graph.calculate(node_id, period)
                            values[period] = value
                        except Exception:
                            logger.warning(f"Error calculating {node_id} for {period}")

                    if values:
                        data[node_id] = values

        return data

    def format_statement(
        self,
        statement_id: str,
        format_type: str = "dataframe",
        **fmt_kwargs: dict[str, object],
    ) -> Union[pd.DataFrame, str]:
        """Format a statement with data from the graph.

        Args:
            statement_id: The ID of the statement to format
            format_type: The type of formatting to apply ('dataframe' or 'html')
            **fmt_kwargs: Additional arguments for the formatter

        Returns:
            Union[pd.DataFrame, str]: The formatted statement (DataFrame or HTML string)

        Raises:
            StatementError: If the statement ID is not registered
            ValueError: If the format type is not supported
        """
        # Retrieve the structure
        statement = self.get_statement(statement_id)
        if statement is None:
            raise StatementError(f"Statement {statement_id!r} not found")

        # Build raw data
        data_dict = self.build_data_dictionary(statement_id)

        # Format using the core StatementFormatter
        formatter = StatementFormatter(statement)
        if format_type == "dataframe":
            return formatter.generate_dataframe(data_dict, **fmt_kwargs)
        elif format_type == "html":
            return formatter.format_html(data_dict, **fmt_kwargs)
        else:
            raise ValueError(f"Unsupported format type: {format_type}")

    def export_to_excel(self, statement_id: str, file_path: str, **kwargs: dict[str, Any]) -> None:
        """Export a statement to an Excel file.

        Args:
            statement_id: The ID of the statement to export
            file_path: Path to save the Excel file
            **kwargs: Additional arguments for the formatter

        Raises:
            StatementError: If the statement ID is not registered
            WriteError: If the export fails
        """
        return self.export_service.to_excel(statement_id, file_path, **kwargs)

    def export_to_json(
        self,
        statement_id: str,
        file_path: str,
        orient: str = "columns",
        **kwargs: dict[str, Any],
    ) -> None:
        """Export a statement to a JSON file.

        Args:
            statement_id: The ID of the statement to export
            file_path: Path to save the JSON file
            orient: JSON orientation format
            **kwargs: Additional arguments for the formatter

        Raises:
            StatementError: If the statement ID is not registered
            WriteError: If the export fails
        """
        return self.export_service.to_json(statement_id, file_path, orient=orient, **kwargs)

    def get_all_statement_ids(self) -> list[str]:
        """Get the IDs of all registered statements.

        Returns:
            List[str]: List of statement IDs
        """
        return list(self.statements.keys())

    def load_statements_from_directory(self, directory_path: str) -> list[str]:
        """Load all statement configurations from a directory.

        Args:
            directory_path: Path to the directory containing statement configurations

        Returns:
            List[str]: List of loaded statement IDs

        Raises:
            ConfigurationError: If the directory does not exist or is not a directory
        """
        loaded_ids = []
        path = Path(directory_path)

        if not path.exists() or not path.is_dir():
            raise ConfigurationError(
                message="Invalid directory path",
                config_path=directory_path,
                errors=[f"Path does not exist or is not a directory: {directory_path}"],
            )

        errors = []

        # Load all JSON and YAML files
        for file_path in path.glob("*.json"):
            try:
                statement = self.load_statement(str(file_path))
                loaded_ids.append(statement.id)
            except Exception as e:
                errors.append(f"Error loading {file_path}: {e!s}")
                logger.exception(f"Error loading statement from {file_path}")

        for file_path in path.glob("*.y*ml"):
            try:
                statement = self.load_statement(str(file_path))
                loaded_ids.append(statement.id)
            except Exception as e:
                errors.append(f"Error loading {file_path}: {e!s}")
                logger.exception(f"Error loading statement from {file_path}")

        if not loaded_ids and errors:
            # If no statements were loaded and there were errors, raise an exception
            raise ConfigurationError(
                message="Failed to load any statements from directory",
                config_path=directory_path,
                errors=errors,
            )

        logger.info(f"Loaded {len(loaded_ids)} statements from {directory_path}")
        return loaded_ids

# --- END FILE: fin_statement_model/statements/manager.py ---

# --- START FILE: fin_statement_model/statements/merging/__init__.py ---
"""Package for statement-level merging helpers."""
__all__ = ["StatementMerger"]

# --- END FILE: fin_statement_model/statements/merging/__init__.py ---

# --- START FILE: fin_statement_model/statements/merging/merger.py ---
"""StatementMerger: encapsulates merge logic for two FinancialStatementGraphs."""

import logging
from typing import Any

logger = logging.getLogger(__name__)

class StatementMerger:
    """Merge-related helpers for a FinancialStatementGraph."""

    def __init__(self, fsg: Any) -> None:
        """Store reference to the host graph."""
        self.fsg = fsg

    def merge_from(self, other_graph: Any) -> None:
        """Merge another FinancialStatementGraph into this one."""
        # 1. Update periods
        for period in other_graph.periods:
            if period not in self.fsg.periods:
                self.fsg.periods.append(period)
        self.fsg.periods.sort()

        # 2. Merge nodes
        for node_name, node in other_graph.nodes.items():
            existing = self.fsg.get_node(node_name)
            if existing is not None:
                # Update existing node's values
                if hasattr(node, "values"):
                    for p, v in node.values.items():
                        existing.values[p] = v  # type: ignore
                self.fsg.manipulator.add_node(existing)
            else:
                # Add entirely new node
                self.fsg.manipulator.add_node(node)

# --- END FILE: fin_statement_model/statements/merging/merger.py ---

# --- START FILE: fin_statement_model/statements/services/__init__.py ---
"""Services package for statements: formatting, calculation, export, etc."""

from .calculation_service import CalculationService
from .export_service import ExportService

__all__ = [
    "CalculationService",
    "ExportService",
]

# --- END FILE: fin_statement_model/statements/services/__init__.py ---

# --- START FILE: fin_statement_model/statements/services/calculation_service.py ---
"""Calculation service for financial statements.

Encapsulates creation of calculation nodes and detection of circular dependencies.
"""

import logging
from typing import Union, Any

from fin_statement_model.core.errors import (
    NodeError,
    CalculationError,
    CircularDependencyError,
)
from fin_statement_model.statements.structure import (
    StatementStructure,
    CalculatedLineItem,
    SubtotalLineItem,
)
from fin_statement_model.core.graph import Graph

__all__ = ["CalculationService"]

logger = logging.getLogger(__name__)


class CalculationService:
    """Service to create calculation nodes in the graph for a given statement."""

    def __init__(self, engine: Graph):
        """Initialize the CalculationService with a Graph instance for calculations.

        Args:
            engine: The Graph instance used for adding and evaluating calculation nodes.
        """
        self.engine = engine
        self._input_values: dict[str, Any] = {}  # Track input values for dependency resolution

    def set_input_values(self, values: dict[str, Any]) -> None:
        """Set input values for calculations.

        Args:
            values: Dictionary mapping node IDs to their values.
        """
        self._input_values = values
        # Update engine's shared registry with input values
        for node_id in values:
            if node_id not in self.engine._nodes:
                logger.warning(
                    f"Input value provided for {node_id}, but node does not exist in engine registry. "
                    "Cannot directly add value without creating a node."
                )
            else:
                # Node exists, potentially update its value if needed
                pass

    def create_calculations(self, statement: StatementStructure) -> list[str]:
        """Create calculation nodes for all calculated items in the statement.

        Args:
            statement (StatementStructure): The statement structure containing calculations.

        Returns:
            List[str]: List of created calculation node IDs.

        Raises:
            CircularDependencyError: If a circular dependency is detected.
            NodeError: If dependencies cannot be satisfied.
        """
        items = statement.get_calculation_items()

        if not items:
            return []

        processed: set[str] = set(self._input_values.keys())  # Start with known input values
        created_nodes: list[str] = []

        # Add input values to the processed set if they exist in the engine's registry
        for node_id in self._input_values:
            if node_id in self.engine._nodes:
                processed.add(node_id)
            else:
                pass

        remaining = items.copy()
        while remaining:
            progress = False

            for item in remaining[:]:
                deps_satisfied = all(
                    (
                        dep_id in processed
                        or dep_id in self._input_values
                        or dep_id in self.engine._nodes
                    )  # Check engine registry
                    for dep_id in item.input_ids
                )

                if deps_satisfied:
                    try:
                        self._create_calculation_node(item)
                        created_nodes.append(item.id)
                        processed.add(item.id)  # Mark this item's ID as processed
                        remaining.remove(item)
                        progress = True
                    except Exception:
                        logger.exception(f"Failed to create calculation node for {item.id}")
                        raise

            if not progress and remaining:
                cycle = self._detect_cycle(remaining)
                if cycle:
                    logger.error(f"Circular dependency detected: {cycle}")
                    raise CircularDependencyError(
                        message=f"Circular dependency detected in calculations: {cycle}",
                        cycle=cycle,
                    )
                else:
                    missing_deps = set()
                    for item in remaining:
                        missing_deps.update(
                            dep_id
                            for dep_id in item.input_ids
                            if not (
                                dep_id in processed
                                or dep_id in self._input_values
                                or dep_id in self.engine._nodes
                            )  # Check engine registry
                        )
                    if missing_deps:
                        raise NodeError(
                            message=f"Missing dependencies for calculations: {missing_deps}",
                            node_id=remaining[0].id,  # Report first problematic item
                        )
                    else:
                        logger.error(
                            f"Calculation stalled for items: {[it.id for it in remaining]}. No progress made, no cycle detected, and no explicit missing dependencies found in registry."
                        )
                        raise CalculationError(
                            message="Calculation stalled without clear cause.",
                            node_id=remaining[0].id,
                        )

        return created_nodes

    def _detect_cycle(self, items: list[Union[CalculatedLineItem, SubtotalLineItem]]) -> list[str]:
        """Detects a cycle in the calculation dependency graph.

        Args:
            items: List of calculation items with unresolved dependencies.

        Returns:
            List[str]: List of item IDs forming a cycle, or empty if none found.
        """
        if not items:
            return []

        item_ids = {item.id for item in items}
        graph_map = {item.id: {i for i in item.input_ids if i in item_ids} for item in items}

        visited = set()
        rec_stack = set()

        def dfs(node: str, path: list[str]) -> list[str]:
            if node in rec_stack:
                try:
                    start = path.index(node)
                    return path[start:]  # Return the cycle path
                except ValueError:
                    logger.exception(
                        f"Internal error: Node {node} in rec_stack but not in path {path}"
                    )
                    return [node]  # Fallback
            if node in visited:
                return []  # Already visited this path, no cycle found here

            visited.add(node)
            rec_stack.add(node)
            path.append(node)

            for nbr in graph_map.get(node, set()):
                cycle = dfs(nbr, path)
                if cycle:
                    return cycle  # Propagate cycle found deeper

            rec_stack.remove(node)
            path.pop()
            return []

        for nid in graph_map:
            if nid not in visited:
                cycle = dfs(nid, [])
                if cycle:
                    return cycle
        return []  # No cycle found

    def _create_calculation_node(self, item: Union[CalculatedLineItem, SubtotalLineItem]) -> None:
        """Create a calculation node in the graph for a given calculation item.

        Args:
            item: The calculation item defining type, inputs, and parameters.

        Raises:
            NodeError: If an input node does not exist in the graph.
            CalculationError: If the calculation type is unsupported.
        """
        calc_type = item.calculation_type
        inputs = item.input_ids

        for input_id in inputs:
            if input_id not in self.engine._nodes and input_id not in self._input_values:
                raise NodeError(
                    message=f"Missing input node '{input_id}' in engine registry for '{item.id}'",
                    node_id=item.id,
                )

        try:
            if calc_type in ["addition", "subtraction", "multiplication", "division"]:
                self.engine.add_calculation(item.id, inputs, calc_type, **item.parameters)
            elif calc_type == "weighted_average":
                weights = item.parameters.get("weights")
                if not weights:
                    raise CalculationError(
                        message="Weights required for weighted_average calculation",
                        node_id=item.id,
                    )
                self.engine.add_calculation(item.id, inputs, "weighted_average", weights=weights)
            elif isinstance(item, SubtotalLineItem):
                self.engine.add_calculation(item.id, inputs, "addition", **item.parameters)
            else:
                raise CalculationError(
                    message=f"Unsupported calculation type: {calc_type}",
                    node_id=item.id,
                )
        except (NodeError, CalculationError, ValueError, TypeError) as e:
            logger.exception(f"CalculationEngine failed to add calculation for '{item.id}'")
            raise CalculationError(
                message=f"Engine failed to create calculation node for {item.id}",
                node_id=item.id,
                details={"original_error": str(e)},
            )

# --- END FILE: fin_statement_model/statements/services/calculation_service.py ---

# --- START FILE: fin_statement_model/statements/services/export_service.py ---
"""Export service for financial statements.

Encapsulates export of formatted statements to various file formats.
"""

from typing import Any

from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.statements.errors import StatementError

__all__ = ["ExportService"]


class ExportService:
    """Service to export formatted statements to files."""

    def __init__(self, manager: Any) -> None:
        """Initialize the ExportService.

        Args:
            manager: The StatementManager instance used for formatting.
        """
        self.manager = manager

    def to_excel(self, statement_id: str, file_path: str, **fmt_kwargs: dict[str, object]) -> None:
        """Export a statement to an Excel file.

        Args:
            statement_id: The ID of the statement to export.
            file_path: Path to save the Excel file.
            **fmt_kwargs: Additional arguments passed to the formatter.

        Raises:
            StatementError: If the statement ID is invalid.
            WriteError: If writing the file fails.
        """
        try:
            df = self.manager.format_statement(statement_id, format_type="dataframe", **fmt_kwargs)
            df.to_excel(file_path, index=False)
        except StatementError as e:
            # Wrap statement lookup issues in WriteError for consistency
            raise WriteError(
                message="Failed to export statement",
                target=file_path,
                writer_type="excel",
                original_error=e,
            ) from e
        except Exception as e:
            raise WriteError(
                message="Failed to export statement to Excel",
                target=file_path,
                format_type="excel",
                original_error=e,
            ) from e

    def to_json(
        self,
        statement_id: str,
        file_path: str,
        orient: str = "columns",
        **fmt_kwargs: dict[str, object],
    ) -> None:
        """Export a statement to a JSON file.

        Args:
            statement_id: The ID of the statement to export.
            file_path: Path to save the JSON file.
            orient: JSON orientation (default "columns").
            **fmt_kwargs: Additional arguments passed to the formatter.

        Raises:
            StatementError: If the statement ID is invalid.
            WriteError: If writing the file fails.
        """
        try:
            df = self.manager.format_statement(statement_id, format_type="dataframe", **fmt_kwargs)
            df.to_json(file_path, orient=orient)
        except StatementError as e:
            # Wrap statement lookup issues in WriteError for consistency
            raise WriteError(
                message="Failed to export statement",
                target=file_path,
                writer_type="json",
                original_error=e,
            ) from e
        except Exception as e:
            raise WriteError(
                message="Failed to export statement to JSON",
                target=file_path,
                format_type="json",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/statements/services/export_service.py ---

# --- START FILE: fin_statement_model/statements/structure/__init__.py ---
"""Statement structure package.

Re-export domain model classes from submodules.
"""

from .items import (
    StatementItem,
    StatementItemType,
    LineItem,
    CalculatedLineItem,
    SubtotalLineItem,
)
from .containers import Section, StatementStructure

__all__ = [
    "CalculatedLineItem",
    "LineItem",
    "Section",
    "StatementItem",
    "StatementItemType",
    "StatementStructure",
    "SubtotalLineItem",
]

# --- END FILE: fin_statement_model/statements/structure/__init__.py ---

# --- START FILE: fin_statement_model/statements/structure/containers.py ---
"""Container classes for defining hierarchical financial statement structures.

This module provides Section and StatementStructure, which organize
LineItem and CalculatedLineItem objects into nested groups.
"""

from typing import Any, Optional, Union

from fin_statement_model.core.errors import StatementError
from fin_statement_model.statements.structure.items import (
    StatementItem,
    StatementItemType,
    CalculatedLineItem,
    SubtotalLineItem,
)


__all__ = ["Section", "StatementStructure"]


class Section:
    """Represents a section in a financial statement.

    Sections group related items and subsections into a hierarchical container.
    """

    def __init__(
        self,
        id: str,
        name: str,
        description: str = "",
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a section.

        Args:
            id: Unique identifier for the section.
            name: Display name for the section.
            description: Optional description text.
            metadata: Optional additional metadata.

        Raises:
            StatementError: If id or name is invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(message=f"Invalid section ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(message=f"Invalid section name: {name} for ID: {id}")

        self._id = id
        self._name = name
        self._description = description
        self._metadata = metadata or {}
        self._items: list[Union[Section, StatementItem]] = []

    @property
    def id(self) -> str:
        """Get the section identifier."""
        return self._id

    @property
    def name(self) -> str:
        """Get the section display name."""
        return self._name

    @property
    def description(self) -> str:
        """Get the section description."""
        return self._description

    @property
    def metadata(self) -> dict[str, Any]:
        """Get the section metadata."""
        return self._metadata

    @property
    def items(self) -> list[Union["Section", StatementItem]]:
        """Get the child items and subsections."""
        return list(self._items)

    @property
    def item_type(self) -> StatementItemType:
        """Get the item type (SECTION)."""
        return StatementItemType.SECTION

    def add_item(self, item: Union["Section", StatementItem]) -> None:
        """Add a child item or subsection to this section.

        Args:
            item: The Section or StatementItem to add.

        Raises:
            StatementError: If an item with the same id already exists.
        """
        if any(existing.id == item.id for existing in self._items):
            raise StatementError(message=f"Duplicate item ID: {item.id} in section: {self.id}")
        self._items.append(item)

    def find_item_by_id(self, item_id: str) -> Optional[Union["Section", StatementItem]]:
        """Recursively find an item by its identifier within this section.

        Args:
            item_id: Identifier of the item to search for.

        Returns:
            The found Section or StatementItem, or None if not found.
        """
        if self.id == item_id:
            return self
        for child in self._items:
            if child.id == item_id:
                return child
            if isinstance(child, Section):
                found = child.find_item_by_id(item_id)
                if found:
                    return found
        return None


class StatementStructure:
    """Top-level container for a financial statement structure.

    Manages a hierarchy of Section objects.
    """

    def __init__(
        self,
        id: str,
        name: str,
        description: str = "",
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a statement structure.

        Args:
            id: Unique identifier for the statement.
            name: Display name for the statement.
            description: Optional description text.
            metadata: Optional additional metadata.

        Raises:
            StatementError: If id or name is invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(message=f"Invalid statement ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(message=f"Invalid statement name: {name} for ID: {id}")

        self._id = id
        self._name = name
        self._description = description
        self._metadata = metadata or {}
        self._sections: list[Section] = []

    @property
    def id(self) -> str:
        """Get the statement identifier."""
        return self._id

    @property
    def name(self) -> str:
        """Get the statement display name."""
        return self._name

    @property
    def description(self) -> str:
        """Get the statement description."""
        return self._description

    @property
    def metadata(self) -> dict[str, Any]:
        """Get the statement metadata."""
        return self._metadata

    @property
    def sections(self) -> list[Section]:
        """Get the top-level sections."""
        return list(self._sections)

    @property
    def items(self) -> list[Section]:
        """Alias for sections to ease iteration."""
        return self.sections

    def add_section(self, section: Section) -> None:
        """Add a section to the statement.

        Args:
            section: Section to add.

        Raises:
            StatementError: If a section with the same id already exists.
        """
        if any(s.id == section.id for s in self._sections):
            raise StatementError(
                message=f"Duplicate section ID: {section.id} in statement: {self.id}"
            )
        self._sections.append(section)

    def find_item_by_id(self, item_id: str) -> Optional[Union[Section, StatementItem]]:
        """Find an item by its ID in the statement structure.

        Args:
            item_id: The ID of the item to find.

        Returns:
            Optional[Union[Section, StatementItem]]: The found item or None if not found.
        """
        for section in self._sections:
            item = section.find_item_by_id(item_id)
            if item:
                return item
        return None

    def get_calculation_items(
        self,
    ) -> list[Union[CalculatedLineItem, SubtotalLineItem]]:
        """Get all calculation items from the statement structure.

        Returns:
            List[Union[CalculatedLineItem, SubtotalLineItem]]: List of calculation items.
        """
        calculation_items = []

        def collect_calculation_items(items: list[Union["Section", "StatementItem"]]):
            for item in items:
                if isinstance(item, (CalculatedLineItem, SubtotalLineItem)):
                    calculation_items.append(item)
                elif isinstance(item, Section):
                    collect_calculation_items(item.items)

        collect_calculation_items(self._sections)
        return calculation_items

    def get_all_items(self) -> list[Union[Section, StatementItem]]:
        """Get all items in the statement structure recursively.

        Returns:
            List[Union[Section, StatementItem]]: List of all items in the statement.
        """
        items = []
        for section in self._sections:
            items.append(section)
            items.extend(section._items)
        return items

# --- END FILE: fin_statement_model/statements/structure/containers.py ---

# --- START FILE: fin_statement_model/statements/structure/items.py ---
"""Statement structure items module defining line items, calculated items, and subtotals."""

from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Optional

from fin_statement_model.core.errors import StatementError

__all__ = [
    "CalculatedLineItem",
    "LineItem",
    "StatementItem",
    "StatementItemType",
    "SubtotalLineItem",
]


class StatementItemType(Enum):
    """Types of statement structure items.

    Attributes:
      SECTION: Section container
      LINE_ITEM: Basic financial line item
      SUBTOTAL: Subtotal of multiple items
      CALCULATED: Derived calculation item
    """

    SECTION = "section"
    LINE_ITEM = "line_item"
    SUBTOTAL = "subtotal"
    CALCULATED = "calculated"


class StatementItem(ABC):
    """Abstract base class for all statement structure items.

    Defines a common interface: id, name, and item_type.
    """

    @property
    @abstractmethod
    def id(self) -> str:
        """Get the unique identifier of the item."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Get the display name of the item."""

    @property
    @abstractmethod
    def item_type(self) -> StatementItemType:
        """Get the type of this statement item."""


class LineItem(StatementItem):
    """Represents a basic line item in a financial statement.

    Args:
      id: Unique ID for the line item
      name: Display name for the line item
      node_id: ID of the core graph node that holds values
      description: Optional explanatory text
      sign_convention: 1 for normal values, -1 for inverted
      metadata: Optional additional attributes

    Raises:
      StatementError: If inputs are invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        node_id: str,
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a basic LineItem.

        Args:
            id: Unique ID for the line item.
            name: Display name for the line item.
            node_id: ID of the core graph node holding values.
            description: Optional explanatory text.
            sign_convention: Sign convention (1 for positive, -1 for negative).
            metadata: Optional additional attributes.

        Raises:
            StatementError: If inputs are invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(message=f"Invalid line item ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(message=f"Invalid line item name: {name} for ID: {id}")
        if not node_id or not isinstance(node_id, str):
            raise StatementError(message=f"Invalid node ID for line item: {id}")
        if sign_convention not in (1, -1):
            raise StatementError(
                message=f"Invalid sign convention {sign_convention} for item: {id}"
            )
        self._id = id
        self._name = name
        self._node_id = node_id
        self._description = description
        self._sign_convention = sign_convention
        self._metadata = metadata or {}

    @property
    def id(self) -> str:
        """Get the unique identifier of the line item."""
        return self._id

    @property
    def name(self) -> str:
        """Get the display name of the line item."""
        return self._name

    @property
    def node_id(self) -> str:
        """Get the core graph node ID for this line item."""
        return self._node_id

    @property
    def description(self) -> str:
        """Get the description for this line item."""
        return self._description

    @property
    def sign_convention(self) -> int:
        """Get the sign convention (1 or -1)."""
        return self._sign_convention

    @property
    def metadata(self) -> dict[str, Any]:
        """Get custom metadata associated with this item."""
        return self._metadata

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (LINE_ITEM)."""
        return StatementItemType.LINE_ITEM


class CalculatedLineItem(LineItem):
    """Represents a calculated line item whose values come from graph calculations.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      calculation: Dict with 'type', 'inputs', optional 'parameters'
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata

    Raises:
      StatementError: If calculation dictionary is invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        calculation: dict[str, Any],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a CalculatedLineItem based on calculation specification.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            calculation: Calculation spec dict with 'type', 'inputs', optional 'parameters'.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.

        Raises:
            StatementError: If calculation dictionary is invalid.
        """
        super().__init__(
            id=id,
            name=name,
            node_id=id,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
        )
        if not isinstance(calculation, dict):
            raise StatementError(message=f"Invalid calculation spec for item: {id}")
        if "type" not in calculation:
            raise StatementError(message=f"Missing calculation type for item: {id}")
        inputs = calculation.get("inputs")
        if not isinstance(inputs, list) or not inputs:
            raise StatementError(
                message=f"Calculation inputs must be a non-empty list for item: {id}"
            )
        self._calculation = calculation

    @property
    def calculation_type(self) -> str:
        """Get the calculation operation type (e.g., 'addition')."""
        return self._calculation["type"]

    @property
    def input_ids(self) -> list[str]:
        """Get the list of input item IDs for this calculation."""
        return self._calculation["inputs"]

    @property
    def parameters(self) -> dict[str, Any]:
        """Get optional parameters for the calculation."""
        return self._calculation.get("parameters", {})

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (CALCULATED)."""
        return StatementItemType.CALCULATED


class SubtotalLineItem(CalculatedLineItem):
    """Represents a subtotal line item summing multiple other items.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      item_ids: List of IDs to sum
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata

    Raises:
      StatementError: If item_ids is empty or not a list
    """

    def __init__(
        self,
        id: str,
        name: str,
        item_ids: list[str],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a SubtotalLineItem summing multiple items.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            item_ids: List of IDs to sum.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.

        Raises:
            StatementError: If item_ids is empty or not a list.
        """
        if not isinstance(item_ids, list) or not item_ids:
            raise StatementError(message=f"Invalid or empty item IDs for subtotal: {id}")
        calculation = {"type": "addition", "inputs": item_ids, "parameters": {}}
        super().__init__(
            id=id,
            name=name,
            calculation=calculation,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
        )
        self._item_ids = item_ids

    @property
    def item_ids(self) -> list[str]:
        """Get the IDs of items summed by this subtotal."""
        return self._item_ids

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (SUBTOTAL)."""
        return StatementItemType.SUBTOTAL

# --- END FILE: fin_statement_model/statements/structure/items.py ---


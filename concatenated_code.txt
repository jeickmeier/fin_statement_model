# --- START FILE: fin_statement_model/__init__.py ---
"""finlib - A Python library for financial statement analysis and forecasting."""

__all__ = [
    "CalculationNode",
    "CurveGrowthForecastNode",
    "CustomGrowthForecastNode",
    "FinancialStatementGraph",
    "FinancialStatementItemNode",
    "FixedGrowthForecastNode",
    "ForecastNode",
    "Graph",
    "LLMClient",
    "LLMConfig",
    "MultiPeriodStatNode",
    "Node",
    "StatisticalGrowthForecastNode",
    "YoYGrowthNode",
]

from .extensions.llm.llm_client import LLMClient, LLMConfig
from .core.graph import Graph
from .core.nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    YoYGrowthNode,
    MultiPeriodStatNode,
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    CustomGrowthForecastNode,
)

# ensure our library-wide logging policy is applied immediately
from . import logging_config  # noqa: F401

# --------------------------------------------------------------------------
# Optional Extensions (via entry points / importlib.metadata)
# --------------------------------------------------------------------------
# Extensions are optional modules that add functionality without modifying
# the core library. They might depend on heavy libraries (e.g., LLMs,
# ML frameworks) and should be lazy-loaded.
# Example entry point group: 'fin_statement_model.extensions.reporting'
# Expected interface: TBD (e.g., a class with specific methods)
# Note: Avoid hard imports from extensions into core/statements/io.
# Goal: Keep core library lean, allow users to install extras like:
# pip install fin-statement-model[openai]
# pip install fin-statement-model[reporting-tools]
# --------------------------------------------------------------------------


# Core API Exports (ensure essential classes/functions are accessible)
# Example:
# from .core.graph import Graph
# from .core.nodes import Node, FinancialStatementItemNode
# from .core.calculation_engine import CalculationEngine
# from .statements.manager import StatementManager

# Placeholder: Explicitly list key public API components later.
# For now, just rely on sub-package __init__ files if they exist.

__version__ = "0.1.0"  # Central version definition

# --- END FILE: fin_statement_model/__init__.py ---

# --- START FILE: fin_statement_model/core/__init__.py ---
"""Core components for the Financial Statement Model.

This package forms the foundation of the library, providing the core infrastructure
for building, calculating, and managing financial models. It includes:

- Graph engine (`core.graph`): For representing financial relationships.
- Base node hierarchy (`core.nodes`): Abstract and concrete node types.
- Calculation engine (`calculation_engine.py`): For evaluating the graph.
- Metric registry and definitions (`core.metrics`): For managing financial metrics.
- Data management (`data_manager.py`): For handling financial data.
- Calculation strategies (`core.strategies`): Reusable calculation logic.
- Core utilities and exceptions (`errors.py`, `node_factory.py`).

This `core` package is designed to be self-contained and does not depend on
other higher-level packages like `statements`, `io`, or `forecasting`.
"""

from .node_factory import NodeFactory
from .graph import Graph
from .nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    YoYGrowthNode,
    MultiPeriodStatNode,
    FormulaCalculationNode,
    CustomCalculationNode,
    TwoPeriodAverageNode,
)
from .calculations import (
    AdditionCalculation,
    SubtractionCalculation,
    MultiplicationCalculation,
    DivisionCalculation,
)
from .errors import (
    FinancialModelError,
    ConfigurationError,
    CalculationError,
    NodeError,
    GraphError,
    DataValidationError,
    CircularDependencyError,
    PeriodError,
    StatementError,
    StrategyError,
    TransformationError,
)

__all__ = [
    "AdditionCalculation",
    "CalculationError",
    "CalculationNode",
    "CircularDependencyError",
    "ConfigurationError",
    "CustomCalculationNode",
    "DataValidationError",
    "DivisionCalculation",
    "FinancialModelError",
    "FinancialStatementItemNode",
    "FormulaCalculationNode",
    "Graph",
    "GraphError",
    "MultiPeriodStatNode",
    "MultiplicationCalculation",
    "Node",
    "NodeError",
    "NodeFactory",
    "PeriodError",
    "StatementError",
    "StrategyError",
    "SubtractionCalculation",
    "TransformationError",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]

# --- END FILE: fin_statement_model/core/__init__.py ---

# --- START FILE: fin_statement_model/core/adjustments/__init__.py ---
"""Core adjustment models and filters."""

# from .manager import AdjustmentManager # Remove unused import

from .models import (
    Adjustment,
    AdjustmentFilter,
    AdjustmentFilterInput,
    AdjustmentType,
    DEFAULT_SCENARIO,
)

__all__ = [
    "DEFAULT_SCENARIO",
    "Adjustment",
    "AdjustmentFilter",
    "AdjustmentFilterInput",
    "AdjustmentType",
]

# --- END FILE: fin_statement_model/core/adjustments/__init__.py ---

# --- START FILE: fin_statement_model/core/adjustments/analytics.py ---
"""Analytics functions for summarizing and analyzing adjustments."""

import logging
from typing import Optional, Union
from collections.abc import Callable

import pandas as pd

from fin_statement_model.core.adjustments.manager import AdjustmentManager
from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentFilter,
    AdjustmentTag,
)
from fin_statement_model.core.adjustments.helpers import tag_matches

logger = logging.getLogger(__name__)


def _filter_adjustments_static(
    all_adjustments: list[Adjustment],
    filter_input: Optional[
        Union[AdjustmentFilter, set[AdjustmentTag], Callable[[Adjustment], bool]]
    ],
) -> list[Adjustment]:
    """Apply filtering to adjustments based on filter_input, excluding period context.

    This helper function centralizes the common filtering logic used by both
    summary() and list_by_tag() methods. It applies filters based on the static
    properties of adjustments (tags, scenario, type, etc.) but ignores any
    period-based filtering since that requires runtime context.

    Args:
        all_adjustments: List of all adjustments to filter.
        filter_input: Filter criteria (AdjustmentFilter, set of tags, callable, or None).

    Returns:
        Filtered list of adjustments matching the filter criteria.
    """
    if filter_input is None:
        # No filter means include all adjustments
        logger.debug("No filter applied.")
        return all_adjustments

    elif isinstance(filter_input, AdjustmentFilter):
        # Apply filter, ignoring its period attribute
        temp_filter = filter_input.model_copy(update={"period": None})
        filtered = [adj for adj in all_adjustments if temp_filter.matches(adj)]
        logger.debug(f"Applied AdjustmentFilter (ignoring period). Filter: {temp_filter}")
        return filtered

    elif isinstance(filter_input, set):
        # Shorthand for include_tags
        filtered = [adj for adj in all_adjustments if tag_matches(adj.tags, filter_input)]
        logger.debug(f"Applied tag filter. Tags: {filter_input}")
        return filtered

    elif callable(filter_input):
        filtered = [adj for adj in all_adjustments if filter_input(adj)]
        logger.debug("Applied callable filter.")
        return filtered

    else:
        # Should not happen due to type hint, but defensive
        logger.warning(f"Invalid filter_input type: {type(filter_input)}. No filtering applied.")
        return all_adjustments


def summary(
    manager: AdjustmentManager,
    filter_input: Optional[
        Union[AdjustmentFilter, set[AdjustmentTag], Callable[[Adjustment], bool]]
    ] = None,
    group_by: list[str] = ["period", "node_name"],
) -> pd.DataFrame:
    """Generate a summary DataFrame of adjustments, optionally filtered and grouped.

    Calculates count, sum of values, and mean of absolute values for adjustments.

    Args:
        manager: The AdjustmentManager instance containing the adjustments.
        filter_input: Optional filter criteria (AdjustmentFilter, set of tags, callable, or None)
                      to apply before summarizing.
        group_by: List of Adjustment attributes to group the summary by.
                  Defaults to ["period", "node_name"]. Valid fields include
                  'period', 'node_name', 'scenario', 'type', 'user'.

    Returns:
        A pandas DataFrame with the summary statistics (count, sum, mean_abs_value)
        indexed by the specified group_by columns.
    """
    logger.debug(f"Generating adjustment summary, grouping by: {group_by}")

    # Get all adjustments first
    # TODO: Optimization - If filtering is very restrictive, could filter first.
    # However, filtering requires period context which isn't directly available here.
    # Get all adjustments and filter based on the filter_input's static properties.
    # The period-based filtering (effective window) cannot be applied generically here.
    all_adjustments = manager.get_all_adjustments()

    # Apply filtering using the helper function
    filtered_adjustments = _filter_adjustments_static(all_adjustments, filter_input)

    if not filtered_adjustments:
        logger.info("No adjustments found matching the filter criteria for summary.")
        # Return empty DataFrame with expected columns
        cols = [*group_by, "count", "sum_value", "mean_abs_value"]
        return pd.DataFrame(columns=cols).set_index(group_by)

    # Convert to DataFrame for easier aggregation
    adj_data = [
        adj.model_dump(include=set([*group_by, "value"]))  # Include value for aggregation
        for adj in filtered_adjustments
    ]
    df = pd.DataFrame(adj_data)

    # Ensure correct types for grouping columns if needed (e.g., type as string)
    if "type" in group_by:
        df["type"] = df["type"].astype(str)

    # Add absolute value for mean calculation
    df["abs_value"] = df["value"].abs()

    # Perform aggregation
    summary_df = df.groupby(group_by).agg(
        count=("value", "size"),
        sum_value=("value", "sum"),
        mean_abs_value=("abs_value", "mean"),
    )

    logger.info(f"Generated adjustment summary with {len(summary_df)} groups.")
    return summary_df


def list_by_tag(
    manager: AdjustmentManager,
    tag_prefix: str,
    filter_input: Optional[
        Union[AdjustmentFilter, set[AdjustmentTag], Callable[[Adjustment], bool]]
    ] = None,
) -> list[Adjustment]:
    """List all adjustments matching a tag prefix, optionally applying further filters.

    Args:
        manager: The AdjustmentManager instance containing the adjustments.
        tag_prefix: The tag prefix string to match (e.g., "NonRecurring").
        filter_input: Optional additional filter criteria (AdjustmentFilter, set of tags,
                      callable, or None).

    Returns:
        A list of Adjustment objects that have at least one tag starting with
        the tag_prefix and also match the optional filter_input.
    """
    logger.debug(f"Listing adjustments by tag prefix: '{tag_prefix}'")

    # Get all adjustments and apply filters using the helper function
    all_adjustments = manager.get_all_adjustments()
    filtered_adjustments = _filter_adjustments_static(all_adjustments, filter_input)

    # Apply the primary tag prefix filter
    prefix_set = {tag_prefix}
    final_list = [adj for adj in filtered_adjustments if tag_matches(adj.tags, prefix_set)]

    logger.info(
        f"Found {len(final_list)} adjustments matching prefix '{tag_prefix}' and other filters."
    )
    # Sort by priority/timestamp for consistent output
    return sorted(final_list, key=lambda x: (x.priority, x.timestamp))

# --- END FILE: fin_statement_model/core/adjustments/analytics.py ---

# --- START FILE: fin_statement_model/core/adjustments/helpers.py ---
"""Helper functions for the adjustments module."""


def tag_matches(target_tags: set[str], prefixes: set[str]) -> bool:
    """Check if any target tag starts with any of the given prefixes.

    Allows for hierarchical matching: a prefix "A/B" matches tag "A/B/C".
    A simple prefix "A" matches tag "A/B".

    Args:
        target_tags: The set of tags on an adjustment.
        prefixes: The set of prefixes to check against (e.g., from a filter).

    Returns:
        True if at least one tag in target_tags starts with at least one
        prefix in prefixes, False otherwise.
    """
    if not prefixes:  # Optimization: if no prefixes specified, it can't match
        return False
    if not target_tags:  # Optimization: if no tags exist, it can't match
        return False

    # Check if any combination of tag and prefix matches
    return any(t.startswith(p) for t in target_tags for p in prefixes)

# --- END FILE: fin_statement_model/core/adjustments/helpers.py ---

# --- START FILE: fin_statement_model/core/adjustments/manager.py ---
"""Manages the storage, retrieval, and application of adjustments."""

from __future__ import annotations

from collections import defaultdict
from typing import Optional
from uuid import UUID

from .models import (
    Adjustment,
    AdjustmentFilter,
    AdjustmentFilterInput,
    AdjustmentType,
    DEFAULT_SCENARIO,
)


class AdjustmentManager:
    """Handles the lifecycle and application of Adjustment objects.

    Provides methods for adding, removing, filtering, and applying adjustments
    to base values.
    """

    def __init__(self) -> None:
        """Initializes the AdjustmentManager with empty storage."""
        # Primary index: (scenario, node_name, period) -> list[Adjustment]
        self._by_location: dict[tuple[str, str, str], list[Adjustment]] = defaultdict(list)
        # Secondary index for quick lookup and removal by ID
        self._by_id: dict[UUID, Adjustment] = {}

    def add_adjustment(self, adj: Adjustment) -> None:
        """Adds an adjustment to the manager, replacing if ID exists."""
        # If an adjustment with the same ID already exists, remove it first
        if adj.id in self._by_id:
            self.remove_adjustment(adj.id)

        self._by_id[adj.id] = adj
        key = (adj.scenario, adj.node_name, adj.period)
        self._by_location[key].append(adj)
        # Keep the list sorted by priority, then timestamp for consistent application order
        self._by_location[key].sort(key=lambda x: (x.priority, x.timestamp))

    def remove_adjustment(self, adj_id: UUID) -> bool:
        """Removes an adjustment by its ID. Returns True if found, False otherwise."""
        if adj_id not in self._by_id:
            return False

        adj_to_remove = self._by_id.pop(adj_id)
        key = (adj_to_remove.scenario, adj_to_remove.node_name, adj_to_remove.period)

        if key in self._by_location:
            # Filter out the specific adjustment object instance
            self._by_location[key] = [a for a in self._by_location[key] if a.id != adj_id]
            # If the list becomes empty, remove the key
            if not self._by_location[key]:
                del self._by_location[key]
        return True

    def _apply_one(self, base_value: float, adj: Adjustment) -> float:
        """Applies a single adjustment to a value based on its type and scale."""
        if adj.type == AdjustmentType.ADDITIVE:
            return base_value + adj.value * adj.scale
        elif adj.type == AdjustmentType.MULTIPLICATIVE:
            # Ensure base_value is not zero to avoid issues with 0**(negative scale)
            # If base is 0, multiplicative adjustment usually results in 0 unless value is 0.
            # We also need to handle potential complex numbers if base is negative and scale is fractional.
            # For simplicity, let's assume standard financial contexts where this is less common
            # or handle it by convention (e.g., multiplicative doesn't apply to zero/negative base).
            # Let's default to returning 0 if base is 0 for multiplicative.
            if base_value == 0:
                return 0.0
            # Consider adding checks or specific handling for negative base + fractional scale if needed.
            return base_value * (adj.value**adj.scale)
        elif adj.type == AdjustmentType.REPLACEMENT:
            # Scale is ignored for replacement type as per spec
            return adj.value
        else:
            # Should not happen with Enum, but defensively return base value
            return base_value  # pragma: no cover

    def apply_adjustments(
        self, base_value: float, adjustments: list[Adjustment]
    ) -> tuple[float, bool]:
        """Applies a list of adjustments sequentially to a base value.

        Adjustments are applied in order of priority (lower first), then timestamp.

        Args:
            base_value: The starting value before adjustments.
            adjustments: A list of Adjustment objects to apply.

        Returns:
            A tuple containing: (final adjusted value, boolean indicating if any adjustment was applied).
        """
        if not adjustments:
            return base_value, False

        current_value = base_value
        applied_flag = False

        # Sort by priority (ascending), then timestamp (ascending) as per spec
        # Note: add_adjustment already sorts the list in _by_location, but
        # this ensures correctness if an unsorted list is passed directly.
        sorted_adjustments = sorted(adjustments, key=lambda x: (x.priority, x.timestamp))

        for adj in sorted_adjustments:
            current_value = self._apply_one(current_value, adj)
            applied_flag = True

        return current_value, applied_flag

    def get_adjustments(
        self, node_name: str, period: str, *, scenario: str = DEFAULT_SCENARIO
    ) -> list[Adjustment]:
        """Retrieves all adjustments for a specific node, period, and scenario."""
        key = (scenario, node_name, period)
        # Return a copy to prevent external modification of the internal list
        return list(self._by_location.get(key, []))

    def _normalize_filter(
        self, filter_input: AdjustmentFilterInput, period: Optional[str] = None
    ) -> AdjustmentFilter:
        """Converts flexible filter input into a standard AdjustmentFilter instance."""
        if filter_input is None:
            # Default filter includes only the default scenario and sets the period context
            return AdjustmentFilter(include_scenarios={DEFAULT_SCENARIO}, period=period)
        elif isinstance(filter_input, AdjustmentFilter):
            # If period context wasn't set on the filter, set it now
            if filter_input.period is None:
                return filter_input.model_copy(update={"period": period})
            return filter_input
        elif isinstance(filter_input, set):
            # Shorthand for include_tags filter
            # Assume shorthand applies only to DEFAULT_SCENARIO unless specified otherwise?
            # Let's keep it simple: Shorthand applies to DEFAULT_SCENARIO only.
            return AdjustmentFilter(
                include_tags=filter_input,
                include_scenarios={DEFAULT_SCENARIO},
                period=period,
            )
        elif callable(filter_input):
            # This case is complex as the callable doesn't inherently know the period.
            # We wrap the callable in a filter, but the effective window check might not work
            # as expected unless the callable itself uses the period.
            # For simplicity, we create a default filter and rely on the callable for matching.
            # The manager will still filter by callable *after* potentially getting adjustments.
            # A more robust solution might involve passing period to the callable.
            # Let's just return a base filter for now and handle callable later.
            # TODO: Revisit handling of callable filters if period context is critical.
            print(
                "Warning: Callable filter used; period context for effective window check might be ignored."
            )
            # Apply callable, but filter to default scenario like other shorthand.
            return AdjustmentFilter(
                include_scenarios={DEFAULT_SCENARIO}, period=period
            )  # Base filter, callable applied later
        else:
            raise TypeError(f"Invalid filter_input type: {type(filter_input)}")

    def get_filtered_adjustments(
        self, node_name: str, period: str, filter_input: AdjustmentFilterInput = None
    ) -> list[Adjustment]:
        """Retrieves adjustments for a node/period that match the given filter criteria.

        Args:
            node_name: The target node name.
            period: The target period.
            filter_input: The filter criteria (AdjustmentFilter, set of tags, callable, or None).

        Returns:
            A list of matching Adjustment objects, sorted by priority and timestamp.
        """
        normalized_filter = self._normalize_filter(filter_input, period)

        candidate_adjustments: list[Adjustment] = []

        # Determine which scenarios to check based on the filter
        scenarios_to_check: set[str]
        if normalized_filter.include_scenarios is not None:
            scenarios_to_check = normalized_filter.include_scenarios.copy()  # Work on a copy
            if normalized_filter.exclude_scenarios is not None:
                scenarios_to_check -= normalized_filter.exclude_scenarios
        elif normalized_filter.exclude_scenarios is not None:
            # Get all scenarios currently known to the manager
            all_known_scenarios = {adj.scenario for adj in self._by_id.values()}
            scenarios_to_check = all_known_scenarios - normalized_filter.exclude_scenarios
        else:
            # No include/exclude specified: check all scenarios relevant for this node/period
            # This requires checking keys in _by_location
            scenarios_to_check = {
                key[0] for key in self._by_location if key[1] == node_name and key[2] == period
            }
            # If no specific adjustments exist for this node/period, we might check default?
            # Let's assume we only check scenarios that *have* adjustments for this location.
            if not scenarios_to_check:
                # Maybe return empty list early if no scenarios found for location?
                # Or should it behave differently? For now, proceed with empty set.
                pass

        # Gather candidates from relevant locations
        for scenario in scenarios_to_check:
            key = (scenario, node_name, period)
            candidate_adjustments.extend(self._by_location.get(key, []))

        # Apply the filter logic
        matching_adjustments: list[Adjustment] = []
        if callable(filter_input):
            # Apply the callable filter directly
            matching_adjustments = [adj for adj in candidate_adjustments if filter_input(adj)]
        else:
            # Apply the normalized AdjustmentFilter's matches method
            matching_adjustments = [
                adj for adj in candidate_adjustments if normalized_filter.matches(adj)
            ]

        # Return sorted list (sorting might be redundant if fetched lists are pre-sorted
        # and filtering maintains order, but ensures correctness)
        return sorted(matching_adjustments, key=lambda x: (x.priority, x.timestamp))

    def get_all_adjustments(self) -> list[Adjustment]:
        """Returns a list of all adjustments currently stored in the manager."""
        # Return a copy to prevent external modification
        return list(self._by_id.values())

    def clear_all(self) -> None:
        """Removes all adjustments from the manager."""
        self._by_location.clear()
        self._by_id.clear()

    def load_adjustments(self, adjustments: list[Adjustment]) -> None:
        """Clears existing adjustments and loads a new list."""
        self.clear_all()
        for adj in adjustments:
            self.add_adjustment(adj)

# --- END FILE: fin_statement_model/core/adjustments/manager.py ---

# --- START FILE: fin_statement_model/core/adjustments/models.py ---
"""Adjustment data models and related types."""

from __future__ import annotations

import uuid
from datetime import datetime
from enum import Enum
from typing import Final, Optional
from collections.abc import Callable

from pydantic import BaseModel, ConfigDict, Field, field_validator
import logging

from .helpers import tag_matches

logger = logging.getLogger(__name__)

# --------------------------------------------------------------------
# Core Types and Constants
# --------------------------------------------------------------------


class AdjustmentType(Enum):
    """Defines how an adjustment modifies a base value."""

    ADDITIVE = "additive"  # base + (value * scale)
    MULTIPLICATIVE = "multiplicative"  # base * (value ** scale)
    REPLACEMENT = "replacement"  # use value (scale ignored)


AdjustmentTag = str  # Slash (/) separates hierarchy levels in tags
DEFAULT_SCENARIO: Final[str] = "default"

# --------------------------------------------------------------------
# Adjustment Model
# --------------------------------------------------------------------


class Adjustment(BaseModel):
    """Immutable record describing a discretionary adjustment to a node's value.

    Attributes:
        id: Unique identifier for the adjustment.
        node_name: The name of the target node.
        period: The primary period the adjustment applies to.
        start_period: The first period the adjustment is effective (inclusive, Phase 2).
        end_period: The last period the adjustment is effective (inclusive, Phase 2).
        value: The numeric value of the adjustment.
        type: How the adjustment combines with the base value.
        scale: Attenuation factor for the adjustment (0.0 to 1.0, Phase 2).
        priority: Tie-breaker for applying multiple adjustments (lower number applied first).
        tags: Set of descriptive tags for filtering and analysis.
        scenario: The named scenario this adjustment belongs to (Phase 2).
        reason: Text description of why the adjustment was made.
        user: Identifier for the user who created the adjustment.
        timestamp: UTC timestamp when the adjustment was created.
    """

    # Target
    id: uuid.UUID = Field(default_factory=uuid.uuid4)
    node_name: str
    period: str  # Primary target period
    start_period: Optional[str] = None  # Phase 2 - effective range start (inclusive)
    end_period: Optional[str] = None  # Phase 2 - effective range end (inclusive)

    # Behaviour
    value: float
    type: AdjustmentType = AdjustmentType.ADDITIVE
    scale: float = 1.0  # Phase 2 - 0.0 <= scale <= 1.0
    priority: int = 0  # Lower value means higher priority (applied first)

    # Classification
    tags: set[AdjustmentTag] = Field(default_factory=set)
    scenario: str = DEFAULT_SCENARIO  # Phase 2 - Scenario grouping

    # Metadata
    reason: str
    user: Optional[str] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)

    model_config = ConfigDict(frozen=True)

    @field_validator("scale")
    @classmethod
    def _scale_bounds(cls, v: float) -> float:
        """Validate that the scale factor is between 0.0 and 1.0."""
        if not 0.0 <= v <= 1.0:
            raise ValueError("Scale must be between 0.0 and 1.0 (inclusive)")
        return v


# --------------------------------------------------------------------
# Adjustment Filtering
# --------------------------------------------------------------------


class AdjustmentFilter(BaseModel):
    """Defines criteria for selecting adjustments.

    Attributes:
        include_scenarios: Only include adjustments from these scenarios.
        exclude_scenarios: Exclude adjustments from these scenarios.
        include_tags: Include adjustments matching any of these tag prefixes.
        exclude_tags: Exclude adjustments matching any of these tag prefixes.
        require_all_tags: Include only adjustments having *all* these exact tags.
        include_types: Only include adjustments of these types.
        exclude_types: Exclude adjustments of these types.
        period: The specific period context for effective window checks.
    """

    # Scenario Filtering
    include_scenarios: Optional[set[str]] = None
    exclude_scenarios: Optional[set[str]] = None

    # Tag Filtering (supports hierarchical matching via helpers.tag_matches)
    # Need to import the helper function first.
    # Let's assume it will be imported at the top level of the module later.

    include_tags: Optional[set[AdjustmentTag]] = None
    exclude_tags: Optional[set[AdjustmentTag]] = None
    require_all_tags: Optional[set[AdjustmentTag]] = None  # Exact match required

    # Type Filtering
    include_types: Optional[set[AdjustmentType]] = None
    exclude_types: Optional[set[AdjustmentType]] = None

    # Context for Effective Window Checks (Phase 2)
    period: Optional[str] = None  # The current period being calculated/viewed

    def matches(self, adj: Adjustment) -> bool:
        """Check if a given adjustment meets the filter criteria."""
        # Need to import the helper function here to avoid circular dependency issues at module level

        # Start assuming it matches, then progressively set to False if any check fails.
        is_match = True

        # --- Scenario Checks ---
        if (self.include_scenarios is not None and adj.scenario not in self.include_scenarios) or (
            self.exclude_scenarios is not None and adj.scenario in self.exclude_scenarios
        ):
            is_match = False

        # --- Tag Checks ---
        # Only check if still potentially a match
        if is_match and (
            (self.include_tags is not None and not tag_matches(adj.tags, self.include_tags))
            or (self.exclude_tags is not None and tag_matches(adj.tags, self.exclude_tags))
            or (self.require_all_tags is not None and not self.require_all_tags.issubset(adj.tags))
        ):
            is_match = False

        # --- Type Checks ---
        # Only check if still potentially a match
        if is_match and (
            (self.include_types is not None and adj.type not in self.include_types)
            or (self.exclude_types is not None and adj.type in self.exclude_types)
        ):
            is_match = False

        # --- Effective Window Check (Phase 2) ---
        # Assumes periods are sortable strings (e.g., 'YYYY-MM' or 'Q1-2023')
        if is_match and self.period is not None:  # Only check if still potentially a match
            logger.debug(
                f"Period check: FilterPeriod={self.period}, AdjStart={adj.start_period}, AdjEnd={adj.end_period}"
            )
            period_match = True  # Assume period is ok unless proven otherwise
            if adj.start_period is not None and self.period < adj.start_period:
                logger.debug("Period check failed: FilterPeriod < AdjStart")
                period_match = False
            # Use 'if period_match' to avoid unnecessary log if start check already failed
            if period_match and adj.end_period is not None and self.period > adj.end_period:
                logger.debug("Period check failed: FilterPeriod > AdjEnd")
                period_match = False

            if period_match:
                logger.debug("Period check passed.")
            else:
                is_match = False  # Period check failed
        # else: # Optional log if period check was skipped
        #     if self.period is not None:
        #         logger.debug("Period check skipped because is_match was already False")
        #     else:
        #         logger.debug("Period check skipped: Filter has no period context.")

        return is_match


# Type alias for flexible filter input
AdjustmentFilterInput = Optional[
    AdjustmentFilter | set[AdjustmentTag] | Callable[[Adjustment], bool]
]

# --- END FILE: fin_statement_model/core/adjustments/models.py ---

# --- START FILE: fin_statement_model/core/calculations/__init__.py ---
"""Calculations module for the Financial Statement Model.

This module provides classes for implementing the Calculation Pattern for calculations
in the Financial Statement Model. It allows different calculation algorithms to be
defined, registered, and applied to financial data.
"""

from .calculation import (
    Calculation,
    AdditionCalculation,
    SubtractionCalculation,
    MultiplicationCalculation,
    DivisionCalculation,
    WeightedAverageCalculation,
    CustomFormulaCalculation,
    FormulaCalculation,
)
from .registry import Registry

# Register calculations
Registry.register(AdditionCalculation)
Registry.register(SubtractionCalculation)
Registry.register(MultiplicationCalculation)
Registry.register(DivisionCalculation)
Registry.register(WeightedAverageCalculation)
Registry.register(CustomFormulaCalculation)
Registry.register(FormulaCalculation)

__all__ = [
    "AdditionCalculation",
    "Calculation",
    "CustomFormulaCalculation",
    "DivisionCalculation",
    "FormulaCalculation",
    "MultiplicationCalculation",
    "Registry",
    "SubtractionCalculation",
    "WeightedAverageCalculation",
]

# --- END FILE: fin_statement_model/core/calculations/__init__.py ---

# --- START FILE: fin_statement_model/core/calculations/calculation.py ---
"""Calculation for the Financial Statement Model.

This module provides the Calculation Pattern implementation for calculations,
allowing different calculation types to be encapsulated in calculation classes.
"""

from abc import ABC, abstractmethod
import ast
import logging
import operator
from typing import Optional, ClassVar
from collections.abc import Callable

from fin_statement_model.core.nodes.base import Node  # Absolute

# Configure logging
logger = logging.getLogger(__name__)


class Calculation(ABC):
    """Abstract base class for all calculations.

    This class defines the interface that all concrete calculation classes must
    implement. It employs a calculation pattern, allowing the algorithm
    used by a CalculationNode to be selected at runtime.

    Each concrete calculation encapsulates a specific method for computing a
    financial value based on a list of input nodes and a given time period.
    """

    @abstractmethod
    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculate a value based on input nodes for a specific period.

        This abstract method must be implemented by all concrete calculation classes.
        It defines the core logic for the calculation.

        Args:
            inputs: A list of input Node objects whose values will be used in
                the calculation.
            period: The time period string (e.g., "2023Q1") for which the
                calculation should be performed.

        Returns:
            The calculated numerical value as a float.

        Raises:
            NotImplementedError: If the method is not implemented by a subclass.
            ValueError: If the inputs are invalid for the specific calculation
                (e.g., wrong number of inputs, incompatible types).
            ZeroDivisionError: If the calculation involves division and a divisor
                is zero.
            Exception: Other exceptions depending on the calculation logic.
        """
        # pragma: no cover

    @property
    def description(self) -> str:
        """Provides a human-readable description of the calculation.

        This is useful for documentation, debugging, and for user interfaces
        that need to explain how a value is derived.

        Returns:
            A string describing the calculation.
        """
        # Default implementation returns the class name. Subclasses should override
        # for more specific descriptions.
        class_name = self.__class__.__name__  # pragma: no cover
        return class_name


class AdditionCalculation(Calculation):
    """Implements an addition calculation, summing values from multiple input nodes.

    This calculation sums the values obtained from calling
    the `calculate` method on each of the provided input nodes for a given period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Sums the calculated values from all input nodes for the specified period.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023Q4") for the calculation.

        Returns:
            The total sum of the values calculated from the input nodes. Returns
            0.0 if the input list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = AdditionCalculation()
            >>> nodes = [MockNode(10), MockNode(20), MockNode(5)]
            >>> strategy.calculate(nodes, "2023")
            35.0
            >>> strategy.calculate([], "2023")
            0.0
        """
        logger.debug(f"Applying addition calculation for period {period}")
        # Using a generator expression for potentially better memory efficiency
        return sum(input_node.calculate(period) for input_node in inputs)

    @property
    def description(self) -> str:
        """Returns a description of the addition calculation."""
        return "Addition (sum of all inputs)"


class SubtractionCalculation(Calculation):
    """Implements a subtraction calculation: first input minus the sum of the rest.

    This calculation takes the calculated value of the first node in the input list
    and subtracts the sum of the calculated values of all subsequent nodes for
    a specific period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the difference: value of the first input minus the sum of others.

        Args:
            inputs: A list of Node objects. Must contain at least one node.
            period: The time period string (e.g., "2024Q1") for the calculation.

        Returns:
            The result of the subtraction. If only one input node is provided,
            its value is returned.

        Raises:
            ValueError: If the `inputs` list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = SubtractionCalculation()
            >>> nodes = [MockNode(100), MockNode(20), MockNode(30)]
            >>> strategy.calculate(nodes, "2023")
            50.0
            >>> nodes_single = [MockNode(100)]
            >>> strategy.calculate(nodes_single, "2023")
            100.0
        """
        if not inputs:
            raise ValueError("Subtraction calculation requires at least one input node")

        logger.debug(f"Applying subtraction calculation for period {period}")
        # Calculate values first to avoid multiple calls if nodes are complex
        values = [node.calculate(period) for node in inputs]
        return values[0] - sum(values[1:])

    @property
    def description(self) -> str:
        """Returns a description of the subtraction calculation."""
        return "Subtraction (first input minus sum of subsequent inputs)"


class MultiplicationCalculation(Calculation):
    """Implements a multiplication calculation, calculating the product of input values.

    This calculation multiplies the calculated values of all provided input nodes
    for a given period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the product of the values from all input nodes.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023FY") for the calculation.

        Returns:
            The product of all input values. Returns 1.0 (multiplicative identity)
            if the input list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = MultiplicationCalculation()
            >>> nodes = [MockNode(2), MockNode(3), MockNode(4)]
            >>> strategy.calculate(nodes, "2023")
            24.0
            >>> strategy.calculate([], "2023")
            1.0
        """
        # Multiplication calculation should ideally return 1.0 for empty inputs.
        # Raising error if empty seems less conventional for multiplication.
        if not inputs:
            logger.warning("Multiplication calculation called with empty inputs, returning 1.0")
            return 1.0

        logger.debug(f"Applying multiplication calculation for period {period}")
        result = 1.0
        for input_node in inputs:
            result *= input_node.calculate(period)
        return result

    @property
    def description(self) -> str:
        """Returns a description of the multiplication calculation."""
        return "Multiplication (product of all inputs)"


class DivisionCalculation(Calculation):
    """Implements a division calculation: first input divided by the product of the rest.

    This calculation takes the calculated value of the first node (numerator) and
    divides it by the product of the calculated values of all subsequent nodes
    (denominator) for a specific period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the division: first input / (product of subsequent inputs).

        Args:
            inputs: A list of Node objects. Must contain at least two nodes.
            period: The time period string (e.g., "2024Q2") for the calculation.

        Returns:
            The result of the division.

        Raises:
            ValueError: If `inputs` list contains fewer than two nodes.
            ZeroDivisionError: If the calculated product of the subsequent nodes
                (denominator) is zero.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = DivisionCalculation()
            >>> nodes = [MockNode(100), MockNode(5), MockNode(2)]
            >>> strategy.calculate(nodes, "2023")
            10.0
            >>> nodes_zero_denom = [MockNode(100), MockNode(5), MockNode(0)]
            >>> try:
            ...     strategy.calculate(nodes_zero_denom, "2023")
            ... except ZeroDivisionError as e:
            ...     # Example: logging the error instead of printing
            ...     logger.error(e)
            Division by zero: Denominator product is zero
        """
        if len(inputs) < 2:
            raise ValueError("Division calculation requires at least two input nodes")

        logger.debug(f"Applying division calculation for period {period}")

        values = [node.calculate(period) for node in inputs]
        numerator = values[0]

        denominator = 1.0
        for val in values[1:]:
            denominator *= val

        if denominator == 0.0:
            raise ZeroDivisionError("Division by zero: Denominator product is zero")

        return numerator / denominator

    @property
    def description(self) -> str:
        """Returns a description of the division calculation."""
        return "Division (first input / product of subsequent inputs)"


class WeightedAverageCalculation(Calculation):
    """Calculates the weighted average of input node values.

    This calculation computes the average of the values from input nodes, where each
    node's contribution is weighted. If no weights are provided during
    initialization, it defaults to an equal weighting (simple average).
    """

    def __init__(self, weights: Optional[list[float]] = None):
        """Initializes the WeightedAverageCalculation.

        Args:
            weights: An optional list of floats representing the weight for each
                corresponding input node. The length of this list must match the
                number of input nodes provided to the `calculate` method. If None,
                equal weights are assumed.
        """
        # Validate weights if provided immediately? No, validation happens in calculate
        # as the number of inputs isn't known here.
        self.weights = weights
        logger.info(f"Initialized WeightedAverageCalculation with weights: {weights}")

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Computes the weighted average of the input node values for the period.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023H1") for the calculation.

        Returns:
            The calculated weighted average as a float.

        Raises:
            ValueError: If the `inputs` list is empty.
            ValueError: If `weights` were provided during initialization and their
                count does not match the number of `inputs`.
            ValueError: If the sum of weights is zero (to prevent division by zero
                if normalization were implemented differently).

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> # Equal weights (simple average)
            >>> strategy_equal = WeightedAverageCalculation()
            >>> nodes = [MockNode(10), MockNode(20), MockNode(30)]
            >>> strategy_equal.calculate(nodes, "2023")
            20.0
            >>> # Custom weights
            >>> strategy_custom = WeightedAverageCalculation(weights=[0.5, 0.3, 0.2])
            >>> strategy_custom.calculate(nodes, "2023")
            17.0
            >>> # Mismatched weights
            >>> strategy_mismatch = WeightedAverageCalculation(weights=[0.5, 0.5])
            >>> try:
            ...     strategy_mismatch.calculate(nodes, "2023")
            ... except ValueError as e:
            ...     # Example: logging the error instead of printing
            ...     logger.error(e)
            Number of weights (2) must match number of inputs (3)
        """
        if not inputs:
            raise ValueError("Weighted average calculation requires at least one input node")

        num_inputs = len(inputs)
        effective_weights: list[float]

        if self.weights is None:
            # Use equal weights if none provided
            if num_inputs == 0:  # Should be caught by the check above, but defensive
                return 0.0
            equal_weight = 1.0 / num_inputs
            effective_weights = [equal_weight] * num_inputs
            logger.debug("Using equal weights for weighted average.")
        elif len(self.weights) == num_inputs:
            effective_weights = self.weights
            logger.debug(f"Using provided weights: {effective_weights}")
        else:
            raise ValueError(
                f"Number of weights ({len(self.weights)}) must match "
                f"number of inputs ({num_inputs})"
            )

        logger.debug(f"Applying weighted average calculation for period {period}")
        weighted_sum = 0.0
        total_weight = sum(effective_weights)
        input_values = [node.calculate(period) for node in inputs]

        if total_weight == 0.0:
            # Avoid division by zero. If weights are all zero, the concept is ill-defined.
            # Returning 0 might be a reasonable default, or raising an error.
            # Let's raise ValueError for clarity.
            raise ValueError("Total weight for weighted average cannot be zero.")

        for value, weight in zip(input_values, effective_weights):
            weighted_sum += value * weight

        # If weights don't sum to 1, this isn't a standard weighted average.
        # Decide whether to normalize or return the weighted sum directly.
        # Normalize by total weight for a true weighted average.
        return weighted_sum / total_weight

    @property
    def description(self) -> str:
        """Returns a description of the weighted average calculation."""
        if self.weights:
            return f"Weighted Average (using provided weights: {self.weights})"
        else:
            return "Weighted Average (using equal weights)"


# Type alias for the custom formula function
FormulaFunc = Callable[[dict[str, float]], float]


class CustomFormulaCalculation(Calculation):
    """Executes a user-defined Python function to calculate a value.

    This calculation provides maximum flexibility by allowing any custom Python
    function to be used for calculation. The function receives a dictionary
    mapping input node names (or fallback names) to their calculated values
    for the period and should return a single float result.
    """

    def __init__(self, formula_function: FormulaFunc):
        """Initializes the CustomFormulaCalculation with a calculation function.

        Args:
            formula_function: A callable (function, lambda, etc.) that accepts
                a single argument: a dictionary mapping string keys (input node
                names or `input_<i>`) to their float values for the period.
                It must return a float.

        Raises:
            TypeError: If `formula_function` is not callable.
        """
        if not callable(formula_function):
            raise TypeError("formula_function must be callable")
        self.formula_function = formula_function
        logger.info(
            f"Initialized CustomFormulaCalculation with function: {formula_function.__name__}"
        )

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Applies the custom formula function to the calculated input values.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2025M1") for the calculation.

        Returns:
            The float result returned by the `formula_function`.

        Raises:
            ValueError: If the `formula_function` encounters an error during execution
                (e.g., incorrect input keys, calculation errors). Wraps the original
                exception.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, name, value): self.name = name; self._value = value
            ...     def calculate(self, period): return self._value
            >>> def my_formula(data):
            ...     # Example: Gross Profit Margin
            ...     return (data['revenue'] - data['cogs']) / data['revenue'] * 100
            >>> strategy = CustomFormulaCalculation(my_formula)
            >>> nodes = [MockNode('revenue', 1000), MockNode('cogs', 600)]
            >>> strategy.calculate(nodes, "2023")
            40.0
            >>> # Example with unnamed nodes
            >>> def simple_sum(data):
            ...     return data['input_0'] + data['input_1']
            >>> strategy_unnamed = CustomFormulaCalculation(simple_sum)
            >>> nodes_unnamed = [MockNode(None, 10), MockNode(None, 20)] # No names
            >>> strategy_unnamed.calculate(nodes_unnamed, "2023")
            30.0
        """
        # Prepare input values dictionary, using names if available
        input_values: dict[str, float] = {}
        for i, node in enumerate(inputs):
            # Prefer node.name if it exists and is a non-empty string
            key = getattr(node, "name", None)
            if not isinstance(key, str) or not key:
                key = f"input_{i}"
            input_values[key] = node.calculate(period)

        logger.debug(
            f"Applying custom formula calculation for period {period} with inputs: {input_values}"
        )
        try:
            # Execute the user-provided function
            result = self.formula_function(input_values)
            if not isinstance(result, int | float):
                logger.warning(
                    f"Custom formula function {self.formula_function.__name__} "
                    f"returned non-numeric type: {type(result)}. Attempting cast."
                )
                # Attempt conversion, but be aware this might fail or be lossy
                try:
                    return float(result)
                except (ValueError, TypeError) as cast_err:
                    raise ValueError(
                        f"Custom formula {self.formula_function.__name__} result "
                        f"({result!r}) could not be cast to float."
                    ) from cast_err
            return float(result)  # Ensure result is float
        except Exception as e:
            # Catch any exception from the custom function and wrap it
            logger.error(
                f"Error executing custom formula '{self.formula_function.__name__}': {e}",
                exc_info=True,
            )
            raise ValueError(
                f"Error in custom formula '{self.formula_function.__name__}': {e}"
            ) from e

    @property
    def description(self) -> str:
        """Returns a description of the custom formula calculation."""
        func_name = getattr(self.formula_function, "__name__", "[anonymous function]")
        return f"Custom Formula (using function: {func_name})"


class FormulaCalculation(Calculation):
    """Evaluates a mathematical formula string as a calculation strategy.

    This calculation parses and evaluates simple mathematical expressions
    involving input nodes. Supports basic arithmetic operators (+, -, *, /)
    and unary negation.

    Attributes:
        formula: The mathematical expression string to evaluate.
        input_variable_names: List of variable names used in the formula,
            corresponding to the order of input nodes.
        _ast: The parsed Abstract Syntax Tree of the formula.
    """

    # Supported AST operators mapping to Python operator functions
    OPERATORS: ClassVar[dict[type, Callable[[float, float], float]]] = {
        ast.Add: operator.add,
        ast.Sub: operator.sub,
        ast.Mult: operator.mul,
        ast.Div: operator.truediv,
        ast.USub: operator.neg,  # type: ignore[dict-item]
    }

    def __init__(self, formula: str, input_variable_names: list[str]):
        """Initialize the FormulaCalculation.

        Args:
            formula: The mathematical formula string (e.g., "a + b / 2").
            input_variable_names: List of variable names used in the formula,
                in the same order as the input nodes that will be provided
                to the calculate method.

        Raises:
            ValueError: If the formula string has invalid syntax.
        """
        self.formula = formula
        self.input_variable_names = input_variable_names
        try:
            # Parse the formula string into an AST expression
            self._ast = ast.parse(formula, mode="eval").body
        except SyntaxError as e:
            raise ValueError(f"Invalid formula syntax: {formula}") from e
        logger.info(
            f"Initialized FormulaCalculation with formula: {formula} and variables: {input_variable_names}"
        )

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculate the value by evaluating the formula with input node values.

        Args:
            inputs: A list of Node objects, in the same order as input_variable_names.
            period: The time period string for the calculation.

        Returns:
            The result of the formula evaluation.

        Raises:
            ValueError: If the number of inputs doesn't match input_variable_names,
                or if an error occurs during evaluation.
        """
        if len(inputs) != len(self.input_variable_names):
            raise ValueError(
                f"Number of inputs ({len(inputs)}) must match number of variable names "
                f"({len(self.input_variable_names)})"
            )

        # Create mapping of variable names to nodes
        variable_map = dict(zip(self.input_variable_names, inputs))

        logger.debug(f"Applying formula calculation for period {period}")
        try:
            return self._evaluate(self._ast, period, variable_map)
        except (ValueError, TypeError, KeyError, ZeroDivisionError) as e:
            raise ValueError(f"Error evaluating formula: {self.formula}. Error: {e!s}") from e

    def _evaluate(self, node: ast.AST, period: str, variable_map: dict[str, Node]) -> float:
        """Recursively evaluate the parsed AST node for the formula.

        Args:
            node: The current AST node to evaluate.
            period: The time period context for the evaluation.
            variable_map: Mapping of variable names to Node objects.

        Returns:
            The result of evaluating the AST node.

        Raises:
            TypeError: If a non-numeric constant or input node value is encountered.
            ValueError: If an unknown variable or unsupported operator/syntax is found.
            ZeroDivisionError: If division by zero occurs.
        """
        # Numeric literal (Constant in Python 3.8+)
        if isinstance(node, ast.Constant):
            if isinstance(node.value, int | float):
                return float(node.value)
            else:
                raise TypeError(
                    f"Unsupported constant type '{type(node.value).__name__}' in formula"
                )

        # Variable reference
        elif isinstance(node, ast.Name):
            var_name = node.id
            if var_name not in variable_map:
                raise ValueError(
                    f"Unknown variable '{var_name}' in formula. Available: {list(variable_map.keys())}"
                )
            input_node = variable_map[var_name]
            # Recursively calculate the value of the input node
            value = input_node.calculate(period)
            if not isinstance(value, int | float):
                raise TypeError(
                    f"Input node '{input_node.name}' (variable '{var_name}') did not return a numeric value for period '{period}'"
                )
            return float(value)

        # Binary operation (e.g., a + b)
        elif isinstance(node, ast.BinOp):
            left_val = self._evaluate(node.left, period, variable_map)
            right_val = self._evaluate(node.right, period, variable_map)
            op_type = type(node.op)
            if op_type not in self.OPERATORS:
                raise ValueError(f"Unsupported binary operator '{op_type.__name__}' in formula")
            # Perform the operation
            return float(self.OPERATORS[op_type](left_val, right_val))

        # Unary operation (e.g., -a)
        elif isinstance(node, ast.UnaryOp):
            operand_val = self._evaluate(node.operand, period, variable_map)
            op_type = type(node.op)
            if op_type not in self.OPERATORS:
                raise ValueError(f"Unsupported unary operator '{op_type.__name__}' in formula")
            # Perform the operation
            return float(self.OPERATORS[op_type](operand_val))

        # If the node type is unsupported
        else:
            raise TypeError(
                f"Unsupported syntax node type '{type(node).__name__}' in formula: {ast.dump(node)}"
            )

    @property
    def description(self) -> str:
        """Returns a description of the formula calculation."""
        return f"Formula: {self.formula}"

# --- END FILE: fin_statement_model/core/calculations/calculation.py ---

# --- START FILE: fin_statement_model/core/calculations/registry.py ---
"""Registry for calculation classes in the Financial Statement Model.

This module provides a central registry for discovering and accessing different
calculation classes. Calculations can be registered using their class
object and later retrieved by their class name.
"""

# Use lowercase built-in types
from typing import ClassVar  # Keep Type for now
import logging

from .calculation import Calculation

# Configure logging
logger = logging.getLogger(__name__)


class Registry:
    """A central registry for managing and accessing calculation classes.

    This class uses class methods to provide a global registry. Calculations
    are stored in a dictionary mapping their class name (string) to the
    calculation class itself.

    Attributes:
        _strategies: A dictionary holding the registered calculation classes.
                     Keys are calculation class names (str), values are calculation
                     types (Type[Calculation]).
    """

    _strategies: ClassVar[dict[str, type[Calculation]]] = {}  # Use dict, type

    @classmethod
    def register(cls, calculation: type[Calculation]) -> None:
        """Register a calculation class with the registry.

        If a calculation with the same name is already registered, it will be
        overwritten.

        Args:
            calculation: The calculation class (Type[Calculation]) to register.
                         The class's __name__ attribute will be used as the key.
        """
        if not issubclass(calculation, Calculation):
            raise TypeError(f"Can only register subclasses of Calculation, not {calculation}")
        cls._strategies[calculation.__name__] = calculation
        logger.debug(f"Registered calculation: {calculation.__name__}")

    @classmethod
    def get(cls, name: str) -> type[Calculation]:
        """Retrieve a calculation class from the registry by its name.

        Args:
            name: The string name of the calculation class to retrieve.

        Returns:
            The calculation class (Type[Calculation]) associated with the given name.

        Raises:
            KeyError: If no calculation with the specified name is found in the
                      registry.
        """
        # Debug print including id of the dictionary
        if name not in cls._strategies:
            logger.error(f"Attempted to access unregistered calculation: {name}")
            raise KeyError(f"Calculation '{name}' not found in registry.")
        return cls._strategies[name]

    @classmethod
    def list(cls) -> dict[str, type[Calculation]]:  # Use dict, type
        """List all registered calculation classes.

        Returns:
            A dictionary containing all registered calculation names (str) and their
            corresponding calculation classes (Type[Calculation]). Returns a copy
            to prevent modification of the internal registry.
        """
        return cls._strategies.copy()

# --- END FILE: fin_statement_model/core/calculations/registry.py ---

# --- START FILE: fin_statement_model/core/errors.py ---
"""Define custom exceptions for the Financial Statement Model.

This module defines exception classes for specific error cases in the
Financial Statement Model, allowing for more precise error handling
and better error messages.
"""

from typing import Optional, Any


class FinancialModelError(Exception):
    """Define the base exception class for all Financial Statement Model errors.

    All custom exceptions raised within the library should inherit from this class.

    Args:
        message: A human-readable description of the error.
    """

    def __init__(self, message: str):
        """Initializes the FinancialModelError."""
        self.message = message
        super().__init__(self.message)


class ConfigurationError(FinancialModelError):
    """Raise an error for invalid configuration files or objects.

    This typically occurs when parsing or validating configuration data,
    such as YAML files defining metrics or statement structures.

    Args:
        message: The base error message.
        config_path: Optional path to the configuration file where the error occurred.
        errors: Optional list of specific validation errors found.

    Examples:
        >>> raise ConfigurationError("Invalid syntax", config_path="config.yaml")
        >>> raise ConfigurationError(
        ...     "Missing required fields",
        ...     config_path="metrics.yaml",
        ...     errors=["Missing 'formula' for 'revenue'"]
        ... )
    """

    def __init__(
        self,
        message: str,
        config_path: Optional[str] = None,
        errors: Optional[list[str]] = None,
    ):
        """Initializes the ConfigurationError."""
        self.config_path = config_path
        self.errors = errors or []

        if config_path and errors:
            full_message = f"{message} in {config_path}: {'; '.join(errors)}"
        elif config_path:
            full_message = f"{message} in {config_path}"
        elif errors:
            full_message = f"{message}: {'; '.join(errors)}"
        else:
            full_message = message

        super().__init__(full_message)


class CalculationError(FinancialModelError):
    """Raise an error during calculation operations.

    This indicates a problem while computing the value of a node, often due
    to issues with the calculation logic, input data, or strategy used.

    Args:
        message: The base error message.
        node_id: Optional ID of the node where the calculation failed.
        period: Optional period for which the calculation failed.
        details: Optional dictionary containing additional context about the error.

    Examples:
        >>> raise CalculationError("Division by zero", node_id="profit_margin", period="2023-Q1")
        >>> raise CalculationError(
        ...     "Incompatible input types",
        ...     node_id="total_assets",
        ...     details={"input_a_type": "str", "input_b_type": "int"}
        ... )
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str] = None,
        period: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
    ):
        """Initializes the CalculationError."""
        self.node_id = node_id
        self.period = period
        self.details = details or {}

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if period:
            context.append(f"period '{period}'")

        full_message = f"{message} for {' and '.join(context)}" if context else message

        # Append details to the message for better context
        if self.details:
            details_str = ", ".join(f'{k}="{v}"' for k, v in self.details.items())
            # Prioritize showing the original underlying error if captured
            original_error_str = self.details.get("original_error")
            if original_error_str:
                full_message = f"{full_message}: {original_error_str}"
            else:
                full_message = f"{full_message} (Details: {details_str})"

        super().__init__(full_message)


class NodeError(FinancialModelError):
    """Raise an error for issues related to graph nodes.

    This covers issues like trying to access a non-existent node,
    invalid node configurations, or type mismatches related to nodes.

    Args:
        message: The base error message.
        node_id: Optional ID of the node related to the error.

    Examples:
        >>> raise NodeError("Node not found", node_id="non_existent_node")
        >>> raise NodeError("Invalid node type for operation", node_id="revenue")
    """

    def __init__(self, message: str, node_id: Optional[str] = None):
        """Initializes the NodeError."""
        self.node_id = node_id

        full_message = f"{message} for node '{node_id}'" if node_id else message

        super().__init__(full_message)


class MissingInputError(FinancialModelError):
    """Raise an error when a required calculation input is missing.

    This occurs when a calculation node needs data from another node for a
    specific period, but that data is unavailable.

    Args:
        message: The base error message.
        node_id: Optional ID of the node requiring the input.
        input_name: Optional name or ID of the missing input node.
        period: Optional period for which the input was missing.

    Examples:
        >>> raise MissingInputError(
        ...     "Required input data unavailable",
        ...     node_id="cogs",
        ...     input_name="inventory",
        ...     period="2023-12-31"
        ... )
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str] = None,
        input_name: Optional[str] = None,
        period: Optional[str] = None,
    ):
        """Initializes the MissingInputError."""
        self.node_id = node_id
        self.input_name = input_name
        self.period = period

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if input_name:
            context.append(f"input '{input_name}'")
        if period:
            context.append(f"period '{period}'")

        full_message = f"{message} for {' in '.join(context)}" if context else message

        super().__init__(full_message)


class GraphError(FinancialModelError):
    """Raise an error for invalid graph structure or operations.

    This covers issues like inconsistencies in the graph (e.g., orphaned nodes),
    problems during graph traversal, or invalid modifications to the graph.

    Args:
        message: The base error message.
        nodes: Optional list of node IDs involved in the graph error.

    Examples:
        >>> raise GraphError("Orphaned node detected", nodes=["unconnected_node"])
        >>> raise GraphError("Failed to add edge due to type mismatch")
    """

    def __init__(self, message: str, nodes: Optional[list[str]] = None):
        """Initializes the GraphError."""
        self.nodes = nodes or []

        full_message = f"{message} involving nodes: {', '.join(nodes)}" if nodes else message

        super().__init__(full_message)


class DataValidationError(FinancialModelError):
    """Raise an error for data validation failures.

    This typically occurs during data import or preprocessing when data
    does not conform to expected formats, types, or constraints.

    Args:
        message: The base error message.
        validation_errors: Optional list of specific validation failures.

    Examples:
        >>> raise DataValidationError(
        ...     "Input data failed validation",
        ...     validation_errors=["Column 'Date' has invalid format", "Value '-100' is not allowed for 'Revenue'"]
        ... )
    """

    def __init__(self, message: str, validation_errors: Optional[list[str]] = None):
        """Initializes the DataValidationError."""
        self.validation_errors = validation_errors or []

        if validation_errors:
            full_message = f"{message}: {'; '.join(validation_errors)}"
        else:
            full_message = message

        super().__init__(full_message)


class CircularDependencyError(FinancialModelError):
    """Raise an error when a circular dependency is detected in calculations.

    This occurs if the calculation graph contains cycles, meaning a node
    directly or indirectly depends on itself.

    Args:
        message: The base error message. Defaults to "Circular dependency detected".
        cycle: Optional list of node IDs forming the detected cycle.

    Examples:
        >>> raise CircularDependencyError(cycle=["node_a", "node_b", "node_c", "node_a"])
    """

    def __init__(
        self,
        message: str = "Circular dependency detected",
        cycle: Optional[list[str]] = None,
    ):
        """Initializes the CircularDependencyError."""
        self.cycle = cycle or []

        if cycle:
            cycle_str = " -> ".join(cycle)
            full_message = f"{message}: {cycle_str}"
        else:
            full_message = message

        super().__init__(full_message)


class PeriodError(FinancialModelError):
    """Raise an error for invalid or missing periods.

    This covers issues like requesting data for a non-existent period or
    using invalid period formats.

    Args:
        message: The base error message.
        period: Optional specific period involved in the error.
        available_periods: Optional list of valid periods.

    Examples:
        >>> raise PeriodError("Invalid period format", period="2023Q5")
        >>> raise PeriodError("Period not found", period="2024-01-01", available_periods=["2023-12-31"])
    """

    def __init__(
        self,
        message: str,
        period: Optional[str] = None,
        available_periods: Optional[list[str]] = None,
    ):
        """Initializes the PeriodError."""
        self.period = period
        self.available_periods = available_periods or []

        if period and available_periods:
            full_message = f"{message} for period '{period}'. Available periods: {', '.join(available_periods)}"
        elif period:
            full_message = f"{message} for period '{period}'"
        else:
            full_message = message

        super().__init__(full_message)


class StatementError(FinancialModelError):
    """Raise an error for issues related to financial statements.

    This is used for errors specific to the structure, definition, or
    processing of financial statements (e.g., Balance Sheet, P&L).

    Args:
        message: The base error message.
        statement_id: Optional ID or name of the statement involved.

    Examples:
        >>> raise StatementError("Balance sheet does not balance", statement_id="BS_2023")
        >>> raise StatementError("Required account missing from P&L", statement_id="PnL_Q1")
    """

    def __init__(self, message: str, statement_id: Optional[str] = None):
        """Initializes the StatementError."""
        self.statement_id = statement_id

        full_message = f"{message} for statement '{statement_id}'" if statement_id else message

        super().__init__(full_message)


class StrategyError(FinancialModelError):
    """Raise an error for issues related to calculation strategies.

    This indicates a problem with the configuration or execution of a
    specific calculation strategy (e.g., Summation, GrowthRate).

    Args:
        message: The base error message.
        strategy_type: Optional name or type of the strategy involved.
        node_id: Optional ID of the node using the strategy.

    Examples:
        >>> raise StrategyError("Invalid parameter for GrowthRate strategy", strategy_type="GrowthRate", node_id="revenue_forecast")
        >>> raise StrategyError("Strategy not applicable to node type", strategy_type="Summation", node_id="text_description")
    """

    def __init__(
        self,
        message: str,
        strategy_type: Optional[str] = None,
        node_id: Optional[str] = None,
    ):
        """Initializes the StrategyError."""
        self.strategy_type = strategy_type
        self.node_id = node_id

        context = []
        if strategy_type:
            context.append(f"strategy type '{strategy_type}'")
        if node_id:
            context.append(f"node '{node_id}'")

        full_message = f"{message} for {' in '.join(context)}" if context else message

        super().__init__(full_message)


class TransformationError(FinancialModelError):
    """Raise an error during data transformation.

    This occurs during preprocessing steps when a specific transformation
    (e.g., normalization, scaling) fails.

    Args:
        message: The base error message.
        transformer_type: Optional name or type of the transformer involved.
        parameters: Optional dictionary of parameters used by the transformer.

    Examples:
        >>> raise TransformationError("Log transform requires positive values", transformer_type="LogTransformer")
        >>> raise TransformationError(
        ...     "Incompatible data type for scaling",
        ...     transformer_type="MinMaxScaler",
        ...     parameters={"feature_range": (0, 1)}
        ... )
    """

    def __init__(
        self,
        message: str,
        transformer_type: Optional[str] = None,
        parameters: Optional[dict[str, Any]] = None,
    ):
        """Initializes the TransformationError."""
        self.transformer_type = transformer_type
        self.parameters = parameters or {}

        if transformer_type:
            full_message = f"{message} in transformer '{transformer_type}'"
            if parameters:
                params_str = ", ".join(f"{k}={v}" for k, v in parameters.items())
                full_message = f"{full_message} with parameters: {params_str}"
        else:
            full_message = message

        super().__init__(full_message)


class MetricError(FinancialModelError):
    """Raise an error for issues related to metric definitions or registry.

    This covers issues with loading, validating, or accessing financial metrics,
    whether defined in YAML or Python code.

    Args:
        message: The base error message.
        metric_name: Optional name of the metric involved in the error.
        details: Optional dictionary containing additional context about the error.

    Examples:
        >>> raise MetricError("Metric definition not found", metric_name="unknown_ratio")
        >>> raise MetricError(
        ...     "Invalid formula syntax in metric definition",
        ...     metric_name="profitability_index",
        ...     details={"formula": "NPV / Initial Investment)"} # Missing parenthesis
        ... )
    """

    def __init__(
        self,
        message: str,
        metric_name: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
    ):
        """Initializes the MetricError."""
        self.metric_name = metric_name
        self.details = details or {}

        full_message = f"{message} related to metric '{metric_name}'" if metric_name else message

        super().__init__(full_message)

# --- END FILE: fin_statement_model/core/errors.py ---

# --- START FILE: fin_statement_model/core/graph/__init__.py ---
"""Graph module for the financial statement model.

This module provides the core graph functionality for building and evaluating
financial statement models.
"""

from fin_statement_model.core.graph.graph import Graph
from fin_statement_model.core.graph.manipulator import GraphManipulator
from fin_statement_model.core.graph.traverser import GraphTraverser

__all__ = ["Graph", "GraphManipulator", "GraphTraverser"]

# --- END FILE: fin_statement_model/core/graph/__init__.py ---

# --- START FILE: fin_statement_model/core/graph/graph.py ---
"""Provide graph operations for the financial statement model.

This module provides the `Graph` class that combines manipulation, traversal,
forecasting, and calculation capabilities for building and evaluating
financial statement models.
"""

import logging
from typing import Any, Optional
from collections.abc import Callable
from uuid import UUID

from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.core.nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
)
from fin_statement_model.core.errors import (
    NodeError,
    ConfigurationError,
    CalculationError,
    CircularDependencyError,
)
from fin_statement_model.core.metrics import metric_registry
from fin_statement_model.core.calculations import Registry
from fin_statement_model.core.graph.manipulator import GraphManipulator
from fin_statement_model.core.graph.traverser import GraphTraverser
from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentType,
    AdjustmentTag,
    DEFAULT_SCENARIO,
    AdjustmentFilterInput,
)
from fin_statement_model.core.adjustments.manager import AdjustmentManager


# Configure logging
logger = logging.getLogger(__name__)

__all__ = ["Graph"]


class Graph:
    """Represent the financial statement model as a directed graph.

    This class integrates graph manipulation, traversal, forecasting, and
    calculation capabilities. It serves as the central orchestrator for nodes,
    periods, caching, and calculation workflows.

    Attributes:
        _nodes: A dict mapping node names (str) to Node objects.
        _periods: A list of time period identifiers (str) managed by the graph.
        _cache: A nested dict caching calculated values per node per period.
        _metric_names: A set of node names (str) corresponding to metric nodes.
        _node_factory: An instance of NodeFactory for creating new nodes.
    """

    def __init__(self, periods: Optional[list[str]] = None):
        """Initialize the Graph instance.

        Set up core components: node registry, `DataManager`, and `CalculationEngine`.
        Optionally initialize the graph with a list of time periods.

        Args:
            periods: An optional list of strings representing the initial time
                     periods for the financial model (e.g., ["2023", "2024"]).
                     The `DataManager` will handle sorting and ensuring uniqueness.

        Raises:
            TypeError: If `periods` is provided but is not a list.

        Examples:
            >>> graph_no_periods = Graph()
            >>> logger.info(graph_no_periods.periods) # Output: []
            >>> graph_with_periods = Graph(periods=["2023", "2022"])
            >>> logger.info(graph_with_periods.periods) # Periods are sorted
            >>> try:
            ...     Graph(periods="2023") # Invalid type
            ... except TypeError as e:
            ...     logger.error(e)
        """
        # No super().__init__() needed as mixins don't have __init__
        # and GraphCore is removed.

        self._nodes: dict[str, Node] = {}

        # Initialize core attributes for periods, cache, metrics, and node factory
        self._periods: list[str] = []
        self._cache: dict[str, dict[str, float]] = {}
        self._metric_names: set[str] = set()
        self._node_factory: NodeFactory = NodeFactory()

        # Handle initial periods directly
        if periods:
            if not isinstance(periods, list):
                raise TypeError("Initial periods must be a list")
            self.add_periods(periods)

        self.manipulator = GraphManipulator(self)
        self.traverser = GraphTraverser(self)

        # --- Adjustment Manager Integration ---
        self.adjustment_manager = AdjustmentManager()
        # --- End Adjustment Manager Integration ---

    @property
    def nodes(self) -> dict[str, Node]:
        """Provide access to the dictionary of all nodes in the graph.

        Returns:
            A dictionary where keys are node names (str) and values are
            `Node` objects. This dictionary represents the shared node registry.

        Examples:
            >>> graph = Graph()
            >>> item_node = graph.add_financial_statement_item("Revenue", {"2023": 100})
            >>> logger.info(list(graph.nodes.keys()))
            >>> logger.info(graph.nodes["Revenue"] == item_node)
        """
        return self._nodes

    @property
    def periods(self) -> list[str]:
        """Retrieve the list of time periods currently managed by the graph.

        Returns:
            A sorted list of unique time period strings managed by the graph.

        Examples:
            >>> graph = Graph(periods=["2024", "2023"])
            >>> logger.info(graph.periods)
            >>> graph.add_periods(["2025"])
            >>> logger.info(graph.periods)
        """
        return self._periods

    def add_periods(self, periods: list[str]) -> None:
        """Add new time periods to the graph.

        Update the internal period list, ensuring uniqueness and sorting.

        Args:
            periods: A list of strings representing the time periods to add.

        Raises:
            TypeError: If `periods` is not a list.
        """
        if not isinstance(periods, list):
            raise TypeError("Periods must be provided as a list.")
        # Ensure unique and sorted periods
        combined = set(self._periods).union(periods)
        self._periods = sorted(combined)
        logger.debug(f"Added periods {periods}; current periods: {self._periods}")

    def add_calculation(
        self,
        name: str,
        input_names: list[str],
        operation_type: str,
        formula_variable_names: Optional[list[str]] = None,
        **calculation_kwargs: Any,
    ) -> Node:
        """Add a new calculation node to the graph using the node factory.

        Resolve input node names to Node objects, create a CalculationNode,
        register it in the graph, and return it.

        Args:
            name: Unique name for the calculation node.
            input_names: List of node names to use as inputs.
            operation_type: Calculation type key (e.g., 'addition').
            formula_variable_names: Optional list of variable names used in the formula
                string, required if creating a FormulaCalculationNode via this method.
            **calculation_kwargs: Additional parameters for the calculation constructor.

        Returns:
            The created calculation node.

        Raises:
            NodeError: If any input node name does not exist.
            ValueError: If the name is invalid or creation fails.
            TypeError: If inputs are invalid.
        """
        # Validate name
        if not name or not isinstance(name, str):
            raise ValueError("Calculation node name must be a non-empty string.")
        # Resolve input node names
        if not isinstance(input_names, list):
            raise TypeError("input_names must be a list of node names.")
        resolved_inputs: list[Node] = []
        missing = []
        for nm in input_names:
            nd = self._nodes.get(nm)
            if nd is None:
                missing.append(nm)
            else:
                resolved_inputs.append(nd)
        if missing:
            raise NodeError(
                f"Cannot create calculation node '{name}': missing input nodes {missing}",
            )
        # Create the node via factory
        try:
            node = self._node_factory.create_calculation_node(
                name=name,
                inputs=resolved_inputs,
                calculation_type=operation_type,
                formula_variable_names=formula_variable_names,
                **calculation_kwargs,
            )
        except (ValueError, TypeError):
            logger.exception(
                f"Failed to create calculation node '{name}' with type '{operation_type}'"
            )
            raise

        # --- Cycle Detection ---
        for input_node in resolved_inputs:
            try:
                bfs_levels = self.traverser.breadth_first_search(
                    start_node=input_node.name, direction="successors"
                )
                reachable_nodes = {n for level in bfs_levels for n in level}
                if name in reachable_nodes:
                    cycle_path_guess = [
                        input_node.name,
                        "...",
                        name,
                    ]  # Simplified guess
                    raise CircularDependencyError(
                        message=f"Adding calculation node '{name}' would create a cycle from input '{input_node.name}'",
                        node_id=name,
                        cycle=cycle_path_guess,
                    )
            except NodeError:
                # Should not happen if inputs were resolved, but handle defensively
                logger.warning(
                    f"NodeError during cycle check pre-computation for {name} from {input_node.name}"
                )
                # Or potentially re-raise depending on desired strictness
            except Exception as e:
                # Catch broader errors during traversal if needed
                logger.error(
                    f"Unexpected error during cycle detection for node {name}: {e}",
                    exc_info=True,
                )
                # Optionally re-raise as a different error type or handle
                raise CalculationError(
                    message=f"Error during cycle detection for {name}",
                    node_id=name,
                    details={"original_error": str(e)},
                ) from e
        # --- End Cycle Detection ---

        # Register node in graph
        if name in self._nodes:
            logger.warning(f"Overwriting existing node '{name}' in graph.")
        self._nodes[name] = node
        logger.info(
            f"Added calculation node '{name}' of type '{operation_type}' with inputs {input_names}"
        )
        return node

    def add_metric(
        self,
        metric_name: str,
        node_name: Optional[str] = None,
        *,
        input_node_map: Optional[dict[str, str]] = None,
    ) -> Node:
        """Add a metric calculation node based on a metric definition.

        If `node_name` is None, uses `metric_name` as the node name.

        Uses the metric registry to load inputs and formula, creates a
        calculation node using the formula strategy, registers it, and stores metric
        metadata on the node itself.

        Args:
            metric_name: Key of the metric definition to add.
            node_name: Optional name for the metric node; defaults to metric_name.
            input_node_map: Optional dictionary mapping metric input variable names
                (from metric definition) to the actual node names present in the graph.
                If None, assumes graph node names match metric input variable names.

        Returns:
            The created calculation node.

        Raises:
            TypeError: If node_name is invalid.
            ValueError: If node_name already exists.
            ConfigurationError: If metric definition is missing or invalid.
            NodeError: If required input nodes (after mapping) are missing.
        """
        # Default node_name to metric_name if not provided
        if node_name is None:
            node_name = metric_name
        if not node_name or not isinstance(node_name, str):
            raise TypeError("Metric node name must be a non-empty string.")
        # Check for name conflict
        if node_name in self._nodes:
            raise ValueError(f"A node with name '{node_name}' already exists in the graph.")

        # Load metric definition (Pydantic model)
        try:
            metric_def = metric_registry.get(metric_name)
        except KeyError as e:
            raise ConfigurationError(f"Unknown metric definition: '{metric_name}'") from e

        # Extract required fields from definition
        required_inputs = metric_def.inputs
        formula = metric_def.formula
        description = metric_def.description

        # Build list of input node names and formula variable names
        input_node_names: list[str] = []
        formula_variable_names: list[str] = []
        missing = []

        for req_input_name in required_inputs:
            # Determine the actual graph node name to look for
            target_node_name = req_input_name  # Default case
            if input_node_map and req_input_name in input_node_map:
                target_node_name = input_node_map[req_input_name]
            elif input_node_map:
                # If map provided but doesn't contain the required input, it's an error in the map
                missing.append(f"{req_input_name} (mapping missing in input_node_map)")
                continue  # Skip trying to find the node

            # Check if the node exists in the graph
            if target_node_name not in self._nodes:
                missing.append(target_node_name)  # Report the name we looked for
            else:
                input_node_names.append(target_node_name)
                formula_variable_names.append(req_input_name)  # Use the metric's variable name

        if missing:
            raise NodeError(
                f"Cannot create metric '{metric_name}': missing required nodes {missing}",
                node_id=node_name,
            )

        # Create calculation node using add_calculation
        try:
            new_node = self.add_calculation(
                name=node_name,
                input_names=input_node_names,
                operation_type="formula",
                formula_variable_names=formula_variable_names,
                formula=formula,
                metric_name=metric_name,  # Pass metric metadata
                metric_description=description,  # Pass metric description
            )
        except Exception as e:
            logger.exception(
                f"Failed to create calculation node for metric '{metric_name}' as node '{node_name}'"
            )
            # Re-raise as ConfigurationError or keep original, depending on desired error reporting
            raise ConfigurationError(f"Error creating node for metric '{metric_name}': {e}") from e

        logger.info(
            f"Added metric '{metric_name}' as calculation node '{node_name}' with inputs {input_node_names}"
        )
        return new_node

    def add_custom_calculation(
        self,
        name: str,
        calculation_func: Callable[..., float],
        inputs: Optional[list[str]] = None,
        description: str = "",
    ) -> Node:
        """Add a custom calculation node using a Python callable.

        Args:
            name: Unique name for the custom calculation node.
            calculation_func: A callable that accepts (period, **inputs) and returns float.
            inputs: Optional list of node names to use as inputs.
            description: Optional description of the calculation.

        Returns:
            The created custom calculation node.

        Raises:
            NodeError: If any specified input nodes are missing.
            TypeError: If calculation_func is not callable.
        """
        # Validate inputs list
        resolved_inputs: list[Node] = []
        if inputs is not None:
            if not isinstance(inputs, list):
                raise TypeError("inputs must be a list of node names.")
            missing = []
            for nm in inputs:
                nd = self._nodes.get(nm)
                if nd is None:
                    missing.append(nm)
                else:
                    resolved_inputs.append(nd)
            if missing:
                raise NodeError(
                    f"Cannot create custom calculation '{name}': missing input nodes {missing}",
                    node_id=name,
                )
        # Validate callable
        if not callable(calculation_func):
            raise TypeError("calculation_func must be callable.")
        # Create custom node via factory
        try:
            custom_node = self._node_factory._create_custom_node_from_callable(
                name=name,
                inputs=resolved_inputs,
                formula=calculation_func,
                description=description,
            )
        except (ValueError, TypeError):
            logger.exception(f"Failed to create custom calculation node '{name}'")
            raise
        # Register custom node
        if name in self._nodes:
            logger.warning(f"Overwriting existing node '{name}' in graph.")
        self._nodes[name] = custom_node
        logger.info(f"Added custom calculation node '{name}' with inputs {inputs}")
        return custom_node

    def change_calculation_method(
        self,
        node_name: str,
        new_method_key: str,
        **kwargs: dict[str, Any],
    ) -> None:
        """Change the calculation method for an existing calculation-based node.

        Args:
            node_name: Name of the existing calculation node.
            new_method_key: Key of the new calculation method to apply.
            **kwargs: Additional parameters required by the new calculation.

        Returns:
            None

        Raises:
            NodeError: If the target node does not exist or is not a CalculationNode.
            ValueError: If `new_method_key` is not a recognized calculation key.
            TypeError: If the new calculation cannot be instantiated with the provided arguments.

        Examples:
            >>> graph.change_calculation_method("GrossProfit", "addition")
        """
        node = self.manipulator.get_node(node_name)
        if node is None:
            raise NodeError("Node not found for calculation change", node_id=node_name)
        if not isinstance(node, CalculationNode):
            raise NodeError(f"Node '{node_name}' is not a CalculationNode", node_id=node_name)
        # Map method key to registry name
        if new_method_key not in self._node_factory._calculation_methods:
            raise ValueError(f"Calculation '{new_method_key}' is not recognized.")
        calculation_class_name = self._node_factory._calculation_methods[new_method_key]
        try:
            calculation_cls = Registry.get(calculation_class_name)
        except KeyError as e:
            raise ValueError(
                f"Calculation class '{calculation_class_name}' not found in registry."
            ) from e
        try:
            calculation_instance = calculation_cls(**kwargs)
        except TypeError as e:
            raise TypeError(f"Failed to instantiate calculation '{new_method_key}': {e}")
        # Apply new calculation
        node.set_calculation(calculation_instance)
        # Clear cached calculations for this node
        if node_name in self._cache:
            del self._cache[node_name]
        logger.info(f"Changed calculation for node '{node_name}' to '{new_method_key}'")

    def get_metric(self, metric_id: str) -> Optional[Node]:
        """Return the metric node for a given metric ID, if present.

        Searches for a node with the given ID that was created as a metric
        (identified by having a `metric_name` attribute).

        Args:
            metric_id: Identifier of the metric node to retrieve.

        Returns:
            The Node corresponding to `metric_id` if it's a metric node, or None.

        Examples:
            >>> m = graph.get_metric("current_ratio")
            >>> if m:
            ...     logger.info(m.name)
        """
        node = self._nodes.get(metric_id)
        # Check if the node exists and has the metric_name attribute populated
        if node and getattr(node, "metric_name", None) == metric_id:
            return node
        return None

    def get_available_metrics(self) -> list[str]:
        """Return a sorted list of all metric node IDs currently in the graph.

        Identifies metric nodes by checking for the presence and non-None value
        of the `metric_name` attribute.

        Returns:
            A sorted list of metric node names.

        Examples:
            >>> graph.get_available_metrics()
            ['current_ratio', 'debt_equity_ratio']
        """
        # Iterate through all nodes and collect names of those that are metrics
        metric_node_names = [
            node.name
            for node in self._nodes.values()
            if getattr(node, "metric_name", None) is not None
        ]
        return sorted(metric_node_names)

    def get_metric_info(self, metric_id: str) -> dict:
        """Return detailed information for a specific metric node.

        Args:
            metric_id: Identifier of the metric node to inspect.

        Returns:
            A dict containing 'id', 'name', 'description', and 'inputs' for the metric.

        Raises:
            ValueError: If `metric_id` does not correspond to a metric node.

        Examples:
            >>> info = graph.get_metric_info("current_ratio")
            >>> logger.info(info['inputs'])
        """
        metric_node = self.get_metric(metric_id)
        if metric_node is None:
            if metric_id in self._nodes:
                raise ValueError(
                    f"Node '{metric_id}' exists but is not a metric (missing metric_name attribute)."
                )
            raise ValueError(f"Metric node '{metric_id}' not found in graph.")

        # Extract info directly from the FormulaCalculationNode
        try:
            # Use getattr for safety, retrieving stored metric metadata
            description = getattr(metric_node, "metric_description", "N/A")
            # metric_name stored on the node is the key from the registry
            registry_key = getattr(metric_node, "metric_name", metric_id)

            # We might want the display name from the original definition.
            # Fetch the definition again if needed for the display name.
            try:
                metric_def = metric_registry.get(registry_key)
                display_name = metric_def.name
            except Exception:
                logger.warning(
                    f"Could not reload metric definition for '{registry_key}' to get display name. Using node name '{metric_id}' instead."
                )
                display_name = metric_id  # Fallback to node name

            inputs = metric_node.get_dependencies()
        except Exception as e:
            # Catch potential attribute errors or other issues
            logger.error(
                f"Error retrieving info for metric node '{metric_id}': {e}",
                exc_info=True,
            )
            raise ValueError(f"Failed to retrieve metric info for '{metric_id}': {e}") from e

        return {
            "id": metric_id,
            "name": display_name,
            "description": description,
            "inputs": inputs,
        }

    def calculate(self, node_name: str, period: str) -> float:
        """Calculate and return the value of a specific node for a given period.

        This method uses internal caching to speed repeated calls, and wraps
        underlying errors in CalculationError for clarity.

        Args:
            node_name: Name of the node to calculate.
            period: Time period identifier for the calculation.

        Returns:
            The calculated float value for the node and period.

        Raises:
            NodeError: If the specified node does not exist.
            TypeError: If the node has no callable `calculate` method.
            CalculationError: If an error occurs during the node's calculation.

        Examples:
            >>> value = graph.calculate("Revenue", "2023")
        """
        # Return cached value if present
        if node_name in self._cache and period in self._cache[node_name]:
            logger.debug(f"Cache hit for node '{node_name}', period '{period}'")
            return self._cache[node_name][period]
        # Resolve node
        node = self.manipulator.get_node(node_name)
        if node is None:
            raise NodeError(f"Node '{node_name}' not found", node_id=node_name)
        # Validate calculate method
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise TypeError(f"Node '{node_name}' has no callable calculate method.")
        # Perform calculation with error handling
        try:
            value = node.calculate(period)
        except (
            NodeError,
            ConfigurationError,
            CalculationError,
            ValueError,
            KeyError,
            ZeroDivisionError,
        ) as e:
            logger.error(
                f"Error calculating node '{node_name}' for period '{period}': {e}",
                exc_info=True,
            )
            raise CalculationError(
                message=f"Failed to calculate node '{node_name}'",
                node_id=node_name,
                period=period,
                details={"original_error": str(e)},
            ) from e
        # Cache and return
        self._cache.setdefault(node_name, {})[period] = value
        logger.debug(f"Cached value for node '{node_name}', period '{period}': {value}")
        return value

    def recalculate_all(self, periods: Optional[list[str]] = None) -> None:
        """Recalculate all nodes for given periods, clearing all caches first.

        Args:
            periods: List of period strings, a single string, or None to use all periods.

        Returns:
            None

        Raises:
            TypeError: If `periods` is not a list, string, or None.

        Examples:
            >>> graph.recalculate_all(["2023", "2024"])
        """
        # Normalize periods input
        if periods is None:
            periods_to_use = self.periods
        elif isinstance(periods, str):
            periods_to_use = [periods]
        elif isinstance(periods, list):
            periods_to_use = periods
        else:
            raise TypeError("Periods must be a list of strings, a single string, or None.")
        # Clear all caches (node-level and central) to force full recalculation
        self.clear_all_caches()
        if not periods_to_use:
            return
        # Recalculate each node for each period
        for node_name in list(self._nodes.keys()):
            for period in periods_to_use:
                try:
                    self.calculate(node_name, period)
                except Exception as e:
                    logger.warning(
                        f"Error recalculating node '{node_name}' for period '{period}': {e}"
                    )

    def clear_all_caches(self) -> None:
        """Clear all node-level and central calculation caches.

        Returns:
            None

        Examples:
            >>> graph.clear_all_caches()
        """
        logger.debug(f"Clearing node-level caches for {len(self.nodes)} nodes.")
        for node in self.nodes.values():
            if hasattr(node, "clear_cache"):
                try:
                    node.clear_cache()
                except Exception as e:
                    logger.warning(f"Failed to clear cache for node '{node.name}': {e}")
        # Clear central calculation cache
        self.clear_calculation_cache()
        logger.debug("Cleared central calculation cache.")

    def clear_calculation_cache(self) -> None:
        """Clear the graph's internal calculation cache.

        Returns:
            None

        Examples:
            >>> graph.clear_calculation_cache()
        """
        self._cache.clear()
        logger.debug("Cleared graph calculation cache.")

    def clear(self) -> None:
        """Reset the graph by clearing nodes, periods, metrics, adjustments, and caches."""
        self._nodes = {}
        self._periods = []
        self._cache = {}
        self._metric_names = set()  # This seems redundant now, metrics identified by attribute

        # --- Adjustment Manager Integration ---
        self.adjustment_manager.clear_all()
        # --- End Adjustment Manager Integration ---

        logger.info("Graph cleared: nodes, periods, metrics, adjustments, and caches reset.")

    def add_financial_statement_item(
        self, name: str, values: dict[str, float]
    ) -> FinancialStatementItemNode:
        """Add a basic financial statement item (data node) to the graph.

        Args:
            name: Unique name for the financial statement item node.
            values: Mapping of period strings to float values for this item.

        Returns:
            The newly created `FinancialStatementItemNode`.

        Raises:
            NodeError: If a node with the same name already exists.
            TypeError: If `values` is not a dict or contains invalid types.

        Examples:
            >>> item_node = graph.add_financial_statement_item("SG&A", {"2023": 50.0})
            >>> item_node.get_value("2023")
            50.0
        """
        if name in self._nodes:
            raise NodeError("Node with name already exists", node_id=name)
        if not isinstance(values, dict):
            raise TypeError("Values must be provided as a dict[str, float]")
        # Create a new financial statement item node
        new_node = self._node_factory.create_financial_statement_item(
            name=name, values=values.copy()
        )
        self._nodes[name] = new_node
        # Update periods from node values
        self.add_periods(list(values.keys()))
        logger.info(f"Added FinancialStatementItemNode '{name}' with periods {list(values.keys())}")
        return new_node

    def update_financial_statement_item(
        self, name: str, values: dict[str, float], replace_existing: bool = False
    ) -> FinancialStatementItemNode:
        """Update values for an existing financial statement item node.

        Args:
            name: Name of the existing financial statement item node.
            values: Mapping of new period strings to float values.
            replace_existing: If True, replace existing values entirely; otherwise merge.

        Returns:
            The updated `FinancialStatementItemNode`.

        Raises:
            NodeError: If the node does not exist.
            TypeError: If the node is not a `FinancialStatementItemNode` or `values` is not a dict.

        Examples:
            >>> graph.update_financial_statement_item("SG&A", {"2024": 60.0})
        """
        node = self.manipulator.get_node(name)
        if node is None:
            raise NodeError("Node not found", node_id=name)
        if not isinstance(node, FinancialStatementItemNode):
            raise TypeError(f"Node '{name}' is not a FinancialStatementItemNode")
        if not isinstance(values, dict):
            raise TypeError("Values must be provided as a dict[str, float]")
        if replace_existing:
            node.values = values.copy()
        else:
            node.values.update(values)
        self.add_periods(list(values.keys()))
        logger.info(
            f"Updated FinancialStatementItemNode '{name}' with periods {list(values.keys())}; replace_existing={replace_existing}"
        )
        return node

    def get_financial_statement_items(self) -> list[Node]:
        """Retrieve all financial statement item nodes from the graph.

        Returns:
            A list of `FinancialStatementItemNode` objects currently in the graph.

        Examples:
            >>> items = graph.get_financial_statement_items()
        """
        from fin_statement_model.core.nodes import (
            FinancialStatementItemNode,
        )  # Keep import local as it's specific

        return [
            node for node in self.nodes.values() if isinstance(node, FinancialStatementItemNode)
        ]

    def __repr__(self) -> str:
        """Provide a concise, developer-friendly string representation of the graph.

        Summarize total nodes, FS items, calculations, dependencies, and periods.

        Returns:
            A string summarizing the graph's structure and contents.

        Examples:
            >>> logger.info(repr(graph))
        """
        from fin_statement_model.core.nodes import (
            FinancialStatementItemNode,
        )  # Keep import local

        num_nodes = len(self.nodes)
        periods_str = ", ".join(map(repr, self.periods)) if self.periods else "None"

        fs_item_count = 0
        calc_node_count = 0
        other_node_count = 0
        dependencies_count = 0

        for node in self.nodes.values():
            if isinstance(node, FinancialStatementItemNode):
                fs_item_count += 1
            elif node.has_calculation():
                calc_node_count += 1
                # Prioritize get_dependencies if available, otherwise check inputs
                if hasattr(node, "get_dependencies"):
                    try:
                        dependencies_count += len(node.get_dependencies())
                    except Exception as e:
                        logger.warning(
                            f"Error calling get_dependencies for node '{node.name}': {e}"
                        )
                elif hasattr(node, "inputs"):
                    try:
                        if isinstance(node.inputs, list):
                            # Ensure inputs are nodes with names
                            dep_names = [inp.name for inp in node.inputs if hasattr(inp, "name")]
                            dependencies_count += len(dep_names)
                        elif isinstance(node.inputs, dict):
                            # Assume keys are dependency names for dict inputs
                            dependencies_count += len(node.inputs)
                    except Exception as e:
                        logger.warning(f"Error processing inputs for node '{node.name}': {e}")
            else:
                other_node_count += 1

        repr_parts = [
            f"Total Nodes: {num_nodes}",
            f"FS Items: {fs_item_count}",
            f"Calculations: {calc_node_count}",
        ]
        if other_node_count > 0:
            repr_parts.append(f"Other: {other_node_count}")
        repr_parts.append(f"Dependencies: {dependencies_count}")
        repr_parts.append(f"Periods: [{periods_str}]")

        return f"<{type(self).__name__}({', '.join(repr_parts)})>"

    def has_cycle(self, source_node: Node, target_node: Node) -> bool:
        """Check if a cycle exists from a source node to a target node.

        This method delegates to GraphTraverser.breadth_first_search to determine
        if `target_node` is reachable from `source_node` via dependencies, indicating
        that adding an edge from `target_node` to `source_node` would create a cycle.

        Args:
            source_node: The starting node for cycle detection.
            target_node: The node to detect return path to.

        Returns:
            True if a cycle exists, False otherwise.
        """
        if source_node.name not in self._nodes or target_node.name not in self._nodes:
            return False

        # Use BFS to check reachability via predecessors (dependencies)
        bfs_levels = self.traverser.breadth_first_search(source_node.name, direction="predecessors")
        reachable_nodes = {n for level in bfs_levels for n in level}
        return target_node.name in reachable_nodes

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its name.

        Args:
            name: The unique name of the node to retrieve.

        Returns:
            The `Node` instance if found, else None.

        Examples:
            >>> node = graph.get_node("Revenue")
        """
        return self.manipulator.get_node(name)

    def add_node(self, node: Node) -> None:
        """Add a node to the graph, replacing any existing node with the same name.

        Args:
            node: The `Node` instance to add.

        Returns:
            None

        Examples:
            >>> graph.add_node(custom_node)
        """
        return self.manipulator.add_node(node)

    def remove_node(self, node_name: str) -> None:
        """Remove a node from the graph by name, updating dependencies.

        Args:
            node_name: The name of the node to remove.

        Returns:
            None

        Examples:
            >>> graph.remove_node("OldItem")
        """
        return self.manipulator.remove_node(node_name)

    def replace_node(self, node_name: str, new_node: Node) -> None:
        """Replace an existing node with a new node instance.

        Args:
            node_name: Name of the node to replace.
            new_node: The new `Node` instance to substitute.

        Returns:
            None

        Examples:
            >>> graph.replace_node("Item", updated_node)
        """
        return self.manipulator.replace_node(node_name, new_node)

    def has_node(self, node_id: str) -> bool:
        """Check if a node with the given ID exists in the graph.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> graph.has_node("Revenue")
        """
        return self.manipulator.has_node(node_id)

    def set_value(self, node_id: str, period: str, value: float) -> None:
        """Set or update the value for a node in a specific period.

        Args:
            node_id: The name of the node.
            period: The period identifier to set the value for.
            value: The float value to assign.

        Returns:
            None

        Raises:
            ValueError: If the period is not recognized by the graph.
            NodeError: If the node does not exist.
            TypeError: If the node does not support setting a value.

        Examples:
            >>> graph.set_value("SG&A", "2024", 55.0)
        """
        return self.manipulator.set_value(node_id, period, value)

    def topological_sort(self) -> list[str]:
        """Perform a topological sort of all graph nodes.

        Returns:
            A list of node IDs in topological order.

        Raises:
            ValueError: If a cycle is detected in the graph.

        Examples:
            >>> order = graph.topological_sort()
        """
        return self.traverser.topological_sort()

    def get_calculation_nodes(self) -> list[str]:
        """Get all calculation node IDs in the graph.

        Returns:
            A list of node names that have associated calculations.

        Examples:
            >>> graph.get_calculation_nodes()
        """
        return self.traverser.get_calculation_nodes()

    def get_dependencies(self, node_id: str) -> list[str]:
        """Get the direct predecessor node IDs (dependencies) for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node depends on.

        Examples:
            >>> graph.get_dependencies("GrossProfit")
        """
        return self.traverser.get_dependencies(node_id)

    def get_dependency_graph(self) -> dict[str, list[str]]:
        """Get the full dependency graph mapping of node IDs to their inputs.

        Returns:
            A dict mapping each node ID to a list of its dependency node IDs.

        Examples:
            >>> graph.get_dependency_graph()
        """
        return self.traverser.get_dependency_graph()

    def detect_cycles(self) -> list[list[str]]:
        """Detect all cycles in the graph's dependency structure.

        Returns:
            A list of cycles, each represented as a list of node IDs.

        Examples:
            >>> graph.detect_cycles()
        """
        return self.traverser.detect_cycles()

    def validate(self) -> list[str]:
        """Validate the graph structure for errors such as cycles or missing nodes.

        Returns:
            A list of validation error messages, empty if valid.

        Examples:
            >>> graph.validate()
        """
        return self.traverser.validate()

    def breadth_first_search(self, start_node: str, direction: str = "successors") -> list[str]:
        """Perform a breadth-first search (BFS) traversal of the graph.

        Args:
            start_node: The starting node ID for BFS.
            direction: Either 'successors' or 'predecessors' to traverse.

        Returns:
            A nested list of node IDs per BFS level.

        Raises:
            ValueError: If `direction` is not 'successors' or 'predecessors'.

        Examples:
            >>> graph.breadth_first_search("Revenue", "successors")
        """
        return self.traverser.breadth_first_search(start_node, direction)

    def get_direct_successors(self, node_id: str) -> list[str]:
        """Get immediate successor node IDs for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that directly follow the given node.

        Examples:
            >>> graph.get_direct_successors("Revenue")
        """
        return self.traverser.get_direct_successors(node_id)

    def get_direct_predecessors(self, node_id: str) -> list[str]:
        """Get immediate predecessor node IDs (inputs) for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node directly depends on.

        Examples:
            >>> graph.get_direct_predecessors("GrossProfit")
        """
        return self.traverser.get_direct_predecessors(node_id)

    def merge_from(self, other_graph: "Graph") -> None:
        """Merge nodes and periods from another Graph into this one.

        Adds periods from the other graph if they don't exist in this graph.
        Adds nodes from the other graph if they don't exist.
        If a node exists in both graphs, attempts to merge the 'values' dictionary
        from the other graph's node into this graph's node.

        Args:
            other_graph: The Graph instance to merge data from.

        Raises:
            TypeError: If other_graph is not a Graph instance.
        """
        if not isinstance(other_graph, Graph):
            raise TypeError("Can only merge from another Graph instance.")

        logger.info(f"Starting merge from graph {other_graph!r} into {self!r}")

        # 1. Update periods
        new_periods = [p for p in other_graph.periods if p not in self.periods]
        if new_periods:
            self.add_periods(new_periods)
            logger.debug(f"Merged periods: {new_periods}")

        # 2. Merge nodes
        nodes_added = 0
        nodes_updated = 0
        for node_name, other_node in other_graph.nodes.items():
            existing_node = self.get_node(node_name)
            if existing_node is not None:
                # Node exists, merge values if applicable
                if (
                    hasattr(existing_node, "values")
                    and hasattr(other_node, "values")
                    and isinstance(getattr(existing_node, "values", None), dict)
                    and isinstance(getattr(other_node, "values", None), dict)
                ):
                    try:
                        # Perform the update
                        existing_node.values.update(other_node.values)  # type: ignore
                        nodes_updated += 1
                        logger.debug(f"Merged values into existing node '{node_name}'")
                        # No need to call self.add_node(existing_node) as it's already there
                    except AttributeError:
                        # Should not happen due to hasattr checks, but defensive
                        logger.warning(
                            f"Could not merge values for node '{node_name}' due to missing 'values' attribute despite hasattr check."
                        )
                    except Exception as e:
                        logger.warning(f"Could not merge values for node '{node_name}': {e}")
                else:
                    # Nodes exist but cannot merge values (e.g., calculation nodes without stored values)
                    logger.debug(
                        f"Node '{node_name}' exists in both graphs, but values not merged (missing/incompatible 'values' attribute). Keeping target graph's node."
                    )
            else:
                # Node doesn't exist in target graph, add it
                try:
                    # Ensure we add a copy if nodes might be shared or mutable in complex ways,
                    # but for now, assume adding the instance is okay.
                    self.add_node(other_node)
                    nodes_added += 1
                except Exception:
                    logger.exception(f"Failed to add new node '{node_name}' during merge:")

        logger.info(
            f"Merge complete. Nodes added: {nodes_added}, Nodes updated (values merged): {nodes_updated}"
        )

    # --- Adjustment Management API ---

    def add_adjustment(
        self,
        node_name: str,
        period: str,
        value: float,
        reason: str,
        adj_type: AdjustmentType = AdjustmentType.ADDITIVE,
        scale: float = 1.0,
        priority: int = 0,
        tags: Optional[set[AdjustmentTag]] = None,
        scenario: Optional[str] = None,
        user: Optional[str] = None,
        start_period: Optional[str] = None,  # Phase 2
        end_period: Optional[str] = None,  # Phase 2
        adj_id: Optional[UUID] = None,  # Allow specifying ID, e.g., for re-creation
    ) -> UUID:
        """Adds a discretionary adjustment to a specific node and period.

        Creates an Adjustment object and delegates storage to the AdjustmentManager.

        Args:
            node_name: The name of the target node.
            period: The primary period the adjustment applies to.
            value: The numeric value of the adjustment.
            reason: Text description of why the adjustment was made.
            adj_type: How the adjustment combines with the base value.
            scale: Attenuation factor for the adjustment (0.0 to 1.0, Phase 2).
            priority: Tie-breaker for applying multiple adjustments (lower number applied first).
            tags: Set of descriptive tags for filtering and analysis.
            scenario: The named scenario this adjustment belongs to. Defaults to DEFAULT_SCENARIO if None.
            user: Identifier for the user who created the adjustment.
            start_period: The first period the adjustment is effective (inclusive, Phase 2).
            end_period: The last period the adjustment is effective (inclusive, Phase 2).
            adj_id: Optional specific UUID to use for the adjustment.

        Returns:
            The UUID of the created or updated adjustment.

        Raises:
            NodeError: If the target node_name does not exist in the graph.
            ValidationError: If adjustment parameters are invalid (e.g., scale out of bounds).
        """
        if not self.has_node(node_name):
            raise NodeError(
                f"Cannot add adjustment: Node '{node_name}' not found.",
                node_id=node_name,
            )

        # Need Pydantic's ValidationError and uuid4
        from pydantic import ValidationError
        from uuid import uuid4

        # Need Adjustment model details
        from fin_statement_model.core.adjustments.models import Adjustment

        # Assign default scenario if None was passed
        actual_scenario = scenario if scenario is not None else DEFAULT_SCENARIO

        # Create the adjustment object - Pydantic handles validation (e.g., scale)
        try:
            adj = Adjustment(
                id=adj_id or uuid4(),  # Generate new ID if not provided
                node_name=node_name,
                period=period,
                start_period=start_period,
                end_period=end_period,
                value=value,
                type=adj_type,
                scale=scale,
                priority=priority,
                tags=tags or set(),
                scenario=actual_scenario,  # Use the actual scenario
                reason=reason,
                user=user,
                # timestamp is added automatically by the model
            )
        except ValidationError:
            logger.exception(f"Failed to create adjustment for node '{node_name}'")
            raise  # Re-raise Pydantic's validation error

        self.adjustment_manager.add_adjustment(adj)
        logger.info(
            f"Added adjustment {adj.id} for node '{node_name}', period '{period}', scenario '{scenario}'."
        )
        return adj.id

    def remove_adjustment(self, adj_id: UUID) -> bool:
        """Removes an adjustment by its unique ID.

        Args:
            adj_id: The UUID of the adjustment to remove.

        Returns:
            True if an adjustment was found and removed, False otherwise.
        """
        removed = self.adjustment_manager.remove_adjustment(adj_id)
        if removed:
            logger.info(f"Removed adjustment {adj_id}.")
        else:
            logger.warning(f"Attempted to remove non-existent adjustment {adj_id}.")
        return removed

    def get_adjustments(
        self, node_name: str, period: str, *, scenario: Optional[str] = None
    ) -> list[Adjustment]:
        """Retrieves all adjustments for a specific node, period, and scenario.

        Args:
            node_name: The name of the target node.
            period: The target period.
            scenario: The scenario to retrieve adjustments for. Defaults to DEFAULT_SCENARIO if None.

        Returns:
            A list of Adjustment objects matching the criteria, sorted by application order.
        """
        if not self.has_node(node_name):
            # Or return empty list? Returning empty seems safer.
            logger.warning(f"Node '{node_name}' not found when getting adjustments.")
            return []
        # Assign default scenario if None was passed
        actual_scenario = scenario if scenario is not None else DEFAULT_SCENARIO
        return self.adjustment_manager.get_adjustments(node_name, period, scenario=actual_scenario)

    def list_all_adjustments(self) -> list[Adjustment]:
        """Returns a list of all adjustments currently managed by the graph.

        Returns:
            A list containing all Adjustment objects across all nodes, periods, and scenarios.
        """
        return self.adjustment_manager.get_all_adjustments()

    def get_adjusted_value(
        self,
        node_name: str,
        period: str,
        filter_input: "AdjustmentFilterInput" = None,
        *,
        return_flag: bool = False,
    ) -> float | tuple[float, bool]:
        """Calculates the value of a node for a period, applying selected adjustments.

        Fetches the base calculated value, retrieves adjustments matching the filter,
        applies them in order, and returns the result.

        Args:
            node_name: The name of the node to calculate.
            period: The time period identifier.
            filter_input: Criteria for selecting which adjustments to apply.
                          Can be an AdjustmentFilter instance, a set of tags (for include_tags),
                          a callable predicate `fn(adj: Adjustment) -> bool`, or None
                          (applies all adjustments in the default scenario).
            return_flag: If True, return a tuple (adjusted_value, was_adjusted_flag).
                         If False (default), return only the adjusted_value.

        Returns:
            The adjusted float value, or a tuple (value, flag) if return_flag is True.

        Raises:
            NodeError: If the specified node does not exist.
            CalculationError: If an error occurs during the base calculation or adjustment application.
            TypeError: If filter_input is an invalid type.
        """
        # 1. Get the base value (result of underlying node calculation)
        try:
            base_value = self.calculate(node_name, period)
        except (NodeError, CalculationError, TypeError):
            # Propagate errors from base calculation
            logger.exception(f"Error getting base value for '{node_name}' in period '{period}'")
            raise

        # 2. Get filtered adjustments from the manager
        # The manager handles filter normalization and matching logic.
        # It needs the period context for effective window checks.
        try:
            adjustments_to_apply = self.adjustment_manager.get_filtered_adjustments(
                node_name=node_name, period=period, filter_input=filter_input
            )
        except TypeError:
            logger.exception("Invalid filter type provided for get_adjusted_value")
            raise

        # 3. Apply the adjustments
        adjusted_value, was_adjusted = self.adjustment_manager.apply_adjustments(
            base_value, adjustments_to_apply
        )

        # 4. Return result based on flag
        if return_flag:
            return adjusted_value, was_adjusted
        else:
            return adjusted_value

    def was_adjusted(
        self, node_name: str, period: str, filter_input: "AdjustmentFilterInput" = None
    ) -> bool:
        """Checks if a node's value for a given period was affected by any selected adjustments.

        Args:
            node_name: The name of the node to check.
            period: The time period identifier.
            filter_input: Criteria for selecting which adjustments to consider (same as get_adjusted_value).

        Returns:
            True if any adjustment matching the filter was applied to the base value, False otherwise.

        Raises:
            NodeError: If the specified node does not exist.
            CalculationError: If an error occurs during the underlying calculation.
            TypeError: If filter_input is an invalid type.
        """
        try:
            _, was_adjusted_flag = self.get_adjusted_value(
                node_name, period, filter_input, return_flag=True
            )
            return was_adjusted_flag
        except (NodeError, CalculationError, TypeError):
            # Propagate errors consistently
            logger.exception(f"Error checking if node '{node_name}' was adjusted")
            raise

    # --- End Adjustment Management API ---

# --- END FILE: fin_statement_model/core/graph/graph.py ---

# --- START FILE: fin_statement_model/core/graph/manipulator.py ---
"""Provide graph manipulation utilities.

This module defines the GraphManipulator class, encapsulating node-level mutation helpers for Graph.
"""

import logging
from typing import Optional, Any
from fin_statement_model.core.errors import NodeError
from fin_statement_model.core.nodes import Node

logger = logging.getLogger(__name__)


class GraphManipulator:
    """Encapsulate node-level mutation helpers for Graph.

    Attributes:
        graph: The Graph instance this manipulator operates on.
    """

    def __init__(self, graph: Any) -> None:
        """Initialize the GraphManipulator with a Graph reference.

        Args:
            graph: The Graph instance to manipulate.
        """
        self.graph = graph

    def add_node(self, node: Node) -> None:
        """Add a node to the graph, replacing any existing node with the same name.

        Args:
            node: The Node instance to add.

        Returns:
            None

        Raises:
            TypeError: If the provided object is not a Node instance.

        Examples:
            >>> manipulator.add_node(node)
        """
        if not isinstance(node, Node):
            raise TypeError(f"Object {node} is not a valid Node instance.")
        if self.has_node(node.name):
            self.remove_node(node.name)
        self.graph._nodes[node.name] = node

    def _update_calculation_nodes(self) -> None:
        """Refresh input references for all calculation nodes after structure changes.

        This method re-resolves `input_names` to current Node objects and clears
        individual node caches.

        Returns:
            None
        """
        for nd in self.graph._nodes.values():
            if nd.has_calculation() and hasattr(nd, "input_names") and nd.input_names:
                try:
                    resolved_inputs: list[Node] = []
                    for name in nd.input_names:
                        input_node = self.get_node(name)
                        if input_node is None:
                            raise NodeError(
                                f"Input node '{name}' not found for calculation node '{nd.name}'"
                            )
                        resolved_inputs.append(input_node)
                    nd.inputs = resolved_inputs
                    if hasattr(nd, "clear_cache"):
                        nd.clear_cache()
                except NodeError:
                    logger.exception(f"Error updating inputs for node '{nd.name}'")
                except AttributeError:
                    logger.warning(
                        f"Node '{nd.name}' has input_names but no 'inputs' attribute to update."
                    )

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its unique name.

        Args:
            name: The unique node name to retrieve.

        Returns:
            The Node instance if found, else None.

        Examples:
            >>> manipulator.get_node("Revenue")
        """
        return self.graph._nodes.get(name)

    def replace_node(self, node_name: str, new_node: Node) -> None:
        """Replace an existing node with a new one, ensuring consistency.

        Args:
            node_name: Name of the node to replace.
            new_node: The new Node instance; its name must match `node_name`.

        Returns:
            None

        Raises:
            NodeError: If `node_name` does not exist.
            ValueError: If `new_node.name` does not match `node_name`.

        Examples:
            >>> manipulator.replace_node("Revenue", updated_node)
        """
        if not self.has_node(node_name):
            raise NodeError(f"Node '{node_name}' not found, cannot replace.")
        if node_name != new_node.name:
            raise ValueError("New node name must match the name of the node being replaced.")
        self.remove_node(node_name)
        self.add_node(new_node)

    def has_node(self, node_id: str) -> bool:
        """Check if a node with the given ID exists.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> manipulator.has_node("Revenue")
        """
        return node_id in self.graph._nodes

    def remove_node(self, node_name: str) -> None:
        """Remove a node from the graph and update calculation nodes.

        Args:
            node_name: The name of the node to remove.

        Returns:
            None

        Examples:
            >>> manipulator.remove_node("OldItem")
        """
        if not self.has_node(node_name):
            return
        self.graph._nodes.pop(node_name, None)
        self._update_calculation_nodes()

    def set_value(self, node_id: str, period: str, value: float) -> None:
        """Set the value for a specific node and period, clearing all caches.

        Args:
            node_id: The name of the node.
            period: The time period identifier.
            value: The numeric value to assign.

        Returns:
            None

        Raises:
            ValueError: If `period` is not recognized by the graph.
            NodeError: If the node does not exist.
            TypeError: If the node does not support setting a value.

        Examples:
            >>> manipulator.set_value("Revenue", "2023", 1100.0)
        """
        if period not in self.graph._periods:
            raise ValueError(f"Period '{period}' not in graph periods")
        nd = self.get_node(node_id)
        if not nd:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if not hasattr(nd, "set_value"):
            raise TypeError(
                f"Node '{node_id}' of type {type(nd).__name__} does not support set_value."
            )
        nd.set_value(period, value)
        self.graph.clear_all_caches()

    def clear_all_caches(self) -> None:
        """Clear caches associated with individual nodes in the graph.

        Returns:
            None

        Examples:
            >>> manipulator.clear_all_caches()
        """
        for nd in self.graph._nodes.values():
            if hasattr(nd, "clear_cache"):
                nd.clear_cache()

# --- END FILE: fin_statement_model/core/graph/manipulator.py ---

# --- START FILE: fin_statement_model/core/graph/traverser.py ---
"""Provide graph traversal and validation utilities.

This module defines the GraphTraverser class, encapsulating read-only graph traversal helpers.
"""

import logging
from typing import Optional, Any
from collections import deque

from fin_statement_model.core.errors import NodeError
from fin_statement_model.core.nodes import Node

logger = logging.getLogger(__name__)


class GraphTraverser:
    """Encapsulate traversal and validation helpers for Graph.

    Attributes:
        graph: The Graph instance this traverser operates on.
    """

    def __init__(self, graph: Any) -> None:
        """Initialize the GraphTraverser with a Graph reference.

        Args:
            graph: The Graph instance to traverse.
        """
        self.graph = graph

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its name.

        Args:
            name: The unique name of the node to retrieve.

        Returns:
            The Node instance if found, else None.

        Examples:
            >>> traverser.get_node("Revenue")
        """
        return self.graph.manipulator.get_node(name)

    def has_node(self, node_id: str) -> bool:
        """Check if a node exists in the graph.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> traverser.has_node("Revenue")
        """
        return self.graph.manipulator.has_node(node_id)

    @property
    def nodes(self) -> dict[str, Node]:
        """Access the full node registry dictionary.

        Returns:
            A dict mapping node names to Node instances.

        Examples:
            >>> list(traverser.nodes.keys())
        """
        return self.graph.nodes

    def get_direct_successors(self, node_id: str) -> list[str]:
        """Get immediate successor node IDs for a given node.

        Args:
            node_id: The name of the node whose successors to retrieve.

        Returns:
            A list of node IDs that directly follow the given node.

        Examples:
            >>> traverser.get_direct_successors("Revenue")
        """
        successors: list[str] = []
        for other_id, node in self.nodes.items():
            if hasattr(node, "inputs"):
                input_nodes: list[Node] = []
                if isinstance(node.inputs, list):
                    input_nodes = node.inputs
                elif isinstance(node.inputs, dict):
                    input_nodes = list(node.inputs.values())

                if any(inp.name == node_id for inp in input_nodes if hasattr(inp, "name")):
                    successors.append(other_id)
        return successors

    def get_direct_predecessors(self, node_id: str) -> list[str]:
        """Get immediate predecessor node IDs (dependencies) for a given node.

        Args:
            node_id: The name of the node whose dependencies to retrieve.

        Returns:
            A list of node IDs that the given node depends on.

        Raises:
            NodeError: If the node does not exist.

        Examples:
            >>> traverser.get_direct_predecessors("GrossProfit")
        """
        node = self.get_node(node_id)
        if not node:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if hasattr(node, "inputs"):
            return [inp.name for inp in node.inputs]
        return []

    def topological_sort(self) -> list[str]:
        """Perform a topological sort of nodes based on dependencies.

        Returns:
            A list of node IDs in topological order.

        Raises:
            ValueError: If a cycle is detected in the graph.

        Examples:
            >>> traverser.topological_sort()
        """
        in_degree: dict[str, int] = {n: 0 for n in self.nodes}
        adjacency: dict[str, list[str]] = {n: [] for n in self.nodes}
        for name, node in self.nodes.items():
            if hasattr(node, "inputs"):
                for inp in node.inputs:
                    adjacency[inp.name].append(name)
                    in_degree[name] += 1
        queue: list[str] = [n for n, d in in_degree.items() if d == 0]
        topo_order: list[str] = []
        while queue:
            current = queue.pop()
            topo_order.append(current)
            for nbr in adjacency[current]:
                in_degree[nbr] -= 1
                if in_degree[nbr] == 0:
                    queue.append(nbr)
        if len(topo_order) != len(self.nodes):
            raise ValueError("Cycle detected in graph, can't do a valid topological sort.")
        return topo_order

    def get_calculation_nodes(self) -> list[str]:
        """Identify all nodes in the graph that represent calculations.

        Returns:
            A list of node IDs for nodes with calculations.

        Examples:
            >>> traverser.get_calculation_nodes()
        """
        return [node_id for node_id, node in self.nodes.items() if node.has_calculation()]

    def get_dependencies(self, node_id: str) -> list[str]:
        """Retrieve the direct dependencies (inputs) of a specific node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node depends on.

        Raises:
            NodeError: If the node does not exist.

        Examples:
            >>> traverser.get_dependencies("GrossProfit")
        """
        node = self.get_node(node_id)
        if not node:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if hasattr(node, "inputs"):
            return [inp.name for inp in node.inputs]
        return []

    def get_dependency_graph(self) -> dict[str, list[str]]:
        """Construct a representation of the full dependency graph.

        Returns:
            A dict mapping each node ID to its list of dependency node IDs.

        Examples:
            >>> traverser.get_dependency_graph()
        """
        dependencies: dict[str, list[str]] = {}
        for node_id, node in self.nodes.items():
            try:
                if hasattr(node, "inputs"):
                    dependencies[node_id] = [inp.name for inp in node.inputs]
                else:
                    dependencies[node_id] = []
            except NodeError:
                dependencies[node_id] = []
        return dependencies

    def detect_cycles(self) -> list[list[str]]:
        """Detect all cycles present in the graph's dependency structure.

        Returns:
            A list of cycles, each cycle is a list of node IDs forming the cycle.

        Examples:
            >>> traverser.detect_cycles()
        """
        dependency_graph = self.get_dependency_graph()
        visited: set[str] = set()
        rec_stack: set[str] = set()
        cycles: list[list[str]] = []

        def dfs_detect_cycles(n_id: str, path: Optional[list[str]] = None) -> None:
            if path is None:
                path = []
            if n_id in rec_stack:
                cycle_start = path.index(n_id)
                cycle = path[cycle_start:] + [n_id]
                if cycle not in cycles:
                    cycles.append(cycle)
                return
            if n_id in visited:
                return
            visited.add(n_id)
            rec_stack.add(n_id)
            path.append(n_id)
            for dep in dependency_graph.get(n_id, []):
                dfs_detect_cycles(dep, path[:])
            rec_stack.remove(n_id)

        for node_id in self.nodes:
            if node_id not in visited:
                dfs_detect_cycles(node_id)
        return cycles

    def validate(self) -> list[str]:
        """Perform validation checks on the graph structure.

        Returns:
            A list of validation error messages; empty list if graph is valid.

        Examples:
            >>> traverser.validate()
        """
        errors: list[str] = [
            f"Circular dependency detected: {' -> '.join(cycle)}" for cycle in self.detect_cycles()
        ]
        errors.extend(
            f"Node '{node_id}' depends on non-existent node '{inp.name}'"
            for node_id, node in self.nodes.items()
            if hasattr(node, "inputs")
            for inp in node.inputs
            if not self.has_node(inp.name)
        )
        return errors

    def breadth_first_search(self, start_node: str, direction: str = "successors") -> list[str]:
        """Perform a breadth-first search (BFS) traversal of the graph.

        Args:
            start_node: The starting node ID for the traversal.
            direction: The traversal direction, either 'successors' or 'predecessors'.

        Returns:
            A list of levels, each level is a list of node IDs visited at that depth.

        Raises:
            ValueError: If `direction` is not 'successors' or 'predecessors'.

        Examples:
            >>> traverser.breadth_first_search("Revenue", "successors")
        """
        if direction not in ["successors", "predecessors"]:
            raise ValueError("Invalid direction. Use 'successors' or 'predecessors'.")

        visited = set()
        queue = deque([start_node])
        visited.add(start_node)
        traversal_order = []

        while queue:
            level_size = len(queue)
            current_level = []

            for _ in range(level_size):
                n_id = queue.popleft()
                current_level.append(n_id)

                if direction == "successors":
                    for successor in self.get_direct_successors(n_id):
                        if successor not in visited:
                            visited.add(successor)
                            queue.append(successor)
                elif direction == "predecessors":
                    for predecessor in self.get_direct_predecessors(n_id):
                        if predecessor not in visited:
                            visited.add(predecessor)
                            queue.append(predecessor)

            traversal_order.append(current_level)

        return traversal_order

# --- END FILE: fin_statement_model/core/graph/traverser.py ---

# --- START FILE: fin_statement_model/core/metrics/__init__.py ---
"""Financial Metrics Module.

This module provides a comprehensive system for defining, calculating, and interpreting
financial metrics. It includes:

- MetricDefinition: Pydantic model for metric definitions
- MetricRegistry: Registry for loading and managing metrics
- MetricInterpreter: System for interpreting metric values with ratings
- Built-in metrics: 75+ professional financial metrics organized by category

The metrics are organized into logical categories:
- Liquidity: Current ratio, quick ratio, working capital analysis
- Leverage: Debt ratios, coverage ratios, capital structure
- Profitability: Margins, returns on assets/equity/capital
- Efficiency: Asset turnover, working capital efficiency
- Valuation: Price multiples, enterprise value ratios
- Cash Flow: Cash generation, cash returns, quality metrics
- Growth: Revenue, earnings, asset growth rates
- Credit Risk: Altman Z-scores, warning flags
- Advanced: DuPont analysis, specialized ratios
"""

import logging
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from fin_statement_model.core.nodes.base import Node

from .models import MetricDefinition, MetricInterpretation
from .registry import MetricRegistry, metric_registry
from .interpretation import MetricInterpreter, MetricRating, interpret_metric

logger = logging.getLogger(__name__)

# Load metrics from the organized structure by default
try:
    # Load from organized structure
    organized_path = Path(__file__).parent / "builtin_organized"
    if organized_path.exists():
        logger.info("Loading metrics from organized structure")
        from .builtin_organized import load_organized_metrics

        organized_count = load_organized_metrics()
        logger.info(f"Successfully loaded {organized_count} metrics from organized structure")
    else:
        logger.warning("Organized metric structure not found - no metrics loaded")

except Exception:
    logger.exception("Failed to load built-in metrics from organized structure")


def calculate_metric(
    metric_name: str,
    data_nodes: dict[str, "Node"],
    period: str,
    node_name: str | None = None,
) -> float:
    """Calculate a metric value using the metric registry and data nodes.

    This helper function simplifies the common pattern of:
    1. Getting a metric definition from the registry
    2. Creating a FormulaCalculationNode with the appropriate inputs
    3. Calculating the result for a specific period

    Args:
        metric_name: Name of the metric in the registry (e.g., "debt_yield")
        data_nodes: Dictionary mapping node names to Node instances
        period: Time period for calculation (e.g., "2023")
        node_name: Optional name for the calculation node (defaults to metric_name)

    Returns:
        The calculated metric value as a float

    Raises:
        KeyError: If the metric is not found in the registry
        ValueError: If required input nodes are missing from data_nodes
        CalculationError: If the calculation fails

    Examples:
        >>> data_nodes = {
        ...     "net_operating_income": FinancialStatementItemNode("noi", {"2023": 1000000}),
        ...     "total_debt": FinancialStatementItemNode("debt", {"2023": 10000000})
        ... }
        >>> debt_yield = calculate_metric("debt_yield", data_nodes, "2023")
        >>> print(f"Debt Yield: {debt_yield:.1f}%")
        Debt Yield: 10.0%
    """
    # Import here to avoid circular imports
    from fin_statement_model.core.nodes.calculation_nodes import FormulaCalculationNode

    # Get metric definition from registry
    try:
        metric_def = metric_registry.get(metric_name)
    except KeyError:
        available_metrics = metric_registry.list_metrics()
        raise KeyError(
            f"Metric '{metric_name}' not found in registry. "
            f"Available metrics: {available_metrics[:10]}..."  # Show first 10
        )

    # Build input mapping for the formula
    inputs = {}
    missing_inputs = []

    for input_name in metric_def.inputs:
        if input_name in data_nodes:
            inputs[input_name] = data_nodes[input_name]
        else:
            missing_inputs.append(input_name)

    if missing_inputs:
        available_nodes = list(data_nodes.keys())
        raise ValueError(
            f"Missing required input nodes for metric '{metric_name}': {missing_inputs}. "
            f"Available nodes: {available_nodes}"
        )

    # Create calculation node
    calc_node_name = node_name or f"{metric_name}_calc"
    calc_node = FormulaCalculationNode(
        calc_node_name,
        inputs=inputs,
        formula=metric_def.formula,
        metric_name=metric_name,
        metric_description=metric_def.description,
    )

    # Calculate and return result
    return calc_node.calculate(period)


__all__ = [
    "MetricDefinition",
    "MetricInterpretation",
    "MetricInterpreter",
    "MetricRating",
    "MetricRegistry",
    "calculate_metric",
    "interpret_metric",
    "metric_registry",
]

# --- END FILE: fin_statement_model/core/metrics/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/__init__.py ---
"""Organized Built-in Metrics Package.

This package contains financial metrics organized by analytical category for easier
maintenance and understanding. All metrics are automatically loaded into the
metric_registry when this package is imported.
"""

import logging
from pathlib import Path
from typing import Optional

from fin_statement_model.core.metrics.registry import metric_registry

logger = logging.getLogger(__name__)


def load_organized_metrics(base_path: Optional[Path] = None) -> int:
    """Load all metrics from the organized structure.

    Args:
        base_path: Base path to the builtin_organized directory. If None, uses default.

    Returns:
        Total number of metrics loaded.
    """
    if base_path is None:
        base_path = Path(__file__).parent

    total_loaded = 0

    # Define the organized metric files to load
    metric_files = [
        # Liquidity metrics
        "liquidity/ratios.yaml",
        "liquidity/working_capital.yaml",
        # Leverage metrics
        "leverage/debt_ratios.yaml",
        "leverage/net_leverage.yaml",
        # Coverage metrics
        "coverage/interest_coverage.yaml",
        "coverage/debt_service.yaml",
        # Profitability metrics
        "profitability/margins.yaml",
        "profitability/returns.yaml",
        # Efficiency metrics
        "efficiency/asset_turnover.yaml",
        "efficiency/component_turnover.yaml",
        # Valuation metrics
        "valuation/price_multiples.yaml",
        "valuation/enterprise_multiples.yaml",
        "valuation/yields.yaml",
        # Cash flow metrics
        "cash_flow/generation.yaml",
        "cash_flow/returns.yaml",
        # Growth metrics
        "growth/growth_rates.yaml",
        # Per share metrics
        "per_share/per_share_metrics.yaml",
        # Credit risk metrics
        "credit_risk/altman_scores.yaml",
        "credit_risk/warning_flags.yaml",
        # Advanced metrics
        "advanced/dupont_analysis.yaml",
        # Special calculated items
        "special/gross_profit.yaml",
        "special/net_income.yaml",
        "special/retained_earnings.yaml",
        # Real estate metrics
        "real_estate/operational_metrics.yaml",
        "real_estate/valuation_metrics.yaml",
        "real_estate/per_share_metrics.yaml",
        "real_estate/debt_metrics.yaml",
        # Banking metrics
        "banking/asset_quality.yaml",
        "banking/capital_adequacy.yaml",
        "banking/profitability.yaml",
        "banking/liquidity.yaml",
    ]

    # Collect unique parent directories from the metric files
    unique_directories = set()
    for file_path in metric_files:
        full_path = base_path / file_path
        if full_path.exists():
            unique_directories.add(full_path.parent)
        else:
            logger.warning(f"Organized metric file not found: {full_path}")

    # Load metrics from each unique directory
    for directory in unique_directories:
        try:
            # load_metrics_from_directory returns the count of metrics loaded
            metrics_count = metric_registry.load_metrics_from_directory(directory)
            total_loaded += metrics_count
            logger.debug(f"Loaded {metrics_count} metrics from {directory}")
        except Exception:
            logger.exception(f"Failed to load metrics from directory {directory}")

    logger.info(f"Loaded {total_loaded} total metrics from organized structure")
    return total_loaded


# Auto-load on import
try:
    load_organized_metrics()
except Exception as e:
    logger.warning(f"Failed to auto-load organized metrics: {e}")

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/advanced/__init__.py ---
"""Advanced Metrics."""

# Advanced metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/advanced/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/banking/__init__.py ---
"""Banking Metrics."""

# Banking metrics organized by subcategory:
# - asset_quality.yaml: NPL ratios, charge-offs, provision coverage
# - capital_adequacy.yaml: Tier 1/2 capital ratios, leverage ratios
# - profitability.yaml: Net interest margin, efficiency ratio, ROA/ROE
# - liquidity.yaml: LCR, NSFR, deposit composition
# - credit_risk.yaml: Credit loss rates, risk-weighted asset ratios
# - regulatory.yaml: Regulatory compliance and stress test metrics

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/banking/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/cash_flow/__init__.py ---
"""Cash_Flow Metrics."""

# Cash_Flow metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/cash_flow/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/coverage/__init__.py ---
"""Coverage Metrics."""

# Coverage metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/coverage/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/credit_risk/__init__.py ---
"""Credit_Risk Metrics."""

# Credit_Risk metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/credit_risk/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/efficiency/__init__.py ---
"""Efficiency Metrics."""

# Efficiency metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/efficiency/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/growth/__init__.py ---
"""Growth Metrics."""

# Growth metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/growth/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/leverage/__init__.py ---
"""Leverage Metrics."""

# Leverage metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/leverage/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/liquidity/__init__.py ---
"""Liquidity Metrics."""

# Liquidity metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/liquidity/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/per_share/__init__.py ---
"""Per_Share Metrics."""

# Per_Share metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/per_share/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/profitability/__init__.py ---
"""Profitability Metrics."""

# Profitability metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/profitability/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/real_estate/__init__.py ---
"""Real Estate Metrics."""

# Real estate metrics organized by subcategory:
# - operational_metrics.yaml: NOI, FFO, AFFO, occupancy
# - valuation_metrics.yaml: Cap rates, price per SF, REIT multiples
# - per_share_metrics.yaml: FFO/share, AFFO/share, NAV/share
# - debt_metrics.yaml: LTV, DSCR, debt yield, interest coverage

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/real_estate/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/builtin_organized/valuation/__init__.py ---
"""Valuation Metrics."""

# Valuation metrics organized by subcategory

# --- END FILE: fin_statement_model/core/metrics/builtin_organized/valuation/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/interpretation.py ---
"""Utilities for interpreting metric values based on defined guidelines."""

from enum import Enum
from typing import Any
from fin_statement_model.core.metrics.models import (
    MetricDefinition,
)


class MetricRating(Enum):
    """Rating levels for metric values."""

    EXCELLENT = "excellent"
    GOOD = "good"
    ADEQUATE = "adequate"
    WARNING = "warning"
    POOR = "poor"
    UNKNOWN = "unknown"


class MetricInterpreter:
    """Interprets metric values based on defined guidelines."""

    def __init__(self, metric_definition: MetricDefinition):
        """Initialize with a metric definition.

        Args:
            metric_definition: The metric definition containing interpretation guidelines.
        """
        self.metric_definition = metric_definition
        self.interpretation = metric_definition.interpretation

    def rate_value(self, value: float) -> MetricRating:
        """Rate a metric value based on interpretation guidelines.

        Args:
            value: The metric value to rate.

        Returns:
            MetricRating indicating the quality of the value.
        """
        if not self.interpretation:
            return MetricRating.UNKNOWN

        # Check for excellent rating
        if (
            self.interpretation.excellent_above is not None
            and value >= self.interpretation.excellent_above
        ):
            return MetricRating.EXCELLENT

        # Check for poor rating
        if self.interpretation.poor_below is not None and value < self.interpretation.poor_below:
            return MetricRating.POOR

        # Check for warning conditions
        warning_conditions = []
        if (
            self.interpretation.warning_below is not None
            and value < self.interpretation.warning_below
        ):
            warning_conditions.append("below_threshold")

        if (
            self.interpretation.warning_above is not None
            and value > self.interpretation.warning_above
        ):
            warning_conditions.append("above_threshold")

        if warning_conditions:
            return MetricRating.WARNING

        # Check if in good range
        if self.interpretation.good_range is not None and len(self.interpretation.good_range) == 2:
            min_good, max_good = self.interpretation.good_range
            if min_good <= value <= max_good:
                return MetricRating.GOOD

        # Default to adequate if no specific conditions met
        return MetricRating.ADEQUATE

    def get_interpretation_message(self, value: float) -> str:
        """Get a human-readable interpretation message for a metric value.

        Args:
            value: The metric value to interpret.

        Returns:
            A descriptive message about the metric value.
        """
        rating = self.rate_value(value)

        # Base message based on rating
        rating_messages = {
            MetricRating.EXCELLENT: f"Excellent performance: {value:.2f}",
            MetricRating.GOOD: f"Good performance: {value:.2f}",
            MetricRating.ADEQUATE: f"Adequate performance: {value:.2f}",
            MetricRating.WARNING: f"Warning level: {value:.2f}",
            MetricRating.POOR: f"Poor performance: {value:.2f}",
            MetricRating.UNKNOWN: f"Value: {value:.2f} (no interpretation guidelines available)",
        }

        return rating_messages[rating]

    def get_detailed_analysis(self, value: float) -> dict[str, Any]:
        """Get a detailed analysis of a metric value.

        Args:
            value: The metric value to analyze.

        Returns:
            Dictionary containing detailed analysis information.
        """
        rating = self.rate_value(value)

        analysis: dict[str, Any] = {
            "value": value,
            "rating": rating.value,
            "metric_name": self.metric_definition.name,
            "units": self.metric_definition.units,
            "category": self.metric_definition.category,
            "interpretation_message": self.get_interpretation_message(value),
        }

        # Add interpretation details if available
        if self.interpretation:
            analysis["guidelines"] = {
                "good_range": self.interpretation.good_range,
                "warning_below": self.interpretation.warning_below,
                "warning_above": self.interpretation.warning_above,
                "excellent_above": self.interpretation.excellent_above,
                "poor_below": self.interpretation.poor_below,
            }

            if self.interpretation.notes:
                analysis["notes"] = self.interpretation.notes

        # Add related metrics for context
        if self.metric_definition.related_metrics:
            analysis["related_metrics"] = self.metric_definition.related_metrics

        return analysis


def interpret_metric(metric_definition: MetricDefinition, value: float) -> dict[str, Any]:
    """Convenience function to interpret a metric value.

    Args:
        metric_definition: The metric definition.
        value: The value to interpret.

    Returns:
        Detailed interpretation analysis.
    """
    interpreter = MetricInterpreter(metric_definition)
    return interpreter.get_detailed_analysis(value)

# --- END FILE: fin_statement_model/core/metrics/interpretation.py ---

# --- START FILE: fin_statement_model/core/metrics/models.py ---
"""Models for metric definitions."""

from typing import Optional, Any
from pydantic import BaseModel, Field, model_validator
from pydantic import ConfigDict


class MetricInterpretation(BaseModel):
    """Interpretation guidelines for a metric."""

    good_range: Optional[list[float]] = Field(
        None, description="Range of values considered good [min, max]"
    )
    warning_below: Optional[float] = Field(
        None, description="Value below which a warning should be issued"
    )
    warning_above: Optional[float] = Field(
        None, description="Value above which a warning should be issued"
    )
    excellent_above: Optional[float] = Field(
        None, description="Value above which the metric is considered excellent"
    )
    poor_below: Optional[float] = Field(
        None, description="Value below which the metric is considered poor"
    )
    notes: Optional[str] = Field(None, description="Additional interpretation notes and context")


class MetricDefinition(BaseModel):
    """Schema for one metric definition loaded from YAML."""

    name: str = Field(..., min_length=1, description="The name of the metric")
    description: str = Field(
        ..., min_length=1, max_length=500, description="The description of the metric"
    )
    inputs: list[str] = Field(..., min_length=1, description="The inputs of the metric")
    formula: str = Field(..., min_length=1, description="The formula of the metric")
    tags: list[str] = Field(default_factory=list, description="The tags of the metric")
    units: Optional[str] = Field(None, description="The units of the metric")
    category: Optional[str] = Field(
        None, description="Category of the metric (e.g., liquidity, profitability)"
    )
    interpretation: Optional[MetricInterpretation] = Field(
        None, description="Guidelines for interpreting the metric values"
    )
    related_metrics: Optional[list[str]] = Field(
        None, description="Names of related metrics that should be considered together"
    )

    model_config = ConfigDict(extra="forbid", frozen=False)

    @model_validator(mode="before")
    def _strip_whitespace(cls, values: dict[str, Any]) -> dict[str, Any]:
        # tiny quality-of-life clean-up
        for k, v in values.items():
            if isinstance(v, str):
                values[k] = v.strip()
        return values

# --- END FILE: fin_statement_model/core/metrics/models.py ---

# --- START FILE: fin_statement_model/core/metrics/registry.py ---
"""Manage loading and accessing metric definitions from YAML files.

This module provides a registry to discover, validate, and retrieve
metric definitions from YAML files and associate them with calculation classes.
"""

import logging
from pathlib import Path
from typing import ClassVar, Union

# Use a try-except block for the YAML import
try:
    import yaml

    HAS_YAML = True
except ImportError:
    HAS_YAML = False

from pydantic import ValidationError

from fin_statement_model.core.metrics.models import MetricDefinition

logger = logging.getLogger(__name__)


class MetricRegistry:
    """Manage loading and accessing metric definitions from YAML files.

    This includes discovering YAML definitions, validating their structure,
    and providing retrieval methods by metric ID.
    """

    _REQUIRED_FIELDS: ClassVar[list[str]] = ["inputs", "formula", "description", "name"]

    def __init__(self) -> None:
        """Initialize the MetricRegistry with an empty metrics store.

        Examples:
            >>> registry = MetricRegistry()
            >>> len(registry)
            0
        """
        self._metrics: dict[str, MetricDefinition] = {}
        logger.info("MetricRegistry initialized.")

    def load_metrics_from_directory(self, directory_path: Union[str, Path]) -> int:
        """Load all metric definitions from a directory.

        This method searches for '*.yaml' files, validates their content,
        and stores them in the registry. Each YAML file can contain either:
        - A single metric definition (a YAML dictionary at the root)
        - A list of metric definitions (a YAML list of dictionaries at the root)

        Args:
            directory_path: Path to the directory containing metric YAML files.

        Returns:
            The number of metrics successfully loaded.

        Raises:
            ImportError: If PyYAML is not installed.
            FileNotFoundError: If the directory_path does not exist.
            ConfigurationError: If a YAML file is invalid or missing required fields.

        Examples:
            >>> registry = MetricRegistry()
            >>> count = registry.load_metrics_from_directory("./metrics")
            >>> print(f"Loaded {count} metrics.")
        """
        if not HAS_YAML:
            logger.error("PyYAML is required to load metrics from YAML files. Please install it.")
            raise ImportError("PyYAML is required to load metrics from YAML files.")

        dir_path = Path(directory_path)
        if not dir_path.is_dir():
            logger.error(f"Metric directory not found: {dir_path}")
            raise FileNotFoundError(f"Metric directory not found: {dir_path}")

        logger.info(f"Loading metrics from directory: {dir_path}")
        loaded_count = 0

        for filepath in dir_path.glob("*.yaml"):
            logger.debug(f"Processing file: {filepath}")
            try:
                with open(filepath, encoding="utf-8") as f:
                    content = f.read()

                # Use standard YAML parsing
                try:
                    data = yaml.safe_load(content)
                except yaml.YAMLError as e:
                    logger.warning(f"Failed to parse YAML file {filepath}: {e}")
                    continue

                if not data:
                    logger.debug(f"Empty or null content in {filepath}, skipping")
                    continue

                # Handle both single metric and list of metrics
                metrics_to_process = []

                if isinstance(data, dict):
                    # Single metric definition
                    metrics_to_process = [data]
                elif isinstance(data, list):
                    # List of metric definitions
                    metrics_to_process = data
                else:
                    logger.warning(
                        f"Invalid YAML structure in {filepath}: "
                        f"expected dict or list, got {type(data).__name__}"
                    )
                    continue

                # Process each metric definition
                for i, metric_data in enumerate(metrics_to_process):
                    if not isinstance(metric_data, dict):
                        logger.warning(
                            f"Invalid metric definition at index {i} in {filepath}: "
                            f"expected dict, got {type(metric_data).__name__}"
                        )
                        continue

                    try:
                        # Validate and register the metric
                        model = MetricDefinition.model_validate(metric_data)
                        self.register_definition(model)
                        loaded_count += 1
                        logger.debug(f"Successfully loaded metric '{model.name}' from {filepath}")

                    except ValidationError as ve:
                        logger.warning(
                            f"Invalid metric definition at index {i} in {filepath}: {ve}"
                        )
                        continue

            except Exception:
                logger.exception(f"Failed to process file {filepath}")
                continue

        logger.info(f"Successfully loaded {loaded_count} metrics from {dir_path}.")
        return loaded_count

    def get(self, metric_id: str) -> MetricDefinition:
        """Retrieve a loaded metric definition by its ID.

        Args:
            metric_id: Identifier of the metric (filename stem).

        Returns:
            A MetricDefinition object containing the metric definition.

        Raises:
            KeyError: If the metric_id is not found in the registry.

        Examples:
            >>> definition = registry.get("gross_profit")
            >>> print(definition["formula"])
        """
        try:
            return self._metrics[metric_id]
        except KeyError:
            logger.warning(f"Metric ID '{metric_id}' not found in registry.")
            raise KeyError(f"Metric ID '{metric_id}' not found. Available: {self.list_metrics()}")

    def list_metrics(self) -> list[str]:
        """Get a sorted list of all loaded metric IDs.

        Returns:
            A sorted list of available metric IDs.

        Examples:
            >>> registry.list_metrics()
            ['current_ratio', 'debt_equity_ratio']
        """
        return sorted(self._metrics.keys())

    def __len__(self) -> int:
        """Return the number of loaded metrics.

        Returns:
            The count of metrics loaded into the registry.

        Examples:
            >>> len(registry)
            5
        """
        return len(self._metrics)

    def __contains__(self, metric_id: str) -> bool:
        """Check if a metric ID exists in the registry.

        Args:
            metric_id: The metric identifier to check.

        Returns:
            True if the metric is present, False otherwise.

        Examples:
            >>> 'current_ratio' in registry
        """
        return metric_id in self._metrics

    def register_definition(self, definition: MetricDefinition) -> None:
        """Register a single metric definition.

        Args:
            definition: The metric definition to register.
        """
        metric_id = definition.name.lower().replace(" ", "_").replace("-", "_")
        if metric_id in self._metrics:
            logger.debug(f"Overwriting existing metric definition for '{metric_id}'")
        self._metrics[metric_id] = definition
        logger.debug(f"Registered metric definition: {metric_id}")


# Create the singleton instance (without auto-loading to prevent duplicates)
metric_registry = MetricRegistry()

# --- END FILE: fin_statement_model/core/metrics/registry.py ---

# --- START FILE: fin_statement_model/core/node_factory.py ---
"""Provide a factory for creating nodes in the financial statement model.

This module centralizes node creation logic and ensures consistent initialization
for different types of nodes used in the financial statement model.
"""

import logging
from typing import Any, Union, Optional, ClassVar
from collections.abc import Callable

# Force import of strategies package to ensure registration happens

from .nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    CustomCalculationNode,
)
from .nodes.forecast_nodes import (
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
)

# Force import of calculations package to ensure registration happens
from fin_statement_model.core.calculations import Registry, Calculation
from fin_statement_model.core.errors import ConfigurationError

# Configure logging
logger = logging.getLogger(__name__)


class NodeFactory:
    """Provide a factory for creating nodes in the financial statement model.

    This class centralizes node creation for financial statement items,
    calculations, metrics, forecasts, and custom logic.

    Attributes:
        _calculation_methods: Maps simple string keys (e.g., 'addition') to
            the class names of Calculation implementations registered in the
            `Registry`. This allows creating CalculationNodes without
            directly importing Calculation classes.
    """

    # Mapping of calculation type strings to Calculation class names (keys in the Registry)
    _calculation_methods: ClassVar[dict[str, str]] = {
        "addition": "AdditionCalculation",
        "subtraction": "SubtractionCalculation",
        "formula": "FormulaCalculation",
        "division": "DivisionCalculation",
        "weighted_average": "WeightedAverageCalculation",
        "custom_formula": "CustomFormulaCalculation",
    }

    # Mapping from node type names to Node classes
    _node_types: ClassVar[dict[str, type[Node]]] = {
        "financial_statement_item": FinancialStatementItemNode,
    }

    @classmethod
    def create_financial_statement_item(
        cls, name: str, values: dict[str, float]
    ) -> FinancialStatementItemNode:
        """Create a FinancialStatementItemNode representing a base financial item.

        This node holds historical or projected values for a specific
        line item (e.g., Revenue, COGS) over different periods.

        Args:
            name: Identifier for the node (e.g., "Revenue").
            values: Mapping of period identifiers to numerical values.

        Returns:
            A FinancialStatementItemNode initialized with the provided values.

        Raises:
            ValueError: If the provided name is empty or not a string.

        Examples:
            >>> revenue_node = NodeFactory.create_financial_statement_item(
            ...     name="Revenue",
            ...     values={"2023": 1000.0, "2024": 1100.0}
            ... )
            >>> revenue_node.get_value("2023")
            1000.0
        """
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        logger.debug(f"Creating financial statement item node: {name}")
        return FinancialStatementItemNode(name, values)

    @classmethod
    def create_calculation_node(
        cls,
        name: str,
        inputs: list[Node],
        calculation_type: str,
        formula_variable_names: Optional[list[str]] = None,
        **calculation_kwargs: Any,
    ) -> CalculationNode:
        """Create a CalculationNode using a pre-defined calculation.

        This method resolves a calculation class from a calculation_type key,
        instantiates it with optional parameters, and wraps it in
        a CalculationNode.

        Args:
            name: Identifier for the calculation node instance.
            inputs: List of Node instances serving as inputs to the calculation.
            calculation_type: Key for the desired calculation in the registry.
            formula_variable_names: Optional list of variable names used in the formula
                string. Required & used only if creating a FormulaCalculationNode
                via the 'custom_formula' type with a 'formula' kwarg.
            **calculation_kwargs: Additional parameters for the calculation constructor.

        Returns:
            A CalculationNode configured with the selected calculation.

        Raises:
            ValueError: If name is invalid, inputs list is empty, or the
                calculation_type is unrecognized.
            TypeError: If the calculation cannot be instantiated with given kwargs.

        Examples:
            >>> gross_profit = NodeFactory.create_calculation_node(
            ...     name="GrossProfit",
            ...     inputs=[revenue, cogs],
            ...     calculation_type="subtraction"
            ... )
        """
        # Validate inputs
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        if not inputs:
            raise ValueError("Calculation node must have at least one input")

        # Check if the calculation type maps to a known calculation name
        if calculation_type not in cls._calculation_methods:
            valid_types = list(cls._calculation_methods.keys())
            raise ValueError(
                f"Invalid calculation type: '{calculation_type}'. Valid types are: {valid_types}"
            )

        # Get the calculation name
        calculation_name = cls._calculation_methods[calculation_type]

        # For other types, resolve the Calculation class from the registry
        try:
            calculation_cls: type[Calculation] = Registry.get(calculation_name)
        except KeyError:
            # This should ideally not happen if _calculation_methods is synced with registry
            raise ValueError(
                f"Calculation class '{calculation_name}' (for type '{calculation_type}') not found in Registry."
            ) from None  # Prevent chaining the KeyError

        # Instantiate the calculation, passing any extra kwargs
        try:
            # Extract any metadata that should be stored on the node, not passed to calculation
            node_kwargs = {}
            if "metric_name" in calculation_kwargs:
                node_kwargs["metric_name"] = calculation_kwargs.pop("metric_name")
            if "metric_description" in calculation_kwargs:
                node_kwargs["metric_description"] = calculation_kwargs.pop("metric_description")

            # Special handling for FormulaCalculation which needs input_variable_names
            if calculation_type == "formula":
                if formula_variable_names and len(formula_variable_names) == len(inputs):
                    calculation_kwargs["input_variable_names"] = formula_variable_names
                elif not formula_variable_names:
                    # Generate default names like var_0, var_1, ...
                    calculation_kwargs["input_variable_names"] = [
                        f"var_{i}" for i in range(len(inputs))
                    ]
                    logger.warning(
                        f"No formula_variable_names provided for formula node '{name}'. Using defaults: {calculation_kwargs['input_variable_names']}"
                    )
                else:
                    # Mismatch between provided names and number of inputs
                    raise ConfigurationError(
                        f"Mismatch between formula_variable_names ({len(formula_variable_names)}) and number of inputs ({len(inputs)}) for node '{name}'"
                    )

            calculation_instance = calculation_cls(**calculation_kwargs)
        except TypeError as e:
            logger.exception(
                f"Failed to instantiate calculation '{calculation_name}' with kwargs {calculation_kwargs}"
            )
            raise TypeError(
                f"Could not instantiate calculation '{calculation_name}' for node '{name}'. "
                f"Check required arguments for {calculation_cls.__name__}. Provided kwargs: {calculation_kwargs}"
            ) from e

        # Create and return a CalculationNode with the instantiated calculation
        logger.debug(f"Creating calculation node '{name}' with '{calculation_name}' calculation.")

        return CalculationNode(name, inputs, calculation_instance, **node_kwargs)

    @classmethod
    def create_forecast_node(
        cls,
        name: str,
        base_node: Node,
        base_period: str,
        forecast_periods: list[str],
        forecast_type: str,
        growth_params: Union[float, list[float], Callable[[], float]],
    ) -> Node:
        """Create a forecast node of the specified type using core forecast classes.

        Args:
            name: Custom name for the forecast node.
            base_node: The Node instance to base projections on.
            base_period: Period identifier providing the base value.
            forecast_periods: List of periods for which to forecast.
            forecast_type: Forecast method ('simple', 'curve', 'statistical',
                'average', 'historical_growth').
            growth_params: Parameters controlling forecast behavior (float,
                list of floats, or callable). Ignored for 'average' and 'historical_growth'.

        Returns:
            A Node instance implementing the chosen forecast.

        Raises:
            ValueError: If an unsupported forecast_type is provided.

        Examples:
            >>> forecast = NodeFactory.create_forecast_node(
            ...     name="RevForecast",
            ...     base_node=revenue,
            ...     base_period="2023",
            ...     forecast_periods=["2024", "2025"],
            ...     forecast_type="simple",
            ...     growth_params=0.05
            ... )
        """
        # Instantiate the appropriate forecast node
        if forecast_type == "simple":
            node = FixedGrowthForecastNode(base_node, base_period, forecast_periods, growth_params)
        elif forecast_type == "curve":
            node = CurveGrowthForecastNode(base_node, base_period, forecast_periods, growth_params)
        elif forecast_type == "statistical":
            node = StatisticalGrowthForecastNode(
                base_node, base_period, forecast_periods, growth_params
            )
        elif forecast_type == "average":
            node = AverageValueForecastNode(base_node, base_period, forecast_periods)
        elif forecast_type == "historical_growth":
            node = AverageHistoricalGrowthForecastNode(base_node, base_period, forecast_periods)
        else:
            raise ValueError(f"Invalid forecast type: {forecast_type}")

        # Override forecast node's name to match factory 'name' argument
        node.name = name
        logger.debug(f"Forecast node created with custom name: {name} (original: {base_node.name})")
        return node

    @classmethod
    def _create_custom_node_from_callable(
        cls,
        name: str,
        inputs: list[Node],
        formula: Callable,
        description: Optional[str] = None,
    ) -> CustomCalculationNode:
        """Create a CustomCalculationNode using a Python callable for the calculation logic.

        This supports ad-hoc or complex calculations not covered by standard
        strategies or metrics. The `formula` callable will be invoked with
        input node values at calculation time.

        Note:
            Renamed from `create_metric_node` to avoid confusion with metric-based nodes.

        Args:
            name: Identifier for the custom calculation node.
            inputs: List of Node instances providing values to the formula.
            formula: Callable that computes a value from input node values.
            description: Optional description of the calculation logic.

        Returns:
            A CustomCalculationNode configured with the provided formula.

        Raises:
            ValueError: If name is empty or not a string.
            TypeError: If formula is not callable or inputs contain non-Node items.

        Examples:
            >>> def complex_tax_logic(revenue, expenses, tax_rate_node):
            ...     profit = revenue - expenses
            ...     if profit <= 0:
            ...         return 0.0
            ...     tax_rate = tax_rate_node
            ...     return profit * tax_rate
            ...
            >>> tax_node = NodeFactory._create_custom_node_from_callable(
            ...     name="CalculatedTaxes",
            ...     inputs=[revenue_node, expenses_node, tax_rate_schedule_node],
            ...     formula=complex_tax_logic,
            ...     description="Calculates income tax based on profit and a variable rate."
            ... )

            Using a lambda for a simple ratio:
            >>> quick_ratio_node = NodeFactory._create_custom_node_from_callable(
            ...    name="QuickRatioCustom",
            ...    inputs=[cash_node, receivables_node, current_liabilities_node],
            ...    formula=lambda cash, rec, liab: (cash + rec) / liab if liab else 0
            ... )
        """
        # Validate inputs
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        if not inputs:
            # Allowing no inputs might be valid for some custom functions (e.g., constants)
            # Reconsider if this check is always needed here.
            logger.warning(f"Creating CustomCalculationNode '{name}' with no inputs.")
            # raise ValueError("Custom node must have at least one input")

        if not callable(formula):
            raise TypeError("Formula must be a callable function")
        if not all(isinstance(i, Node) for i in inputs):
            raise TypeError("All items in inputs must be Node instances.")

        # Use the imported CustomCalculationNode
        logger.debug(f"Creating CustomCalculationNode: {name} using provided callable.")
        return CustomCalculationNode(name, inputs, formula_func=formula, description=description)

    # Consider adding a method for creating FormulaCalculationNode if needed directly
    # @classmethod
    # def create_formula_node(cls, name: str, inputs: Dict[str, Node], formula: str) -> FormulaCalculationNode:
    #     ...

# --- END FILE: fin_statement_model/core/node_factory.py ---

# --- START FILE: fin_statement_model/core/nodes/__init__.py ---
"""Core Node Implementations for the Financial Statement Model.

This package exports the base `Node` class and various concrete node types
used to build the financial model graph. These include:

- Data Nodes:
    - `FinancialStatementItemNode`: Stores raw numerical data for specific periods.

- Calculation Nodes:
    - `FormulaCalculationNode`: Calculates based on mathematical string formulas.
    - `CalculationNode`: Uses a calculation object for calculation logic.
    - `CustomCalculationNode`: Uses arbitrary Python functions for calculation.

- Statistical Nodes:
    - `YoYGrowthNode`: Calculates year-over-year percentage growth.
    - `MultiPeriodStatNode`: Computes statistics (mean, stddev) over multiple periods.
    - `TwoPeriodAverageNode`: Calculates the average over two specific periods.

- Forecast Nodes:
    - `ForecastNode`: Base class for forecasting nodes.
    - Various forecast implementations for different forecasting strategies.

Standard Node Registry:
The package also provides a registry of standardized node names for financial
statement items, ensuring consistency across models and enabling metrics to work properly.
Standard nodes are organized by category:
- Balance Sheet: Assets, liabilities, equity
- Income Statement: Revenue, expenses, profit measures
- Cash Flow: Operating, investing, financing activities
- Calculated Items: EBITDA, working capital, leverage measures
- Market Data: Stock price, market cap, per-share metrics
"""

import logging
from pathlib import Path

# Import all node classes using actual file names
from .base import Node
from .item_node import FinancialStatementItemNode
from .calculation_nodes import (
    CalculationNode,
    FormulaCalculationNode,
    CustomCalculationNode,
)
from .stats_nodes import (
    YoYGrowthNode,
    MultiPeriodStatNode,
    TwoPeriodAverageNode,
)
from .forecast_nodes import (
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    CustomGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
)

# Import standard registry
from .standard_registry import standard_node_registry

logger = logging.getLogger(__name__)

# Load standard nodes - prefer organized structure, fallback to flat file
_nodes_loaded = False

if not _nodes_loaded:
    try:
        # First try to load from organized structure
        organized_path = Path(__file__).parent / "standard_nodes"
        if organized_path.exists() and (organized_path / "__init__.py").exists():
            logger.info("Loading standard nodes from organized structure")
            from .standard_nodes import load_all_standard_nodes

            organized_count = load_all_standard_nodes()
            logger.info(
                f"Successfully loaded {organized_count} standard nodes from organized structure"
            )
            _nodes_loaded = True
        else:
            # Fallback to flat structure if organized doesn't exist
            logger.info("Organized structure not found, loading from flat standard_nodes.yaml")
            flat_file = Path(__file__).parent / "standard_nodes.yaml"
            if flat_file.exists():
                flat_count = standard_node_registry.load_from_yaml(flat_file)
                logger.info(f"Successfully loaded {flat_count} standard nodes from flat file")
                _nodes_loaded = True
            else:
                logger.warning("No standard node files found in either organized or flat structure")

    except Exception:
        logger.exception("Failed to load standard nodes")
        # Try fallback to flat structure only if organized loading failed
        if not _nodes_loaded:
            try:
                flat_file = Path(__file__).parent / "standard_nodes.yaml"
                if flat_file.exists():
                    flat_count = standard_node_registry.load_from_yaml(flat_file)
                    logger.info(f"Fallback: loaded {flat_count} standard nodes from flat file")
                    _nodes_loaded = True
            except Exception:
                logger.exception("Fallback loading also failed")

__all__ = [
    "AverageHistoricalGrowthForecastNode",
    "AverageValueForecastNode",
    "CalculationNode",
    "CurveGrowthForecastNode",
    "CustomCalculationNode",
    "CustomGrowthForecastNode",
    "FinancialStatementItemNode",
    "FixedGrowthForecastNode",
    "ForecastNode",
    "FormulaCalculationNode",
    "MultiPeriodStatNode",
    "Node",
    "StatisticalGrowthForecastNode",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
    "standard_node_registry",
]

# --- END FILE: fin_statement_model/core/nodes/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/base.py ---
"""Define the abstract base class for all nodes in the graph.

This module provides the Node base class with interfaces for calculation,
attribute access, and optional caching behavior.
"""

from abc import ABC, abstractmethod


class Node(ABC):
    """Define the abstract base class for graph nodes.

    Provide the essential interface for all nodes in the financial statement
    model graph, including calculation, caching, and attribute access.

    Attributes:
    name (str): Unique identifier for the node instance.
    """

    name: str

    def __init__(self, name: str):
        """Initialize the Node instance with a unique name.

        Args:
            name: Unique identifier for the node. Must be a non-empty string.

        Raises:
            ValueError: If `name` is empty or not a string.

        Examples:
            >>> class Dummy(Node):
            ...     def calculate(self, period): return 0.0
            >>> dn = Dummy("Revenue")
            >>> dn.name
            'Revenue'
        """
        # Check if name is a non-empty string
        if not isinstance(name, str) or not name:
            raise ValueError("Node name must be a non-empty string.")
        # Check for invalid characters (including newline, tab)
        if "\n" in name or "\t" in name:
            raise ValueError(f"Invalid node name: '{name}'. Contains invalid characters.")
        # Check for leading/trailing whitespace
        if name != name.strip():
            raise ValueError(
                f"Invalid node name: '{name}'. Cannot have leading/trailing whitespace."
            )
        self.name = name

    @abstractmethod
    def calculate(self, period: str) -> float:
        """Calculate the node's value for a specific period.

        This abstract method must be implemented by subclasses to define how to
        determine the node's value for a given time period.

        Args:
            period: The time period identifier for the calculation.

        Returns:
            The calculated float value for the specified period.

        Raises:
            NotImplementedError: If the subclass does not implement this method.

        Examples:
            >>> class Dummy(Node):
            ...     def calculate(self, period): return 100.0
            >>> d = Dummy("Test")
            >>> d.calculate("2023")
            100.0
        """

    def clear_cache(self):
        """Clear cached calculation results for this node.

        Subclasses with caching should override this method to clear their internal cache.

        Returns:
            None

        Examples:
            >>> node.clear_cache()
        """
        # Default: no cache to clear

    def has_attribute(self, attr_name: str) -> bool:
        """Check if the node has a specific attribute.

        Args:
            attr_name: The name of the attribute to check.

        Returns:
            True if the attribute exists, otherwise False.

        Examples:
            >>> node.has_attribute("name")
            True
        """
        return hasattr(self, attr_name)

    def get_attribute(self, attribute_name: str) -> object:
        """Get a named attribute from the node.

        Args:
            attribute_name: The name of the attribute to retrieve.

        Returns:
            The value of the specified attribute.

        Raises:
            AttributeError: If the attribute does not exist.

        Examples:
            >>> node.get_attribute("name")
            'Revenue'
        """
        try:
            return getattr(self, attribute_name)
        except AttributeError:
            raise AttributeError(f"Node '{self.name}' has no attribute '{attribute_name}'")

    def has_value(self, period: str) -> bool:
        """Indicate whether the node stores a direct value for a period.

        Primarily for data-bearing nodes; calculation nodes override has_calculation.

        Args:
            period: The time period to check for a stored value.

        Returns:
            True if a direct value is stored, otherwise False.

        Examples:
            >>> node.has_value("2023")
            False
        """
        return False

    def get_value(self, period: str) -> float:
        """Retrieve the node's directly stored value for a period.

        This method must be overridden by data-bearing nodes to return stored values.

        Args:
            period: The time period string for which to retrieve the value.

        Returns:
            The float value stored for the given period.

        Raises:
            NotImplementedError: If the node does not store direct values.

        Examples:
            >>> node.get_value("2023")
        """
        raise NotImplementedError(f"Node {self.name} does not implement get_value")

    def has_calculation(self) -> bool:
        """Indicate whether this node performs calculation.

        Distinguish calculation nodes from data-holding nodes.

        Returns:
            True if the node performs calculations, otherwise False.

        Examples:
            >>> node.has_calculation()
            False
        """
        return False

# --- END FILE: fin_statement_model/core/nodes/base.py ---

# --- START FILE: fin_statement_model/core/nodes/calculation_nodes.py ---
"""Provide node implementations for performing calculations in the financial statement model.

This module defines the different types of calculation nodes available in the system:
- FormulaCalculationNode: Evaluates a formula expression string (e.g., "a + b / 2")
- CalculationNode: Uses a calculation object for calculation logic
- CustomCalculationNode: Calculates using a Python callable/function
"""

from typing import Optional, Any
from collections.abc import Callable

from fin_statement_model.core.calculations.calculation import Calculation, FormulaCalculation
from fin_statement_model.core.errors import (
    CalculationError,
)
from fin_statement_model.core.nodes.base import Node


# === CalculationNode ===


class CalculationNode(Node):
    """Delegate calculation logic to a separate calculation object.

    Uses a calculation object. The actual calculation algorithm is
    encapsulated in a `calculation` object provided during initialization.
    This allows for flexible and interchangeable calculation logic.

    Attributes:
        name (str): Identifier for this node.
        inputs (List[Node]): A list of input nodes required by the calculation.
        calculation (Any): An object possessing a `calculate(inputs: List[Node], period: str) -> float` method.
        _values (Dict[str, float]): Internal cache for calculated results.

    Examples:
        >>> class SumCalculation:
        ...     def calculate(self, inputs: List[Node], period: str) -> float:
        ...         return sum(node.calculate(period) for node in inputs)
        >>> node_a = FinancialStatementItemNode("a", {"2023": 10})
        >>> node_b = FinancialStatementItemNode("b", {"2023": 20})
        >>> sum_node = CalculationNode(
        ...     "sum_ab",
        ...     inputs=[node_a, node_b],
        ...     calculation=SumCalculation()
        ... )
        >>> print(sum_node.calculate("2023"))
        30.0
    """

    def __init__(self, name: str, inputs: list[Node], calculation: Calculation, **kwargs: Any):
        """Initialize the CalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (List[Node]): List of input nodes needed by the calculation.
            calculation (Any): The calculation object implementing the calculation.
                Must have a `calculate` method.
            **kwargs: Additional attributes to store on the node (e.g., metric_name, metric_description).

        Raises:
            TypeError: If `inputs` is not a list of Nodes, or if `calculation`
                does not have a callable `calculate` method.
        """
        super().__init__(name)
        if not isinstance(inputs, list) or not all(isinstance(n, Node) for n in inputs):
            raise TypeError("CalculationNode inputs must be a list of Node instances.")
        if not hasattr(calculation, "calculate") or not callable(getattr(calculation, "calculate")):
            raise TypeError("Calculation object must have a callable 'calculate' method.")

        self.inputs = inputs
        self.calculation = calculation
        self._values: dict[str, float] = {}  # Cache for calculated values

        # Store any additional attributes passed via kwargs
        for key, value in kwargs.items():
            setattr(self, key, value)

    def calculate(self, period: str) -> float:
        """Calculate the value for a period using the assigned calculation.

        Checks the cache first. If not found, delegates to the calculation's
        `calculate` method and stores the result.

        Args:
            period (str): The time period for the calculation.

        Returns:
            float: The calculated value from the calculation.

        Raises:
            CalculationError: If the calculation fails or returns
                a non-numeric value.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Delegate to the calculation object's calculate method
            result = self.calculation.calculate(self.inputs, period)
            if not isinstance(result, int | float):
                raise TypeError(
                    f"Calculation for node '{self.name}' did not return a numeric value (got {type(self.calculation).__name__})."
                )
            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the calculation
            raise CalculationError(
                message=f"Error during calculation for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={
                    "calculation": type(self.calculation).__name__,
                    "error": str(e),
                },
            ) from e

    def set_calculation(self, calculation: Calculation) -> None:
        """Change the calculation object for the node.

        Args:
            calculation (Any): The new calculation object. Must have a callable
                `calculate` method.

        Raises:
            TypeError: If the new calculation is invalid.
        """
        if not hasattr(calculation, "calculate") or not callable(getattr(calculation, "calculate")):
            raise TypeError("New calculation object must have a callable 'calculate' method.")
        self.calculation = calculation
        self.clear_cache()  # Clear cache as logic has changed

    def clear_cache(self) -> None:
        """Clear the internal cache of calculated values.

        Returns:
            None
        """
        self._values.clear()

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used by the calculation.

        Returns:
            A list of input node names.
        """
        return [node.name for node in self.inputs]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as CalculationNode performs calculations.
        """
        return True


# === FormulaCalculationNode ===


class FormulaCalculationNode(CalculationNode):
    """Calculate a value based on a mathematical formula string.

    This node extends CalculationNode and uses a FormulaCalculation strategy
    internally to parse and evaluate mathematical expressions.

    Attributes:
        name (str): Identifier for this node.
        inputs (Dict[str, Node]): Mapping of variable names used in the formula
            to their corresponding input Node instances.
        formula (str): The mathematical expression string to evaluate (e.g., "a + b").
        metric_name (Optional[str]): The original metric identifier from the registry, if applicable.
        metric_description (Optional[str]): The description from the metric definition, if applicable.

    Examples:
        >>> # Assume revenue and cogs are Node instances
        >>> revenue = FinancialStatementItemNode("revenue", {"2023": 100})
        >>> cogs = FinancialStatementItemNode("cogs", {"2023": 60})
        >>> gross_profit = FormulaCalculationNode(
        ...     "gross_profit",
        ...     inputs={"rev": revenue, "cost": cogs},
        ...     formula="rev - cost"
        ... )
        >>> print(gross_profit.calculate("2023"))
        40.0
    """

    def __init__(
        self,
        name: str,
        inputs: dict[str, Node],
        formula: str,
        metric_name: Optional[str] = None,
        metric_description: Optional[str] = None,
    ):
        """Initialize the FormulaCalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (Dict[str, Node]): Dictionary mapping variable names in the
                formula to the corresponding input nodes.
            formula (str): The mathematical formula string.
            metric_name (Optional[str]): The original metric identifier from the
                registry, if this node represents a defined metric. Defaults to None.
            metric_description (Optional[str]): The description from the metric
                definition, if applicable. Defaults to None.

        Raises:
            ValueError: If the formula string has invalid syntax.
            TypeError: If any value in `inputs` is not a Node instance.
        """
        if not isinstance(inputs, dict) or not all(isinstance(n, Node) for n in inputs.values()):
            raise TypeError("FormulaCalculationNode inputs must be a dict of Node instances.")

        # Store the formula and metric attributes
        self.formula = formula
        self.metric_name = metric_name
        self.metric_description = metric_description

        # Extract variable names and input nodes in consistent order
        input_variable_names = list(inputs.keys())
        input_nodes = list(inputs.values())

        # Create FormulaCalculation strategy
        formula_calculation = FormulaCalculation(formula, input_variable_names)

        # Initialize parent CalculationNode with the strategy
        super().__init__(name, input_nodes, formula_calculation)

        # Store the inputs dict for compatibility (separate from parent's inputs list)
        self.inputs_dict = inputs

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used in the formula.

        Returns:
            A list of variable names corresponding to the formula inputs.
        """
        return [node.name for node in self.inputs_dict.values()]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as FormulaCalculationNode performs calculations.
        """
        return True


# === CustomCalculationNode ===


class CustomCalculationNode(Node):
    """Calculate a value using a Python callable/function.

    Uses a Python callable/function to calculate the value for a node.
    The function is provided during initialization.

    Attributes:
        name (str): Identifier for this node.
        inputs (List[Node]): List of input nodes needed for calculation.
        formula_func (Callable): The Python callable function to use for calculation.
        description (str, optional): Description of what this calculation does.
        _values (Dict[str, float]): Internal cache for calculated results.

    Examples:
        >>> def custom_calculation(a, b):
        ...     return a + b
        >>> node_a = FinancialStatementItemNode("NodeA", values={"2023": 10.0})
        >>> node_b = FinancialStatementItemNode("NodeB", values={"2023": 5.0})
        >>> node = CustomCalculationNode(
        ...     "custom_calc",
        ...     inputs=[node_a, node_b],
        ...     formula_func=custom_calculation
        ... )
        >>> print(node.calculate("2023"))
        15.0
    """

    def __init__(
        self,
        name: str,
        inputs: list[Node],
        formula_func: Callable,
        description: Optional[str] = None,
    ):
        """Initialize the CustomCalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (List[Node]): The input nodes whose values will be passed to formula_func.
            formula_func (Callable): The Python callable function to use for calculation.
            description (str, optional): Description of what this calculation does.

        Raises:
            TypeError: If `inputs` is not a list of Nodes or `formula_func` is not a callable.
        """
        super().__init__(name)
        if not isinstance(inputs, list) or not all(isinstance(n, Node) for n in inputs):
            raise TypeError("CustomCalculationNode inputs must be a list of Node instances")
        if not callable(formula_func):
            raise TypeError("CustomCalculationNode formula_func must be a callable function")

        self.inputs = inputs
        self.formula_func = formula_func
        self.description = description
        self._values: dict[str, float] = {}  # Cache for calculated results

    def calculate(self, period: str) -> float:
        """Calculate the node's value for a period using the provided function.

        Args:
            period (str): The time period for which to perform the calculation.

        Returns:
            float: The calculated value from the function.

        Raises:
            CalculationError: If an error occurs during calculation, such as
                if an input node fails to provide a numeric value for the period.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Get input values
            input_values = []
            for node in self.inputs:
                value = node.calculate(period)
                if not isinstance(value, int | float):
                    raise TypeError(
                        f"Input node '{node.name}' did not return a numeric value for period '{period}'. Got {type(value).__name__}."
                    )
                input_values.append(value)

            # Calculate the value using the provided function
            result = self.formula_func(*input_values)
            if not isinstance(result, int | float):
                raise TypeError(
                    f"Formula did not return a numeric value. Got {type(result).__name__}."
                )

            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the function
            raise CalculationError(
                message=f"Error during custom calculation for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={"function": self.formula_func.__name__, "error": str(e)},
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used in the function.

        Returns:
            A list of input node names.
        """
        return [node.name for node in self.inputs]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as CustomCalculationNode performs calculations.
        """
        return True

# --- END FILE: fin_statement_model/core/nodes/calculation_nodes.py ---

# --- START FILE: fin_statement_model/core/nodes/forecast_nodes.py ---
"""Provide forecast nodes to project future values from historical data.

This module defines the base `ForecastNode` class and its subclasses,
implementing various forecasting strategies (fixed, curve, statistical,
custom, average, and historical growth).
"""

import logging
from collections.abc import Callable

# Use absolute imports
from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class ForecastNode(Node):
    """Define base class for forecast nodes to project future values from historical data.

    A forecast node takes an input node (typically a financial statement item) and projects its
    future values using various growth methods. The node caches calculated values to avoid
    redundant computations.

    Attributes:
        name (str): Identifier for the forecast node (derived from input_node.name)
        input_node (Node): Source node containing historical values to forecast from
        base_period (str): Last historical period to use as basis for forecasting
        forecast_periods (List[str]): List of future periods to generate forecasts for
        _cache (dict): Internal cache of calculated values
        values (dict): Dictionary mapping periods to values (including historical)

    Methods:
        calculate(period): Get value for a specific period (historical or forecast)
        _calculate_value(period): Core calculation logic for a period
        _get_previous_period(period): Helper to get chronologically previous period
        _get_growth_factor_for_period(): Abstract method for growth rate calculation

    Examples:
        # Create 5% fixed growth forecast for revenue
        base = "FY2022"
        forecasts = ["FY2023", "FY2024", "FY2025"]
        node = FixedGrowthForecastNode(revenue_node, base, forecasts, 0.05)

        # Get forecasted value
        fy2024_revenue = node.calculate("FY2024")
    """

    def __init__(self, input_node: Node, base_period: str, forecast_periods: list[str]):
        """Initialize ForecastNode with input node and forecast periods.

        Args:
            input_node: Source node containing historical values.
            base_period: The last historical period serving as the forecast base.
            forecast_periods: List of future periods for which forecasts will be generated.
        """
        self.name = input_node.name
        self.input_node = input_node
        self.base_period = base_period
        self.forecast_periods = forecast_periods
        self._cache = {}

        # Copy historical values from input node
        if hasattr(input_node, "values"):
            self.values = input_node.values.copy()
        else:
            self.values = {}

    def calculate(self, period: str) -> float:
        """Calculate the value for a specific period, using cached results if available.

        This method returns historical values for periods up to the base period, and
        calculates forecasted values for future periods. Results are cached to avoid
        redundant calculations.

        Args:
            period (str): The period to calculate the value for (e.g. "FY2023")

        Returns:
            float: The calculated value for the specified period

        Raises:
            ValueError: If the requested period is not in base_period or forecast_periods

        Examples:
            # Get historical value
            base_value = node.calculate("FY2022")  # Returns actual historical value

            # Get forecasted value
            forecast_value = node.calculate("FY2024")  # Returns projected value
        """
        if period not in self._cache:
            self._cache[period] = self._calculate_value(period)
        return self._cache[period]

    def clear_cache(self):
        """Clear the calculation cache.

        This method clears any cached calculation results, forcing future calls to
        calculate() to recompute values rather than using cached results.

        Examples:
            # Clear cached calculations
            node.clear_cache()  # Future calculate() calls will recompute values
        """
        self._cache.clear()

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True

    def _calculate_value(self, period: str) -> float:
        """Calculate the value for a specific period.

        For historical periods (up to base_period), returns the actual value.
        For forecast periods, calculates the value using the growth rate.

        Args:
            period: The period to calculate the value for

        Returns:
            float: The calculated value for the period

        Raises:
            ValueError: If the period is not in base_period or forecast_periods
        """
        # For historical periods, return the actual value
        if period <= self.base_period:
            return self.values.get(period, 0.0)

        # For forecast periods, calculate using growth rate
        if period not in self.forecast_periods:
            raise ValueError(f"Period '{period}' not in forecast periods for {self.name}")

        # Get the previous period's value
        prev_period = self._get_previous_period(period)
        prev_value = self.calculate(prev_period)

        # Get the growth rate for this period
        growth_factor = self._get_growth_factor_for_period(period, prev_period, prev_value)

        # Calculate the new value
        return prev_value * (1 + growth_factor)

    def _get_previous_period(self, current_period: str) -> str:
        all_periods = sorted([self.base_period, *self.forecast_periods])
        idx = all_periods.index(current_period)
        return all_periods[idx - 1]

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        raise NotImplementedError("Implement in subclass.")


class FixedGrowthForecastNode(ForecastNode):
    """A forecast node that applies a fixed growth rate to project future values.

    This node takes a constant growth rate and applies it to each forecast period,
    compounding from the base period value. It's useful for simple forecasting scenarios
    where steady growth is expected.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_rate (float): The fixed growth rate to apply (e.g. 0.05 for 5% growth)

    Examples:
        # Create node forecasting 5% annual revenue growth
        forecast = FixedGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            0.05
        )

        # Get forecasted value
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1.05)^2
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_rate: float,
    ):
        """Initialize FixedGrowthForecastNode with a constant growth rate.

        Args:
            input_node: Node containing historical values to base the forecast on.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_rate: Fixed growth rate (e.g., 0.05 for 5% growth).
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.growth_rate = float(growth_rate)  # Ensure it's a float
        logger.debug(f"Created FixedGrowthForecastNode with growth rate: {self.growth_rate}")

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        logger.debug(
            f"FixedGrowthForecastNode: Using growth rate {self.growth_rate} for period {period}"
        )
        return self.growth_rate


class CurveGrowthForecastNode(ForecastNode):
    """A forecast node that applies different growth rates for each forecast period.

    This node takes a list of growth rates corresponding to each forecast period,
    allowing for varying growth assumptions over time. This is useful when you expect
    growth patterns to change, such as high initial growth followed by moderation.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_rates (List[float]): List of growth rates for each period (e.g. [0.08, 0.06, 0.04])
                                   Must match length of forecast_periods.

    Raises:
        ValueError: If length of growth_rates doesn't match forecast_periods

    Examples:
        # Create node with declining growth rates
        forecast = CurveGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            [0.08, 0.06, 0.04]  # 8% then 6% then 4% growth
        )

        # Get forecasted value
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1.08) * (1.06)
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_rates: list[float],
    ):
        """Initialize CurveGrowthForecastNode with variable growth rates per period.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_rates: List of growth rates matching each forecast period.
        """
        super().__init__(input_node, base_period, forecast_periods)
        if len(growth_rates) != len(forecast_periods):
            raise ValueError("Number of growth rates must match forecast periods.")
        self.growth_rates = [float(rate) for rate in growth_rates]  # Ensure all are floats
        logger.debug(f"Created CurveGrowthForecastNode with growth rates: {self.growth_rates}")
        logger.debug(f"  Base period: {base_period}")
        logger.debug(f"  Forecast periods: {forecast_periods}")
        logger.debug(f"  Base value: {input_node.calculate(base_period)}")

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Get the growth factor for a specific period."""
        idx = self.forecast_periods.index(period)
        growth_rate = self.growth_rates[idx]
        logger.debug(
            f"CurveGrowthForecastNode: Using growth rate {growth_rate} for period {period}"
        )
        logger.debug(f"  Previous period: {prev_period}")
        logger.debug(f"  Previous value: {prev_value}")
        return growth_rate


class StatisticalGrowthForecastNode(ForecastNode):
    """A forecast node that generates growth rates from a statistical distribution.

    This node uses a provided statistical distribution function to randomly generate
    growth rates for each forecast period. This is useful for modeling uncertainty
    and running Monte Carlo simulations of different growth scenarios.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        distribution_callable (Callable[[], float]): Function that returns random growth rates
                                                   from a statistical distribution

    Examples:
        # Create node with normally distributed growth rates
        from numpy.random import normal
        forecast = StatisticalGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            lambda: normal(0.05, 0.02)  # Mean 5% growth, 2% std dev
        )

        # Get forecasted value (will vary due to randomness)
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1 + r1) * (1 + r2) where r1,r2 are random
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        distribution_callable: Callable[[], float],
    ):
        """Initialize StatisticalGrowthForecastNode with a distribution function.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            distribution_callable: Function that returns a random growth rate.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.distribution_callable = distribution_callable

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.distribution_callable()


class CustomGrowthForecastNode(ForecastNode):
    """A forecast node that uses a custom function to determine growth rates.

    This node allows complete flexibility in how growth rates are calculated by accepting
    a custom function that can incorporate any logic or external data to determine the
    growth rate for each period.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_function (Callable[[str, str, float], float]): Function that returns growth rate
            given current period, previous period, and previous value

    The growth_function should accept three parameters:
        - current_period (str): The period being forecasted
        - prev_period (str): The previous period
        - prev_value (float): The value from the previous period
    And return a float representing the growth rate for that period.

    Examples:
        def custom_growth(period, prev_period, prev_value):
            # Growth rate increases by 1% each year, starting at 5%
            year_diff = int(period[-4:]) - int(prev_period[-4:])
            return 0.05 + (0.01 * year_diff)

        forecast = CustomGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            custom_growth
        )
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_function: Callable[[str, str, float], float],
    ):
        """Initialize CustomGrowthForecastNode with a custom growth function.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_function: Callable(period, prev_period, prev_value) -> growth rate.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.growth_function = growth_function

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.growth_function(period, prev_period, prev_value)


class AverageValueForecastNode(ForecastNode):
    """A forecast node that uses the average of historical values for all forecast periods.

    This node calculates the average of historical values and returns that constant value
    for all forecast periods. It's useful when you want to project future values based
    on the historical average, without any growth.
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
    ):
        """Initialize AverageValueForecastNode by computing historical average.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.

        """
        super().__init__(input_node, base_period, forecast_periods)
        self.average_value = self._calculate_average_value()
        logger.debug(f"Created AverageValueForecastNode with average value: {self.average_value}")

    def _calculate_average_value(self) -> float:
        """Calculate the average historical value up to the base period.

        Returns:
            float: The average of historical values or 0.0 if none.
        """
        values = [value for period, value in self.values.items() if period <= self.base_period]
        if not values:
            logger.warning(f"No historical values found for {self.name}, using 0.0 as average")
            return 0.0
        return sum(values) / len(values)

    def _calculate_value(self, period: str) -> float:
        """Calculate the value for a specific period using the computed average value."""
        # For historical periods, return the actual value
        if period <= self.base_period:
            return self.values.get(period, 0.0)

        # For forecast periods, return the constant average value
        if period not in self.forecast_periods:
            raise ValueError(f"Period '{period}' not in forecast periods for {self.name}")

        return self.average_value

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Not used for average value forecasts."""
        return 0.0


class AverageHistoricalGrowthForecastNode(ForecastNode):
    """A forecast node that uses the average historical growth rate for forecasting.

    This node calculates the average growth rate from historical values and applies
    that same growth rate consistently to all forecast periods. It's useful when you
    want to project future values based on the historical growth pattern.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
    ):
        """Initialize AverageHistoricalGrowthForecastNode by computing average growth.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.avg_growth_rate = self._calculate_average_growth_rate()
        logger.debug(
            f"Created AverageHistoricalGrowthForecastNode with growth rate: {self.avg_growth_rate}"
        )

    def _calculate_average_growth_rate(self) -> float:
        """Calculate the average historical growth rate from input node values.

        Returns:
            float: The average growth rate across historical periods
        """
        if not self.values:
            logger.warning(f"No historical values found for {self.name}, using 0% growth")
            return 0.0

        # Get sorted historical periods up to base period
        historical_periods = sorted([p for p in self.values if p <= self.base_period])
        if len(historical_periods) < 2:
            logger.warning(f"Insufficient historical data for {self.name}, using 0% growth")
            return 0.0

        # Calculate growth rates between consecutive periods
        growth_rates = []
        for i in range(1, len(historical_periods)):
            prev_period = historical_periods[i - 1]
            curr_period = historical_periods[i]
            prev_value = self.values[prev_period]
            curr_value = self.values[curr_period]

            if prev_value == 0:
                logger.warning(
                    f"Zero value found for {self.name} in period {prev_period}, skipping growth rate"
                )
                continue

            growth_rate = (curr_value - prev_value) / prev_value
            growth_rates.append(growth_rate)

        if not growth_rates:
            logger.warning(f"No valid growth rates calculated for {self.name}, using 0% growth")
            return 0.0

        # Calculate and return average growth rate
        avg_growth = sum(growth_rates) / len(growth_rates)
        logger.debug(f"Calculated average growth rate for {self.name}: {avg_growth}")
        return avg_growth

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Get the growth factor for a specific period.

        Args:
            period (str): The current period
            prev_period (str): The previous period
            prev_value (float): The value from the previous period

        Returns:
            float: The growth rate to apply
        """
        logger.debug(
            f"AverageHistoricalGrowthForecastNode: Using growth rate {self.avg_growth_rate} for period {period}"
        )
        return self.avg_growth_rate

# --- END FILE: fin_statement_model/core/nodes/forecast_nodes.py ---

# --- START FILE: fin_statement_model/core/nodes/item_node.py ---
"""Define a node representing a basic financial statement item."""

import logging

# Use absolute imports
from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class FinancialStatementItemNode(Node):
    """Define a leaf node containing raw financial statement data.

    This node type typically stores actual reported values (e.g., Revenue,
    COGS) for different time periods.

    Attributes:
        name (str): The unique identifier for the financial item (e.g., "Revenue").
        values (Dict[str, float]): A dictionary mapping time periods (str)
            to their corresponding numerical values (float).

    Examples:
        >>> revenue_data = {"2022": 1000.0, "2023": 1200.0}
        >>> revenue_node = FinancialStatementItemNode("Revenue", revenue_data)
        >>> print(revenue_node.name)
        Revenue
        >>> print(revenue_node.get_value("2023"))
        1200.0
        >>> print(revenue_node.calculate("2022")) # Calculate retrieves the value
        1000.0
        >>> revenue_node.set_value("2024", 1500.0)
        >>> print(revenue_node.get_value("2024"))
        1500.0
        >>> print(revenue_node.has_value("2021"))
        False
    """

    def __init__(self, name: str, values: dict[str, float]):
        """Initialize the financial statement item node.

        Args:
            name (str): The name of the financial statement item.
            values (Dict[str, float]): Dictionary of period-value pairs.
        """
        # Call base Node init if it requires name
        # super().__init__(name)  # Assuming base Node init takes name
        self.name = name
        self.values = values

    def calculate(self, period: str) -> float:
        """Retrieve the value for the specified period.

        For this node type, calculation simply means retrieving the stored value.

        Args:
            period (str): The time period for which to retrieve the value.

        Returns:
            float: The value for the given period, or 0.0 if the period is not found.
        """
        return self.get_value(period)

    def set_value(self, period: str, value: float) -> None:
        """Update or add a value for a specific period.

        Modifies the stored data for the given period.

        Args:
            period (str): The time period to set the value for.
            value (float): The numerical value to store for the period.
        """
        self.values[period] = value

    def has_value(self, period: str) -> bool:
        """Check if a value exists for the specified period.

        Args:
            period (str): The time period to check.

        Returns:
            bool: True if a value is stored for the period, False otherwise.
        """
        return period in self.values

    def get_value(self, period: str) -> float:
        """Retrieve the stored value for a specific period.

        Args:
            period (str): The time period for which to get the value.

        Returns:
            float: The stored value, defaulting to 0.0 if the period is not found.
        """
        return self.values.get(period, 0.0)

# --- END FILE: fin_statement_model/core/nodes/item_node.py ---

# --- START FILE: fin_statement_model/core/nodes/standard_nodes/__init__.py ---
"""Standard Node Definitions Package.

This package contains organized standard node definitions split into logical categories.
All definitions are automatically loaded into the standard_node_registry.
"""

import logging
from pathlib import Path
from typing import Optional

from fin_statement_model.core.nodes.standard_registry import standard_node_registry

logger = logging.getLogger(__name__)


def load_all_standard_nodes(base_path: Optional[Path] = None) -> int:
    """Load all standard node definitions from organized YAML files.

    Args:
        base_path: Base path to the standard_nodes directory. If None, uses default.

    Returns:
        Total number of nodes loaded.
    """
    if base_path is None:
        base_path = Path(__file__).parent

    total_loaded = 0

    # Define the organized node files to load
    node_files = [
        # Balance sheet nodes
        "balance_sheet/assets.yaml",
        "balance_sheet/liabilities.yaml",
        "balance_sheet/equity.yaml",
        # Income statement nodes
        "income_statement/revenue_costs.yaml",
        "income_statement/operating.yaml",
        "income_statement/non_operating.yaml",
        "income_statement/shares.yaml",
        # Cash flow nodes
        "cash_flow/operating.yaml",
        "cash_flow/investing.yaml",
        "cash_flow/financing.yaml",
        # Calculated items
        "calculated/profitability.yaml",
        "calculated/liquidity.yaml",
        "calculated/leverage.yaml",
        "calculated/valuation.yaml",
        # Market data
        "market_data/market_data.yaml",
        # Real estate nodes
        "real_estate/property_operations.yaml",
        "real_estate/reit_specific.yaml",
        "real_estate/debt_financing.yaml",
        # Banking nodes
        "banking/assets.yaml",
        "banking/liabilities.yaml",
        "banking/income_statement.yaml",
        "banking/regulatory_capital.yaml",
        "banking/off_balance_sheet.yaml",
    ]

    for file_path in node_files:
        full_path = base_path / file_path
        if full_path.exists():
            try:
                count = standard_node_registry.load_from_yaml_file(full_path)
                total_loaded += count
                logger.debug(f"Loaded {count} nodes from {file_path}")
            except Exception:
                logger.exception(f"Failed to load {file_path}")
        else:
            logger.warning(f"Organized node file not found: {full_path}")

    logger.info(f"Loaded {total_loaded} total standard nodes from organized structure")
    return total_loaded


# Auto-load on import
try:
    load_all_standard_nodes()
except Exception as e:
    logger.warning(f"Failed to auto-load standard nodes: {e}")

# --- END FILE: fin_statement_model/core/nodes/standard_nodes/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/standard_nodes/balance_sheet/__init__.py ---
"""Balance Sheet Standard Node Definitions."""

# This package contains balance sheet node definitions split into:
# - assets.yaml: Current and non-current assets
# - liabilities.yaml: Current and non-current liabilities
# - equity.yaml: Equity components

# --- END FILE: fin_statement_model/core/nodes/standard_nodes/balance_sheet/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/standard_nodes/banking/__init__.py ---
"""Banking Standard Node Definitions."""

# This package contains banking node definitions split into:
# - assets.yaml: Loans, securities, cash, and other banking assets
# - liabilities.yaml: Deposits, borrowings, and other banking liabilities
# - income_statement.yaml: Interest income/expense, fees, provisions
# - regulatory_capital.yaml: Tier 1/2 capital, risk-weighted assets
# - off_balance_sheet.yaml: Commitments, guarantees, derivatives

# --- END FILE: fin_statement_model/core/nodes/standard_nodes/banking/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/standard_nodes/calculated/__init__.py ---
"""Calculated Standard Node Definitions."""

# This package contains calculated node definitions split into:
# - profitability.yaml: EBITDA, NOPAT, etc.
# - liquidity.yaml: Working capital measures
# - leverage.yaml: Net debt, leverage measures
# - valuation.yaml: Enterprise value, etc.

# --- END FILE: fin_statement_model/core/nodes/standard_nodes/calculated/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/standard_nodes/cash_flow/__init__.py ---
"""Cash Flow Statement Standard Node Definitions."""

# This package contains cash flow statement node definitions split into:
# - operating.yaml: Operating activities
# - investing.yaml: Investing activities
# - financing.yaml: Financing activities

# --- END FILE: fin_statement_model/core/nodes/standard_nodes/cash_flow/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/standard_nodes/income_statement/__init__.py ---
"""Income Statement Standard Node Definitions."""

# This package contains income statement node definitions split into:
# - revenue_costs.yaml: Revenue and direct costs
# - operating.yaml: Operating expenses and income
# - non_operating.yaml: Interest, other income, taxes
# - shares.yaml: Share-related items

# --- END FILE: fin_statement_model/core/nodes/standard_nodes/income_statement/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/standard_nodes/market_data/__init__.py ---
"""Market Data Standard Node Definitions."""

# This package contains market data node definitions:
# - market_data.yaml: Market prices and per-share data

# --- END FILE: fin_statement_model/core/nodes/standard_nodes/market_data/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/standard_nodes/real_estate/__init__.py ---
"""Real Estate Standard Node Definitions."""

# This package contains real estate node definitions split into:
# - property_operations.yaml: Property income, expenses, and operational metrics
# - reit_specific.yaml: REIT-specific items like FFO, AFFO, and adjustments
# - property_metrics.yaml: Property-level metrics and measurements
# - debt_financing.yaml: Debt, financing, and loan-related nodes

# --- END FILE: fin_statement_model/core/nodes/standard_nodes/real_estate/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/standard_registry.py ---
"""Registry for standard node names.

This module provides a registry system for standardized node names,
ensuring consistency across financial models and enabling metrics to work properly.
"""

import logging
from pathlib import Path
from typing import Optional
import yaml
from pydantic import BaseModel, ConfigDict

logger = logging.getLogger(__name__)


class StandardNodeDefinition(BaseModel):
    """Define a standard node with metadata.

    Attributes:
        category: The main category (e.g., balance_sheet_assets, income_statement)
        subcategory: The subcategory within the main category
        description: Human-readable description of the node
        alternate_names: List of alternate names that should map to this standard name
        sign_convention: Whether values are typically positive or negative
    """

    model_config = ConfigDict(str_strip_whitespace=True)

    category: str
    subcategory: str
    description: str
    alternate_names: list[str] = []
    sign_convention: str = "positive"  # 'positive' or 'negative'


class StandardNodeRegistry:
    """Registry for standard node definitions.

    This class manages loading and accessing standardized node definitions,
    providing validation and alternate name resolution capabilities.

    Attributes:
        _standard_nodes: Dict mapping standard node names to their definitions
        _alternate_to_standard: Dict mapping alternate names to standard names
    """

    def __init__(self) -> None:
        """Initialize an empty registry."""
        self._standard_nodes: dict[str, StandardNodeDefinition] = {}
        self._alternate_to_standard: dict[str, str] = {}
        self._categories: set[str] = set()

    def load_from_yaml(self, yaml_path: Path) -> int:
        """Load standard node definitions from a YAML file.

        Args:
            yaml_path: Path to the YAML file containing node definitions.

        Returns:
            Number of nodes loaded.

        Raises:
            ValueError: If the YAML file has invalid structure or duplicate names.
            FileNotFoundError: If the YAML file doesn't exist.
        """
        if not yaml_path.exists():
            raise FileNotFoundError(f"Standard nodes file not found: {yaml_path}")

        try:
            with open(yaml_path, encoding="utf-8") as f:
                data = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML in {yaml_path}: {e}") from e

        if not isinstance(data, dict):
            raise TypeError(f"Expected dict at root of {yaml_path}, got {type(data)}")

        # Clear existing data
        self._standard_nodes.clear()
        self._alternate_to_standard.clear()
        self._categories.clear()

        nodes_loaded = 0

        # Load each node definition
        for node_name, node_data in data.items():
            if not isinstance(node_data, dict):
                logger.warning(f"Skipping invalid node definition '{node_name}': not a dict")
                continue

            try:
                definition = StandardNodeDefinition(**node_data)

                # Check for duplicate standard names
                if node_name in self._standard_nodes:
                    raise ValueError(f"Duplicate standard node name: {node_name}")

                # Add to main registry
                self._standard_nodes[node_name] = definition
                self._categories.add(definition.category)

                # Map alternate names
                for alt_name in definition.alternate_names:
                    if alt_name in self._alternate_to_standard:
                        existing_standard = self._alternate_to_standard[alt_name]
                        raise ValueError(
                            f"Alternate name '{alt_name}' already maps to '{existing_standard}', "
                            f"cannot also map to '{node_name}'"
                        )
                    self._alternate_to_standard[alt_name] = node_name

                nodes_loaded += 1

            except Exception as e:
                logger.exception(f"Error loading node '{node_name}'")
                raise ValueError(f"Invalid node definition for '{node_name}': {e}") from e

        logger.info(
            f"Loaded {nodes_loaded} standard node definitions "
            f"with {len(self._alternate_to_standard)} alternate names"
        )
        return nodes_loaded

    def load_from_yaml_file(self, yaml_path: Path) -> int:
        """Load standard node definitions from a single YAML file without clearing existing data.

        Args:
            yaml_path: Path to the YAML file containing node definitions.

        Returns:
            Number of nodes loaded from this file.

        Raises:
            ValueError: If the YAML file has invalid structure or duplicate names.
            FileNotFoundError: If the YAML file doesn't exist.
        """
        if not yaml_path.exists():
            raise FileNotFoundError(f"Standard nodes file not found: {yaml_path}")

        try:
            with open(yaml_path, encoding="utf-8") as f:
                data = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML in {yaml_path}: {e}") from e

        if not isinstance(data, dict):
            raise TypeError(f"Expected dict at root of {yaml_path}, got {type(data)}")

        nodes_loaded = 0

        # Load each node definition
        for node_name, node_data in data.items():
            if not isinstance(node_data, dict):
                logger.warning(f"Skipping invalid node definition '{node_name}': not a dict")
                continue

            try:
                definition = StandardNodeDefinition(**node_data)

                # Check for duplicate standard names
                if node_name in self._standard_nodes:
                    logger.debug(f"Overwriting existing standard node: {node_name}")

                # Add to main registry
                self._standard_nodes[node_name] = definition
                self._categories.add(definition.category)

                # Map alternate names
                for alt_name in definition.alternate_names:
                    if alt_name in self._alternate_to_standard:
                        existing_standard = self._alternate_to_standard[alt_name]
                        if existing_standard != node_name:
                            logger.debug(
                                f"Alternate name '{alt_name}' already maps to '{existing_standard}', "
                                f"now mapping to '{node_name}'"
                            )
                    self._alternate_to_standard[alt_name] = node_name

                nodes_loaded += 1

            except Exception as e:
                logger.exception(f"Error loading node '{node_name}' from {yaml_path}")
                raise ValueError(f"Invalid node definition for '{node_name}': {e}") from e

        logger.debug(f"Loaded {nodes_loaded} nodes from {yaml_path}")
        return nodes_loaded

    def get_standard_name(self, name: str) -> str:
        """Get the standard name for a given node name.

        If the name is already standard, returns it unchanged.
        If it's an alternate name, returns the corresponding standard name.
        If it's not recognized, returns the original name.

        Args:
            name: The node name to standardize.

        Returns:
            The standardized node name.
        """
        # Check if it's already a standard name
        if name in self._standard_nodes:
            return name

        # Check if it's an alternate name
        if name in self._alternate_to_standard:
            return self._alternate_to_standard[name]

        # Not recognized, return as-is
        return name

    def is_standard_name(self, name: str) -> bool:
        """Check if a name is a recognized standard node name.

        Args:
            name: The node name to check.

        Returns:
            True if the name is a standard node name, False otherwise.
        """
        return name in self._standard_nodes

    def is_alternate_name(self, name: str) -> bool:
        """Check if a name is a recognized alternate name.

        Args:
            name: The node name to check.

        Returns:
            True if the name is an alternate name, False otherwise.
        """
        return name in self._alternate_to_standard

    def is_recognized_name(self, name: str) -> bool:
        """Check if a name is either standard or alternate.

        Args:
            name: The node name to check.

        Returns:
            True if the name is recognized, False otherwise.
        """
        return self.is_standard_name(name) or self.is_alternate_name(name)

    def get_definition(self, name: str) -> Optional[StandardNodeDefinition]:
        """Get the definition for a node name.

        Works with both standard and alternate names.

        Args:
            name: The node name to look up.

        Returns:
            The node definition if found, None otherwise.
        """
        standard_name = self.get_standard_name(name)
        return self._standard_nodes.get(standard_name)

    def list_standard_names(self, category: Optional[str] = None) -> list[str]:
        """List all standard node names, optionally filtered by category.

        Args:
            category: Optional category to filter by.

        Returns:
            Sorted list of standard node names.
        """
        if category:
            names = [
                name for name, defn in self._standard_nodes.items() if defn.category == category
            ]
        else:
            names = list(self._standard_nodes.keys())

        return sorted(names)

    def list_categories(self) -> list[str]:
        """List all available categories.

        Returns:
            Sorted list of categories.
        """
        return sorted(self._categories)

    def validate_node_name(self, name: str, strict: bool = False) -> tuple[bool, str]:
        """Validate a node name against standards.

        Args:
            name: The node name to validate.
            strict: If True, only standard names are valid.
                   If False, alternate names are also valid.

        Returns:
            Tuple of (is_valid, message).
        """
        if strict:
            if self.is_standard_name(name):
                return True, f"'{name}' is a standard node name"
            elif self.is_alternate_name(name):
                standard = self.get_standard_name(name)
                return (
                    False,
                    f"'{name}' is an alternate name. Use standard name '{standard}'",
                )
            else:
                return False, f"'{name}' is not a recognized node name"
        elif self.is_recognized_name(name):
            if self.is_alternate_name(name):
                standard = self.get_standard_name(name)
                return True, f"'{name}' is valid (alternate for '{standard}')"
            else:
                return True, f"'{name}' is a standard node name"
        else:
            return False, f"'{name}' is not a recognized node name"

    def get_sign_convention(self, name: str) -> Optional[str]:
        """Get the sign convention for a node.

        Args:
            name: The node name (standard or alternate).

        Returns:
            The sign convention ('positive' or 'negative') if found, None otherwise.
        """
        definition = self.get_definition(name)
        return definition.sign_convention if definition else None

    def __len__(self) -> int:
        """Return the number of standard nodes in the registry."""
        return len(self._standard_nodes)


# Global registry instance
standard_node_registry = StandardNodeRegistry()


def load_standard_nodes(yaml_path: Optional[Path] = None) -> None:
    """Load standard nodes into the global registry.

    Args:
        yaml_path: Path to YAML file. If None, uses default location.
    """
    if yaml_path is None:
        # Default location relative to this file
        yaml_path = Path(__file__).parent / "standard_nodes.yaml"

    count = standard_node_registry.load_from_yaml(yaml_path)
    logger.info(f"Loaded {count} standard node definitions")


# NOTE: Auto-loading is disabled to prevent conflicts with organized structure
# The nodes/__init__.py will handle loading from the appropriate source
#
# # Auto-load standard nodes on import if file exists
# default_yaml = Path(__file__).parent / "standard_nodes.yaml"
# if default_yaml.exists():
#     try:
#         load_standard_nodes(default_yaml)
#     except Exception as e:
#         logger.warning(f"Failed to auto-load standard nodes: {e}")

# --- END FILE: fin_statement_model/core/nodes/standard_registry.py ---

# --- START FILE: fin_statement_model/core/nodes/stats_nodes.py ---
"""Provide nodes for statistical calculations on financial data across periods.

This module provides nodes for common time-series statistical analyses:
- `YoYGrowthNode`: Calculates year-over-year percentage growth.
- `MultiPeriodStatNode`: Computes statistics (mean, stddev, etc.) over a range of periods.
- `TwoPeriodAverageNode`: Calculates the simple average over two specific periods.
"""

import logging
import math
import statistics

# Use lowercase built-in types for annotations
from typing import Optional, Union
from collections.abc import Callable
from collections.abc import Sequence

# Use absolute imports
from fin_statement_model.core.nodes.base import Node
from fin_statement_model.core.errors import CalculationError

# Added logger instance
logger = logging.getLogger(__name__)

Numeric = Union[int, float]
StatFunc = Callable[[Sequence[Numeric]], Numeric]


class YoYGrowthNode(Node):
    """Calculate year-over-year (YoY) percentage growth.

    Compares the value of an input node between two specified periods
    (prior and current) and calculates the relative change.

    Growth = (Current Value - Prior Value) / Prior Value

    Attributes:
        name (str): The node's identifier.
        input_node (Node): The node providing the values for comparison.
        prior_period (str): Identifier for the earlier time period.
        current_period (str): Identifier for the later time period.

    Examples:
        >>> # Assume revenue_node holds {"2022": 100, "2023": 120}
        >>> revenue_node = FinancialStatementItemNode("revenue", {"2022": 100.0, "2023": 120.0})
        >>> yoy_growth = YoYGrowthNode(
        ...     "revenue_yoy",
        ...     input_node=revenue_node,
        ...     prior_period="2022",
        ...     current_period="2023"
        ... )
        >>> print(yoy_growth.calculate("any_period")) # Period arg is ignored
        0.2
    """

    def __init__(self, name: str, input_node: Node, prior_period: str, current_period: str):
        """Initialize the YoY Growth node.

        Args:
            name (str): The identifier for this growth node.
            input_node (Node): The node whose values will be compared.
            prior_period (str): The identifier for the earlier period.
            current_period (str): The identifier for the later period.

        Raises:
            TypeError: If `input_node` is not a Node instance or periods are not strings.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError("YoYGrowthNode input_node must be a Node instance.")
        if not isinstance(prior_period, str) or not isinstance(current_period, str):
            raise TypeError("YoYGrowthNode prior_period and current_period must be strings.")

        self.input_node = input_node
        self.prior_period = prior_period
        self.current_period = current_period

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the year-over-year growth rate.

        Retrieves values for the prior and current periods from the input node
        and computes the percentage growth. The `period` argument is ignored
        as the calculation periods are fixed during initialization.

        Args:
            period (Optional[str]): Ignored. The calculation uses the periods
                defined during initialization.

        Returns:
            float: The calculated growth rate (e.g., 0.2 for 20% growth).
                   Returns `float('nan')` if the prior period value is zero
                   or non-numeric.

        Raises:
            CalculationError: If the input node fails to provide numeric values
                for the required periods.
        """
        try:
            prior_value = self.input_node.calculate(self.prior_period)
            current_value = self.input_node.calculate(self.current_period)

            # Validate input types
            if not isinstance(prior_value, int | float):
                raise TypeError(f"Prior period ('{self.prior_period}') value is non-numeric.")
            if not isinstance(current_value, int | float):
                raise TypeError(f"Current period ('{self.current_period}') value is non-numeric.")

            # Handle division by zero or non-finite prior value
            if prior_value == 0 or not math.isfinite(prior_value):
                logger.warning(
                    f"YoYGrowthNode '{self.name}': Prior period '{self.prior_period}' value is zero or non-finite ({prior_value}). Returning NaN."
                )
                return float("nan")

            # Calculate growth
            growth = (float(current_value) - float(prior_value)) / float(prior_value)
            return growth

        except Exception as e:
            # Wrap any exception during calculation
            raise CalculationError(
                message=f"Failed to calculate YoY growth for node '{self.name}'",
                node_id=self.name,
                period=f"{self.prior_period}_to_{self.current_period}",  # Indicate period span
                details={
                    "input_node": self.input_node.name,
                    "prior_period": self.prior_period,
                    "current_period": self.current_period,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True


class MultiPeriodStatNode(Node):
    """Calculate a statistical measure across multiple periods.

    Applies a specified statistical function (e.g., mean, standard deviation)
    to the values of an input node over a list of periods.

    Attributes:
        name (str): The node's identifier.
        input_node (Node): The node providing the values for analysis.
        periods (List[str]): The list of period identifiers to include.
        stat_func (StatFunc): The statistical function to apply (e.g.,
            `statistics.mean`, `statistics.stdev`). Must accept a sequence
            of numbers and return a single number.

    Examples:
        >>> # Assume sales_node holds {"Q1": 10, "Q2": 12, "Q3": 11, "Q4": 13}
        >>> sales_node = FinancialStatementItemNode("sales", {"Q1": 10, "Q2": 12, "Q3": 11, "Q4": 13})
        >>> mean_sales = MultiPeriodStatNode(
        ...     "avg_quarterly_sales",
        ...     input_node=sales_node,
        ...     periods=["Q1", "Q2", "Q3", "Q4"],
        ...     stat_func=statistics.mean
        ... )
        >>> print(mean_sales.calculate()) # Period arg is ignored
        11.5
        >>> stddev_sales = MultiPeriodStatNode(
        ...     "sales_volatility",
        ...     input_node=sales_node,
        ...     periods=["Q1", "Q2", "Q3", "Q4"],
        ...     stat_func=statistics.stdev # Default
        ... )
        >>> print(round(stddev_sales.calculate(), 2))
        1.29
    """

    def __init__(
        self,
        name: str,
        input_node: Node,
        periods: list[str],
        stat_func: StatFunc = statistics.stdev,  # Default to standard deviation
    ):
        """Initialize the multi-period statistics node.

        Args:
            name (str): The identifier for this statistical node.
            input_node (Node): The node providing the source values.
            periods (List[str]): A list of period identifiers to analyze.
            stat_func (StatFunc): The statistical function to apply. Defaults to
                `statistics.stdev`. It must accept a sequence of numerics and
                return a numeric value.

        Raises:
            ValueError: If `periods` is not a list or is empty.
            TypeError: If `input_node` is not a Node, `periods` contains non-strings,
                or `stat_func` is not callable.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError("MultiPeriodStatNode input_node must be a Node instance.")
        if not isinstance(periods, list) or not periods:
            raise ValueError("MultiPeriodStatNode periods must be a non-empty list.")
        if not all(isinstance(p, str) for p in periods):
            raise TypeError("MultiPeriodStatNode periods must contain only strings.")
        if not callable(stat_func):
            raise TypeError("MultiPeriodStatNode stat_func must be a callable function.")

        self.input_node = input_node
        self.periods = periods
        self.stat_func = stat_func

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the statistical measure across the specified periods.

        Retrieves values from the input node for each period in the configured list,
        then applies the `stat_func`. The `period` argument is ignored.

        Args:
            period (Optional[str]): Ignored. Calculation uses the periods defined
                during initialization.

        Returns:
            float: The result of the statistical function. Returns `float('nan')`
                   if the statistical function requires more data points than
                   available (e.g., standard deviation with < 2 values) or if
                   no valid numeric data is found.

        Raises:
            CalculationError: If retrieving input node values fails or if the
                statistical function itself raises an unexpected error.
        """
        values: list[Numeric] = []
        retrieval_errors = []
        try:
            for p in self.periods:
                try:
                    value = self.input_node.calculate(p)
                    if isinstance(value, int | float) and math.isfinite(value):
                        values.append(float(value))
                    else:
                        # Log non-numeric/non-finite values but continue if possible
                        logger.warning(
                            f"MultiPeriodStatNode '{self.name}': Input '{self.input_node.name}' gave non-numeric/non-finite value ({value}) for period '{p}'. Skipping."
                        )
                except Exception as node_err:
                    # Log error fetching data for a specific period but continue
                    logger.error(
                        f"MultiPeriodStatNode '{self.name}': Error getting value for period '{p}' from '{self.input_node.name}': {node_err}",
                        exc_info=True,
                    )
                    retrieval_errors.append(p)

            # If no valid numeric values were collected
            if not values:
                logger.warning(
                    f"MultiPeriodStatNode '{self.name}': No valid numeric data points found across periods {self.periods}. Returning NaN."
                )
                return float("nan")

            # Attempt the statistical calculation
            try:
                result = self.stat_func(values)
                # Ensure result is float, handle potential NaN from stat_func
                return float(result) if math.isfinite(result) else float("nan")
            except (statistics.StatisticsError, ValueError, TypeError) as stat_err:
                # Handle errors specific to statistical functions (e.g., stdev needs >= 2 points)
                logger.warning(
                    f"MultiPeriodStatNode '{self.name}': Stat function '{self.stat_func.__name__}' failed ({stat_err}). Values: {values}. Returning NaN."
                )
                return float("nan")

        except Exception as e:
            # Catch any other unexpected errors during the process
            raise CalculationError(
                message=f"Failed to calculate multi-period stat for node '{self.name}'",
                node_id=self.name,
                period="multi-period",  # Indicate calculation context
                details={
                    "input_node": self.input_node.name,
                    "periods": self.periods,
                    "stat_func": self.stat_func.__name__,
                    "collected_values_count": len(values),
                    "retrieval_errors_periods": retrieval_errors,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True


class TwoPeriodAverageNode(Node):
    """Compute the simple average of an input node's value over two periods.

    Calculates (Value at Period 1 + Value at Period 2) / 2.

    Attributes:
        name (str): Identifier for this node.
        input_node (Node): Node providing the values to be averaged.
        period1 (str): Identifier for the first period.
        period2 (str): Identifier for the second period.

    Examples:
        >>> # Assume price_node holds {"Jan": 10.0, "Feb": 11.0}
        >>> price_node = FinancialStatementItemNode("price", {"Jan": 10.0, "Feb": 11.0})
        >>> avg_price = TwoPeriodAverageNode(
        ...     "jan_feb_avg_price",
        ...     input_node=price_node,
        ...     period1="Jan",
        ...     period2="Feb"
        ... )
        >>> print(avg_price.calculate()) # Period arg is ignored
        10.5
    """

    def __init__(self, name: str, input_node: Node, period1: str, period2: str):
        """Initialize the two-period average node.

        Args:
            name (str): The identifier for this node.
            input_node (Node): The node providing values.
            period1 (str): The identifier for the first period.
            period2 (str): The identifier for the second period.

        Raises:
            TypeError: If `input_node` is not a Node, or periods are not strings.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError(
                f"TwoPeriodAverageNode input_node must be a Node instance, got {type(input_node).__name__}"
            )
        if not isinstance(period1, str) or not isinstance(period2, str):
            raise TypeError("TwoPeriodAverageNode period1 and period2 must be strings.")

        self.input_node = input_node
        self.period1 = period1
        self.period2 = period2

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the average of the input node for the two fixed periods.

        Ignores the `period` argument, using `period1` and `period2` defined
        during initialization.

        Args:
            period (Optional[str]): Ignored.

        Returns:
            float: The average of the input node's values for `period1` and `period2`.
                   Returns `float('nan')` if either input value is non-numeric.

        Raises:
            CalculationError: If retrieving values from the input node fails.
        """
        try:
            val1 = self.input_node.calculate(self.period1)
            val2 = self.input_node.calculate(self.period2)

            # Ensure values are numeric and finite
            if not isinstance(val1, int | float) or not math.isfinite(val1):
                logger.warning(
                    f"TwoPeriodAverageNode '{self.name}': Value for period '{self.period1}' is non-numeric/non-finite ({val1}). Returning NaN."
                )
                return float("nan")
            if not isinstance(val2, int | float) or not math.isfinite(val2):
                logger.warning(
                    f"TwoPeriodAverageNode '{self.name}': Value for period '{self.period2}' is non-numeric/non-finite ({val2}). Returning NaN."
                )
                return float("nan")

            # Calculate the average
            return (float(val1) + float(val2)) / 2.0

        except Exception as e:
            # Wrap potential errors during input node calculation
            raise CalculationError(
                message=f"Failed to calculate two-period average for node '{self.name}'",
                node_id=self.name,
                period=f"{self.period1}_and_{self.period2}",  # Indicate context
                details={
                    "input_node": self.input_node.name,
                    "period1": self.period1,
                    "period2": self.period2,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True


__all__ = [
    "MultiPeriodStatNode",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]

# --- END FILE: fin_statement_model/core/nodes/stats_nodes.py ---

# --- START FILE: fin_statement_model/extensions/__init__.py ---
"""Extensions package for fin_statement_model.

This package hosts optional in-repo extensions and third-party plugins discovered via entry-points under
`fin_statement_model.extensions`.
"""

# --- END FILE: fin_statement_model/extensions/__init__.py ---

# --- START FILE: fin_statement_model/extensions/llm/__init__.py ---
"""LLM extension subpackage for fin_statement_model.

Provides built-in OpenAI-based LLM client extension for generating and injecting content.
"""

# --- END FILE: fin_statement_model/extensions/llm/__init__.py ---

# --- START FILE: fin_statement_model/extensions/llm/llm_client.py ---
"""LLM client module for OpenAI and backoff integration.

This module provides `LLMConfig` for client configuration and `LLMClient` for
asynchronous interactions with OpenAI's ChatCompletion API, including retry logic.
"""

import logging
import openai
import backoff
from dataclasses import dataclass
from typing import Optional, Any
from types import TracebackType

logger = logging.getLogger(__name__)


@dataclass
class LLMConfig:
    """Configuration data for LLMClient.

    Attributes:
        api_key: API key for OpenAI authentication.
        model_name: Model to use (e.g., 'gpt-4o').
        temperature: Sampling temperature setting.
        max_tokens: Maximum tokens to generate.
        timeout: Request timeout in seconds.
        max_retries: Number of retries on failure.
    """

    api_key: str
    model_name: str = "gpt-4o"
    temperature: float = 0.7
    max_tokens: int = 1500
    timeout: int = 30
    max_retries: int = 3
    # base_url is no longer needed as the openai library handles the endpoint configuration.


class LLMClientError(Exception):
    """Base exception for LLM client errors."""


class LLMTimeoutError(LLMClientError):
    """Exception for timeout errors."""


class LLMClient:
    """Asynchronous client for OpenAI ChatCompletion API with retry logic.

    Utilizes `LLMConfig` and supports retries on rate limits and timeouts.

    Methods:
        _make_api_call: Internal method for performing the API call with retry logic.
        get_completion: High-level method to obtain chat completions.
    """

    def __init__(self, config: Optional[LLMConfig] = None):
        """Initialize the async LLM client with configuration."""
        self.config = config or LLMConfig(api_key="")
        openai.api_key = self.config.api_key

    @backoff.on_exception(
        backoff.expo,
        (Exception, LLMTimeoutError),
        max_tries=3,
        giveup=lambda e: isinstance(e, LLMTimeoutError),
    )
    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=8)
    async def _make_api_call(self, messages: list[dict[str, str]]) -> dict[str, Any]:
        """Make the async API call to OpenAI with retry logic.

        Args:
            messages: List of message dicts for the ChatCompletion API

        Returns:
            Dict containing the API response

        Raises:
            LLMClientError: For any client-related errors, including timeout if applicable
        """
        try:
            logger.debug(f"Sending async request to OpenAI API with model {self.config.model_name}")
            response = await openai.ChatCompletion.acreate(
                model=self.config.model_name,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                timeout=self.config.timeout,
            )

            if not response.get("choices"):
                logger.error("No suggestions received from OpenAI API")
                raise LLMClientError("No suggestions received from API")

            return response
        except Exception as e:
            if "timeout" in str(e).lower():
                logger.exception("Async request timed out")
                raise LLMTimeoutError("Request timed out") from e
            logger.exception("OpenAI async API request failed")
            raise LLMClientError(f"API request failed: {e!s}") from e

    async def get_completion(self, messages: list[dict[str, str]]) -> str:
        """Get a completion result from the LLM using async API.

        Args:
            messages: List of message dictionaries for the ChatCompletion API

        Returns:
            str: The suggested completion from the LLM

        Raises:
            LLMClientError: For any client-related errors
        """
        try:
            logger.info("Requesting completion from OpenAI async API")
            response = await self._make_api_call(messages)
            completion = response["choices"][0]["message"]["content"].strip()
            logger.info("Successfully received completion")
            return completion
        except Exception as e:
            logger.exception("Error getting completion")
            raise LLMClientError(f"Failed to get completion: {e!s}") from e

    async def __aenter__(self) -> "LLMClient":
        """Enter the asynchronous context manager, returning the client."""
        return self

    async def __aexit__(
        self,
        exc_type: Optional[type[BaseException]],
        exc: Optional[BaseException],
        tb: Optional[TracebackType],
    ) -> None:
        """Exit the asynchronous context manager, performing cleanup."""
        # Ensure method has a body

# --- END FILE: fin_statement_model/extensions/llm/llm_client.py ---

# --- START FILE: fin_statement_model/forecasting/__init__.py ---
"""Forecasting sub-module for financial statement models.

This module provides comprehensive forecasting capabilities including:
- Multiple forecast methods (simple, curve, statistical, average, historical growth)
- Mutating and non-mutating forecast operations
- Extensible architecture for custom forecast methods
- Period management and validation utilities

Example:
    >>> from fin_statement_model.forecasting import StatementForecaster
    >>> forecaster = StatementForecaster(graph)
    >>>
    >>> # Mutating forecast - modifies the graph
    >>> forecaster.create_forecast(
    ...     forecast_periods=['2024', '2025'],
    ...     node_configs={
    ...         'revenue': {'method': 'simple', 'config': 0.05},
    ...         'costs': {'method': 'curve', 'config': [0.03, 0.04]}
    ...     }
    ... )
    >>>
    >>> # Non-mutating forecast - returns values without modifying graph
    >>> values = forecaster.forecast_value(
    ...     'revenue',
    ...     forecast_periods=['2024', '2025'],
    ...     forecast_config={'method': 'simple', 'config': 0.05}
    ... )
"""

# Main forecaster class
from .forecaster import StatementForecaster

# Forecast methods
from .methods import (
    ForecastMethod,
    BaseForecastMethod,
    SimpleForecastMethod,
    CurveForecastMethod,
    StatisticalForecastMethod,
    AverageForecastMethod,
    HistoricalGrowthForecastMethod,
)

# Registry and strategies
from .strategies import (
    ForecastMethodRegistry,
    forecast_registry,
    get_forecast_method,
    register_forecast_method,
)

# Utilities
from .period_manager import PeriodManager
from .validators import ForecastValidator

# Types
from .types import (
    ForecastMethodType,
    ForecastConfig,
    StatisticalConfig,
    ForecastResult,
)

__all__ = [
    "AverageForecastMethod",
    "BaseForecastMethod",
    "CurveForecastMethod",
    "ForecastConfig",
    "ForecastMethod",
    "ForecastMethodRegistry",
    "ForecastMethodType",
    "ForecastResult",
    "ForecastValidator",
    "HistoricalGrowthForecastMethod",
    "PeriodManager",
    "SimpleForecastMethod",
    "StatementForecaster",
    "StatisticalConfig",
    "StatisticalForecastMethod",
    "forecast_registry",
    "get_forecast_method",
    "register_forecast_method",
]

# --- END FILE: fin_statement_model/forecasting/__init__.py ---

# --- START FILE: fin_statement_model/forecasting/forecaster.py ---
"""Forecasting operations dedicated to statement-level financial graphs.

This module provides the StatementForecaster class, which handles forecasting
operations for financial statement graphs. It offers both mutating operations
(that modify the graph) and non-mutating operations (that return forecast values
without changing the graph state).
"""

import logging
from typing import Any, Optional, cast
import numpy as np

# Core imports
from fin_statement_model.core.nodes import Node
from fin_statement_model.core.node_factory import NodeFactory

# Forecasting module imports
from .period_manager import PeriodManager
from .validators import ForecastValidator
from .strategies import get_forecast_method
from .types import ForecastConfig, ForecastResult
from .methods import BaseForecastMethod

logger = logging.getLogger(__name__)


class StatementForecaster:
    """Handles forecasting operations specifically for a FinancialStatementGraph.

    This class provides two main approaches to forecasting:

    1. **Mutating operations** (`create_forecast`): Modifies the graph by adding
       forecast periods and updating node values directly. This is useful when
       you want to extend the graph with forecast data for further analysis.

    2. **Non-mutating operations** (`forecast_value`): Returns forecast values
       without modifying the graph state. This is useful for what-if scenarios
       or when you need forecast values without altering the original data.

    The forecaster supports multiple forecasting methods:
    - simple: Simple growth rate
    - curve: Variable growth rates per period
    - statistical: Random sampling from distributions
    - average: Average of historical values
    - historical_growth: Based on historical growth patterns
    """

    def __init__(self, fsg: Any) -> None:
        """Initialize the forecaster.

        Args:
            fsg: The FinancialStatementGraph instance this forecaster will operate on.
        """
        self.fsg = fsg

    def create_forecast(
        self,
        forecast_periods: list[str],
        node_configs: Optional[dict[str, dict[str, Any]]] = None,
        historical_periods: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> None:
        """Create forecasts for financial statement items on the graph.

        **IMPORTANT**: This method MUTATES the graph by:
        - Adding new periods to the graph if they don't exist
        - Updating node values with forecast data
        - Clearing node caches after updates

        Use `forecast_value` instead if you need forecast values without
        modifying the graph.

        Args:
            forecast_periods: List of future periods to forecast.
            node_configs: Mapping of node names to their forecast configurations.
                Each config should contain:
                - 'method': Forecasting method ('simple', 'curve', 'statistical',
                           'average', 'historical_growth')
                - 'config': Method-specific parameters (growth rate, distribution, etc.)
            historical_periods: Optional list of historical periods to use as base.
                If not provided, will be inferred from the graph's existing periods.
            **kwargs: Additional arguments passed to the forecasting logic.

        Returns:
            None (modifies the graph in-place)

        Raises:
            ValueError: If no historical periods found, no forecast periods provided,
                       or invalid forecasting method/configuration.

        Example:
            >>> forecaster = StatementForecaster(graph)
            >>> forecaster.create_forecast(
            ...     forecast_periods=['2024', '2025'],
            ...     node_configs={
            ...         'revenue': {'method': 'simple', 'config': 0.05},  # 5% growth
            ...         'costs': {'method': 'curve', 'config': [0.03, 0.04]}  # Variable growth
            ...     }
            ... )
        """
        logger.info(f"StatementForecaster: Creating forecast for periods {forecast_periods}")
        try:
            # Use PeriodManager to infer historical periods
            historical_periods = PeriodManager.infer_historical_periods(
                self.fsg, forecast_periods, historical_periods
            )

            # Validate inputs using ForecastValidator
            ForecastValidator.validate_forecast_inputs(
                historical_periods, forecast_periods, node_configs
            )

            # Ensure forecast periods exist in the graph
            PeriodManager.ensure_periods_exist(self.fsg, forecast_periods, add_missing=True)

            if node_configs is None:
                node_configs = {}

            for node_name, config in node_configs.items():
                node = self.fsg.get_node(node_name)
                if node is None:
                    raise ValueError(f"Node {node_name} not found in graph")

                # Validate node can be forecasted
                forecast_config = ForecastValidator.validate_forecast_config(config)
                ForecastValidator.validate_node_for_forecast(node, forecast_config.method)

                self._forecast_node(node, historical_periods, forecast_periods, forecast_config)

            logger.info(
                f"Created forecast for {len(forecast_periods)} periods and {len(node_configs)} nodes"
            )
        except Exception as e:
            logger.error(f"Error creating forecast: {e}", exc_info=True)
            raise

    def _forecast_node(
        self,
        node: Node,
        historical_periods: list[str],
        forecast_periods: list[str],
        forecast_config: ForecastConfig,
        **kwargs: Any,
    ) -> None:
        """Calculate forecast values and update the original node.

        **IMPORTANT**: This is an internal MUTATING method that:
        - Creates a temporary forecast node for calculations
        - Updates the original node's values dictionary with forecast results
        - Clears the original node's cache after updates

        This method should not be called directly. Use `create_forecast` for
        mutating operations or `forecast_value` for non-mutating operations.

        Args:
            node: The graph node to forecast. Must have a 'values' dictionary.
            historical_periods: List of historical periods for base values.
            forecast_periods: List of periods for which to calculate forecasts.
            forecast_config: Validated forecast configuration.
            **kwargs: Additional arguments passed to growth logic.

        Returns:
            None (modifies the node in-place)

        Raises:
            ValueError: If no historical periods provided or invalid method.

        Side Effects:
            - Modifies node.values dictionary with forecast data
            - Clears node cache if clear_cache method exists
        """
        logger.debug(
            f"StatementForecaster: Forecasting node {node.name} using method {forecast_config.method}"
        )

        # Determine base period using PeriodManager
        base_period = PeriodManager.determine_base_period(node, historical_periods)

        # Get the forecast method from registry
        method = get_forecast_method(forecast_config.method)

        # Get normalized parameters for NodeFactory
        # All built-in methods extend BaseForecastMethod which has get_forecast_params
        base_method = cast(BaseForecastMethod, method)
        params = base_method.get_forecast_params(forecast_config.config, forecast_periods)

        # Create a temporary node to perform calculations
        tmp_node = NodeFactory.create_forecast_node(
            name=f"{node.name}_forecast_temp",
            base_node=node,
            base_period=base_period,
            forecast_periods=forecast_periods,
            forecast_type=params["forecast_type"],
            growth_params=params["growth_params"],
        )

        # Ensure the original node has a values dictionary
        if not hasattr(node, "values") or not isinstance(node.values, dict):
            logger.error(
                f"Cannot store forecast for node {node.name}: node does not have a 'values' dictionary."
            )
            return  # Cannot proceed

        # Calculate and update original node's values
        for period in forecast_periods:
            try:
                val = tmp_node.calculate(period)
                if np.isnan(val) or np.isinf(val):
                    logger.warning(
                        f"Bad forecast {val} for {node.name}@{period}; defaulting to 0.0"
                    )
                    val = 0.0
                node.values[period] = float(val)  # Update the original node
            except Exception as e:
                logger.error(f"Error forecasting {node.name}@{period}: {e}", exc_info=True)
                node.values[period] = 0.0  # Set default on error

        # Clear cache of the original node as its values have changed
        if hasattr(node, "clear_cache") and callable(node.clear_cache):
            node.clear_cache()  # type: ignore[no-untyped-call]
            logger.debug(f"Cleared cache for node {node.name} after forecast update.")

    def forecast_value(
        self,
        node_name: str,
        forecast_periods: list[str],
        base_period: Optional[str] = None,
        forecast_config: Optional[dict[str, Any]] = None,
        **kwargs: Any,
    ) -> dict[str, float]:
        """Forecast and return values for a node without mutating the graph.

        **IMPORTANT**: This is a NON-MUTATING method that:
        - Does NOT add periods to the graph
        - Does NOT modify any node values
        - Does NOT affect the graph state in any way
        - Returns forecast values as a separate dictionary

        This method is ideal for:
        - What-if analysis
        - Comparing different forecast scenarios
        - Getting forecast values without committing them to the graph
        - API responses where you don't want to modify server state

        Args:
            node_name: Name of the node to forecast.
            forecast_periods: List of future periods to forecast.
            base_period: Optional base period to use for forecasting.
                        If omitted, will be inferred from the node's historical data.
            forecast_config: Forecast configuration dict with:
                - 'method': Forecasting method ('simple', 'curve', 'statistical',
                           'average', 'historical_growth')
                - 'config': Method-specific parameters
                If not provided, defaults to simple method with 0% growth.
            **kwargs: Additional arguments passed to the internal forecasting logic.

        Returns:
            A dictionary mapping forecast periods to their calculated values.
            Example: {'2024': 1050.0, '2025': 1102.5}

        Raises:
            ValueError: If node not found, no historical periods available,
                       or invalid forecast configuration.

        Example:
            >>> forecaster = StatementForecaster(graph)
            >>> # Get forecast without modifying the graph
            >>> values = forecaster.forecast_value(
            ...     'revenue',
            ...     forecast_periods=['2024', '2025'],
            ...     forecast_config={'method': 'simple', 'config': 0.05}
            ... )
            >>> print(values)  # {'2024': 1050.0, '2025': 1102.5}
            >>> # Original graph remains unchanged
        """
        # Locate the node
        node = self.fsg.get_node(node_name)
        if node is None:
            raise ValueError(f"Node {node_name} not found in graph")

        # Determine historical periods
        if base_period:
            historical_periods = [base_period]
        else:
            historical_periods = PeriodManager.infer_historical_periods(self.fsg, forecast_periods)

        # Validate inputs
        ForecastValidator.validate_forecast_inputs(historical_periods, forecast_periods)

        # Set default config if not provided
        if forecast_config is None:
            forecast_config = {"method": "simple", "config": 0.0}

        # Validate and create ForecastConfig
        validated_config = ForecastValidator.validate_forecast_config(forecast_config)
        ForecastValidator.validate_node_for_forecast(node, validated_config.method)

        # Determine base period
        calc_base_period = PeriodManager.determine_base_period(
            node, historical_periods, base_period
        )

        # Get the forecast method from registry
        method = get_forecast_method(validated_config.method)

        # Get normalized parameters for NodeFactory
        # All built-in methods extend BaseForecastMethod which has get_forecast_params
        base_method = cast(BaseForecastMethod, method)
        params = base_method.get_forecast_params(validated_config.config, forecast_periods)

        # Create a temporary forecast node (DO NOT add to graph)
        try:
            temp_forecast_node = NodeFactory.create_forecast_node(
                name=f"{node_name}_temp_forecast",
                base_node=node,
                base_period=calc_base_period,
                forecast_periods=forecast_periods,
                forecast_type=params["forecast_type"],
                growth_params=params["growth_params"],
            )
        except Exception as e:
            logger.error(
                f"Failed to create temporary forecast node for '{node_name}': {e}",
                exc_info=True,
            )
            raise ValueError(f"Could not create temporary forecast node: {e}") from e

        # Calculate results using the temporary node
        results: dict[str, float] = {}
        for period in forecast_periods:
            try:
                value = temp_forecast_node.calculate(period)
                # Handle potential NaN/Inf results from calculation
                results[period] = 0.0 if not np.isfinite(value) else float(value)
            except Exception as e:
                logger.warning(
                    f"Error calculating temporary forecast for {node_name}@{period}: {e}. Returning 0.0"
                )
                results[period] = 0.0

        # Validate results before returning
        ForecastValidator.validate_forecast_result(results, forecast_periods, node_name)

        return results

    def forecast_multiple(
        self,
        node_names: list[str],
        forecast_periods: list[str],
        forecast_configs: Optional[dict[str, dict[str, Any]]] = None,
        base_period: Optional[str] = None,
        **kwargs: Any,
    ) -> dict[str, ForecastResult]:
        """Forecast multiple nodes without mutating the graph.

        This is a convenience method that forecasts multiple nodes at once
        and returns structured results.

        Args:
            node_names: List of node names to forecast.
            forecast_periods: List of future periods to forecast.
            forecast_configs: Optional mapping of node names to their forecast configs.
                             If not provided, uses simple method with 0% growth for all.
            base_period: Optional base period to use for all nodes.
            **kwargs: Additional arguments passed to forecast_value.

        Returns:
            Dictionary mapping node names to ForecastResult objects.

        Example:
            >>> results = forecaster.forecast_multiple(
            ...     ['revenue', 'costs'],
            ...     ['2024', '2025'],
            ...     {'revenue': {'method': 'simple', 'config': 0.05}}
            ... )
            >>> print(results['revenue'].get_value('2024'))
        """
        results = {}
        configs = forecast_configs or {}

        for node_name in node_names:
            try:
                # Get config for this node or use default
                node_config = configs.get(node_name)

                # Forecast the node
                values = self.forecast_value(
                    node_name, forecast_periods, base_period, node_config, **kwargs
                )

                # Determine actual base period used
                node = self.fsg.get_node(node_name)
                historical_periods = PeriodManager.infer_historical_periods(
                    self.fsg, forecast_periods
                )
                actual_base_period = PeriodManager.determine_base_period(
                    node, historical_periods, base_period
                )

                # Create ForecastResult
                config = ForecastValidator.validate_forecast_config(
                    node_config or {"method": "simple", "config": 0.0}
                )

                results[node_name] = ForecastResult(
                    node_name=node_name,
                    periods=forecast_periods,
                    values=values,
                    method=config.method,
                    base_period=actual_base_period,
                )
            except Exception:
                logger.exception(f"Error forecasting node {node_name}")
                # Continue with other nodes
                continue

        return results

# --- END FILE: fin_statement_model/forecasting/forecaster.py ---

# --- START FILE: fin_statement_model/forecasting/methods/__init__.py ---
"""Forecast method implementations.

This module contains all the built-in forecast methods available in the library.
"""

from .base import ForecastMethod, BaseForecastMethod
from .simple import SimpleForecastMethod
from .curve import CurveForecastMethod
from .statistical import StatisticalForecastMethod
from .average import AverageForecastMethod
from .historical_growth import HistoricalGrowthForecastMethod

__all__ = [
    "AverageForecastMethod",
    "BaseForecastMethod",
    "CurveForecastMethod",
    "ForecastMethod",
    "HistoricalGrowthForecastMethod",
    "SimpleForecastMethod",
    "StatisticalForecastMethod",
]

# --- END FILE: fin_statement_model/forecasting/methods/__init__.py ---

# --- START FILE: fin_statement_model/forecasting/methods/average.py ---
"""Average forecasting method based on historical values.

This method forecasts future values as the average of historical values.
"""

import logging
from typing import Any, Optional
import numpy as np

from fin_statement_model.core.nodes import Node
from .base import BaseForecastMethod

logger = logging.getLogger(__name__)


class AverageForecastMethod(BaseForecastMethod):
    """Historical average forecasting.

    This method calculates forecast values as the average of historical
    values. Useful for stable metrics or when expecting mean reversion.

    Configuration:
        - Not required (pass 0 or None)
        - The method will automatically use all available historical data

    Example:
        >>> method = AverageForecastMethod()
        >>> params = method.get_forecast_params(None, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'average', 'growth_params': None}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "average"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "average"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for average method.

        Args:
            config: Not used for average method, can be None or 0.

        Note:
            Average method doesn't require configuration as it uses
            historical data automatically.
        """
        # Average method doesn't need specific configuration
        # Accept None, 0, or any placeholder value

    def normalize_params(self, config: Any, forecast_periods: list[str]) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Not used for average method.
            forecast_periods: List of periods to forecast (not used).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
            For average method, growth_params is None.
        """
        return {
            "forecast_type": self.internal_type,
            "growth_params": None,  # Average method doesn't use growth params
        }

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for average calculation.

        Args:
            node: The node to extract historical data from.
            historical_periods: List of historical periods.

        Returns:
            List of valid historical values.

        Raises:
            ValueError: If no valid historical data is available.
        """
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise ValueError(f"Node {node.name} cannot be calculated for average method")

        if not hasattr(node, "values") or not isinstance(node.values, dict):
            raise ValueError(f"Node {node.name} does not have values dictionary for average method")

        # Extract historical values
        historical_values = []
        for period in historical_periods:
            if period in node.values:
                try:
                    value = node.calculate(period)
                    if value is not None and not np.isnan(value) and not np.isinf(value):
                        historical_values.append(float(value))
                except Exception as e:
                    # Log the exception and skip this period
                    logger.debug(
                        f"Skipping period {period} for node {node.name} in average calculation: {e}"
                    )
                    continue

        if not historical_values:
            raise ValueError(
                f"No valid historical data available for node {node.name} to compute average"
            )

        return historical_values

# --- END FILE: fin_statement_model/forecasting/methods/average.py ---

# --- START FILE: fin_statement_model/forecasting/methods/base.py ---
"""Base protocol and abstract class for forecast methods.

This module defines the interface that all forecast methods must implement.
"""

from typing import Protocol, Any, Optional, runtime_checkable
from abc import ABC, abstractmethod

from fin_statement_model.core.nodes import Node


@runtime_checkable
class ForecastMethod(Protocol):
    """Protocol that all forecast methods must implement."""

    @property
    def name(self) -> str:
        """Return the method name."""
        ...

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for this method.

        Args:
            config: The method-specific configuration to validate.

        Raises:
            ValueError: If configuration is invalid.
        """
        ...

    def normalize_params(self, config: Any, forecast_periods: list[str]) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: The method-specific configuration.
            forecast_periods: List of periods to forecast.

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
        """
        ...

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for methods that need it.

        Args:
            node: The node to extract historical data from.
            historical_periods: List of historical periods.

        Returns:
            List of historical values or None if not needed.
        """
        ...


class BaseForecastMethod(ABC):
    """Abstract base class for forecast methods.

    Provides common functionality and enforces the interface.
    """

    @property
    @abstractmethod
    def name(self) -> str:
        """Return the method name."""

    @property
    @abstractmethod
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""

    @abstractmethod
    def validate_config(self, config: Any) -> None:
        """Validate the configuration for this method."""

    @abstractmethod
    def normalize_params(self, config: Any, forecast_periods: list[str]) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory."""

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for methods that need it.

        Default implementation returns None (not needed).
        Override in methods that require historical data.
        """
        return None

    def get_forecast_params(self, config: Any, forecast_periods: list[str]) -> dict[str, Any]:
        """Get complete forecast parameters.

        This is a convenience method that validates and normalizes in one call.

        Args:
            config: The method-specific configuration.
            forecast_periods: List of periods to forecast.

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.

        Raises:
            ValueError: If configuration is invalid.
        """
        self.validate_config(config)
        return self.normalize_params(config, forecast_periods)

# --- END FILE: fin_statement_model/forecasting/methods/base.py ---

# --- START FILE: fin_statement_model/forecasting/methods/curve.py ---
"""Curve forecasting method with variable growth rates.

This method applies different growth rates for each forecast period.
"""

from typing import Any

from .base import BaseForecastMethod


class CurveForecastMethod(BaseForecastMethod):
    """Variable growth rate forecasting.

    This method applies different growth rates for each forecast period,
    allowing for non-linear growth patterns.

    Configuration:
        - List of numeric values: One growth rate per forecast period
        - Single numeric value: Will be expanded to match forecast periods

    Example:
        >>> method = CurveForecastMethod()
        >>> params = method.get_forecast_params([0.05, 0.04, 0.03], ['2024', '2025', '2026'])
        >>> # Returns: {'forecast_type': 'curve', 'growth_params': [0.05, 0.04, 0.03]}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "curve"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "curve"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for curve method.

        Args:
            config: Should be a numeric value or a list of numeric values.

        Raises:
            ValueError: If config is empty list.
            TypeError: If config is not numeric or list of numerics.
        """
        if isinstance(config, list):
            if not config:
                raise ValueError("Curve method: empty list provided")
            for i, value in enumerate(config):
                if not isinstance(value, int | float):
                    raise TypeError(f"Curve method: non-numeric value at index {i}: {type(value)}")
        elif not isinstance(config, int | float):
            raise TypeError(
                f"Curve method requires numeric or list of numeric values, got {type(config)}"
            )

    def normalize_params(self, config: Any, forecast_periods: list[str]) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Growth rates (single value or list).
            forecast_periods: List of periods to forecast.

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.

        Raises:
            ValueError: If list length doesn't match forecast periods.
        """
        if not isinstance(config, list):
            # Single value - expand to match forecast periods
            growth_rates = [float(config)] * len(forecast_periods)
        else:
            # List of values - must match forecast periods length
            if len(config) != len(forecast_periods):
                raise ValueError(
                    f"Curve method: growth rate list length ({len(config)}) "
                    f"must match forecast periods ({len(forecast_periods)})"
                )
            growth_rates = [float(x) for x in config]

        return {"forecast_type": self.internal_type, "growth_params": growth_rates}

# --- END FILE: fin_statement_model/forecasting/methods/curve.py ---

# --- START FILE: fin_statement_model/forecasting/methods/historical_growth.py ---
"""Historical growth forecasting method based on past growth patterns.

This method calculates future values based on the average historical growth rate.
"""

import logging
from typing import Any, Optional
import numpy as np

from fin_statement_model.core.nodes import Node
from .base import BaseForecastMethod

logger = logging.getLogger(__name__)


class HistoricalGrowthForecastMethod(BaseForecastMethod):
    """Historical growth pattern forecasting.

    This method calculates the average historical growth rate and applies
    it to forecast future values. It's useful when past growth patterns
    are expected to continue.

    Configuration:
        - Not required (pass 0 or None)
        - The method will automatically calculate growth from historical data

    Example:
        >>> method = HistoricalGrowthForecastMethod()
        >>> params = method.get_forecast_params(None, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'historical_growth', 'growth_params': None}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "historical_growth"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "historical_growth"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for historical growth method.

        Args:
            config: Not used for historical growth method, can be None or 0.

        Note:
            Historical growth method doesn't require configuration as it
            calculates growth from historical data automatically.
        """
        # Historical growth method doesn't need specific configuration
        # Accept None, 0, or any placeholder value

    def normalize_params(self, config: Any, forecast_periods: list[str]) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Not used for historical growth method.
            forecast_periods: List of periods to forecast (not used).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
            For historical growth method, growth_params is None.
        """
        return {
            "forecast_type": self.internal_type,
            "growth_params": None,  # Historical growth method calculates internally
        }

    def prepare_historical_data(
        self, node: Node, historical_periods: list[str]
    ) -> Optional[list[float]]:
        """Prepare historical data for growth calculation.

        Args:
            node: The node to extract historical data from.
            historical_periods: List of historical periods.

        Returns:
            List of valid historical values (at least 2 needed for growth).

        Raises:
            ValueError: If insufficient historical data is available.
        """
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise ValueError(f"Node {node.name} cannot be calculated for historical growth method")

        if not hasattr(node, "values") or not isinstance(node.values, dict):
            raise ValueError(
                f"Node {node.name} does not have values dictionary for historical growth method"
            )

        # Extract historical values in chronological order
        historical_values = []
        for period in historical_periods:
            if period in node.values:
                try:
                    value = node.calculate(period)
                    if value is not None and not np.isnan(value) and not np.isinf(value):
                        historical_values.append(float(value))
                except Exception as e:
                    # Log the exception and skip this period
                    logger.debug(
                        f"Skipping period {period} for node {node.name} in historical growth calculation: {e}"
                    )
                    continue

        if len(historical_values) < 2:
            raise ValueError(
                f"Need at least 2 historical data points for node {node.name} "
                f"to compute growth rate, found {len(historical_values)}"
            )

        return historical_values

    def calculate_average_growth_rate(self, historical_values: list[float]) -> float:
        """Calculate the average growth rate from historical values.

        Args:
            historical_values: List of historical values in chronological order.

        Returns:
            Average growth rate.

        Note:
            This is a helper method that can be used by the forecast node
            implementation to calculate the growth rate.
        """
        if len(historical_values) < 2:
            return 0.0

        # Calculate period-over-period growth rates
        growth_rates = []
        for i in range(1, len(historical_values)):
            if historical_values[i - 1] != 0:
                growth_rate = (historical_values[i] - historical_values[i - 1]) / historical_values[
                    i - 1
                ]
                growth_rates.append(growth_rate)

        # Return average growth rate, or 0 if no valid rates
        return float(np.mean(growth_rates)) if growth_rates else 0.0

# --- END FILE: fin_statement_model/forecasting/methods/historical_growth.py ---

# --- START FILE: fin_statement_model/forecasting/methods/simple.py ---
"""Simple growth rate forecasting method.

This method applies a constant growth rate to forecast future values.
"""

from typing import Any

from .base import BaseForecastMethod


class SimpleForecastMethod(BaseForecastMethod):
    """Simple growth rate forecasting.

    This method applies a constant growth rate to the base value
    for all forecast periods.

    Configuration:
        - Single numeric value: The growth rate (e.g., 0.05 for 5% growth)
        - List with single value: Will use the first value

    Example:
        >>> method = SimpleForecastMethod()
        >>> params = method.get_forecast_params(0.05, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'simple', 'growth_params': 0.05}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "simple"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "simple"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for simple method.

        Args:
            config: Should be a numeric value or a list containing a numeric value.

        Raises:
            ValueError: If config is empty list.
            TypeError: If config is not numeric or a list with numeric value.
        """
        if isinstance(config, list):
            if not config:
                raise ValueError("Simple method: empty list provided")
            if not isinstance(config[0], int | float):
                raise TypeError(
                    f"Simple method requires numeric growth rate, got {type(config[0])}"
                )
        elif not isinstance(config, int | float):
            raise TypeError(f"Simple method requires numeric growth rate, got {type(config)}")

    def normalize_params(self, config: Any, forecast_periods: list[str]) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: The growth rate (numeric or list with numeric).
            forecast_periods: List of periods to forecast (not used for simple method).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
        """
        # Handle list input - take first value
        growth_rate = float(config[0]) if isinstance(config, list) else float(config)

        return {"forecast_type": self.internal_type, "growth_params": growth_rate}

# --- END FILE: fin_statement_model/forecasting/methods/simple.py ---

# --- START FILE: fin_statement_model/forecasting/methods/statistical.py ---
"""Statistical forecasting method using random sampling.

This method generates forecast values by sampling from statistical distributions.
"""

from typing import Any
import numpy as np

from .base import BaseForecastMethod
from fin_statement_model.forecasting.types import StatisticalConfig


class StatisticalForecastMethod(BaseForecastMethod):
    """Statistical forecasting using random distributions.

    This method generates forecast values by sampling from specified
    statistical distributions, useful for Monte Carlo simulations
    and uncertainty analysis.

    Configuration:
        Dict with:
        - 'distribution': 'normal' or 'uniform'
        - 'params': Distribution-specific parameters
            - For 'normal': {'mean': float, 'std': float}
            - For 'uniform': {'low': float, 'high': float}

    Example:
        >>> method = StatisticalForecastMethod()
        >>> config = {
        ...     'distribution': 'normal',
        ...     'params': {'mean': 0.05, 'std': 0.02}
        ... }
        >>> params = method.get_forecast_params(config, ['2024', '2025'])
        >>> # Returns: {'forecast_type': 'statistical', 'growth_params': <callable>}
    """

    @property
    def name(self) -> str:
        """Return the method name."""
        return "statistical"

    @property
    def internal_type(self) -> str:
        """Return the internal forecast type for NodeFactory."""
        return "statistical"

    def validate_config(self, config: Any) -> None:
        """Validate the configuration for statistical method.

        Args:
            config: Should be a dict with 'distribution' and 'params' keys.

        Raises:
            TypeError: If config is invalid.
        """
        if not isinstance(config, dict):
            raise TypeError(f"Statistical method requires dict configuration, got {type(config)}")

        if "distribution" not in config:
            raise ValueError("Statistical method requires 'distribution' key")

        if "params" not in config:
            raise ValueError("Statistical method requires 'params' key")

        # Validate using StatisticalConfig dataclass
        try:
            StatisticalConfig(distribution=config["distribution"], params=config["params"])
        except (ValueError, TypeError) as e:
            raise ValueError(f"Invalid statistical configuration: {e}") from e

    def normalize_params(self, config: Any, forecast_periods: list[str]) -> dict[str, Any]:
        """Normalize parameters for the NodeFactory.

        Args:
            config: Statistical distribution configuration.
            forecast_periods: List of periods to forecast (not used).

        Returns:
            Dict with 'forecast_type' and 'growth_params' keys.
            The 'growth_params' value is a callable that generates random values.
        """
        # Create validated config
        stat_config = StatisticalConfig(
            distribution=config["distribution"], params=config["params"]
        )

        # Create generator function based on distribution
        def generator() -> float:
            """Generate a random growth rate from the specified distribution."""
            if stat_config.distribution == "normal":
                return float(
                    np.random.normal(stat_config.params["mean"], stat_config.params["std"])
                )
            elif stat_config.distribution == "uniform":
                return float(
                    np.random.uniform(stat_config.params["low"], stat_config.params["high"])
                )
            else:
                # This shouldn't happen due to validation, but just in case
                raise ValueError(f"Unsupported distribution: {stat_config.distribution}")

        return {"forecast_type": self.internal_type, "growth_params": generator}

# --- END FILE: fin_statement_model/forecasting/methods/statistical.py ---

# --- START FILE: fin_statement_model/forecasting/period_manager.py ---
"""Period inference and management utilities for forecasting.

This module handles the logic for determining historical periods,
base periods, and managing period-related operations.
"""

import logging
from typing import Any, Optional

from fin_statement_model.core.nodes import Node

logger = logging.getLogger(__name__)


class PeriodManager:
    """Handles period inference and management for forecasting.

    This class provides utilities for:
    - Inferring historical periods from graph state
    - Determining base periods for forecasting
    - Validating period sequences
    - Managing period transitions
    """

    @staticmethod
    def infer_historical_periods(
        graph: Any,
        forecast_periods: list[str],
        provided_periods: Optional[list[str]] = None,
    ) -> list[str]:
        """Infer historical periods from graph state.

        Args:
            graph: The financial statement graph instance.
            forecast_periods: List of periods to forecast.
            provided_periods: Optional explicitly provided historical periods.

        Returns:
            List of historical periods.

        Raises:
            ValueError: If historical periods cannot be determined.
        """
        # If explicitly provided, use them
        if provided_periods is not None:
            logger.debug(f"Using explicitly provided historical periods: {provided_periods}")
            return provided_periods

        # Check if graph has a custom method for getting historical periods
        if hasattr(graph, "get_historical_periods") and callable(graph.get_historical_periods):
            historical = graph.get_historical_periods()
            logger.debug(f"Using graph's get_historical_periods method: {historical}")
            return historical

        # Otherwise, infer from graph periods and forecast periods
        if not hasattr(graph, "periods") or not graph.periods:
            raise ValueError("Cannot infer historical periods: graph has no periods attribute")

        if not forecast_periods:
            raise ValueError("Cannot infer historical periods: no forecast periods provided")

        # Try to find where forecast periods start
        first_forecast = forecast_periods[0]
        try:
            idx = graph.periods.index(first_forecast)
            historical_periods = graph.periods[:idx]
            logger.debug(
                f"Inferred historical periods by splitting at {first_forecast}: "
                f"{historical_periods}"
            )
        except ValueError:
            # First forecast period not in graph periods
            # Assume all current periods are historical
            historical_periods = list(graph.periods)
            logger.warning(
                f"First forecast period {first_forecast} not found in graph periods. "
                f"Using all existing periods as historical: {historical_periods}"
            )

        if not historical_periods:
            raise ValueError(
                "No historical periods found. Ensure graph has periods before "
                "the first forecast period."
            )

        return historical_periods

    @staticmethod
    def determine_base_period(
        node: Node,
        historical_periods: list[str],
        preferred_period: Optional[str] = None,
    ) -> str:
        """Determine the base period for forecasting a node.

        Args:
            node: The node to forecast.
            historical_periods: List of available historical periods.
            preferred_period: Optional preferred base period.

        Returns:
            The base period to use for forecasting.

        Raises:
            ValueError: If no valid base period can be determined.
        """
        if not historical_periods:
            raise ValueError("No historical periods provided")

        # If preferred period is specified and valid, use it
        if (
            preferred_period
            and preferred_period in historical_periods
            and hasattr(node, "values")
            and isinstance(node.values, dict)
            and preferred_period in node.values
        ):
            logger.debug(f"Using preferred base period {preferred_period} for {node.name}")
            return preferred_period

        # Try to find the most recent period with data for this node
        if hasattr(node, "values") and isinstance(node.values, dict):
            # Get periods that have values for this node
            available_periods = [p for p in historical_periods if p in node.values]

            if available_periods:
                # Use the most recent available period
                # Assuming periods are in chronological order
                base_period = available_periods[-1]
                logger.debug(
                    f"Using most recent period with data as base for {node.name}: {base_period}"
                )
                return base_period

        # Fallback: use the last historical period
        base_period = historical_periods[-1]
        logger.info(
            f"Using last historical period as base for {node.name}: {base_period} "
            f"(node may lack values)"
        )
        return base_period

    @staticmethod
    def validate_period_sequence(periods: list[str]) -> None:
        """Validate that a period sequence is valid.

        Args:
            periods: List of periods to validate.

        Raises:
            ValueError: If the period sequence is invalid.
        """
        if not periods:
            raise ValueError("Period sequence cannot be empty")

        if len(periods) != len(set(periods)):
            duplicates = [p for p in periods if periods.count(p) > 1]
            raise ValueError(f"Period sequence contains duplicates: {set(duplicates)}")

    @staticmethod
    def get_period_index(period: str, periods: list[str]) -> int:
        """Get the index of a period in a period list.

        Args:
            period: The period to find.
            periods: List of periods.

        Returns:
            The index of the period.

        Raises:
            ValueError: If period not found in list.
        """
        try:
            return periods.index(period)
        except ValueError:
            raise ValueError(f"Period '{period}' not found in period list") from None

    @staticmethod
    def ensure_periods_exist(graph: Any, periods: list[str], add_missing: bool = True) -> list[str]:
        """Ensure periods exist in the graph.

        Args:
            graph: The financial statement graph instance.
            periods: List of periods that should exist.
            add_missing: Whether to add missing periods to the graph.

        Returns:
            List of periods that were added (empty if none).

        Raises:
            ValueError: If add_missing is False and periods are missing.
        """
        if not hasattr(graph, "periods"):
            raise ValueError("Graph does not have a periods attribute")

        existing_periods = set(graph.periods)
        missing_periods = [p for p in periods if p not in existing_periods]

        if missing_periods:
            if add_missing:
                # Add missing periods to graph
                if hasattr(graph, "add_periods") and callable(graph.add_periods):
                    graph.add_periods(missing_periods)
                    logger.info(f"Added missing periods to graph: {missing_periods}")
                else:
                    raise ValueError(
                        f"Graph is missing periods {missing_periods} but has no add_periods method"
                    )
            else:
                raise ValueError(
                    f"The following periods do not exist in the graph: {missing_periods}"
                )

        return missing_periods

# --- END FILE: fin_statement_model/forecasting/period_manager.py ---

# --- START FILE: fin_statement_model/forecasting/strategies.py ---
"""Forecast method registry and selection strategies.

This module provides a registry for forecast methods and handles method
selection and configuration.
"""

import logging
from typing import Any

from .methods import (
    ForecastMethod,
    SimpleForecastMethod,
    CurveForecastMethod,
    StatisticalForecastMethod,
    AverageForecastMethod,
    HistoricalGrowthForecastMethod,
)

logger = logging.getLogger(__name__)


class ForecastMethodRegistry:
    """Registry for forecast methods.

    This class manages the available forecast methods and provides
    a centralized way to access and register them.

    Example:
        >>> registry = ForecastMethodRegistry()
        >>> method = registry.get_method('simple')
        >>> print(registry.list_methods())
        ['simple', 'curve', 'statistical', 'average', 'historical_growth']
    """

    def __init__(self) -> None:
        """Initialize the registry with built-in methods."""
        self._methods: dict[str, ForecastMethod] = {}
        self._register_builtin_methods()

    def _register_builtin_methods(self) -> None:
        """Register all built-in forecast methods."""
        builtin_methods = [
            SimpleForecastMethod(),
            CurveForecastMethod(),
            StatisticalForecastMethod(),
            AverageForecastMethod(),
            HistoricalGrowthForecastMethod(),
        ]

        for method in builtin_methods:
            self.register(method)
            logger.debug(f"Registered built-in forecast method: {method.name}")

    def register(self, method: ForecastMethod) -> None:
        """Register a new forecast method.

        Args:
            method: The forecast method to register.

        Raises:
            ValueError: If a method with the same name is already registered.
        """
        if method.name in self._methods:
            raise ValueError(f"Forecast method '{method.name}' is already registered")

        self._methods[method.name] = method
        logger.info(f"Registered forecast method: {method.name}")

    def unregister(self, name: str) -> None:
        """Unregister a forecast method.

        Args:
            name: The name of the method to unregister.

        Raises:
            KeyError: If the method is not registered.
        """
        if name not in self._methods:
            raise KeyError(f"Forecast method '{name}' is not registered")

        del self._methods[name]
        logger.info(f"Unregistered forecast method: {name}")

    def get_method(self, name: str) -> ForecastMethod:
        """Get a forecast method by name.

        Args:
            name: The name of the method to retrieve.

        Returns:
            The requested forecast method.

        Raises:
            ValueError: If the method is not registered.
        """
        if name not in self._methods:
            available = ", ".join(sorted(self._methods.keys()))
            raise ValueError(f"Unknown forecast method: '{name}'. Available methods: {available}")

        return self._methods[name]

    def list_methods(self) -> list[str]:
        """List all available forecast methods.

        Returns:
            Sorted list of registered method names.
        """
        return sorted(self._methods.keys())

    def has_method(self, name: str) -> bool:
        """Check if a method is registered.

        Args:
            name: The name of the method to check.

        Returns:
            True if the method is registered, False otherwise.
        """
        return name in self._methods

    def get_method_info(self, name: str) -> dict[str, Any]:
        """Get information about a forecast method.

        Args:
            name: The name of the method.

        Returns:
            Dictionary with method information including docstring.

        Raises:
            ValueError: If the method is not registered.
        """
        method = self.get_method(name)
        return {
            "name": method.name,
            "class": method.__class__.__name__,
            "description": method.__class__.__doc__ or "No description available",
            "module": method.__class__.__module__,
        }


# Global registry instance
forecast_registry = ForecastMethodRegistry()


def get_forecast_method(name: str) -> ForecastMethod:
    """Get a forecast method from the global registry.

    This is a convenience function that uses the global registry.

    Args:
        name: The name of the method to retrieve.

    Returns:
        The requested forecast method.

    Raises:
        ValueError: If the method is not registered.
    """
    return forecast_registry.get_method(name)


def register_forecast_method(method: ForecastMethod) -> None:
    """Register a custom forecast method in the global registry.

    This is a convenience function that uses the global registry.

    Args:
        method: The forecast method to register.

    Raises:
        ValueError: If a method with the same name is already registered.
    """
    forecast_registry.register(method)

# --- END FILE: fin_statement_model/forecasting/strategies.py ---

# --- START FILE: fin_statement_model/forecasting/types.py ---
"""Type definitions and data structures for forecasting module.

This module contains all the type aliases, enums, and data structures
used throughout the forecasting sub-module.
"""

from typing import Any, Union, Literal
from collections.abc import Callable
import numpy as np
from dataclasses import dataclass

# Type aliases for clarity
Numeric = Union[int, float, np.number[Any]]
GrowthRate = Union[float, list[float], Callable[[], float]]
PeriodValue = dict[str, float]

# Forecast method types
ForecastMethodType = Literal["simple", "curve", "statistical", "average", "historical_growth"]


@dataclass
class ForecastConfig:
    """Configuration for a forecast operation."""

    method: ForecastMethodType
    config: Any  # Method-specific configuration

    def __post_init__(self) -> None:
        """Validate the forecast configuration."""
        if self.method not in [
            "simple",
            "curve",
            "statistical",
            "average",
            "historical_growth",
        ]:
            raise ValueError(f"Invalid forecast method: {self.method}")


@dataclass
class StatisticalConfig:
    """Configuration for statistical forecasting methods."""

    distribution: Literal["normal", "uniform"]
    params: dict[str, float]

    def __post_init__(self) -> None:
        """Validate the statistical configuration."""
        if self.distribution == "normal":
            if "mean" not in self.params or "std" not in self.params:
                raise ValueError("Normal distribution requires 'mean' and 'std' parameters")
        elif self.distribution == "uniform":
            if "low" not in self.params or "high" not in self.params:
                raise ValueError("Uniform distribution requires 'low' and 'high' parameters")
        else:
            raise ValueError(f"Unsupported distribution: {self.distribution}")


@dataclass
class ForecastResult:
    """Result of a forecast operation."""

    node_name: str
    periods: list[str]
    values: PeriodValue
    method: ForecastMethodType
    base_period: str

    def get_value(self, period: str) -> float:
        """Get the forecast value for a specific period."""
        if period not in self.values:
            raise KeyError(f"Period {period} not found in forecast results")
        return self.values[period]

# --- END FILE: fin_statement_model/forecasting/types.py ---

# --- START FILE: fin_statement_model/forecasting/validators.py ---
"""Input validation and error checking for forecasting operations.

This module provides validation utilities to ensure forecast inputs
are valid before processing.
"""

import logging
from typing import Any, Optional

from fin_statement_model.core.nodes import Node
from .types import ForecastMethodType, ForecastConfig

logger = logging.getLogger(__name__)


class ForecastValidator:
    """Validates inputs for forecasting operations.

    This class provides methods to validate various aspects of forecast
    inputs including periods, node configurations, and method parameters.
    """

    @staticmethod
    def validate_forecast_inputs(
        historical_periods: list[str],
        forecast_periods: list[str],
        node_configs: Optional[dict[str, dict[str, Any]]] = None,
    ) -> None:
        """Validate basic forecast inputs.

        Args:
            historical_periods: List of historical periods.
            forecast_periods: List of periods to forecast.
            node_configs: Optional node configuration mapping.

        Raises:
            ValueError: If inputs are logically invalid.
            TypeError: If inputs are of wrong type.
        """
        # Validate historical periods
        if not historical_periods:
            raise ValueError("No historical periods provided for forecasting")

        if not isinstance(historical_periods, list):
            raise TypeError(f"Historical periods must be a list, got {type(historical_periods)}")

        # Validate forecast periods
        if not forecast_periods:
            raise ValueError("No forecast periods provided")

        if not isinstance(forecast_periods, list):
            raise TypeError(f"Forecast periods must be a list, got {type(forecast_periods)}")

        # Check for overlapping periods
        historical_set = set(historical_periods)
        forecast_set = set(forecast_periods)
        overlap = historical_set & forecast_set
        if overlap:
            logger.warning(
                f"Forecast periods overlap with historical periods: {overlap}. "
                f"This may overwrite historical data."
            )

        # Validate node configs if provided
        if node_configs is not None:
            if not isinstance(node_configs, dict):
                raise TypeError(f"Node configs must be a dict, got {type(node_configs)}")

            for node_name, config in node_configs.items():
                ForecastValidator.validate_node_config(node_name, config)

    @staticmethod
    def validate_node_config(node_name: str, config: dict[str, Any]) -> None:
        """Validate configuration for a single node.

        Args:
            node_name: Name of the node being configured.
            config: Configuration dictionary for the node.

        Raises:
            ValueError: If configuration is logically invalid.
            TypeError: If configuration is of wrong type.
        """
        if not isinstance(config, dict):
            raise TypeError(
                f"Configuration for node '{node_name}' must be a dict, got {type(config)}"
            )

        # Validate method
        if "method" not in config:
            raise ValueError(f"Configuration for node '{node_name}' missing required 'method' key")

        method = config["method"]
        valid_methods: list[ForecastMethodType] = [
            "simple",
            "curve",
            "statistical",
            "average",
            "historical_growth",
        ]
        if method not in valid_methods:
            raise ValueError(
                f"Invalid forecast method '{method}' for node '{node_name}'. "
                f"Valid methods: {valid_methods}"
            )

        # Validate config exists (can be None for some methods)
        if "config" not in config:
            raise ValueError(f"Configuration for node '{node_name}' missing required 'config' key")

    @staticmethod
    def validate_node_for_forecast(node: Node, method: str) -> None:
        """Validate that a node can be forecasted with the given method.

        Args:
            node: The node to validate.
            method: The forecast method to use.

        Raises:
            ValueError: If node cannot be forecasted.
        """
        # Check if node has values dictionary
        if not hasattr(node, "values") or not isinstance(node.values, dict):
            raise ValueError(
                f"Node '{node.name}' cannot be forecasted: missing or invalid "
                f"'values' attribute. Only nodes with values dictionaries can "
                f"be forecasted."
            )

        # Check if node has calculate method for certain forecast types
        if method in ["average", "historical_growth"] and (
            not hasattr(node, "calculate") or not callable(node.calculate)
        ):
            raise ValueError(
                f"Node '{node.name}' cannot use '{method}' forecast method: "
                f"missing calculate() method"
            )

    @staticmethod
    def validate_forecast_config(config: dict[str, Any]) -> ForecastConfig:
        """Validate and convert a forecast configuration dictionary.

        Args:
            config: Raw configuration dictionary.

        Returns:
            Validated ForecastConfig instance.

        Raises:
            ValueError: If configuration is logically invalid.
            TypeError: If configuration is of wrong type.
        """
        if not isinstance(config, dict):
            raise TypeError(f"Forecast config must be a dict, got {type(config)}")

        if "method" not in config:
            raise ValueError("Forecast config missing required 'method' key")

        if "config" not in config:
            raise ValueError("Forecast config missing required 'config' key")

        # Create and validate using dataclass
        return ForecastConfig(method=config["method"], config=config["config"])

    @staticmethod
    def validate_base_period(
        base_period: str, available_periods: list[str], node_name: str
    ) -> None:
        """Validate that a base period is valid for forecasting.

        Args:
            base_period: The proposed base period.
            available_periods: List of available periods.
            node_name: Name of the node (for error messages).

        Raises:
            ValueError: If base period is invalid.
        """
        if not base_period:
            raise ValueError(f"No base period determined for node '{node_name}'")

        if base_period not in available_periods:
            raise ValueError(
                f"Base period '{base_period}' for node '{node_name}' not found in available periods"
            )

    @staticmethod
    def validate_forecast_result(
        result: dict[str, float], expected_periods: list[str], node_name: str
    ) -> None:
        """Validate forecast results.

        Args:
            result: Dictionary of period -> value mappings.
            expected_periods: List of expected forecast periods.
            node_name: Name of the node (for error messages).

        Raises:
            ValueError: If results are logically invalid or incomplete.
            TypeError: If results are of wrong type.
        """
        if not isinstance(result, dict):
            raise TypeError(
                f"Forecast result for node '{node_name}' must be a dict, got {type(result)}"
            )

        # Check all expected periods are present
        missing_periods = set(expected_periods) - set(result.keys())
        if missing_periods:
            raise ValueError(
                f"Forecast result for node '{node_name}' missing periods: {missing_periods}"
            )

        # Validate all values are numeric
        for period, value in result.items():
            if not isinstance(value, int | float):
                raise TypeError(
                    f"Forecast value for node '{node_name}' period '{period}' "
                    f"must be numeric, got {type(value)}"
                )

# --- END FILE: fin_statement_model/forecasting/validators.py ---

# --- START FILE: fin_statement_model/io/__init__.py ---
"""Input/Output components for the Financial Statement Model.

This package provides a unified interface for reading and writing financial model
data from/to various formats using a registry-based approach.
"""

import logging

from .core import (
    DataReader,
    DataWriter,
    get_reader,
    get_writer,
    list_readers,
    list_writers,
    read_data,
    write_data,
)
from .exceptions import IOError, ReadError, WriteError, FormatNotSupportedError

# Import specialized functions for convenience
from .specialized import (
    import_from_cells,
    load_adjustments_from_excel,
    export_adjustments_to_excel,
    list_available_builtin_configs,
    read_builtin_statement_config,
    read_statement_config_from_path,
    read_statement_configs_from_directory,
    write_statement_to_excel,
    write_statement_to_json,
)

# Configure logging for the io package
logger = logging.getLogger(__name__)

# --- Trigger Registration ---
# Import format modules to ensure their @register decorators run.
# This makes them available in the registry when the io package is imported.
try:
    from . import formats  # noqa: F401
    from . import specialized  # noqa: F401
except ImportError:
    # This might happen during setup or if directories are missing
    logger.warning("Could not automatically import formats/specialized modules")


# --- Public API ---

__all__ = [
    # Base classes
    "DataReader",
    "DataWriter",
    # Exceptions
    "FormatNotSupportedError",
    "IOError",
    "ReadError",
    "WriteError",
    # Specialized functions
    "export_adjustments_to_excel",
    # Registry functions
    "get_reader",
    "get_writer",
    "import_from_cells",
    "list_available_builtin_configs",
    "list_readers",
    "list_writers",
    "load_adjustments_from_excel",
    "read_builtin_statement_config",
    # Facade functions
    "read_data",
    "read_statement_config_from_path",
    "read_statement_configs_from_directory",
    "write_data",
    "write_statement_to_excel",
    "write_statement_to_json",
]

# --- END FILE: fin_statement_model/io/__init__.py ---

# --- START FILE: fin_statement_model/io/config/models.py ---
"""Pydantic models for IO reader and writer configuration.

This module provides declarative schemas for validating configuration passed to IO readers.
"""

from __future__ import annotations

from typing import Optional, Literal, Any, Union
from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator

# Import the type used in MarkdownWriterConfig
from fin_statement_model.core.adjustments.models import (
    AdjustmentFilterInput,
)

# Define MappingConfig locally to avoid circular import
MappingConfig = Union[dict[str, str], dict[Optional[str], dict[str, str]]]


class BaseReaderConfig(BaseModel):
    """Base configuration for IO readers."""

    source: str = Field(..., description="URI or path to data source (file path, ticker, etc.)")
    format_type: Literal["csv", "excel", "dataframe", "dict", "fmp"] = Field(
        ..., description="Type of reader (csv, excel, dataframe, dict, fmp)."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class CsvReaderConfig(BaseReaderConfig):
    """CSV reader options."""

    delimiter: str = Field(",", description="Field delimiter for CSV files.")
    header_row: int = Field(1, description="Row number containing column names (1-indexed).")
    index_col: Optional[int] = Field(None, description="1-indexed column for row labels.")
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    @model_validator(mode="after")
    def check_header_row(cls, cfg: CsvReaderConfig) -> CsvReaderConfig:
        """Ensure header_row is at least 1."""
        if cfg.header_row < 1:
            raise ValueError("header_row must be >= 1")
        return cfg


class ExcelReaderConfig(BaseReaderConfig):
    """Excel reader options."""

    sheet_name: Optional[str] = Field(None, description="Worksheet name or index.")
    items_col: int = Field(1, description="1-indexed column where item names reside.")
    periods_row: int = Field(1, description="1-indexed row where periods reside.")
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    @model_validator(mode="after")
    def check_indices(cls, cfg: ExcelReaderConfig) -> ExcelReaderConfig:
        """Ensure items_col and periods_row are at least 1."""
        if cfg.items_col < 1 or cfg.periods_row < 1:
            raise ValueError("items_col and periods_row must be >= 1")
        return cfg


class FmpReaderConfig(BaseReaderConfig):
    """Financial Modeling Prep API reader options."""

    statement_type: Literal["income_statement", "balance_sheet", "cash_flow"] = Field(
        ..., description="Type of financial statement to fetch."
    )
    period_type: Literal["FY", "QTR"] = Field("FY", description="Period type: 'FY' or 'QTR'.")
    limit: int = Field(5, description="Number of periods to fetch.")
    api_key: Optional[str] = Field(None, description="Financial Modeling Prep API key.")
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    @field_validator("api_key", mode="before")
    def load_api_key_env(cls, value: Optional[str]) -> Optional[str]:
        """Load api_key from FMP_API_KEY env var if not provided."""
        if not value:
            import os

            return os.getenv("FMP_API_KEY")
        return value

    @model_validator(mode="after")
    def check_api_key(cls, cfg: FmpReaderConfig) -> FmpReaderConfig:
        """Ensure an API key is provided."""
        if not cfg.api_key:
            raise ValueError("api_key is required (env var FMP_API_KEY or param)")
        return cfg


# --- New Reader Configs for DataFrame and Dict readers ---


class DataFrameReaderConfig(BaseReaderConfig):
    """Configuration for DataFrameReader.

    No additional reader-specific options are required at the moment because
    the reader consumes an in-memory :class:`pandas.DataFrame` supplied to
    :py:meth:`DataFrameReader.read`.  The `source` field therefore serves only
    to preserve a consistent registry-initialisation contract.
    """

    source: Any = Field(..., description="In-memory pandas DataFrame source")
    format_type: Literal["dataframe"] = "dataframe"


class DictReaderConfig(BaseReaderConfig):
    """Configuration for DictReader.

    Mirrors :class:`DataFrameReaderConfig` - no custom options yet.  The
    placeholder keeps the IO registry symmetric and future-proof.
    """

    source: dict[str, dict[str, float]] = Field(..., description="In-memory dictionary source")
    format_type: Literal["dict"] = "dict"


# --- Writer-side Pydantic configuration models ---
class BaseWriterConfig(BaseModel):
    """Base configuration for IO writers."""

    target: Optional[str] = Field(
        None,
        description="URI or path to data target (file path, in-memory target, etc.)",
    )
    format_type: Literal["excel", "dataframe", "dict", "markdown"] = Field(
        ..., description="Type of writer (excel, dataframe, dict, markdown)."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class ExcelWriterConfig(BaseWriterConfig):
    """Excel writer options."""

    sheet_name: str = Field("Sheet1", description="Name of the sheet to write to.")
    recalculate: bool = Field(True, description="Whether to recalculate graph before export.")
    include_nodes: Optional[list[str]] = Field(
        None, description="Optional list of node names to include in export."
    )
    excel_writer_kwargs: dict[str, Any] = Field(
        default_factory=dict,
        description="Additional kwargs for pandas.DataFrame.to_excel.",
    )


class DataFrameWriterConfig(BaseWriterConfig):
    """DataFrame writer options."""

    target: Optional[str] = Field(
        None, description="Optional target path (ignored by DataFrameWriter)."
    )
    recalculate: bool = Field(True, description="Whether to recalculate graph before export.")
    include_nodes: Optional[list[str]] = Field(
        None, description="Optional list of node names to include in export."
    )


class DictWriterConfig(BaseWriterConfig):
    """Dict writer has no additional options."""

    target: Optional[str] = Field(None, description="Optional target (ignored by DictWriter).")


class MarkdownWriterConfig(BaseWriterConfig):
    """Markdown writer options."""

    statement_config_path: str = Field(
        ..., description="Path to the statement definition YAML file."
    )
    historical_periods: Optional[list[str]] = Field(
        None, description="List of historical period names."
    )
    forecast_periods: Optional[list[str]] = Field(
        None, description="List of forecast period names."
    )
    adjustment_filter: Optional[AdjustmentFilterInput] = Field(
        None, description="Adjustment filter to apply."
    )
    forecast_configs: Optional[dict] = Field(
        None,
        description="Dictionary mapping node IDs to forecast configurations for notes.",
    )
    indent_spaces: int = Field(4, description="Number of spaces per indentation level.")
    target: Optional[str] = Field(
        None, description="Optional target path (ignored by MarkdownWriter)."
    )

# --- END FILE: fin_statement_model/io/config/models.py ---

# --- START FILE: fin_statement_model/io/core/__init__.py ---
"""Core IO components including base classes, registry, and utilities."""

from .base import DataReader, DataWriter
from .facade import read_data, write_data
from .mixins import (
    FileBasedReader,
    ConfigurableReaderMixin,
    DataFrameBasedWriter,
    ValueExtractionMixin,
    BatchProcessingMixin,
    ValidationResultCollector,
    handle_read_errors,
    handle_write_errors,
)
from .registry import (
    HandlerRegistry,
    get_reader,
    get_writer,
    list_readers,
    list_writers,
    register_reader,
    register_writer,
)

__all__ = [
    # Mixins and utilities
    "BatchProcessingMixin",
    "ConfigurableReaderMixin",
    "DataFrameBasedWriter",
    # Base classes
    "DataReader",
    "DataWriter",
    "FileBasedReader",
    # Registry
    "HandlerRegistry",
    "ValidationResultCollector",
    "ValueExtractionMixin",
    "get_reader",
    "get_writer",
    "handle_read_errors",
    "handle_write_errors",
    "list_readers",
    "list_writers",
    # Facade functions
    "read_data",
    "register_reader",
    "register_writer",
    "write_data",
]

# --- END FILE: fin_statement_model/io/core/__init__.py ---

# --- START FILE: fin_statement_model/io/core/base.py ---
"""Base classes for data readers and writers."""

from abc import ABC, abstractmethod
from typing import Any

# Use absolute import based on project structure
from fin_statement_model.core.graph import Graph


class DataReader(ABC):
    """Abstract base class for all data readers.

    Defines the interface for classes that read data from various sources
    and typically populate or return a Graph object.
    """

    @abstractmethod
    def read(self, source: Any, **kwargs: dict[str, Any]) -> Graph:
        """Read data from the specified source and return a Graph.

        Args:
            source: The data source. Type depends on the reader implementation
                (e.g., file path `str`, ticker `str`, `pd.DataFrame`, `dict`).
            **kwargs: Additional format-specific options for reading.

        Returns:
            A Graph object populated with the data from the source.

        Raises:
            ReadError: If an error occurs during the reading process.
            NotImplementedError: If the method is not implemented by a subclass.
        """
        raise NotImplementedError


class DataWriter(ABC):
    """Abstract base class for all data writers.

    Defines the interface for classes that write graph data to various targets.
    """

    @abstractmethod
    def write(self, graph: Graph, target: Any, **kwargs: dict[str, Any]) -> object:
        """Write data from the Graph object to the specified target.

        Args:
            graph: The Graph object containing the data to write.
            target: The destination target. Type depends on the writer implementation
                (e.g., file path `str`, or ignored if the writer returns an object).
            **kwargs: Additional format-specific options for writing.

        Raises:
            WriteError: If an error occurs during the writing process.
            NotImplementedError: If the method is not implemented by a subclass.
        """
        raise NotImplementedError

# --- END FILE: fin_statement_model/io/core/base.py ---

# --- START FILE: fin_statement_model/io/core/facade.py ---
"""Facade functions for simplified IO operations.

This module provides the main public API for reading and writing data,
abstracting away the complexity of the registry system.
"""

import logging
from typing import Union, Any

from fin_statement_model.core.graph import Graph
from .registry import get_reader, get_writer
from fin_statement_model.io.exceptions import (
    IOError,
    ReadError,
    WriteError,
    FormatNotSupportedError,
)

logger = logging.getLogger(__name__)

# Define known keyword arguments for reader/writer initialization
_READER_INIT_KWARGS = {"api_key", "mapping_config"}
_WRITER_INIT_KWARGS = {"target"}  # Use 'target' as init kwarg for writer Pydantic config


def read_data(
    format_type: str, source: Any, **kwargs: dict[str, Union[str, int, float, bool]]
) -> Graph:
    """Reads data from a source using the specified format.

    This function acts as a facade for the underlying reader implementations.
    It uses the `format_type` to look up the appropriate reader class in the registry.
    The `source` and `**kwargs` are combined and validated against the specific
    reader's Pydantic configuration model (e.g., `CsvReaderConfig`).

    The validated configuration is used to initialize the reader instance.
    The `source` (which might be the original object for dict/dataframe formats, or
    the validated string path/ticker otherwise) and the original `**kwargs` are then
    passed to the reader instance's `.read()` method, which handles format-specific
    read-time options.

    Args:
        format_type (str): The format identifier (e.g., 'excel', 'csv', 'fmp', 'dict').
        source (Any): The data source. Its type depends on `format_type`:
            - `str`: file path (for 'excel', 'csv'), ticker symbol (for 'fmp').
            - `pd.DataFrame`: for 'dataframe'.
            - `dict`: for 'dict'.
        **kwargs: Additional keyword arguments used for reader configuration (e.g.,
            `api_key`, `delimiter`, `sheet_name`, `mapping_config`) and potentially
            passed to the reader's `.read()` method (e.g., `periods`). Consult the
            specific reader's Pydantic config model and `.read()` docstring.

    Returns:
        Graph: A new Graph object populated with the read data.

    Raises:
        ReadError: If reading fails.
        FormatNotSupportedError: If the format_type is not registered.
        Exception: Other errors during reader initialization or reading.
    """
    logger.info(
        f"Attempting to read data using format '{format_type}' from source type '{type(source).__name__}'"
    )

    # Prepare kwargs for registry validation (includes source and format_type)
    config_kwargs = {**kwargs, "source": source, "format_type": format_type}
    # Keep separate kwargs for the read method itself (e.g., 'periods')
    # This assumes Pydantic configs *don't* capture read-time args.

    try:
        # Pass the config kwargs directly to get_reader
        reader = get_reader(**config_kwargs)

        # Determine the actual source object for the read method
        actual_source = source if format_type in ("dict", "dataframe") else config_kwargs["source"]

        # Pass the determined source and the original kwargs (excluding config keys potentially)
        # to the read method. Specific readers handle relevant kwargs.
        return reader.read(actual_source, **kwargs)
    except (IOError, FormatNotSupportedError):
        logger.exception("IO Error reading data")
        raise  # Re-raise specific IO errors
    except Exception as e:
        logger.exception(f"Unexpected error reading data with format '{format_type}'")
        # Wrap unexpected errors in ReadError for consistency?
        raise ReadError(
            "Unexpected error during read",
            source=str(source),
            reader_type=format_type,
            original_error=e,
        ) from e


def write_data(
    format_type: str,
    graph: Graph,
    target: Any,
    **kwargs: dict[str, Union[str, int, float, bool]],
) -> object:
    """Writes graph data to a target using the specified format.

    Similar to `read_data`, this acts as a facade for writer implementations.
    It uses `format_type` to find the writer class in the registry.
    The `target` and `**kwargs` are combined and validated against the specific
    writer's Pydantic configuration model (e.g., `ExcelWriterConfig`).

    The validated configuration initializes the writer instance.
    The original `graph`, `target`, and `**kwargs` are then passed to the writer
    instance's `.write()` method for format-specific write-time options.

    Args:
        format_type (str): The format identifier (e.g., 'excel', 'dataframe', 'dict').
        graph (Graph): The graph object containing data to write.
        target (Any): The destination target. Its type depends on `format_type`:
            - `str`: file path (usually required for file-based writers like 'excel').
            - Ignored: for writers that return objects (like 'dataframe', 'dict').
        **kwargs: Additional keyword arguments used for writer configuration (e.g.,
            `sheet_name`, `recalculate`) and potentially passed to the writer's
            `.write()` method. Consult the specific writer's Pydantic config model
            and `.write()` docstring.

    Returns:
        object: The result of the write operation. For writers like DataFrameWriter
                or DictWriter, this is the created object. For file writers, it's None.

    Raises:
        WriteError: If writing fails.
        FormatNotSupportedError: If the format_type is not registered.
        Exception: Other errors during writer initialization or writing.
    """
    logger.info(
        f"Attempting to write graph data using format '{format_type}' to target type '{type(target).__name__}'"
    )

    # Prepare kwargs for registry validation (includes target and format_type)
    config_kwargs = {**kwargs, "target": target, "format_type": format_type}

    # Pass the config kwargs directly to get_writer
    writer = get_writer(**config_kwargs)
    # Now call write with all writer-specific kwargs
    try:
        # Pass original graph, target, and non-config kwargs to write()
        result = writer.write(graph, target, **kwargs)

        # If the writer returns a string and target is a path, write it to the file.
        if isinstance(result, str) and isinstance(target, str):
            try:
                logger.debug(
                    f"Writing string result from writer '{type(writer).__name__}' to file: {target}"
                )
                with open(target, "w", encoding="utf-8") as f:
                    f.write(result)
                return None  # Consistent return for file writers
            except OSError as e:
                logger.exception(f"Failed to write writer output to target file: {target}")
                raise WriteError(
                    f"Failed to write writer output to file: {target}",
                    target=target,
                    writer_type=format_type,
                    original_error=e,
                ) from e
        else:
            # Otherwise, return the original result (e.g., DataFrame, dict)
            return result
    except (IOError, FormatNotSupportedError):
        logger.exception("IO Error writing data")
        raise  # Re-raise specific IO errors
    except Exception as e:
        logger.exception(f"Unexpected error writing data with format '{format_type}'")
        # Wrap unexpected errors
        raise WriteError(
            "Unexpected error during write",
            target=str(target),
            writer_type=format_type,
            original_error=e,
        ) from e


__all__ = ["read_data", "write_data"]

# --- END FILE: fin_statement_model/io/core/facade.py ---

# --- START FILE: fin_statement_model/io/core/mixins.py ---
"""Reusable mixins and decorators for IO operations.

This module provides shared functionality for readers and writers including
error handling decorators and mixins for consistent behavior.
"""

import os
import functools
import logging
from abc import abstractmethod
from typing import Any, TypeVar, Optional
from collections.abc import Callable

from fin_statement_model.core.graph import Graph
from .base import DataReader
from fin_statement_model.io.exceptions import ReadError, WriteError

logger = logging.getLogger(__name__)

F = TypeVar("F", bound=Callable[..., Any])


# ===== Error Handling Decorators =====


def handle_read_errors(source_attr: str = "source") -> Callable[[F], F]:
    """Decorator to standardize error handling for readers.

    This decorator catches common exceptions during read operations and
    converts them to appropriate ReadError instances with consistent
    error messages and context.

    Args:
        source_attr: Name of the attribute containing the source identifier.
                    Defaults to "source".

    Returns:
        Decorated function that handles errors consistently.
    """

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(self: Any, source: Any, **kwargs: Any) -> Any:
            try:
                return func(self, source, **kwargs)
            except ReadError:
                raise  # Re-raise our own errors without modification
            except FileNotFoundError as e:
                raise ReadError(
                    f"File not found: {source}",
                    source=str(source),
                    reader_type=self.__class__.__name__,
                    original_error=e,
                )
            except ValueError as e:
                raise ReadError(
                    f"Invalid value encountered: {e}",
                    source=str(source),
                    reader_type=self.__class__.__name__,
                    original_error=e,
                )
            except Exception as e:
                logger.error(f"Failed to read from {source}: {e}", exc_info=True)
                raise ReadError(
                    f"Failed to process source: {e}",
                    source=str(source),
                    reader_type=self.__class__.__name__,
                    original_error=e,
                ) from e

        return wrapper  # type: ignore

    return decorator


def handle_write_errors(target_attr: str = "target") -> Callable[[F], F]:
    """Decorator to standardize error handling for writers.

    Similar to handle_read_errors but for write operations.

    Args:
        target_attr: Name of the attribute containing the target identifier.
                    Defaults to "target".

    Returns:
        Decorated function that handles errors consistently.
    """

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(self: Any, graph: Any, target: Any = None, **kwargs: Any) -> Any:
            try:
                return func(self, graph, target, **kwargs)
            except WriteError:
                raise  # Re-raise our own errors without modification
            except Exception as e:
                logger.error(f"Failed to write to {target}: {e}", exc_info=True)
                raise WriteError(
                    f"Failed to write data: {e}",
                    target=str(target) if target else "unknown",
                    writer_type=self.__class__.__name__,
                    original_error=e,
                ) from e

        return wrapper  # type: ignore

    return decorator


# ===== Reader Mixins =====


class FileBasedReader(DataReader):
    """Base class for file-based readers with common validation.

    This class provides common file validation methods and ensures
    consistent error handling for all file-based readers.

    Note: Subclasses should apply the @handle_read_errors() decorator
    to their read() method implementation for consistent error handling.
    """

    def validate_file_exists(self, path: str) -> None:
        """Validate that file exists.

        Args:
            path: Path to the file to validate.

        Raises:
            ReadError: If the file does not exist.
        """
        if not os.path.exists(path):
            raise ReadError(
                f"File not found: {path}", source=path, reader_type=self.__class__.__name__
            )

    def validate_file_extension(self, path: str, valid_extensions: tuple[str, ...]) -> None:
        """Validate file has correct extension.

        Args:
            path: Path to the file to validate.
            valid_extensions: Tuple of valid file extensions (e.g., ('.csv', '.txt')).

        Raises:
            ReadError: If the file extension is not valid.
        """
        if not path.lower().endswith(valid_extensions):
            raise ReadError(
                f"Invalid file extension. Expected one of {valid_extensions}, "
                f"got '{os.path.splitext(path)[1]}'",
                source=path,
                reader_type=self.__class__.__name__,
            )

    @abstractmethod
    def read(self, source: str, **kwargs: Any) -> Graph:
        """Read from file source.

        Subclasses must implement this method with their specific
        file reading logic. It's recommended to apply the @handle_read_errors()
        decorator to the implementation.

        Args:
            source: Path to the file to read.
            **kwargs: Additional reader-specific options.

        Returns:
            Graph populated with data from the file.
        """


class ConfigurableReaderMixin:
    """Mixin for readers that use configuration objects.

    Provides common methods for accessing configuration values
    with proper error handling.
    """

    def get_config_value(self, key: str, default: Any = None) -> Any:
        """Safely get a configuration value.

        Args:
            key: Configuration key to retrieve.
            default: Default value if key is not found.

        Returns:
            Configuration value or default.
        """
        if hasattr(self, "cfg") and self.cfg:
            return getattr(self.cfg, key, default)
        return default

    def require_config_value(self, key: str) -> Any:
        """Get a required configuration value.

        Args:
            key: Configuration key to retrieve.

        Returns:
            Configuration value.

        Raises:
            ReadError: If the configuration value is missing.
        """
        if not hasattr(self, "cfg") or not self.cfg:
            raise ReadError(
                "Reader not properly configured: missing configuration object",
                reader_type=self.__class__.__name__,
            )

        value = getattr(self.cfg, key, None)
        if value is None:
            raise ReadError(
                f"Required configuration value '{key}' is missing",
                reader_type=self.__class__.__name__,
            )

        return value


# ===== Writer Mixins =====


class ValueExtractionMixin:
    """Mixin for consistent value extraction from nodes.

    This mixin provides a standardized way to extract values from nodes,
    handling both calculated values and stored values with proper error
    handling.
    """

    def extract_node_value(
        self,
        node: Any,  # Avoid circular import with Node type
        period: str,
        calculate: bool = True,
    ) -> Optional[float]:
        """Extract value from node with consistent error handling.

        Args:
            node: The node to extract value from.
            period: The period to get the value for.
            calculate: If True, attempt to calculate the value using node.calculate().
                      If False, only look for stored values.

        Returns:
            The extracted value as a float, or None if no value could be extracted.
        """
        try:
            # First try calculation if enabled and method exists
            if calculate and hasattr(node, "calculate") and callable(node.calculate):
                value = node.calculate(period)
                if isinstance(value, int | float):
                    return float(value)

            # Fall back to stored values
            if hasattr(node, "values") and isinstance(node.values, dict):
                value = node.values.get(period)
                if isinstance(value, int | float):
                    return float(value)

            return None

        except Exception as e:
            logger.debug(
                f"Failed to extract value from node '{getattr(node, 'name', 'unknown')}' "
                f"for period '{period}': {e}"
            )
            return None


class DataFrameBasedWriter(ValueExtractionMixin):
    """Base class for writers that convert to DataFrame format.

    This base class provides common functionality for writers that
    need to extract data from a graph into a tabular format.

    Note: Subclasses should apply the @handle_write_errors() decorator
    to their write() method implementation for consistent error handling.
    """

    def extract_graph_data(
        self, graph: Graph, include_nodes: Optional[list[str]] = None, calculate: bool = True
    ) -> dict[str, dict[str, float]]:
        """Extract data from graph nodes into a dictionary format.

        Args:
            graph: The graph to extract data from.
            include_nodes: Optional list of node names to include.
                          If None, includes all nodes.
            calculate: Whether to calculate values or just use stored values.

        Returns:
            Dictionary mapping node names to period-value dictionaries.
        """
        import numpy as np

        periods = sorted(graph.periods) if graph.periods else []
        data: dict[str, dict[str, float]] = {}

        # Determine which nodes to process
        nodes_to_process = include_nodes if include_nodes else list(graph.nodes.keys())

        # Validate requested nodes exist
        if include_nodes:
            missing_nodes = [n for n in include_nodes if n not in graph.nodes]
            if missing_nodes:
                logger.warning(f"Requested nodes not found in graph: {missing_nodes}")
                nodes_to_process = [n for n in include_nodes if n in graph.nodes]

        # Extract data for each node
        for node_id in nodes_to_process:
            node = graph.nodes[node_id]
            row: dict[str, float] = {}

            for period in periods:
                # Use the mixin's extract method for consistent value extraction
                value = self.extract_node_value(node, period, calculate=calculate)

                # Convert None to NaN for DataFrame compatibility
                if (
                    value is None
                    or not isinstance(value, int | float | np.number)
                    or not np.isfinite(value)
                ):
                    value = np.nan

                row[period] = float(value)

            data[node_id] = row

        return data

    @abstractmethod
    def write(self, graph: Graph, target: Any = None, **kwargs: Any) -> Any:
        """Write graph data to target.

        Subclasses must implement this method with their specific
        writing logic. It's recommended to apply the @handle_write_errors()
        decorator to the implementation.

        Args:
            graph: Graph containing data to write.
            target: Target for the output (file path, etc.).
            **kwargs: Additional writer-specific options.

        Returns:
            Writer-specific return value.
        """


# ===== Utility Classes =====


class BatchProcessingMixin:
    """Mixin for readers/writers that process data in batches.

    Provides utilities for chunking data and progress reporting.
    """

    def __init__(self, batch_size: int = 1000):
        """Initialize with batch size.

        Args:
            batch_size: Number of items to process in each batch.
        """
        self.batch_size = batch_size
        self._processed_count = 0
        self._total_count = 0

    def process_in_batches(
        self, items: list[Any], process_func: callable, progress_callback: Optional[callable] = None
    ) -> list[Any]:
        """Process items in batches.

        Args:
            items: List of items to process.
            process_func: Function to apply to each batch.
            progress_callback: Optional callback for progress updates.

        Returns:
            List of results from processing all batches.
        """
        results = []
        self._total_count = len(items)
        self._processed_count = 0

        for i in range(0, len(items), self.batch_size):
            batch = items[i : i + self.batch_size]
            batch_results = process_func(batch)
            results.extend(batch_results)

            self._processed_count += len(batch)

            if progress_callback:
                progress_callback(self._processed_count, self._total_count)

            # Log progress
            if self._processed_count % (self.batch_size * 10) == 0:
                logger.info(
                    f"Processed {self._processed_count}/{self._total_count} items "
                    f"({self._processed_count / self._total_count * 100:.1f}%)"
                )

        return results

    def get_progress(self) -> tuple[int, int]:
        """Get current progress.

        Returns:
            Tuple of (processed_count, total_count).
        """
        return self._processed_count, self._total_count


class ValidationResultCollector:
    """Utility class for collecting and summarizing validation results.

    Useful for batch operations where you want to collect all validation
    results and report them together.
    """

    def __init__(self):
        self.results: list[tuple[str, bool, str]] = []
        self.errors: list[str] = []
        self.warnings: list[str] = []

    def add_result(self, item_name: str, is_valid: bool, message: str) -> None:
        """Add a validation result."""
        self.results.append((item_name, is_valid, message))

        if not is_valid:
            self.errors.append(f"{item_name}: {message}")
        elif "warning" in message.lower():
            self.warnings.append(f"{item_name}: {message}")

    def has_errors(self) -> bool:
        """Check if any errors were collected."""
        return len(self.errors) > 0

    def get_summary(self) -> dict[str, Any]:
        """Get a summary of all validation results."""
        total = len(self.results)
        valid = sum(1 for _, is_valid, _ in self.results if is_valid)

        return {
            "total": total,
            "valid": valid,
            "invalid": total - valid,
            "errors": self.errors.copy(),
            "warnings": self.warnings.copy(),
            "error_rate": (total - valid) / total if total > 0 else 0.0,
        }

# --- END FILE: fin_statement_model/io/core/mixins.py ---

# --- START FILE: fin_statement_model/io/core/registry.py ---
"""Registry system for managing IO format handlers.

This module provides a generic registry implementation and specific registries
for readers and writers, along with registration decorators and access functions.
"""

import logging
from typing import TypeVar, Generic, Optional, Any, Union
from collections.abc import Callable

from pydantic import ValidationError

from fin_statement_model.io.core.base import DataReader, DataWriter
from fin_statement_model.io.exceptions import FormatNotSupportedError, ReadError, WriteError
from fin_statement_model.io.config.models import (
    CsvReaderConfig,
    ExcelReaderConfig,
    FmpReaderConfig,
    DataFrameReaderConfig,
    DictReaderConfig,
    ExcelWriterConfig,
    DataFrameWriterConfig,
    DictWriterConfig,
    MarkdownWriterConfig,
)

logger = logging.getLogger(__name__)

# Type variable for the handler type (DataReader or DataWriter)
T = TypeVar("T")


# ===== Generic Registry Implementation =====


class HandlerRegistry(Generic[T]):
    """Generic registry for managing format handlers (readers or writers).

    This class provides a reusable registry pattern for registering and
    retrieving handler classes by format type.

    Attributes:
        _registry: Internal dictionary mapping format types to handler classes.
        _handler_type: String describing the handler type ('reader' or 'writer').
    """

    def __init__(self, handler_type: str):
        """Initialize the registry.

        Args:
            handler_type: Type of handlers ('reader' or 'writer') for error messages.
        """
        self._registry: dict[str, type[T]] = {}
        self._handler_type = handler_type

    def register(self, format_type: str) -> Callable[[type[T]], type[T]]:
        """Create a decorator to register a handler class for a format type.

        Args:
            format_type: The format identifier (e.g., 'excel', 'csv').

        Returns:
            A decorator function that registers the class.

        Raises:
            ValueError: If the format is already registered to a different class.
        """

        def decorator(cls: type[T]) -> type[T]:
            if format_type in self._registry:
                # Allow re-registration of the same class (idempotent)
                if self._registry[format_type] is not cls:
                    raise ValueError(
                        f"{self._handler_type.capitalize()} format type '{format_type}' "
                        f"already registered to {self._registry[format_type]}."
                    )
                logger.debug(
                    f"Re-registering {self._handler_type} format type '{format_type}' "
                    f"to {cls.__name__}"
                )
            else:
                logger.debug(
                    f"Registering {self._handler_type} format type '{format_type}' "
                    f"to {cls.__name__}"
                )

            self._registry[format_type] = cls
            return cls

        return decorator

    def get(self, format_type: str) -> type[T]:
        """Get the registered handler class for a format type.

        Args:
            format_type: The format identifier.

        Returns:
            The registered handler class.

        Raises:
            FormatNotSupportedError: If no handler is registered for the format.
        """
        if format_type not in self._registry:
            raise FormatNotSupportedError(
                format_type=format_type, operation=f"{self._handler_type} operations"
            )

        return self._registry[format_type]

    def list_formats(self) -> dict[str, type[T]]:
        """Return a copy of all registered format handlers.

        Returns:
            Dictionary mapping format types to handler classes.
        """
        return self._registry.copy()

    def is_registered(self, format_type: str) -> bool:
        """Check if a format type is registered.

        Args:
            format_type: The format identifier to check.

        Returns:
            True if the format is registered, False otherwise.
        """
        return format_type in self._registry

    def unregister(self, format_type: str) -> Optional[type[T]]:
        """Remove a format handler from the registry.

        This method is primarily useful for testing.

        Args:
            format_type: The format identifier to remove.

        Returns:
            The removed handler class, or None if not found.
        """
        return self._registry.pop(format_type, None)

    def clear(self) -> None:
        """Clear all registered handlers.

        This method is primarily useful for testing.
        """
        self._registry.clear()

    def __contains__(self, format_type: str) -> bool:
        """Check if a format type is registered using 'in' operator.

        Args:
            format_type: The format identifier to check.

        Returns:
            True if the format is registered, False otherwise.
        """
        return format_type in self._registry

    def __len__(self) -> int:
        """Return the number of registered formats.

        Returns:
            Number of registered format handlers.
        """
        return len(self._registry)


# ===== Registry Instances =====

# Create registry instances for readers and writers
_reader_registry = HandlerRegistry[DataReader]("reader")
_writer_registry = HandlerRegistry[DataWriter]("writer")

# Schema mappings for configuration validation
_READER_SCHEMA_MAP = {
    "csv": CsvReaderConfig,
    "excel": ExcelReaderConfig,
    "fmp": FmpReaderConfig,
    "dataframe": DataFrameReaderConfig,
    "dict": DictReaderConfig,
}

_WRITER_SCHEMA_MAP = {
    "excel": ExcelWriterConfig,
    "dataframe": DataFrameWriterConfig,
    "dict": DictWriterConfig,
    "markdown": MarkdownWriterConfig,
}


# ===== Registration Decorators =====


def register_reader(format_type: str) -> Callable[[type[DataReader]], type[DataReader]]:
    """Decorator to register a DataReader class for a specific format type.

    Args:
        format_type: The string identifier for the format (e.g., 'excel', 'csv').

    Returns:
        A decorator function that registers the class and returns it unmodified.

    Raises:
        ValueError: If the format_type is already registered for a reader.
    """
    return _reader_registry.register(format_type)


def register_writer(format_type: str) -> Callable[[type[DataWriter]], type[DataWriter]]:
    """Decorator to register a DataWriter class for a specific format type.

    Args:
        format_type: The string identifier for the format (e.g., 'excel', 'json').

    Returns:
        A decorator function that registers the class and returns it unmodified.

    Raises:
        ValueError: If the format_type is already registered for a writer.
    """
    return _writer_registry.register(format_type)


# ===== Generic Handler Function =====


def _get_handler(
    format_type: str,
    registry: HandlerRegistry[Union[DataReader, DataWriter]],
    schema_map: dict[str, type],
    handler_type: str,
    error_class: type[Union[ReadError, WriteError]],
    **kwargs: Any,
) -> Union[DataReader, DataWriter]:
    """Generic handler instantiation logic.

    This function encapsulates the common pattern for instantiating
    readers and writers, including configuration validation and error handling.

    Args:
        format_type: The format identifier (e.g., 'excel', 'csv').
        registry: The registry instance containing handler classes.
        schema_map: Mapping of format types to Pydantic config schemas.
        handler_type: Either 'read' or 'write' for error messages.
        error_class: Either ReadError or WriteError class.
        **kwargs: Configuration parameters for the handler.

    Returns:
        An initialized handler instance.

    Raises:
        FormatNotSupportedError: If format_type is not in registry.
        ReadError/WriteError: If configuration validation or instantiation fails.
    """
    # Get handler class from registry (may raise FormatNotSupportedError)
    handler_class = registry.get(format_type)

    schema = schema_map.get(format_type)

    # Prepare error context based on handler type
    error_context = {}
    if handler_type == "read":
        error_context["source"] = kwargs.get("source")
        error_context["reader_type"] = format_type
    else:  # write
        error_context["target"] = kwargs.get("target")
        error_context["writer_type"] = format_type

    if schema:
        # Validate configuration using Pydantic schema
        try:
            cfg = schema.model_validate({**kwargs, "format_type": format_type})
        except ValidationError as ve:
            raise error_class(
                message=f"Invalid {handler_type}er configuration",
                original_error=ve,
                **error_context,
            ) from ve

        # Instantiate handler with validated config
        try:
            return handler_class(cfg)
        except Exception as e:
            logger.error(
                f"Failed to instantiate {handler_type}er for format '{format_type}' "
                f"({handler_class.__name__}): {e}",
                exc_info=True,
            )
            raise error_class(
                message=f"Failed to initialize {handler_type}er", original_error=e, **error_context
            ) from e

    # Fallback for handlers without config schema (legacy support)
    try:
        return handler_class(**kwargs)
    except Exception as e:
        logger.error(
            f"Failed to instantiate {handler_type}er for format '{format_type}' "
            f"({handler_class.__name__}): {e}",
            exc_info=True,
        )
        raise error_class(
            message=f"Failed to initialize {handler_type}er", original_error=e, **error_context
        ) from e


# ===== Registry Access Functions =====


def get_reader(format_type: str, **kwargs: Any) -> DataReader:
    """Get an instance of the registered DataReader for the given format type.

    Args:
        format_type: The string identifier for the format.
        **kwargs: Keyword arguments to pass to the reader's constructor.

    Returns:
        An initialized DataReader instance.

    Raises:
        FormatNotSupportedError: If no reader is registered for the format type.
        ReadError: If validation fails for known reader types.
    """
    return _get_handler(
        format_type=format_type,
        registry=_reader_registry,
        schema_map=_READER_SCHEMA_MAP,
        handler_type="read",
        error_class=ReadError,
        **kwargs,
    )


def get_writer(format_type: str, **kwargs: Any) -> DataWriter:
    """Get an instance of the registered DataWriter for the given format type.

    Args:
        format_type: The string identifier for the format.
        **kwargs: Keyword arguments to pass to the writer's constructor.

    Returns:
        An initialized DataWriter instance.

    Raises:
        FormatNotSupportedError: If no writer is registered for the format type.
        WriteError: If validation fails for known writer types.
    """
    return _get_handler(
        format_type=format_type,
        registry=_writer_registry,
        schema_map=_WRITER_SCHEMA_MAP,
        handler_type="write",
        error_class=WriteError,
        **kwargs,
    )


def list_readers() -> dict[str, type[DataReader]]:
    """Return a copy of the registered reader classes."""
    return _reader_registry.list_formats()


def list_writers() -> dict[str, type[DataWriter]]:
    """Return a copy of the registered writer classes."""
    return _writer_registry.list_formats()


# ===== Backward Compatibility =====

# Expose internal registries for backward compatibility with tests
# These should be considered deprecated and may be removed in future versions
_readers = _reader_registry._registry
_writers = _writer_registry._registry


__all__ = [
    "HandlerRegistry",
    "get_reader",
    "get_writer",
    "list_readers",
    "list_writers",
    "register_reader",
    "register_writer",
]

# --- END FILE: fin_statement_model/io/core/registry.py ---

# --- START FILE: fin_statement_model/io/core/registry_base.py ---
"""Base registry class for managing handler registrations.

This module provides a generic registry implementation that can be used
for both readers and writers, reducing code duplication.
"""

import logging
from typing import TypeVar, Generic, Optional
from collections.abc import Callable

from fin_statement_model.io.exceptions import FormatNotSupportedError

logger = logging.getLogger(__name__)

# Type variable for the handler type (DataReader or DataWriter)
T = TypeVar("T")


class HandlerRegistry(Generic[T]):
    """Generic registry for managing format handlers (readers or writers).

    This class provides a reusable registry pattern for registering and
    retrieving handler classes by format type.

    Attributes:
        _registry: Internal dictionary mapping format types to handler classes.
        _handler_type: String describing the handler type ('reader' or 'writer').
    """

    def __init__(self, handler_type: str):
        """Initialize the registry.

        Args:
            handler_type: Type of handlers ('reader' or 'writer') for error messages.
        """
        self._registry: dict[str, type[T]] = {}
        self._handler_type = handler_type

    def register(self, format_type: str) -> Callable[[type[T]], type[T]]:
        """Create a decorator to register a handler class for a format type.

        Args:
            format_type: The format identifier (e.g., 'excel', 'csv').

        Returns:
            A decorator function that registers the class.

        Raises:
            ValueError: If the format is already registered to a different class.
        """

        def decorator(cls: type[T]) -> type[T]:
            if format_type in self._registry:
                # Allow re-registration of the same class (idempotent)
                if self._registry[format_type] is not cls:
                    raise ValueError(
                        f"{self._handler_type.capitalize()} format type '{format_type}' "
                        f"already registered to {self._registry[format_type]}."
                    )
                logger.debug(
                    f"Re-registering {self._handler_type} format type '{format_type}' "
                    f"to {cls.__name__}"
                )
            else:
                logger.debug(
                    f"Registering {self._handler_type} format type '{format_type}' "
                    f"to {cls.__name__}"
                )

            self._registry[format_type] = cls
            return cls

        return decorator

    def get(self, format_type: str) -> type[T]:
        """Get the registered handler class for a format type.

        Args:
            format_type: The format identifier.

        Returns:
            The registered handler class.

        Raises:
            FormatNotSupportedError: If no handler is registered for the format.
        """
        if format_type not in self._registry:
            raise FormatNotSupportedError(
                format_type=format_type, operation=f"{self._handler_type} operations"
            )

        return self._registry[format_type]

    def list_formats(self) -> dict[str, type[T]]:
        """Return a copy of all registered format handlers.

        Returns:
            Dictionary mapping format types to handler classes.
        """
        return self._registry.copy()

    def is_registered(self, format_type: str) -> bool:
        """Check if a format type is registered.

        Args:
            format_type: The format identifier to check.

        Returns:
            True if the format is registered, False otherwise.
        """
        return format_type in self._registry

    def unregister(self, format_type: str) -> Optional[type[T]]:
        """Remove a format handler from the registry.

        This method is primarily useful for testing.

        Args:
            format_type: The format identifier to remove.

        Returns:
            The removed handler class, or None if not found.
        """
        return self._registry.pop(format_type, None)

    def clear(self) -> None:
        """Clear all registered handlers.

        This method is primarily useful for testing.
        """
        self._registry.clear()

    def __contains__(self, format_type: str) -> bool:
        """Check if a format type is registered using 'in' operator.

        Args:
            format_type: The format identifier to check.

        Returns:
            True if the format is registered, False otherwise.
        """
        return format_type in self._registry

    def __len__(self) -> int:
        """Return the number of registered formats.

        Returns:
            Number of registered format handlers.
        """
        return len(self._registry)

# --- END FILE: fin_statement_model/io/core/registry_base.py ---

# --- START FILE: fin_statement_model/io/core/utils.py ---
"""Utility functions for IO operations."""

from typing import Optional, Union

# Type alias for mapping configurations
MappingConfig = Union[dict[str, str], dict[Optional[str], dict[str, str]]]


def normalize_mapping(
    mapping_config: MappingConfig = None, context_key: Optional[str] = None
) -> dict[str, str]:
    """Turn a scoped MappingConfig into a unified flat dict with a required default mapping under None.

    Args:
        mapping_config: MappingConfig object defining name mappings.
        context_key: Optional key (e.g., sheet name or statement type) to select
            a scoped mapping within a scoped config.

    Returns:
        A flat dict mapping original names to canonical names.

    Raises:
        TypeError: If the provided mapping_config is not of a supported structure.
    """
    if mapping_config is None:
        return {}
    if not isinstance(mapping_config, dict):
        raise TypeError(f"mapping_config must be a dict, got {type(mapping_config).__name__}")
    if None not in mapping_config:
        # Flat mapping
        return mapping_config
    else:
        # Scoped mapping
        default_mapping = mapping_config.get(None, {})
        if context_key and context_key in mapping_config:
            context_mapping = mapping_config[context_key]
            # Merge with context-specific overriding default
            return {**default_mapping, **context_mapping}
        else:
            return default_mapping


__all__ = ["MappingConfig", "normalize_mapping"]

# --- END FILE: fin_statement_model/io/core/utils.py ---

# --- START FILE: fin_statement_model/io/exceptions.py ---
"""IO specific exceptions."""

from typing import Optional

# Use absolute import based on project structure
from fin_statement_model.core.errors import FinancialModelError


class IOError(FinancialModelError):
    """Base exception for all Input/Output errors in the IO package."""

    def __init__(
        self,
        message: str,
        source_or_target: Optional[str] = None,
        format_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the IOError.

        Args:
            message: The base error message.
            source_or_target: Optional identifier for the source (read) or target (write).
            format_type: Optional name of the format or handler involved.
            original_error: Optional underlying exception that caused the failure.
        """
        self.source_or_target = source_or_target
        self.format_type = format_type
        self.original_error = original_error

        context = []
        if source_or_target:
            context.append(f"source/target '{source_or_target}'")
        if format_type:
            context.append(f"format '{format_type}'")

        full_message = f"{message} involving {' and '.join(context)}" if context else message

        if original_error:
            full_message = f"{full_message}: {original_error!s}"

        super().__init__(full_message)


class ReadError(IOError):
    """Exception raised specifically for errors during data read/import operations."""

    def __init__(
        self,
        message: str,
        source: Optional[str] = None,
        reader_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the ReadError.

        Args:
            message: The base error message.
            source: Optional identifier for the data source (e.g., file path, URL).
            reader_type: Optional name of the reader class used for importing.
            original_error: Optional underlying exception that caused the import failure.
        """
        super().__init__(
            message=message,
            source_or_target=source,
            format_type=reader_type,
            original_error=original_error,
        )


class WriteError(IOError):
    """Exception raised specifically for errors during data write/export operations."""

    def __init__(
        self,
        message: str,
        target: Optional[str] = None,
        writer_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the WriteError.

        Args:
            message: The base error message.
            target: Optional identifier for the export destination (e.g., file path).
            writer_type: Optional name of the writer class being used.
            original_error: Optional underlying exception that caused the export failure.
        """
        super().__init__(
            message=message,
            source_or_target=target,
            format_type=writer_type,
            original_error=original_error,
        )


class FormatNotSupportedError(IOError):
    """Exception raised when a requested IO format is not registered or supported."""

    def __init__(self, format_type: str, operation: str = "read/write"):
        """Initializes the FormatNotSupportedError.

        Args:
            format_type: The requested format identifier (e.g., 'excel', 'json').
            operation: The operation being attempted ('read' or 'write').
        """
        message = f"Format '{format_type}' is not supported for {operation} operations."
        super().__init__(message=message, format_type=format_type)

# --- END FILE: fin_statement_model/io/exceptions.py ---

# --- START FILE: fin_statement_model/io/formats/__init__.py ---
"""Format-specific IO implementations.

This module contains readers and writers for various data formats.
Each format is organized in its own submodule.
"""

# Import all format handlers to ensure they're registered
from .csv import CsvReader
from .dataframe import DataFrameReader, DataFrameWriter
from .dict import DictReader, DictWriter
from .excel import ExcelReader, ExcelWriter
from .api import FmpReader
from .markdown import MarkdownWriter

__all__ = [
    # CSV
    "CsvReader",
    # DataFrame
    "DataFrameReader",
    "DataFrameWriter",
    # Dict
    "DictReader",
    "DictWriter",
    # Excel
    "ExcelReader",
    "ExcelWriter",
    # API
    "FmpReader",
    # Markdown
    "MarkdownWriter",
]

# --- END FILE: fin_statement_model/io/formats/__init__.py ---

# --- START FILE: fin_statement_model/io/formats/api/__init__.py ---
"""API format IO operations."""

from .fmp import FmpReader

__all__ = ["FmpReader"]

# --- END FILE: fin_statement_model/io/formats/api/__init__.py ---

# --- START FILE: fin_statement_model/io/formats/api/fmp.py ---
"""Data reader for the Financial Modeling Prep (FMP) API."""

import logging
import requests
from typing import Optional, ClassVar, Any
import numpy as np
import yaml
import importlib.resources

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.base import DataReader
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.core.utils import normalize_mapping
from fin_statement_model.io.config.models import FmpReaderConfig

logger = logging.getLogger(__name__)


@register_reader("fmp")
class FmpReader(DataReader):
    """Reads financial statement data from the FMP API into a Graph.

    Fetches data for a specific ticker and statement type.
    Requires an API key, either passed directly or via the FMP_API_KEY env var.

    Supports a `mapping_config` constructor parameter for mapping API field names to canonical node names,
    accepting either a flat mapping or a statement-type keyed mapping.

    Configuration (api_key, statement_type, period_type, limit, mapping_config)
    is passed via an `FmpReaderConfig` object during initialization (typically by
    the `read_data` facade). The `.read()` method currently takes no specific
    keyword arguments beyond the `source` (ticker).

    Stateful Use:
        For advanced use cases involving repeated API calls, consider instantiating
        and reusing a single `FmpReader` instance to avoid redundant API key
        validations and improve performance.
    """

    BASE_URL = "https://financialmodelingprep.com/api/v3"

    # Load default mappings from YAML configuration
    DEFAULT_MAPPINGS: ClassVar[dict[str, dict[str, str]]] = {}

    @classmethod
    def _load_default_mappings(cls) -> None:
        """Load default mapping configurations from YAML file into DEFAULT_MAPPINGS."""
        # Load mapping YAML from config directory relative to this file
        try:
            # Use importlib.resources for robust package data loading
            yaml_content = (
                importlib.resources.files("fin_statement_model.io.config.mappings")
                .joinpath("fmp_default_mappings.yaml")
                .read_text(encoding="utf-8")
            )
            cls.DEFAULT_MAPPINGS = yaml.safe_load(yaml_content)
        except FileNotFoundError:
            logger.error("Default FMP mapping file not found.", exc_info=True)
            # Keep DEFAULT_MAPPINGS as empty dict if file is missing
        except Exception as e:
            logger.error(f"Error loading default FMP mapping file: {e}", exc_info=True)
            # Keep DEFAULT_MAPPINGS as empty dict on other errors

    def __init__(self, cfg: FmpReaderConfig) -> None:
        """Initialize the FmpReader with validated configuration.

        Args:
            cfg: A validated `FmpReaderConfig` instance containing parameters like
                 `source` (ticker), `api_key`, `statement_type`, `period_type`,
                 `limit`, and `mapping_config`.
        """
        self.cfg = cfg

    def _get_mapping(
        self,
        statement_type: Optional[str],
    ) -> dict[str, str]:
        """Get the appropriate mapping based on statement type and the stored config."""
        # Start with defaults based on statement type loaded from config
        mapping = dict(self.DEFAULT_MAPPINGS.get(statement_type, {}))

        # Use mapping config from the validated Pydantic config object
        config = self.cfg.mapping_config
        # Normalize and overlay user-provided mappings
        user_map = normalize_mapping(config, context_key=statement_type)
        mapping.update(user_map)
        return mapping

    def _validate_api_key(self):
        """Perform a simple check if the API key seems valid."""
        # API key is now guaranteed by FmpReaderConfig validation
        api_key = self.cfg.api_key
        if not api_key:  # Should not happen if validation passed, but defensive check
            raise ReadError(
                "FMP API key is required for reading.",
                source="FMP API",
                reader_type="FmpReader",
            )
        try:
            # Use a cheap endpoint for validation
            test_url = f"{self.BASE_URL}/profile/AAPL?apikey={api_key}"  # Example
            response = requests.get(test_url, timeout=10)
            response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
            # Basic check on response content if needed
            if not response.json():
                raise ReadError(
                    "API key validation returned empty response.",
                    source="FMP API",
                    reader_type="FmpReader",
                )
            logger.debug("FMP API key validated successfully.")
        except requests.exceptions.RequestException as e:
            logger.error(f"FMP API key validation failed: {e}", exc_info=True)
            raise ReadError(
                f"FMP API key validation failed: {e}",
                source="FMP API",
                reader_type="FmpReader",
                original_error=e,
            )

    def read(self, source: str, **kwargs: dict[str, Any]) -> Graph:
        """Fetch data from FMP API and return a Graph.

        Args:
            source (str): The stock ticker symbol (e.g., "AAPL").
            **kwargs: Currently unused. Configuration is handled by the `FmpReaderConfig`
                      object passed during initialization.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If API key is missing/invalid, API request fails, or data format is unexpected.
        """
        ticker = source
        # Parameters now come directly from the validated config
        statement_type = self.cfg.statement_type
        period_type_arg = self.cfg.period_type
        limit = self.cfg.limit
        api_key = self.cfg.api_key

        # --- Validate Inputs ---
        if not ticker or not isinstance(ticker, str):
            raise ReadError(
                "Invalid source (ticker) provided. Expected a non-empty string.",
                source=ticker,
                reader_type="FmpReader",
            )
        # statement_type and period_type are validated by FmpReaderConfig

        self._validate_api_key()  # Ensure API key is usable

        # Determine mapping for this operation, allowing override via kwargs
        # Mapping is now determined solely by the config passed during __init__
        try:
            mapping = self._get_mapping(statement_type)
        except TypeError as te:
            raise ReadError(
                "Invalid mapping_config provided.",
                source=ticker,
                reader_type="FmpReader",
                original_error=te,
            )
        logger.debug(f"Using mapping for {ticker} {statement_type}: {mapping}")

        # --- Fetch API Data ---
        # Correct endpoint construction based on FMP v3 docs
        # e.g., /income-statement/AAPL, not /income_statement-statement/AAPL
        endpoint_path = statement_type.replace("_", "-")
        endpoint = f"{self.BASE_URL}/{endpoint_path}/{ticker}"
        params = {"apikey": api_key, "limit": limit}
        if period_type_arg == "QTR":
            params["period"] = "quarter"

        try:
            logger.info(
                f"Fetching {period_type_arg} {statement_type} for {ticker} from FMP API (limit={limit})."
            )
            response = requests.get(endpoint, params=params, timeout=30)  # Increased timeout
            response.raise_for_status()  # Check for HTTP errors
            api_data = response.json()

            if not isinstance(api_data, list):
                raise ReadError(
                    f"Unexpected API response format. Expected list, got {type(api_data)}. Response: {str(api_data)[:100]}...",
                    source=f"FMP API ({ticker})",
                    reader_type="FmpReader",
                )
            if not api_data:
                logger.warning(f"FMP API returned empty list for {ticker} {statement_type}.")
                # Return empty graph or raise? Returning empty for now.
                return Graph(periods=[])

        except requests.exceptions.RequestException as e:
            logger.error(
                f"FMP API request failed for {ticker} {statement_type}: {e}",
                exc_info=True,
            )
            raise ReadError(
                f"FMP API request failed: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            )
        except Exception as e:
            logger.error(f"Failed to process FMP API response: {e}", exc_info=True)
            raise ReadError(
                f"Failed to process FMP API response: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            )

        # --- Process Data and Populate Graph ---
        try:
            # FMP data is usually newest first, reverse to process chronologically
            api_data.reverse()

            # Extract periods (e.g., 'date' or 'fillingDate')
            # Using 'date' as it usually represents the period end date
            periods = [item.get("date") for item in api_data if item.get("date")]
            if not periods:
                raise ReadError(
                    "Could not extract periods ('date' field) from FMP API response.",
                    source=f"FMP API ({ticker})",
                    reader_type="FmpReader",
                )

            graph = Graph(periods=periods)
            all_item_data: dict[str, dict[str, float]] = {}

            # Collect data for all items across all periods
            for period_data in api_data:
                period = period_data.get("date")
                if not period:
                    continue  # Skip records without a date

                for api_field, value in period_data.items():
                    node_name = mapping.get(api_field, api_field)  # Use mapping or fallback

                    # Initialize node data dict if first time seeing this node
                    if node_name not in all_item_data:
                        all_item_data[node_name] = {p: np.nan for p in periods}  # Pre-fill with NaN

                    # Store value for this period
                    if isinstance(value, int | float):
                        all_item_data[node_name][period] = float(value)

            # Create nodes from collected data
            nodes_added = 0
            for node_name, period_values in all_item_data.items():
                # Filter out periods that only have NaN
                valid_period_values = {p: v for p, v in period_values.items() if not np.isnan(v)}
                if valid_period_values:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=valid_period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

            logger.info(
                f"Successfully created graph with {nodes_added} nodes from FMP API for {ticker} {statement_type}."
            )
            return graph

        except Exception as e:
            logger.error(f"Failed to parse FMP data and build graph: {e}", exc_info=True)
            raise ReadError(
                message=f"Failed to parse FMP data: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            ) from e


# After class definition, load default mappings
FmpReader._load_default_mappings()

# --- END FILE: fin_statement_model/io/formats/api/fmp.py ---

# --- START FILE: fin_statement_model/io/formats/csv/__init__.py ---
"""CSV format IO operations."""

from .reader import CsvReader

__all__ = ["CsvReader"]

# --- END FILE: fin_statement_model/io/formats/csv/__init__.py ---

# --- START FILE: fin_statement_model/io/formats/csv/reader.py ---
"""Data reader for CSV files."""

import logging
from typing import Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.mixins import (
    FileBasedReader,
    ConfigurableReaderMixin,
    handle_read_errors,
    ValidationResultCollector,
)
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import CsvReaderConfig

logger = logging.getLogger(__name__)


@register_reader("csv")
class CsvReader(FileBasedReader, ConfigurableReaderMixin):
    """Reads financial statement data from a CSV file into a Graph.

    Assumes a 'long' format where each row represents a single data point
    (item, period, value).
    Requires specifying the columns containing item names, period identifiers,
    and values.

    Supports a `mapping_config` constructor parameter for name mapping,
    accepting either a flat mapping or a statement-type scoped mapping.

    Configuration (delimiter, header_row, index_col, mapping_config) is passed
    via a `CsvReaderConfig` object during initialization (typically by the `read_data` facade).
    Method-specific options (`item_col`, `period_col`, `value_col`, `pandas_read_csv_kwargs`)
    are passed as keyword arguments to the `read()` method.
    """

    def __init__(self, cfg: CsvReaderConfig) -> None:
        """Initialize the CsvReader with validated configuration.

        Args:
            cfg: A validated `CsvReaderConfig` instance containing parameters like
                 `source`, `delimiter`, `header_row`, `index_col`, and `mapping_config`.
        """
        self.cfg = cfg

    @handle_read_errors()
    def read(self, source: str, **kwargs: dict[str, Any]) -> Graph:
        """Read data from a CSV file into a new Graph.

        Args:
            source (str): Path to the CSV file.
            **kwargs: Read-time keyword arguments:
                item_col (str): Name of the column containing item identifiers.
                period_col (str): Name of the column containing period identifiers.
                value_col (str): Name of the column containing numeric values.
                pandas_read_csv_kwargs (dict): Additional arguments passed
                    directly to `pandas.read_csv()`. These can override settings
                    from the `CsvReaderConfig` (e.g., `delimiter`).

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the file cannot be read or required columns are missing.
        """
        file_path = source
        logger.info(f"Starting import from CSV file: {file_path}")

        # Use base class validation
        self.validate_file_exists(file_path)
        self.validate_file_extension(file_path, (".csv", ".txt"))

        # Get required column names
        item_col = kwargs.get("item_col")
        period_col = kwargs.get("period_col")
        value_col = kwargs.get("value_col")

        if not all([item_col, period_col, value_col]):
            raise ReadError(
                "Missing required arguments: 'item_col', 'period_col', 'value_col' must be provided.",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        # Read CSV Data
        df = self._read_csv_file(file_path, kwargs.get("pandas_read_csv_kwargs", {}))

        # Validate columns
        self._validate_columns(df, item_col, period_col, value_col, file_path)

        # Process data
        return self._process_dataframe(df, item_col, period_col, value_col, file_path)

    def _read_csv_file(self, file_path: str, user_options: dict[str, Any]) -> pd.DataFrame:
        """Read CSV file with configuration options."""
        # Use configuration from self.cfg, allow overrides via user_options
        read_options = {
            "delimiter": self.get_config_value("delimiter", ","),
            "header": self.get_config_value("header_row", 1) - 1,  # Convert to 0-indexed
        }

        # Handle optional index_col
        index_col = self.get_config_value("index_col")
        if index_col is not None:
            read_options["index_col"] = index_col - 1  # Convert to 0-indexed

        # Merge user-provided kwargs, allowing them to override config
        read_options.update(user_options)

        return pd.read_csv(file_path, **read_options)

    def _validate_columns(
        self, df: pd.DataFrame, item_col: str, period_col: str, value_col: str, file_path: str
    ) -> None:
        """Validate that required columns exist in the DataFrame."""
        required_cols = {item_col, period_col, value_col}
        missing_cols = required_cols - set(df.columns)
        if missing_cols:
            raise ReadError(
                f"Missing required columns in CSV: {missing_cols}",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

    def _process_dataframe(
        self, df: pd.DataFrame, item_col: str, period_col: str, value_col: str, file_path: str
    ) -> Graph:
        """Process the DataFrame and create a Graph."""
        # Convert period column to string
        df[period_col] = df[period_col].astype(str)
        all_periods = sorted(df[period_col].unique().tolist())

        if not all_periods:
            raise ReadError(
                "No periods found in the specified period column.",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        logger.info(f"Identified periods: {all_periods}")
        graph = Graph(periods=all_periods)

        # Use validation collector for better error reporting
        validator = ValidationResultCollector()

        # Group data by item name
        grouped = df.groupby(item_col)
        nodes_added = 0

        # Get mapping config
        mapping_config = self.get_config_value("mapping_config", {})
        if mapping_config is None:
            mapping_config = {}

        for item_name_csv, group in grouped:
            if pd.isna(item_name_csv) or not item_name_csv:
                logger.debug("Skipping group with empty item name.")
                continue

            item_name_csv_str = str(item_name_csv).strip()
            node_name = mapping_config.get(item_name_csv_str, item_name_csv_str)

            period_values = self._extract_period_values(
                group, period_col, value_col, item_name_csv_str, node_name, validator
            )

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' (from CSV item '{item_name_csv_str}') already exists. "
                        "Overwriting data is not standard for readers."
                    )
                else:
                    new_node = FinancialStatementItemNode(name=node_name, values=period_values)
                    graph.add_node(new_node)
                    nodes_added += 1

        # Check for validation errors
        if validator.has_errors():
            summary = validator.get_summary()
            raise ReadError(
                f"Validation errors occurred while reading {file_path}: {'; '.join(summary['errors'])}",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        logger.info(f"Successfully created graph with {nodes_added} nodes from {file_path}.")
        return graph

    def _extract_period_values(
        self,
        group: pd.DataFrame,
        period_col: str,
        value_col: str,
        item_name_csv: str,
        node_name: str,
        validator: ValidationResultCollector,
    ) -> dict[str, float]:
        """Extract period values from a group with validation."""
        period_values: dict[str, float] = {}

        for _, row in group.iterrows():
            period = row[period_col]
            value = row[value_col]

            if pd.isna(value):
                continue  # Skip missing values

            # Validate and convert value
            if not isinstance(value, int | float):
                try:
                    value = float(value)
                    logger.warning(
                        f"Converted non-numeric value '{row[value_col]}' to float "
                        f"for node '{node_name}' period '{period}'"
                    )
                except (ValueError, TypeError):
                    validator.add_result(
                        item_name_csv, False, f"Non-numeric value '{value}' for period '{period}'"
                    )
                    continue

            if period in period_values:
                logger.warning(
                    f"Duplicate value found for node '{node_name}' "
                    f"(from CSV item '{item_name_csv}') period '{period}'. "
                    "Using the last one found."
                )

            period_values[period] = float(value)

        return period_values

# --- END FILE: fin_statement_model/io/formats/csv/reader.py ---

# --- START FILE: fin_statement_model/io/formats/dataframe/__init__.py ---
"""DataFrame format IO operations."""

from .reader import DataFrameReader
from .writer import DataFrameWriter

__all__ = ["DataFrameReader", "DataFrameWriter"]

# --- END FILE: fin_statement_model/io/formats/dataframe/__init__.py ---

# --- START FILE: fin_statement_model/io/formats/dataframe/reader.py ---
"""Data reader for pandas DataFrames."""

import logging
import pandas as pd
import numpy as np
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.base import DataReader
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import DataFrameReaderConfig

logger = logging.getLogger(__name__)


@register_reader("dataframe")
class DataFrameReader(DataReader):
    """Reads data from a pandas DataFrame into a Graph.

    Assumes the DataFrame index contains node names and columns contain periods.
    Values should be numeric.
    """

    def __init__(self, cfg: Optional[DataFrameReaderConfig] = None) -> None:
        """Initialize the DataFrameReader.

        Args:
            cfg: Optional validated `DataFrameReaderConfig` instance.
                 Currently unused but kept for registry symmetry and future options.
        """
        self.cfg = cfg  # For future use; currently no configuration options.

    def read(self, source: pd.DataFrame, **kwargs: Any) -> Graph:
        """Read data from a pandas DataFrame into a new Graph.

        Assumes DataFrame index = node names, columns = periods.

        Args:
            source (pd.DataFrame): The DataFrame to read data from.
            **kwargs: Read-time keyword arguments:
                periods (list[str], optional): Explicit list of periods (columns) to include.
                    If None, all columns are assumed to be periods.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the source is not a DataFrame or has invalid structure.
        """
        df = source
        logger.info("Starting import from DataFrame.")

        # --- Validate Inputs ---
        if not isinstance(df, pd.DataFrame):
            raise ReadError(
                "Source is not a pandas DataFrame.",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        if df.index.name is None and df.index.empty:
            logger.warning(
                "DataFrame index is unnamed and empty, assuming columns are nodes if periods kwarg is provided."
            )
            # Handle case where DF might be oriented differently if periods kwarg is present?
            # For now, stick to index=nodes assumption.

        # Determine periods: use explicit list or infer from columns
        graph_periods_arg = kwargs.get("periods")
        if graph_periods_arg:
            if not isinstance(graph_periods_arg, list):
                raise ReadError("'periods' argument must be a list of column names.")
            missing_cols = [p for p in graph_periods_arg if p not in df.columns]
            if missing_cols:
                raise ReadError(
                    f"Specified periods (columns) not found in DataFrame: {missing_cols}"
                )
            graph_periods = sorted(graph_periods_arg)
            df_subset = df[graph_periods]  # Select only specified period columns
        else:
            # Assume all columns are periods
            graph_periods = sorted(df.columns.astype(str).tolist())
            df_subset = df

        if not graph_periods:
            raise ReadError(
                "No periods identified in DataFrame columns.",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        logger.info(f"Using periods (columns): {graph_periods}")
        graph = Graph(periods=graph_periods)

        # --- Populate Graph ---
        validation_errors = []
        nodes_added = 0
        for node_name_df, row in df_subset.iterrows():
            if pd.isna(node_name_df) or not node_name_df:
                logger.debug("Skipping row with empty index name.")
                continue

            node_name = str(node_name_df).strip()
            period_values: dict[str, float] = {}
            for period in graph_periods:
                value = row[period]
                if pd.isna(value):
                    continue  # Skip NaN values

                if not isinstance(value, int | float | np.number):
                    try:
                        value = float(value)
                        logger.warning(
                            f"Converted non-numeric value '{row[period]}' to float for node '{node_name}' period '{period}'"
                        )
                    except (ValueError, TypeError):
                        validation_errors.append(
                            f"Node '{node_name}': Non-numeric value '{value}' for period '{period}'"
                        )
                        continue  # Skip invalid value

                period_values[period] = float(value)

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' already exists. Overwriting data is not standard for readers."
                    )
                    # Update existing? Log for now.
                else:
                    new_node = FinancialStatementItemNode(name=node_name, values=period_values)
                    graph.add_node(new_node)
                    nodes_added += 1

        if validation_errors:
            raise ReadError(
                f"Validation errors occurred while reading DataFrame: {'; '.join(validation_errors)}",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        logger.info(f"Successfully created graph with {nodes_added} nodes from DataFrame.")
        return graph

        # No specific file operations, so less need for broad Exception catch
        # Specific errors handled above (TypeError, ValueError from float conversion)

# --- END FILE: fin_statement_model/io/formats/dataframe/reader.py ---

# --- START FILE: fin_statement_model/io/formats/dataframe/writer.py ---
"""Data writer for pandas DataFrames."""

import logging
from typing import Optional, Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.mixins import (
    DataFrameBasedWriter,
    ConfigurableReaderMixin,
    handle_write_errors,
)
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.io.config.models import DataFrameWriterConfig

logger = logging.getLogger(__name__)


@register_writer("dataframe")
class DataFrameWriter(DataFrameBasedWriter, ConfigurableReaderMixin):
    """Writes graph data to a pandas DataFrame.

    Converts the graph to a DataFrame with node names as index and periods as columns.

    Configuration options `recalculate` and `include_nodes` are controlled by
    the `DataFrameWriterConfig` object passed during initialization.
    """

    def __init__(self, cfg: Optional[DataFrameWriterConfig] = None) -> None:
        """Initialize the DataFrameWriter.

        Args:
            cfg: Optional validated `DataFrameWriterConfig` instance.
        """
        self.cfg = cfg

    @handle_write_errors()
    def write(self, graph: Graph, target: Any = None, **kwargs: dict[str, object]) -> pd.DataFrame:
        """Convert the graph data to a pandas DataFrame based on instance configuration.

        Args:
            graph (Graph): The Graph instance to export.
            target (Any): Ignored by this writer; the DataFrame is returned directly.
            **kwargs: Currently unused by this method.

        Returns:
            pd.DataFrame: DataFrame with node names as index and periods as columns.

        Raises:
            WriteError: If an error occurs during conversion.
        """
        # Get configuration values using the mixin
        recalculate = self.get_config_value("recalculate", True)
        include_nodes = self.get_config_value("include_nodes")

        logger.info("Exporting graph to DataFrame format.")

        # Handle recalculation if requested
        if recalculate:
            self._recalculate_graph(graph)

        # Extract data using base class method
        data = self.extract_graph_data(graph, include_nodes=include_nodes, calculate=True)

        # Convert to DataFrame
        periods = sorted(graph.periods) if graph.periods else []
        df = pd.DataFrame.from_dict(data, orient="index", columns=periods)
        df.index.name = "node_name"

        logger.info(f"Successfully exported {len(df)} nodes to DataFrame.")
        return df

    def _recalculate_graph(self, graph: Graph) -> None:
        """Recalculate the graph if it has periods defined.

        Args:
            graph: The graph to recalculate.
        """
        try:
            if graph.periods:
                graph.recalculate_all(periods=graph.periods)
                logger.info("Recalculated graph before exporting to DataFrame.")
            else:
                logger.warning("Graph has no periods defined, skipping recalculation.")
        except Exception as e:
            logger.error(
                f"Error during recalculation for DataFrame export: {e}",
                exc_info=True,
            )
            logger.warning("Proceeding to export DataFrame without successful recalculation.")

# --- END FILE: fin_statement_model/io/formats/dataframe/writer.py ---

# --- START FILE: fin_statement_model/io/formats/dict/__init__.py ---
"""Dictionary format IO operations."""

from .reader import DictReader
from .writer import DictWriter

__all__ = ["DictReader", "DictWriter"]

# --- END FILE: fin_statement_model/io/formats/dict/__init__.py ---

# --- START FILE: fin_statement_model/io/formats/dict/reader.py ---
"""Data reader for Python dictionaries."""

import logging
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.base import DataReader
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import DictReaderConfig

logger = logging.getLogger(__name__)


@register_reader("dict")
class DictReader(DataReader):
    """Reads data from a Python dictionary to create a new Graph.

    Expects a dictionary format: {node_name: {period: value, ...}, ...}
    Creates FinancialStatementItemNode instances for each entry.

    Note:
        Configuration is handled via `DictReaderConfig` during initialization.
        The `read()` method takes the source dictionary directly and an optional
        `periods` keyword argument.
    """

    def __init__(self, cfg: Optional[DictReaderConfig] = None) -> None:
        """Initialize the DictReader.

        Args:
            cfg: Optional validated `DictReaderConfig` instance.
                 Currently unused but kept for registry symmetry and future options.
        """
        self.cfg = cfg

    def read(self, source: dict[str, dict[str, float]], **kwargs: Any) -> Graph:
        """Create a new Graph from a dictionary.

        Args:
            source: Dictionary mapping node names to period-value dictionaries.
                    Format: {node_name: {period: value, ...}, ...}
            **kwargs: Read-time keyword arguments:
                periods (list[str], optional): Explicit list of periods for the new graph.
                    If None, inferred from data keys.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the source data format is invalid or processing fails.
            # DataValidationError: If data values are not numeric.
        """
        logger.info("Starting import from dictionary to create a new graph.")

        if not isinstance(source, dict):
            raise ReadError(
                message="Invalid source type for DictReader. Expected dict.",
                source="dict_input",
                reader_type="DictReader",
            )

        # Validate data structure and collect all periods
        all_periods = set()
        validation_errors = []
        try:
            for node_name, period_values in source.items():
                if not isinstance(period_values, dict):
                    validation_errors.append(
                        f"Node '{node_name}': Invalid format - expected dict, got {type(period_values).__name__}"
                    )
                    continue  # Skip further checks for this node
                for period, value in period_values.items():
                    # Basic type checks - can be expanded
                    if not isinstance(period, str):
                        validation_errors.append(
                            f"Node '{node_name}': Invalid period format '{period}' - expected string."
                        )
                    if not isinstance(value, int | float):
                        validation_errors.append(
                            f"Node '{node_name}' period '{period}': Invalid value type {type(value).__name__} - expected number."
                        )
                    all_periods.add(str(period))

            if validation_errors:
                # Use core DataValidationError if it exists and is suitable
                # Otherwise, stick to ReadError or a specific IOValidationError
                # raise DataValidationError(
                #     message="Input dictionary failed validation",
                #     validation_errors=validation_errors
                # )
                raise ReadError(
                    f"Input dictionary failed validation: {'; '.join(validation_errors)}",
                    source="dict_input",
                    reader_type="DictReader",
                )

        except Exception as e:
            # Catch unexpected validation errors
            raise ReadError(
                message=f"Error validating input dictionary: {e}",
                source="dict_input",
                reader_type="DictReader",
                original_error=e,
            ) from e

        # Determine graph periods
        graph_periods = kwargs.get("periods")
        if graph_periods is None:
            graph_periods = sorted(list(all_periods))
            logger.debug(f"Inferred graph periods from data: {graph_periods}")
        # Optional: Validate if all data periods are within the provided list
        elif not all_periods.issubset(set(graph_periods)):
            missing = all_periods - set(graph_periods)
            logger.warning(f"Data contains periods not in specified graph periods: {missing}")
            # Decide whether to error or just ignore extra data

        # Create graph and add nodes
        try:
            graph = Graph(periods=graph_periods)
            for node_name, period_values in source.items():
                # Filter values to only include those matching graph_periods
                filtered_values = {p: v for p, v in period_values.items() if p in graph_periods}
                if filtered_values:
                    # Create FinancialStatementItemNode directly
                    # Assumes FinancialStatementItemNode takes name and values dict
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=filtered_values.copy()
                    )
                    graph.add_node(new_node)
                else:
                    logger.debug(
                        f"Node '{node_name}' has no data for specified graph periods. Skipping."
                    )

            logger.info(
                f"Successfully created graph with {len(graph.nodes)} nodes from dictionary."
            )
            return graph

        except Exception as e:
            # Catch errors during graph/node creation
            logger.error(f"Failed to create graph from dictionary: {e}", exc_info=True)
            raise ReadError(
                message="Failed to build graph from dictionary data",
                source="dict_input",
                reader_type="DictReader",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/io/formats/dict/reader.py ---

# --- START FILE: fin_statement_model/io/formats/dict/writer.py ---
"""Data writer for Python dictionaries."""

import logging
from typing import Any, Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.mixins import (
    DataFrameBasedWriter,
    ConfigurableReaderMixin,
    handle_write_errors,
)
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.io.config.models import DictWriterConfig

logger = logging.getLogger(__name__)


@register_writer("dict")
class DictWriter(DataFrameBasedWriter, ConfigurableReaderMixin):
    """Writes graph data to a Python dictionary.

    Extracts values for each node and period in the graph, attempting to
    calculate values where possible.

    Initialized via `DictWriterConfig` (typically by the `write_data` facade),
    although the config currently has no options.
    """

    def __init__(self, cfg: Optional[DictWriterConfig] = None) -> None:
        """Initialize the DictWriter.

        Args:
            cfg: Optional validated `DictWriterConfig` instance.
                 Currently unused but kept for registry symmetry.
        """
        self.cfg = cfg

    @handle_write_errors()
    def write(
        self, graph: Graph, target: Any = None, **kwargs: dict[str, Any]
    ) -> dict[str, dict[str, float]]:
        """Export calculated data from all graph nodes to a dictionary.

        Args:
            graph (Graph): The Graph instance to export data from.
            target (Any): Ignored by this writer; the dictionary is returned directly.
            **kwargs: Currently unused.

        Returns:
            Dict[str, Dict[str, float]]: Mapping node names to period-value dicts.
                                         Includes values for all nodes in the graph
                                         for all defined periods. NaN represents
                                         uncalculable values.

        Raises:
            WriteError: If an unexpected error occurs during export.
        """
        logger.info(f"Starting export of graph '{graph}' to dictionary format.")

        if not graph.periods:
            logger.warning("Graph has no periods defined. Exported dictionary will be empty.")
            return {}

        # Use base class method to extract all data
        # This handles calculation attempts and error handling consistently
        result = self.extract_graph_data(graph, include_nodes=None, calculate=True)

        logger.info(f"Successfully exported {len(result)} nodes to dictionary.")
        return result

# --- END FILE: fin_statement_model/io/formats/dict/writer.py ---

# --- START FILE: fin_statement_model/io/formats/excel/__init__.py ---
"""Excel format IO operations."""

from .reader import ExcelReader
from .writer import ExcelWriter

__all__ = ["ExcelReader", "ExcelWriter"]

# --- END FILE: fin_statement_model/io/formats/excel/__init__.py ---

# --- START FILE: fin_statement_model/io/formats/excel/reader.py ---
"""Data reader for Excel files."""

import logging
from typing import Optional, Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.core.mixins import (
    FileBasedReader,
    ConfigurableReaderMixin,
    handle_read_errors,
    ValidationResultCollector,
)
from fin_statement_model.io.core.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.core.utils import normalize_mapping
from fin_statement_model.io.config.models import ExcelReaderConfig

logger = logging.getLogger(__name__)


@register_reader("excel")
class ExcelReader(FileBasedReader, ConfigurableReaderMixin):
    """Reads financial statement data from an Excel file into a Graph.

    Expects data in a tabular format where rows typically represent items
    and columns represent periods, or vice-versa.
    Requires specifying sheet name, period identification, and item identification.

    Configuration (sheet_name, items_col, periods_row, mapping_config) is passed
    via an `ExcelReaderConfig` object during initialization (typically by the `read_data` facade).
    Method-specific options (`statement_type`, `header_row`, `nrows`, `skiprows`)
    are passed as keyword arguments to the `read()` method.
    """

    def __init__(self, cfg: ExcelReaderConfig) -> None:
        """Initialize the ExcelReader with validated configuration.

        Args:
            cfg: A validated `ExcelReaderConfig` instance containing parameters like
                 `source`, `sheet_name`, `items_col`, `periods_row`, and `mapping_config`.
        """
        self.cfg = cfg

    def _get_mapping(self, statement_type: Optional[str]) -> dict[str, str]:
        """Get the appropriate mapping based on statement type and the stored config."""
        # Use the mapping config stored in the validated Pydantic config object
        config = self.get_config_value("mapping_config")
        # Normalize and overlay user-provided mappings
        mapping = normalize_mapping(config, context_key=statement_type)
        return mapping

    @handle_read_errors()
    def read(self, source: str, **kwargs: dict[str, Any]) -> Graph:
        """Read data from an Excel file sheet into a new Graph based on instance config.

        Args:
            source (str): Path to the Excel file.
            **kwargs: Optional runtime keyword arguments:
                statement_type (str, optional): Type of statement ('income_statement', 'balance_sheet', 'cash_flow').
                    Used to select a scope within the `mapping_config` provided during initialization.
                header_row (int, optional): 1-based index for pandas header reading.
                    Defaults to `self.cfg.periods_row` if not provided.
                nrows (int, optional): Number of rows to read from the sheet.
                skiprows (int, optional): Number of rows to skip at the beginning.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the file cannot be read or the configuration is invalid.
        """
        file_path = source
        logger.info(f"Starting import from Excel file: {file_path}")

        # Use base class validation
        self.validate_file_exists(file_path)
        self.validate_file_extension(file_path, (".xls", ".xlsx", ".xlsm"))

        # Get configuration values
        sheet_name = self.require_config_value("sheet_name")
        periods_row = self.require_config_value("periods_row")
        items_col = self.require_config_value("items_col")

        # Runtime options from kwargs
        statement_type = kwargs.get("statement_type")
        header_row = kwargs.get("header_row", periods_row)
        nrows = kwargs.get("nrows")
        skiprows = kwargs.get("skiprows")

        # Get mapping
        mapping = self._get_mapping(statement_type)
        logger.debug(f"Using mapping for statement type '{statement_type}': {mapping}")

        # Read Excel data
        df, period_headers = self._read_excel_data(
            file_path, sheet_name, periods_row, items_col, header_row, nrows, skiprows
        )

        # Extract periods
        graph_periods = self._extract_periods(period_headers, items_col)

        # Create and populate graph
        return self._create_graph(df, graph_periods, items_col, mapping, file_path, sheet_name)

    def _read_excel_data(
        self,
        file_path: str,
        sheet_name: str,
        periods_row: int,
        items_col: int,
        header_row: int,
        nrows: Optional[int],
        skiprows: Optional[int],
    ) -> tuple[pd.DataFrame, list[str]]:
        """Read Excel file and extract data and period headers."""
        # Convert to 0-based indices for pandas
        periods_row_0idx = periods_row - 1
        items_col_0idx = items_col - 1
        header_row_0idx = header_row - 1

        # Read the main data
        df = pd.read_excel(
            file_path,
            sheet_name=sheet_name,
            header=header_row_0idx,
            skiprows=skiprows,
            nrows=nrows,
        )

        # Get period headers
        if header_row_0idx != periods_row_0idx:
            # Read periods row separately if different from header
            periods_df = pd.read_excel(
                file_path,
                sheet_name=sheet_name,
                header=None,
                skiprows=periods_row_0idx,
                nrows=1,
            )
            period_headers = periods_df.iloc[0].astype(str).tolist()
        else:
            # Periods are in the main header row
            period_headers = df.columns.astype(str).tolist()

        # Validate items column index
        if items_col_0idx >= len(df.columns):
            raise ReadError(
                f"items_col index ({items_col}) is out of bounds for sheet '{sheet_name}'. "
                f"Found {len(df.columns)} columns.",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        return df, period_headers

    def _extract_periods(self, period_headers: list[str], items_col: int) -> list[str]:
        """Extract valid period names from headers."""
        items_col_0idx = items_col - 1

        # Filter period headers: exclude the item column and empty values
        graph_periods = [
            p for i, p in enumerate(period_headers) if i > items_col_0idx and p and p.strip()
        ]

        if not graph_periods:
            raise ReadError(
                f"Could not identify period columns after column {items_col}. "
                f"Headers found: {period_headers}",
                source="Excel file",
                reader_type=self.__class__.__name__,
            )

        logger.info(f"Identified periods: {graph_periods}")
        return graph_periods

    def _create_graph(
        self,
        df: pd.DataFrame,
        graph_periods: list[str],
        items_col: int,
        mapping: dict[str, str],
        file_path: str,
        sheet_name: str,
    ) -> Graph:
        """Create and populate the graph from DataFrame."""
        items_col_0idx = items_col - 1
        graph = Graph(periods=graph_periods)

        # Use validation collector
        validator = ValidationResultCollector()
        nodes_added = 0

        for index, row in df.iterrows():
            # Get item name
            item_name_excel = row.iloc[items_col_0idx]
            if pd.isna(item_name_excel) or not item_name_excel:
                continue

            item_name_excel = str(item_name_excel).strip()
            node_name = mapping.get(item_name_excel, item_name_excel)

            # Extract values for all periods
            period_values = self._extract_row_values(
                row, df, graph_periods, node_name, item_name_excel, index, validator
            )

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' (from Excel item '{item_name_excel}') already exists. "
                        "Overwriting data is not standard for readers."
                    )
                else:
                    new_node = FinancialStatementItemNode(name=node_name, values=period_values)
                    graph.add_node(new_node)
                    nodes_added += 1

        # Check for validation errors
        if validator.has_errors():
            summary = validator.get_summary()
            raise ReadError(
                f"Validation errors occurred while reading {file_path} sheet '{sheet_name}': "
                f"{'; '.join(summary['errors'])}",
                source=file_path,
                reader_type=self.__class__.__name__,
            )

        logger.info(
            f"Successfully created graph with {nodes_added} nodes from {file_path} sheet '{sheet_name}'."
        )
        return graph

    def _extract_row_values(
        self,
        row: pd.Series,
        df: pd.DataFrame,
        graph_periods: list[str],
        node_name: str,
        item_name_excel: str,
        row_index: int,
        validator: ValidationResultCollector,
    ) -> dict[str, float]:
        """Extract period values from a row with validation."""
        period_values: dict[str, float] = {}

        for period in graph_periods:
            if period not in df.columns:
                logger.warning(
                    f"Period header '{period}' not found in DataFrame columns for row {row_index}."
                )
                continue

            value = row[period]

            if pd.isna(value):
                continue  # Skip NaN values

            if isinstance(value, int | float):
                period_values[period] = float(value)
            else:
                # Try to convert to float
                try:
                    period_values[period] = float(value)
                    logger.warning(
                        f"Converted non-numeric value '{value}' to float for "
                        f"node '{node_name}' period '{period}'"
                    )
                except (ValueError, TypeError):
                    validator.add_result(
                        f"Row {row_index}",
                        False,
                        f"Non-numeric value '{value}' for node '{node_name}' "
                        f"(from '{item_name_excel}') period '{period}'",
                    )

        return period_values

# --- END FILE: fin_statement_model/io/formats/excel/reader.py ---

# --- START FILE: fin_statement_model/io/formats/excel/writer.py ---
"""Data writer for Excel files."""

import logging
from pathlib import Path
from typing import Any, Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.core.base import DataWriter
from fin_statement_model.io.core.mixins import ConfigurableReaderMixin, handle_write_errors
from fin_statement_model.io.core.registry import register_writer
from fin_statement_model.io.formats.dataframe.writer import DataFrameWriter
from fin_statement_model.io.config.models import ExcelWriterConfig, DataFrameWriterConfig

logger = logging.getLogger(__name__)


@register_writer("excel")
class ExcelWriter(DataWriter, ConfigurableReaderMixin):
    """Writes graph data to an Excel file.

    Converts the graph data to a pandas DataFrame first (using `DataFrameWriter`),
    then writes that DataFrame to an Excel file using `pandas.to_excel()`.

    Configuration (sheet_name, recalculate, include_nodes, excel_writer_kwargs) is
    provided via an `ExcelWriterConfig` object during initialization.
    """

    def __init__(self, cfg: Optional[ExcelWriterConfig] = None) -> None:
        """Initialize the ExcelWriter.

        Args:
            cfg: Optional validated `ExcelWriterConfig` instance.
        """
        self.cfg = cfg

    @handle_write_errors()
    def write(self, graph: Graph, target: str, **kwargs: dict[str, Any]) -> None:
        """Write graph data to an Excel file, converting via DataFrame first.

        Args:
            graph (Graph): The Graph object containing the data to write.
            target (str): Path to the target Excel file.
            **kwargs: Currently unused. Configuration is handled by the ExcelWriterConfig.

        Raises:
            WriteError: If an error occurs during the writing process.
        """
        file_path = target

        # Get configuration values using the mixin
        sheet_name = self.get_config_value("sheet_name", "Sheet1")
        recalculate = self.get_config_value("recalculate", True)
        include_nodes = self.get_config_value("include_nodes")
        excel_writer_options = self.get_config_value("excel_writer_kwargs", {})

        logger.info(f"Exporting graph to Excel file: {file_path}, sheet: {sheet_name}")

        # Convert graph to DataFrame
        df = self._create_dataframe(graph, recalculate, include_nodes)

        # Write DataFrame to Excel
        self._write_to_excel(df, file_path, sheet_name, excel_writer_options)

        logger.info(f"Successfully exported graph to {file_path}, sheet '{sheet_name}'")

    def _create_dataframe(
        self, graph: Graph, recalculate: bool, include_nodes: Optional[list[str]]
    ) -> Any:
        """Convert graph to DataFrame using DataFrameWriter.

        Args:
            graph: The graph to convert.
            recalculate: Whether to recalculate before export.
            include_nodes: Optional list of nodes to include.

        Returns:
            pandas DataFrame with the graph data.
        """
        # Create a config for DataFrameWriter
        df_config = DataFrameWriterConfig(
            format_type="dataframe", recalculate=recalculate, include_nodes=include_nodes
        )

        df_writer = DataFrameWriter(df_config)
        return df_writer.write(graph=graph, target=None)

    def _write_to_excel(
        self, df: Any, file_path: str, sheet_name: str, excel_writer_options: dict[str, Any]
    ) -> None:
        """Write DataFrame to Excel file.

        Args:
            df: The pandas DataFrame to write.
            file_path: Path to the output file.
            sheet_name: Name of the Excel sheet.
            excel_writer_options: Additional options for pandas.to_excel().
        """
        output_path = Path(file_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        df.to_excel(
            output_path,
            sheet_name=sheet_name,
            index=True,  # Keep node names as index column
            **excel_writer_options,
        )

# --- END FILE: fin_statement_model/io/formats/excel/writer.py ---

# --- START FILE: fin_statement_model/io/formats/markdown/__init__.py ---
"""Markdown format IO operations."""

from .writer import MarkdownWriter

__all__ = ["MarkdownWriter"]

# --- END FILE: fin_statement_model/io/formats/markdown/__init__.py ---

# --- START FILE: fin_statement_model/io/formats/markdown/writer.py ---
"""Writes a financial statement graph to a Markdown table."""

import logging
import yaml  # Added for parsing statement config
from typing import Any, Optional, TypedDict, Union
from collections.abc import Iterable

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentFilter,
    DEFAULT_SCENARIO,
)
from fin_statement_model.io.core.base import DataWriter
from fin_statement_model.io.config.models import BaseWriterConfig
from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.io.core.registry import register_writer

logger = logging.getLogger(__name__)


class MarkdownWriterConfig(BaseWriterConfig):
    """Configuration specific to the Markdown writer."""

    indent_spaces: int = 4  # Number of spaces per indentation level
    # Add other Markdown-specific config options here if needed


# Define a structure for the items to be written
class StatementItem(TypedDict):
    """Represents a line item with its values for Markdown output."""

    name: str
    # value: Union[float, int, str, None] # Replaced single value
    values: dict[str, Union[float, int, str, None]]  # Values per period
    level: int
    is_subtotal: bool  # Indicates if the row is a subtotal or section header


@register_writer("markdown")
class MarkdownWriter(DataWriter):
    """Writes a financial statement structure to a Markdown table."""

    def __init__(self, config: Optional[MarkdownWriterConfig] = None):
        """Initializes the MarkdownWriter."""
        self.config = config or MarkdownWriterConfig(format_type="markdown")
        logger.debug(f"Initialized MarkdownWriter with config: {self.config}")

    def _format_value(self, value: Union[float, int, str, None]) -> str:
        """Formats the value for display in the table."""
        if value is None:
            return ""
        if isinstance(value, float | int):
            # Basic number formatting, could be enhanced (e.g., commas)
            return f"{value:,.2f}" if isinstance(value, float) else str(value)
        return str(value)

    def _get_statement_items(
        self, graph: Graph, statement_config_path: str
    ) -> Iterable[StatementItem]:
        """Dynamically extracts and orders statement items based on YAML config and graph data.

        Args:
            graph: The financial statement graph containing node values.
            statement_config_path: Path to the statement definition YAML file.

        Returns:
            An iterable of StatementItem dictionaries.

        Raises:
            WriteError: If the config file is not found, invalid, or nodes are missing.
        """
        try:
            with open(statement_config_path) as f:
                config_data = yaml.safe_load(f)
            if not config_data or not isinstance(config_data, dict):
                raise WriteError(f"Invalid or empty YAML structure in {statement_config_path}")

        except FileNotFoundError:
            logger.exception(f"Statement configuration file not found: {statement_config_path}")
            raise WriteError(
                f"Statement configuration file not found: {statement_config_path}"
            ) from None
        except yaml.YAMLError as e:
            logger.exception(
                f"Error parsing statement configuration YAML '{statement_config_path}'"
            )
            raise WriteError(f"Error parsing statement configuration YAML: {e}") from e

        periods = sorted(list(graph.periods))
        logger.debug(
            f"Extracting statement items for periods: {periods} using config: {statement_config_path}"
        )

        # Define the recursive processing function within the scope
        def process_level(items_or_sections: list, level: int):
            for config_item in items_or_sections:
                # Check if this dictionary represents a section container
                # by seeing if it has an 'items' list.
                inner_items = config_item.get("items")
                if isinstance(inner_items, list):
                    # It's a section-like structure. Process its inner items.
                    # Optionally, yield a header for the section here if needed
                    # section_name = config_item.get("name", "Unnamed Section")
                    # yield StatementItem(name=f"**{section_name}**", values={p: "" for p in periods}, level=level, is_subtotal=True) # Treat as subtotal for bolding?

                    yield from process_level(
                        inner_items, level + 1
                    )  # Process items within the section

                    # Also process any nested subsections
                    subsections = config_item.get("subsections")
                    if isinstance(subsections, list):
                        yield from process_level(subsections, level + 1)

                    # Process subtotal for this section if it exists
                    section_subtotal_config = config_item.get("subtotal")
                    if section_subtotal_config:
                        # Ensure subtotal has correct type and id before processing
                        if section_subtotal_config.get("id") and section_subtotal_config.get(
                            "type"
                        ):
                            subtotal_item = process_item(
                                section_subtotal_config,
                                level + 1,  # Indent subtotal
                            )
                            if subtotal_item:
                                yield subtotal_item
                        else:
                            logger.warning(
                                f"Skipping section subtotal due to missing 'id' or 'type': {section_subtotal_config}"
                            )

                else:
                    # This is likely a direct item (line_item, metric, etc.)
                    # Check for required fields before processing
                    item_id = config_item.get("id")
                    item_type = config_item.get("type")
                    if not item_id or not item_type:
                        logger.warning(
                            f"Skipping item due to missing 'id' or 'type' in config: {config_item}"
                        )
                        continue  # Skip this malformed item

                    # Process the item using the existing function
                    item_data = process_item(config_item, level)
                    if item_data:
                        yield item_data

        # Define the process_item function (handles non-section items)
        def process_item(item_config: dict, level: int) -> Union[StatementItem, None]:
            item_id = item_config.get("id")
            item_name = item_config.get("name", "Unknown Item")
            item_type = item_config.get("type")
            sign_convention = item_config.get("sign_convention", 1)

            # No need for the redundant check here as it's done in process_level now
            # if not item_id or not item_type:
            #     logger.warning(
            #         f"Skipping item due to missing 'id' or 'type' in config: {item_config}"
            #     )
            #     return None

            values = {}
            is_subtotal = item_type == "subtotal"
            node_id = None

            if item_type == "line_item":
                node_id = item_config.get("node_id")
                if not node_id:
                    logger.warning(
                        f"Skipping line_item '{item_name}' (ID: {item_id}) - missing 'node_id' in config."
                    )
                    return None
            elif item_type in ["calculated", "subtotal", "metric"]:
                node_id = item_id
            else:
                logger.warning(
                    f"Unsupported item type '{item_type}' for item '{item_name}'. Skipping."
                )
                return None

            # Check if node_id could be determined (might be None if type wasn't handled)
            if node_id is None:
                logger.warning(
                    f"Could not determine node ID for item '{item_name}' (Type: {item_type}). Skipping value fetch."
                )
                for period in periods:
                    values[period] = None
                # Still return the item structure but with None values
                return StatementItem(
                    name=item_name, values=values, level=level, is_subtotal=is_subtotal
                )

            # Fetch values from the graph node
            try:
                node = graph.get_node(node_id)
                for period in periods:
                    raw_value = None
                    if item_type == "line_item":
                        # For line items, get the stored value directly
                        raw_value = node.get_value(period)
                    elif item_type in ["calculated", "subtotal", "metric"]:
                        # For calculated/subtotal/metric items, use the graph's calculation engine
                        try:
                            raw_value = graph.calculate(node_id, period)
                        except Exception:  # Catch potential calculation errors
                            logger.exception(
                                f"Calculation failed for node '{node_id}' period '{period}'"
                            )
                            raw_value = "CALC_ERR"
                    else:
                        # Should not happen based on earlier check, but as a safeguard
                        logger.warning(
                            f"Unexpected item type '{item_type}' during value fetch for '{item_name}'."
                        )
                        raw_value = None

                    # Apply sign convention
                    # Handle potential string values from errors
                    if isinstance(raw_value, int | float):
                        values[period] = raw_value * sign_convention
                    elif raw_value is None:
                        values[period] = None
                    else:  # Keep error strings as is
                        values[period] = raw_value

            except KeyError:
                logger.warning(
                    f"Node '{node_id}' for item '{item_name}' (Type: {item_type}) not found in graph. Values will be missing."
                )
                for period in periods:
                    values[period] = None  # Fill with None if node is missing
            except Exception as e:
                logger.error(
                    f"Error fetching value for node '{node_id}' (Item: '{item_name}'): {e}",
                    exc_info=True,
                )
                for period in periods:
                    values[period] = "ERROR"  # Indicate error in output

            return StatementItem(
                name=item_name, values=values, level=level, is_subtotal=is_subtotal
            )

        # Start processing from the top-level sections
        yield from process_level(config_data.get("sections", []), level=0)

    def write(self, graph: Graph, target: Any = None, **kwargs: Any) -> str:
        """Writes the financial statement graph to a Markdown table string.

        Args:
            graph: The Graph object containing the financial data.
            target: Ignored by this writer, as it returns a string.
            **kwargs: Additional options. Expected args:
                statement_config_path (str): Path to the statement definition YAML.
                historical_periods (list[str], optional): List of historical period names.
                forecast_periods (list[str], optional): List of forecast period names.
                forecast_configs (dict, optional): Maps node IDs to forecast methods/configs for notes.

        Returns:
            A string containing the formatted statement in Markdown.

        Raises:
            WriteError: If statement_config_path is missing/invalid or an error occurs during writing.
        """
        logger.info(
            f"Writing graph to Markdown format (target ignored: {target}) using kwargs: {kwargs.keys()}"
        )

        statement_config_path = kwargs.get("statement_config_path")
        if not statement_config_path or not isinstance(statement_config_path, str):
            raise WriteError(
                "Missing or invalid 'statement_config_path' keyword argument for MarkdownWriter."
            )

        try:
            items = list(self._get_statement_items(graph, statement_config_path))
            if not items:
                logger.warning("No statement items generated from config and graph.")
                return ""  # Return empty string

            # --- Get periods and determine historical/forecast ---
            # Get periods from graph, ensuring order
            periods = sorted(list(graph.periods))
            # Default historical/forecast periods if not provided - attempt to infer or use all
            all_periods = set(periods)
            historical_periods = set(
                kwargs.get("historical_periods", [])
            )  # Get from kwargs if provided
            forecast_periods = set(
                kwargs.get("forecast_periods", [])
            )  # Get from kwargs if provided

            # Simple inference if not provided fully
            if not historical_periods and not forecast_periods:
                # Assume all periods are historical if no forecast info given (basic fallback)
                logger.warning(
                    "No historical/forecast period info provided; assuming all periods are historical for formatting."
                )
                historical_periods = all_periods
            elif not historical_periods:
                historical_periods = all_periods - forecast_periods
            elif not forecast_periods:
                forecast_periods = all_periods - historical_periods

            # --- Calculate dynamic padding ---
            max_desc_width = 0
            period_max_value_widths: dict[str, int] = {p: 0 for p in periods}
            formatted_lines = []

            # First pass: format data and calculate max widths
            for item in items:
                indent = " " * (item["level"] * self.config.indent_spaces)
                name = f"{indent}{item['name']}"
                is_subtotal = item["is_subtotal"]
                values_formatted: dict[str, str] = {}

                if is_subtotal:
                    name = f"**{name}**"

                max_desc_width = max(max_desc_width, len(name))

                for period in periods:
                    raw_value = item["values"].get(period)
                    value_str = self._format_value(raw_value)
                    if is_subtotal:
                        value_str = f"**{value_str}**"
                    values_formatted[period] = value_str
                    period_max_value_widths[period] = max(
                        period_max_value_widths[period], len(value_str)
                    )

                formatted_lines.append(
                    {
                        "name": name,
                        "values": values_formatted,
                        "is_subtotal": is_subtotal,
                    }
                )

            # Add some spacing between columns

            # --- Build the final string ---
            output_lines = []

            # Build header row
            header_parts = ["Description".ljust(max_desc_width)]
            for period in periods:
                period_label = period
                if period in historical_periods:
                    period_label += " (H)"
                elif period in forecast_periods:
                    period_label += " (F)"
                header_parts.append(period_label.rjust(period_max_value_widths[period]))
            # Join with | and add start/end |
            output_lines.append(f"| {(' | ').join(header_parts)} |")

            # Add separator line
            separator_parts = ["-" * max_desc_width]
            separator_parts.extend("-" * period_max_value_widths[period] for period in periods)
            output_lines.append(f"| {(' | ').join(separator_parts)} |")

            # Build data rows
            for line_data in formatted_lines:
                row_parts = [line_data["name"].ljust(max_desc_width)]
                for period in periods:
                    value = line_data["values"].get(period, "")  # Get value or empty string
                    row_parts.append(value.rjust(period_max_value_widths[period]))
                # Join with | and add start/end |
                output_lines.append(f"| {(' | ').join(row_parts)} |")

            # --- Add Forecast Notes ---
            forecast_configs = kwargs.get("forecast_configs")
            if forecast_configs:
                notes = ["", "## Forecast Notes"]  # Add blank line before header
                for node_id, config in forecast_configs.items():
                    method = config.get("method", "N/A")
                    cfg_details = config.get("config")
                    desc = f"- **{node_id}**: Forecasted using method '{method}'"
                    if method == "simple" and cfg_details is not None:
                        desc += f" (e.g., simple growth rate: {cfg_details:.1%})."
                    elif method == "curve" and cfg_details:
                        rates_str = ", ".join([f"{r:.1%}" for r in cfg_details])
                        desc += f" (e.g., specific growth rates: [{rates_str}])."
                    elif method == "historical_growth":
                        desc += " (based on average historical growth)."
                    elif method == "average":
                        desc += " (based on historical average value)."
                    elif method == "statistical":
                        dist_name = cfg_details.get("distribution", "unknown")
                        params_dict = cfg_details.get("params", {})
                        params_str = ", ".join(
                            [
                                f"{k}={v:.3f}" if isinstance(v, float) else f"{k}={v}"
                                for k, v in params_dict.items()
                            ]
                        )
                        desc += f" (using '{dist_name}' distribution with params: {params_str})."
                    else:
                        desc += "."
                    notes.append(desc)
                # Append notes if any were generated
                if len(notes) > 2:  # Header + at least one note
                    output_lines.extend(notes)

            # --- Add Adjustment Notes --- #
            adj_filter_input = kwargs.get("adjustment_filter")
            all_adjustments: list[Adjustment] = graph.list_all_adjustments()
            filtered_adjustments: list[Adjustment] = []

            if all_adjustments:
                # Create a filter instance based on the input
                filt: AdjustmentFilter
                if isinstance(adj_filter_input, AdjustmentFilter):
                    filt = adj_filter_input.model_copy(
                        update={"period": None}
                    )  # Ignore period context
                elif isinstance(adj_filter_input, set):
                    filt = AdjustmentFilter(
                        include_tags=adj_filter_input,
                        include_scenarios={
                            DEFAULT_SCENARIO
                        },  # Assume default scenario for tag shorthand
                        period=None,
                    )
                else:  # Includes None or other types
                    filt = AdjustmentFilter(include_scenarios={DEFAULT_SCENARIO}, period=None)

                # Apply the filter
                filtered_adjustments = [adj for adj in all_adjustments if filt.matches(adj)]

            if filtered_adjustments:
                output_lines.append("")  # Blank line
                output_lines.append("## Adjustment Notes (Matching Filter)")
                for adj in sorted(
                    filtered_adjustments,
                    key=lambda x: (x.node_name, x.period, x.priority, x.timestamp),
                ):
                    tags_str = ", ".join(sorted(adj.tags)) if adj.tags else "None"
                    details = (
                        f"- **{adj.node_name}** ({adj.period}, Scenario: {adj.scenario}, Prio: {adj.priority}): "
                        f"{adj.type.name.capitalize()} adjustment of {adj.value:.2f}. "
                        f"Reason: {adj.reason}. Tags: [{tags_str}]. (ID: {adj.id})"
                    )
                    output_lines.append(details)
            # --- End Adjustment Notes --- #

            return "\n".join(output_lines)

        except NotImplementedError as nie:
            logger.exception("Markdown write failed")
            raise WriteError(
                message=f"Markdown writer requires graph traversal logic: {nie}",
                target=target,
                writer_type="markdown",
                original_error=nie,
            ) from nie
        except Exception as e:
            logger.exception("Error writing Markdown for graph", exc_info=True)
            raise WriteError(
                message=f"Failed to generate Markdown table: {e}",
                target=target,
                writer_type="markdown",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/io/formats/markdown/writer.py ---

# --- START FILE: fin_statement_model/io/specialized/__init__.py ---
"""Specialized IO operations for domain-specific functionality."""

# Adjustments
from .adjustments import (
    read_excel,
    write_excel,
    load_adjustments_from_excel,
    export_adjustments_to_excel,
)

# Cells
from .cells import import_from_cells

# Graph serialization
from .graph import (
    GraphDefinitionReader,
    GraphDefinitionWriter,
    save_graph_definition,
    load_graph_definition,
)

# Statement utilities
from .statements import (
    list_available_builtin_configs,
    read_builtin_statement_config,
    read_statement_config_from_path,
    read_statement_configs_from_directory,
    write_statement_to_excel,
    write_statement_to_json,
)

__all__ = [
    # Graph
    "GraphDefinitionReader",
    "GraphDefinitionWriter",
    # Adjustments
    "export_adjustments_to_excel",
    # Cells
    "import_from_cells",
    # Statements
    "list_available_builtin_configs",
    "load_adjustments_from_excel",
    "load_graph_definition",
    "read_builtin_statement_config",
    "read_excel",
    "read_statement_config_from_path",
    "read_statement_configs_from_directory",
    "save_graph_definition",
    "write_excel",
    "write_statement_to_excel",
    "write_statement_to_json",
]

# --- END FILE: fin_statement_model/io/specialized/__init__.py ---

# --- START FILE: fin_statement_model/io/specialized/adjustments.py ---
"""Functions for bulk import and export of adjustments via Excel files."""

import logging
from typing import Optional, Any
from pathlib import Path
from collections import defaultdict

import pandas as pd

from fin_statement_model.core.adjustments.models import (
    Adjustment,
    AdjustmentType,
    AdjustmentTag,
    DEFAULT_SCENARIO,
)
from fin_statement_model.core.graph import Graph  # Needed for Graph convenience methods
from fin_statement_model.io.exceptions import ReadError, WriteError

logger = logging.getLogger(__name__)

# Define expected column names (case-insensitive matching during read)
# Required columns per spec:
REQ_COLS = {"node_name", "period", "value", "reason"}
# Optional columns per spec:
OPT_COLS = {
    "type",
    "tags",
    "scale",
    "scenario",
    "start_period",
    "end_period",
    "priority",
    "user",
    "id",
}
ALL_COLS = REQ_COLS.union(OPT_COLS)

# Map DataFrame column names (lowercase) to Adjustment model fields
COL_TO_FIELD_MAP = {
    "node_name": "node_name",
    "period": "period",
    "value": "value",
    "reason": "reason",
    "type": "type",
    "tags": "tags",
    "scale": "scale",
    "scenario": "scenario",
    "start_period": "start_period",
    "end_period": "end_period",
    "priority": "priority",
    "user": "user",
    "id": "id",
    # Note: timestamp is not expected in input file, generated on creation
}


def _parse_tags(tag_str: Optional[str]) -> set[AdjustmentTag]:
    """Parse a comma-separated tag string into a set."""
    if pd.isna(tag_str) or not isinstance(tag_str, str) or not tag_str.strip():
        return set()
    return set(tag.strip() for tag in tag_str.split(",") if tag.strip())


def read_excel(path: str | Path) -> tuple[list[Adjustment], pd.DataFrame]:
    """Read adjustments from an Excel file.

    Expects the first sheet to contain adjustment data.
    Validates required columns and attempts to parse each row into an Adjustment object.
    Rows that fail validation are collected into an error report DataFrame.

    Args:
        path: Path to the Excel file.

    Returns:
        A tuple containing:
            - list[Adjustment]: A list of successfully parsed Adjustment objects.
            - pd.DataFrame: A DataFrame containing rows that failed validation,
                          with an added 'error' column explaining the issue.

    Raises:
        ReadError: If the file cannot be read, sheet is missing, or required
                   columns are not found.
    """
    file_path = Path(path)
    logger.info(f"Reading adjustments from Excel file: {file_path}")

    try:
        # Read the first sheet by default
        df = pd.read_excel(file_path, sheet_name=0)
    except FileNotFoundError:
        raise ReadError(f"Adjustment Excel file not found: {file_path}", source=str(file_path))
    except Exception as e:
        # Catch other potential pandas read errors (e.g., bad format, permissions)
        raise ReadError(
            f"Failed to read Excel file {file_path}: {e}",
            source=str(file_path),
            original_error=e,
        )

    # Normalize column names to lowercase for case-insensitive matching
    df.columns = [str(col).lower().strip() for col in df.columns]
    actual_cols = set(df.columns)

    # Check for required columns
    missing_req_cols = REQ_COLS - actual_cols
    if missing_req_cols:
        raise ReadError(
            f"Missing required columns in adjustment Excel file: {missing_req_cols}",
            source=str(file_path),
        )

    valid_adjustments: list[Adjustment] = []
    error_rows: list[dict[str, Any]] = []

    for index, row in df.iterrows():
        adj_data: dict[str, Any] = {}
        parse_errors: list[str] = []

        # Map columns to Adjustment fields
        for col_name, field_name in COL_TO_FIELD_MAP.items():
            if col_name in df.columns:
                value = row[col_name]
                # Handle potential NaNs from Excel
                if pd.isna(value):
                    adj_data[field_name] = None
                else:
                    # Specific type conversions / parsing
                    try:
                        if field_name == "tags":
                            adj_data[field_name] = _parse_tags(str(value))
                        elif field_name == "type":
                            adj_data[field_name] = AdjustmentType(str(value).lower())
                        elif field_name == "priority":
                            adj_data[field_name] = int(value)
                        elif field_name in {"scale", "value"}:
                            adj_data[field_name] = float(value)
                        elif field_name == "id":
                            # Allow specific UUIDs from input
                            from uuid import UUID  # Local import

                            adj_data[field_name] = UUID(str(value))
                        else:
                            # Default to string conversion for others (node_name, period, etc.)
                            adj_data[field_name] = str(value)
                    except ValueError as e:
                        parse_errors.append(f"Column '{col_name}': Invalid value '{value}' ({e})")
                    except Exception as e:
                        parse_errors.append(
                            f"Column '{col_name}': Error parsing value '{value}' ({e})"
                        )
            else:
                # Optional field not present
                adj_data[field_name] = None

        # Fill defaults for optional fields if not provided/mapped
        adj_data.setdefault("scenario", DEFAULT_SCENARIO)
        adj_data.setdefault("scale", 1.0)
        adj_data.setdefault("priority", 0)
        adj_data.setdefault("type", AdjustmentType.ADDITIVE)
        adj_data.setdefault("tags", set())

        # Remove None values for fields that shouldn't be None if missing (handled by Pydantic defaults later)
        # This is mainly for fields where None might cause issues if explicitly passed to Pydantic
        # e.g., pydantic might handle default factories better if key is absent vs. key=None
        # Let's be explicit for required ones:
        if adj_data.get("node_name") is None:
            parse_errors.append("Column 'node_name': Missing value")
        if adj_data.get("period") is None:
            parse_errors.append("Column 'period': Missing value")
        if adj_data.get("value") is None:
            parse_errors.append("Column 'value': Missing value")
        if adj_data.get("reason") is None:
            parse_errors.append("Column 'reason': Missing value")

        if parse_errors:
            error_detail = "; ".join(parse_errors)
            error_row = row.to_dict()
            error_row["error"] = error_detail
            error_rows.append(error_row)
            logger.debug(f"Row {index + 2}: Validation failed - {error_detail}")
            continue

        # Attempt to create the Adjustment object (final validation)
        try:
            # Filter out keys with None values before passing to Adjustment, let Pydantic handle defaults
            final_adj_data = {k: v for k, v in adj_data.items() if v is not None}
            adjustment = Adjustment(**final_adj_data)
            valid_adjustments.append(adjustment)
        except Exception as e:
            error_detail = f"Pydantic validation failed: {e}"
            error_row = row.to_dict()
            error_row["error"] = error_detail
            error_rows.append(error_row)
            logger.debug(f"Row {index + 2}: Pydantic validation failed - {e}")

    error_report_df = pd.DataFrame(error_rows)
    if not error_report_df.empty:
        logger.warning(
            f"Completed reading adjustments from {file_path}. Found {len(valid_adjustments)} valid adjustments and {len(error_rows)} errors."
        )
    else:
        logger.info(
            f"Successfully read {len(valid_adjustments)} adjustments from {file_path} with no errors."
        )

    return valid_adjustments, error_report_df


def write_excel(adjustments: list[Adjustment], path: str | Path) -> None:
    """Write a list of adjustments to an Excel file.

    Writes adjustments to separate sheets based on their scenario.
    The columns will match the optional fields defined for reading.

    Args:
        adjustments: A list of Adjustment objects to write.
        path: Path for the output Excel file.

    Raises:
        WriteError: If writing to the file fails.
    """
    file_path = Path(path)
    logger.info(f"Writing {len(adjustments)} adjustments to Excel file: {file_path}")

    # Group adjustments by scenario
    grouped_by_scenario: dict[str, list[dict]] = defaultdict(list)
    for adj in adjustments:
        # Use model_dump for serialization, exclude fields we don't usually export
        adj_dict = adj.model_dump(exclude={"timestamp"})  # Exclude timestamp by default
        # Convert complex types to simple types for Excel
        adj_dict["id"] = str(adj_dict.get("id"))
        adj_dict["type"] = adj_dict.get("type").value if adj_dict.get("type") else None
        adj_dict["tags"] = ",".join(sorted(adj_dict.get("tags", set())))
        grouped_by_scenario[adj.scenario].append(adj_dict)

    if not grouped_by_scenario:
        logger.warning("No adjustments provided to write_excel. Creating empty file.")
        # Create an empty file or handle as desired
        try:
            pd.DataFrame().to_excel(file_path, index=False)
        except Exception as e:
            raise WriteError(
                f"Failed to write empty Excel file {file_path}: {e}",
                target=str(file_path),
                original_error=e,
            )
        return

    try:
        with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
            for scenario, scenario_adjustments in grouped_by_scenario.items():
                df = pd.DataFrame(scenario_adjustments)
                # Reorder columns for consistency
                cols_ordered = [c for c in COL_TO_FIELD_MAP if c in df.columns]
                cols_ordered += [c for c in df.columns if c not in cols_ordered]
                df = df[cols_ordered]
                # Sheet names must be valid
                safe_scenario_name = (
                    scenario.replace(":", "-").replace("/", "-").replace("\\", "-")[:31]
                )
                df.to_excel(writer, sheet_name=safe_scenario_name, index=False)
        logger.info(f"Successfully wrote adjustments to {file_path}")
    except Exception as e:
        logger.error(f"Failed to write adjustments to Excel file {file_path}: {e}", exc_info=True)
        raise WriteError(
            f"Failed to write adjustments to Excel: {e}",
            target=str(file_path),
            original_error=e,
        )


# --- Graph Convenience Methods ---


def load_adjustments_from_excel(
    graph: Graph, path: str | Path, replace: bool = False
) -> pd.DataFrame:
    """Reads adjustments from Excel and adds them to the graph.

    Args:
        graph: The Graph instance to add adjustments to.
        path: Path to the Excel file.
        replace: If True, clear existing adjustments in the manager before adding new ones.

    Returns:
        pd.DataFrame: The error report DataFrame from `read_excel`.
                   Empty if no errors occurred.
    """
    logger.info(f"Loading adjustments from Excel ({path}) into graph. Replace={replace}")
    valid_adjustments, error_report_df = read_excel(path)

    if replace:
        logger.debug("Clearing existing adjustments before loading.")
        graph.adjustment_manager.clear_all()

    added_count = 0
    for adj in valid_adjustments:
        try:
            graph.adjustment_manager.add_adjustment(adj)
            added_count += 1
        except Exception as e:
            logger.error(f"Failed to add valid adjustment {adj.id} to graph: {e}", exc_info=True)
            # Optionally add this failure to the error report?
            error_row = adj.model_dump(mode="json")
            error_row["error"] = f"Failed to add to graph: {e}"
            # Need to handle DataFrame append carefully if modifying during iteration
            # Simplest is to report read errors, log add errors.

    logger.info(f"Added {added_count} adjustments to the graph from {path}.")
    if not error_report_df.empty:
        logger.warning(f"Encountered {len(error_report_df)} errors during Excel read process.")

    return error_report_df


def export_adjustments_to_excel(graph: Graph, path: str | Path) -> None:
    """Exports all adjustments from the graph to an Excel file.

    Args:
        graph: The Graph instance containing adjustments.
        path: Path for the output Excel file.
    """
    logger.info(f"Exporting all adjustments from graph to Excel ({path}).")
    all_adjustments = graph.list_all_adjustments()
    write_excel(all_adjustments, path)


# Add convenience methods to Graph class directly?
# This uses module patching which can sometimes be debated, but keeps the Graph API clean.
# Alternatively, users would call fin_statement_model.io.adjustments_excel.load_adjustments_from_excel(graph, path)
Graph.load_adjustments_from_excel = load_adjustments_from_excel
Graph.export_adjustments_to_excel = export_adjustments_to_excel

# --- END FILE: fin_statement_model/io/specialized/adjustments.py ---

# --- START FILE: fin_statement_model/io/specialized/cells.py ---
"""Importer module for reading cell-based financial statement data into a Graph."""

from typing import Any

# from fin_statement_model.statements.graph.financial_graph import FinancialStatementGraph # Removed
from fin_statement_model.core.graph import Graph  # Added

__all__ = ["import_from_cells"]


def import_from_cells(cells_info: list[dict[str, Any]]) -> Graph:  # Changed return type
    """Import a list of cell dictionaries into a core Graph.

    Each cell dict should include at minimum:
    - 'row_name': identifier for the line item (becomes node ID)
    - 'column_name': the period label
    - 'value': the numeric value

    Args:
        cells_info: List of cell metadata dictionaries.

    Returns:
        A core Graph populated with detected periods and data nodes.
    """
    # Group cells by row_name to aggregate values per financial statement item
    items: dict[str, dict[str, Any]] = {}
    unique_periods: set = set()

    for cell in cells_info:
        # Clean the item name and period
        item_name = cell.get("row_name", "").strip()
        period = cell.get("column_name", "").strip()
        value = cell.get("value")

        if not item_name or not period:
            continue

        unique_periods.add(period)
        if item_name not in items:
            items[item_name] = {}
        items[item_name][period] = value

    # Sort periods and create the graph
    sorted_periods = sorted(list(unique_periods))  # Ensure list for Graph constructor
    graph = Graph(periods=sorted_periods)  # Changed to Graph

    # Add each financial statement item as a data node to the graph
    for name, values in items.items():
        # Use add_financial_statement_item based on Graph API
        graph.add_financial_statement_item(name, values)

    return graph

# --- END FILE: fin_statement_model/io/specialized/cells.py ---

# --- START FILE: fin_statement_model/io/specialized/graph.py ---
"""Graph definition serialization and deserialization.

This module provides functionality to save and load complete graph definitions,
including all nodes, periods, and adjustments.
"""

import logging
from typing import Any, Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.adjustments.models import Adjustment
from fin_statement_model.core.nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    FormulaCalculationNode,
)
from fin_statement_model.core.nodes.forecast_nodes import (
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
    CustomGrowthForecastNode,
)
from fin_statement_model.core.errors import NodeError, ConfigurationError
from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.io.core import DataReader, DataWriter, register_reader, register_writer
from fin_statement_model.io.exceptions import ReadError, WriteError
from fin_statement_model.io.config.models import BaseWriterConfig

logger = logging.getLogger(__name__)

# Define a type for the serialized node dictionary for clarity
SerializedNode = dict[str, Any]


# ===== Reader Implementation =====


@register_reader("graph_definition_dict")
class GraphDefinitionReader(DataReader):
    """Reads a graph definition dictionary to reconstruct a Graph object.

    Handles reconstructing nodes based on their serialized type and configuration,
    and loads adjustments.
    """

    def __init__(self, cfg: Optional[Any] = None) -> None:
        """Initialize the GraphDefinitionReader. Config currently unused."""
        self.cfg = cfg

    def _add_nodes_iteratively(self, graph: Graph, nodes_dict: dict[str, SerializedNode]) -> None:
        """Add nodes to the graph, handling potential dependency order issues."""
        nodes_to_add = nodes_dict.copy()
        added_nodes: set[str] = set()
        max_passes = len(nodes_to_add) + 1  # Failsafe against infinite loops
        passes = 0

        while nodes_to_add and passes < max_passes:
            added_in_pass = 0
            pending_nodes = nodes_to_add.copy()
            nodes_to_add = {}

            for node_name, node_def in pending_nodes.items():
                node_type = node_def.get("type")
                # Get actual dependency names (might differ from formula vars)
                dependency_names = node_def.get("inputs", [])

                # For forecast nodes, the dependency is the base node
                if node_type == "forecast":
                    base_node_name = node_def.get("base_node_name")
                    if base_node_name:
                        dependency_names = [base_node_name]

                # Check if all dependencies are already added
                dependencies_met = all(dep_name in added_nodes for dep_name in dependency_names)

                if dependencies_met:
                    try:
                        # Logic to add the node based on its type
                        if node_type == "financial_statement_item":
                            graph.add_financial_statement_item(
                                name=node_name, values=node_def.get("values", {})
                            )
                        elif node_type == "formula_calculation":
                            # Always reconstruct using add_calculation for formula type.
                            # The metric_name attribute will be preserved if present.
                            formula_str = node_def.get("formula")
                            if not formula_str:
                                logger.error(
                                    f"Cannot reconstruct FormulaCalculationNode '{node_name}': missing formula string."
                                )
                                continue  # Skip node

                            graph.add_calculation(
                                name=node_name,
                                input_names=dependency_names,
                                operation_type="formula",
                                formula_variable_names=node_def.get("formula_variable_names"),
                                formula=formula_str,
                                metric_name=node_def.get("metric_name"),
                                metric_description=node_def.get("metric_description"),
                            )
                        elif node_type == "calculation":
                            # Reconstruct generic CalculationNode using the saved type key
                            calc_type_key = node_def.get("calculation_type")
                            if not calc_type_key:
                                logger.error(
                                    f"Missing 'calculation_type' key for node '{node_name}'. Skipping."
                                )
                                continue

                            # Retrieve calculation_args if they were saved
                            calculation_args = node_def.get("calculation_args", {})

                            # Special handling for formula type to ensure formula_variable_names is passed correctly
                            if calc_type_key == "formula" and "formula_variable_names" in node_def:
                                # Pass formula_variable_names as a separate parameter, not in calculation_args
                                formula_variable_names = node_def.get("formula_variable_names")
                                logger.debug(
                                    f"Reconstructing CalculationNode '{node_name}' with type '{calc_type_key}', "
                                    f"formula_variable_names: {formula_variable_names}, and args: {calculation_args}"
                                )
                                graph.add_calculation(
                                    name=node_name,
                                    input_names=dependency_names,
                                    operation_type=calc_type_key,
                                    formula_variable_names=formula_variable_names,
                                    **calculation_args,
                                )
                            else:
                                logger.debug(
                                    f"Reconstructing CalculationNode '{node_name}' with type '{calc_type_key}' and args: {calculation_args}"
                                )
                                graph.add_calculation(
                                    name=node_name,
                                    input_names=dependency_names,
                                    operation_type=calc_type_key,
                                    **calculation_args,
                                )
                        elif node_type == "forecast":
                            # Reconstruct ForecastNode
                            base_node_name = node_def.get("base_node_name")
                            base_period = node_def.get("base_period")
                            forecast_periods = node_def.get("forecast_periods")
                            forecast_type = node_def.get("forecast_type")
                            growth_params = node_def.get(
                                "growth_params", 0.0
                            )  # Default to 0.0 if not provided

                            if not all(
                                [
                                    base_node_name,
                                    base_period,
                                    forecast_periods,
                                    forecast_type,
                                ]
                            ):
                                logger.error(
                                    f"Missing required fields for forecast node '{node_name}'. Skipping."
                                )
                                continue

                            # Get the base node from the graph
                            base_node = graph.nodes.get(base_node_name) if base_node_name else None
                            if not base_node:
                                logger.error(
                                    f"Base node '{base_node_name}' not found for forecast node '{node_name}'. Skipping."
                                )
                                continue

                            # Handle special cases where growth_params might not be serializable
                            if forecast_type in ["statistical", "custom"]:
                                logger.warning(
                                    f"Forecast node '{node_name}' of type '{forecast_type}' uses non-serializable "
                                    f"parameters. Using default values. Manual reconstruction may be needed."
                                )
                                if forecast_type == "statistical":
                                    # Use a default function that returns 0 growth
                                    def default_statistical_growth() -> float:
                                        return 0.0

                                    growth_params = default_statistical_growth
                                elif forecast_type == "custom":
                                    # Use a default function that returns 0 growth
                                    def default_custom_growth(
                                        period: str, prev_period: str, prev_value: float
                                    ) -> float:
                                        return 0.0

                                    growth_params = default_custom_growth
                            elif forecast_type in ["average", "historical_growth"]:
                                # These types don't need growth_params
                                growth_params = None

                            # Create the forecast node
                            try:
                                # Ensure all required parameters are not None
                                if (
                                    not isinstance(base_period, str)
                                    or not isinstance(forecast_periods, list)
                                    or not isinstance(forecast_type, str)
                                ):
                                    logger.error(
                                        f"Invalid types for forecast node '{node_name}' parameters. Skipping."
                                    )
                                    continue

                                forecast_node = NodeFactory.create_forecast_node(
                                    name=node_name,
                                    base_node=base_node,
                                    base_period=base_period,
                                    forecast_periods=forecast_periods,
                                    forecast_type=forecast_type,
                                    growth_params=growth_params,
                                )
                                graph.add_node(forecast_node)
                                logger.debug(
                                    f"Added forecast node '{node_name}' of type '{forecast_type}'."
                                )
                            except Exception:
                                logger.exception(
                                    f"Failed to create forecast node '{node_name}'. Skipping."
                                )
                                continue
                        else:
                            logger.warning(
                                f"Unknown node type '{node_type}' for node '{node_name}' during deserialization. Skipping."
                            )
                            # Don't add to added_nodes if skipped
                            continue

                        added_nodes.add(node_name)
                        added_in_pass += 1
                        logger.debug(f"Added node '{node_name}' in pass {passes + 1}.")

                    except (NodeError, ConfigurationError, ValueError, TypeError):
                        # Log error but try to continue with other nodes
                        logger.exception(f"Failed to add node '{node_name}' during iterative build")
                        # Keep it in nodes_to_add to potentially retry if it was a temporary dependency issue
                        # or if error handling allows partial graph load.
                        # For now, let's keep it for retry, but could decide to fail hard.
                        nodes_to_add[node_name] = node_def
                else:
                    # Dependencies not met, keep for next pass
                    nodes_to_add[node_name] = node_def

            if added_in_pass == 0 and nodes_to_add:
                # No progress made in this pass, indicates missing nodes or cycle
                missing_deps = set()
                for node_name, node_def in nodes_to_add.items():
                    deps = node_def.get("inputs", [])
                    if node_def.get("type") == "forecast":
                        deps = [node_def.get("base_node_name", "")]
                    for dep in deps:
                        if (
                            dep not in added_nodes and dep not in nodes_to_add
                        ):  # Check if dep itself is missing entirely
                            missing_deps.add(dep)
                error_msg = f"Failed to add all nodes. Possible missing dependencies ({missing_deps}) or circular dependency in definition for nodes: {list(nodes_to_add.keys())}"
                logger.error(error_msg)
                raise ReadError(error_msg, source="graph_definition_dict")

            passes += 1

        if nodes_to_add:
            logger.error(
                f"Failed to add the following nodes after {max_passes} passes: {list(nodes_to_add.keys())}"
            )
            raise ReadError(
                f"Failed to reconstruct graph, could not add nodes: {list(nodes_to_add.keys())}",
                source="graph_definition_dict",
            )

    def read(self, source: dict[str, Any], **kwargs: Any) -> Graph:
        """Reconstruct a Graph instance from its definition dictionary.

        Args:
            source: Dictionary containing the graph definition (periods, nodes, adjustments).
            **kwargs: Currently unused.

        Returns:
            A new Graph instance populated from the definition.

        Raises:
            ReadError: If the source format is invalid or graph reconstruction fails.
        """
        logger.info("Starting graph reconstruction from definition dictionary.")

        if not isinstance(source, dict) or "periods" not in source or "nodes" not in source:
            raise ReadError(
                message="Invalid source format for GraphDefinitionReader. Expected dict with 'periods' and 'nodes' keys.",
                source="graph_definition_dict",
                reader_type="GraphDefinitionReader",
            )

        try:
            # 1. Initialize Graph with Periods
            periods = source.get("periods", [])
            if not isinstance(periods, list):
                raise ReadError("Invalid format: 'periods' must be a list.")
            graph = Graph(periods=periods)

            # 2. Reconstruct Nodes iteratively
            nodes_dict = source.get("nodes", {})
            if not isinstance(nodes_dict, dict):
                raise ReadError("Invalid format: 'nodes' must be a dictionary.")
            self._add_nodes_iteratively(graph, nodes_dict)

            # 3. Load Adjustments
            adjustments_list = source.get("adjustments")  # Optional
            if adjustments_list is not None:
                if not isinstance(adjustments_list, list):
                    raise ReadError("Invalid format: 'adjustments' must be a list if present.")

                deserialized_adjustments = []
                for i, adj_dict in enumerate(adjustments_list):
                    try:
                        # Use model_validate for Pydantic V2
                        adj = Adjustment.model_validate(adj_dict)
                        deserialized_adjustments.append(adj)
                    except Exception:
                        # Log error but try to continue with other nodes
                        logger.exception(
                            f"Failed to deserialize adjustment at index {i}: {adj_dict}. Skipping."
                        )
                        # Optionally raise ReadError here to fail fast

                if deserialized_adjustments:
                    graph.adjustment_manager.load_adjustments(deserialized_adjustments)
                    logger.info(
                        f"Loaded {len(deserialized_adjustments)} adjustments into the graph."
                    )

            logger.info(f"Successfully reconstructed graph with {len(graph.nodes)} nodes.")
            return graph

        except ReadError:  # Re-raise ReadErrors directly
            raise
        except Exception as e:
            logger.error(f"Failed to reconstruct graph from definition: {e}", exc_info=True)
            raise ReadError(
                message=f"Failed to reconstruct graph from definition: {e}",
                source="graph_definition_dict",
                reader_type="GraphDefinitionReader",
                original_error=e,
            ) from e


# ===== Writer Implementation =====


@register_writer("graph_definition_dict")
class GraphDefinitionWriter(DataWriter):
    """Writes the full graph definition (nodes, periods, adjustments) to a dictionary.

    This writer serializes the structure and configuration of the graph, suitable
    for saving and reloading the entire model state.
    """

    def __init__(self, cfg: Optional[BaseWriterConfig] = None) -> None:
        """Initialize the GraphDefinitionWriter."""
        self.cfg = cfg

    def _serialize_node(self, node: Node) -> Optional[SerializedNode]:
        """Serialize a single node into a dictionary definition."""
        node_def: SerializedNode = {"name": node.name}

        if isinstance(node, FinancialStatementItemNode):
            node_def["type"] = "financial_statement_item"
            # Store values directly, assuming they are serializable (float)
            node_def["values"] = node.values.copy()
            # Add other relevant attributes if needed (e.g., sign_convention)
        elif isinstance(node, FormulaCalculationNode):
            node_def["type"] = "formula_calculation"
            # Store the *actual* dependency node names
            node_def["inputs"] = node.get_dependencies()  # Store actual dependency names
            # Store the variable names used in the formula
            node_def["formula_variable_names"] = list(
                node.inputs_dict.keys()  # Use inputs_dict instead of inputs for FormulaCalculationNode
            )  # Store input names (which are keys in formula node)
            node_def["formula"] = node.formula
            # Include metric info if it's a metric node
            if getattr(node, "metric_name", None):
                node_def["metric_name"] = node.metric_name
                node_def["metric_description"] = getattr(node, "metric_description", None)
            # Explicitly set the calculation type key for formula nodes
            node_def["calculation_type"] = "formula"
        elif isinstance(
            node, CalculationNode
        ):  # Catch general CalculationNodes after specific ones
            node_def["type"] = "calculation"
            # Assuming inputs are stored as a list of Nodes:
            node_def["inputs"] = node.get_dependencies()
            calc_instance = getattr(node, "calculation", None)
            if calc_instance:
                node_def["calculation_type_class"] = type(calc_instance).__name__
                # Find and save the type key
                inv_map = {v: k for k, v in NodeFactory._calculation_methods.items()}
                type_key = inv_map.get(type(calc_instance).__name__)
                if type_key:
                    node_def["calculation_type"] = type_key  # Save the type key

                    # Extract calculation arguments based on the calculation type
                    calculation_args = {}

                    # WeightedAverageCalculation has weights attribute
                    if type_key == "weighted_average" and hasattr(calc_instance, "weights"):
                        calculation_args["weights"] = calc_instance.weights

                    # FormulaCalculation has formula and input_variable_names
                    elif type_key == "formula" and hasattr(calc_instance, "formula"):
                        calculation_args["formula"] = calc_instance.formula
                        if hasattr(calc_instance, "input_variable_names"):
                            # Store input_variable_names at the node level for proper deserialization
                            node_def["formula_variable_names"] = calc_instance.input_variable_names

                    # CustomFormulaCalculation has formula_function (not easily serializable)
                    elif type_key == "custom_formula":
                        logger.warning(
                            f"CustomFormulaCalculation for node '{node.name}' uses a Python function "
                            "which cannot be serialized. This node will need manual reconstruction."
                        )

                    # Store calculation args if any were extracted
                    if calculation_args:
                        node_def["calculation_args"] = calculation_args
                else:
                    logger.warning(
                        f"Could not find type key in NodeFactory._calculation_methods for calculation class {type(calc_instance).__name__}"
                    )
            else:
                logger.warning(
                    f"CalculationNode '{node.name}' has no internal calculation instance to serialize type."
                )
        elif isinstance(node, ForecastNode):
            node_def["type"] = "forecast"
            node_def["base_node_name"] = node.input_node.name
            node_def["base_period"] = node.base_period
            node_def["forecast_periods"] = node.forecast_periods

            # Determine the forecast type based on the node class
            if isinstance(node, FixedGrowthForecastNode):
                node_def["forecast_type"] = "simple"
                node_def["growth_params"] = node.growth_rate
            elif isinstance(node, CurveGrowthForecastNode):
                node_def["forecast_type"] = "curve"
                node_def["growth_params"] = node.growth_rates
            elif isinstance(node, StatisticalGrowthForecastNode):
                node_def["forecast_type"] = "statistical"
                logger.warning(
                    f"StatisticalGrowthForecastNode '{node.name}' uses a distribution callable "
                    "which cannot be serialized. This node will need manual reconstruction."
                )
                # We can't serialize the callable, so we'll skip growth_params
            elif isinstance(node, AverageValueForecastNode):
                node_def["forecast_type"] = "average"
                # No growth_params needed for average value
            elif isinstance(node, AverageHistoricalGrowthForecastNode):
                node_def["forecast_type"] = "historical_growth"
                # No growth_params needed for historical growth
            elif isinstance(node, CustomGrowthForecastNode):
                node_def["forecast_type"] = "custom"
                logger.warning(
                    f"CustomGrowthForecastNode '{node.name}' uses a growth function "
                    "which cannot be serialized. This node will need manual reconstruction."
                )
                # We can't serialize the callable, so we'll skip growth_params
            else:
                logger.warning(
                    f"Unknown ForecastNode subclass '{type(node).__name__}' for node '{node.name}'. "
                    "Using generic forecast serialization."
                )
        else:
            logger.warning(
                f"Node type '{type(node).__name__}' for node '{node.name}' is not explicitly handled by GraphDefinitionWriter. Skipping."
            )
            return None  # Skip nodes we don't know how to serialize

        return node_def

    def write(self, graph: Graph, target: Any = None, **kwargs: dict[str, Any]) -> dict[str, Any]:
        """Export the full graph definition to a dictionary.

        Args:
            graph (Graph): The Graph instance to serialize.
            target (Any): Ignored by this writer; the dictionary is returned directly.
            **kwargs: Currently unused.

        Returns:
            Dict[str, Any]: Dictionary representing the graph definition, including
                            periods, node definitions, and adjustments.

        Raises:
            WriteError: If an unexpected error occurs during export.
        """
        logger.info(f"Starting export of graph definition for: {graph!r}")
        graph_definition: dict[str, Any] = {
            "periods": [],
            "nodes": {},
            "adjustments": [],
        }

        try:
            # 1. Serialize Periods
            graph_definition["periods"] = list(graph.periods)

            # 2. Serialize Nodes
            serialized_nodes: dict[str, SerializedNode] = {}
            for node_name, node in graph.nodes.items():
                node_dict = self._serialize_node(node)
                if node_dict:
                    serialized_nodes[node_name] = node_dict
            graph_definition["nodes"] = serialized_nodes

            # 3. Serialize Adjustments
            adjustments = graph.list_all_adjustments()
            serialized_adjustments = []
            for adj in adjustments:
                try:
                    # Use model_dump for Pydantic V2, ensure mode='json' for types like UUID/datetime
                    serialized_adjustments.append(adj.model_dump(mode="json"))
                except Exception as e:
                    logger.warning(f"Failed to serialize adjustment {adj.id}: {e}. Skipping.")
            graph_definition["adjustments"] = serialized_adjustments

            logger.info(
                f"Successfully created graph definition dictionary with {len(serialized_nodes)} nodes and {len(serialized_adjustments)} adjustments."
            )
            return graph_definition

        except Exception as e:
            logger.error(f"Failed to create graph definition dictionary: {e}", exc_info=True)
            raise WriteError(
                message=f"Failed to create graph definition dictionary: {e}",
                target="graph_definition_dict",
                writer_type="GraphDefinitionWriter",
                original_error=e,
            ) from e


# ===== Convenience Functions =====


def save_graph_definition(graph: Graph, filepath: str) -> None:
    """Save a graph definition to a JSON file.

    Args:
        graph: The graph to save.
        filepath: Path to the output JSON file.
    """
    import json

    writer = GraphDefinitionWriter()
    definition = writer.write(graph)

    with open(filepath, "w") as f:
        json.dump(definition, f, indent=2)

    logger.info(f"Saved graph definition to {filepath}")


def load_graph_definition(filepath: str) -> Graph:
    """Load a graph definition from a JSON file.

    Args:
        filepath: Path to the JSON file containing the graph definition.

    Returns:
        The reconstructed Graph object.
    """
    import json

    with open(filepath) as f:
        definition = json.load(f)

    reader = GraphDefinitionReader()
    graph = reader.read(definition)

    logger.info(f"Loaded graph definition from {filepath}")
    return graph


__all__ = [
    "GraphDefinitionReader",
    "GraphDefinitionWriter",
    "load_graph_definition",
    "save_graph_definition",
]

# --- END FILE: fin_statement_model/io/specialized/graph.py ---

# --- START FILE: fin_statement_model/io/specialized/statements.py ---
"""Statement-related IO utilities.

This module provides functionality for reading statement configurations
and writing formatted statement data.
"""

import json
import yaml
import logging
import importlib.resources
import importlib.util  # Needed for checking resource type
from pathlib import Path
from typing import Any, Optional

import pandas as pd

from fin_statement_model.io.exceptions import ReadError, WriteError

logger = logging.getLogger(__name__)


# ===== Statement Configuration Reading =====


def read_statement_config_from_path(config_path: str) -> dict[str, Any]:
    """Reads and parses a statement configuration file from a given path.

    Supports JSON and YAML formats.

    Args:
        config_path: Absolute or relative path to the configuration file.

    Returns:
        The parsed configuration data as a dictionary.

    Raises:
        ReadError: If the file is not found, has an unsupported extension,
                   or cannot be parsed.
    """
    path = Path(config_path)

    if not path.exists() or not path.is_file():
        raise ReadError(
            message="Configuration file not found or is not a file",
            source=config_path,
        )

    extension = path.suffix.lower()
    config_data = {}

    try:
        with open(path, encoding="utf-8") as f:
            if extension == ".json":
                config_data = json.load(f)
            elif extension in [".yaml", ".yml"]:
                config_data = yaml.safe_load(f)
            else:
                raise ReadError(
                    message="Unsupported file extension for statement config",
                    source=config_path,
                    details=f"Use .json, .yaml, or .yml instead of {extension}",
                )
        logger.debug(f"Successfully read and parsed config file: {config_path}")
        return config_data

    except json.JSONDecodeError as e:
        logger.exception(f"Error parsing JSON configuration file {config_path}")
        raise ReadError(
            message="Invalid JSON format in configuration file",
            source=config_path,
            original_error=e,
            details=f"JSON decode error at line {e.lineno}, column {e.colno}: {e.msg}",
        ) from e
    except yaml.YAMLError as e:
        logger.exception(f"Error parsing YAML configuration file {config_path}")
        str(e)
        if hasattr(e, "problem_mark") and getattr(e, "problem_mark", None) is not None:
            mark = e.problem_mark
            if mark:
                (
                    f"YAML parse error near line {mark.line + 1}, column {mark.column + 1}: "
                    f"{getattr(e, 'problem', '')}"
                )
        raise ReadError(
            message="Invalid YAML format in configuration file",
            source=config_path,
            original_error=e,
        ) from e
    except OSError as e:
        logger.exception(f"IO Error reading configuration file {config_path}")
        raise ReadError(
            message="Failed to read configuration file",
            source=config_path,
            original_error=e,
        ) from e
    except Exception as e:
        logger.exception(f"Unexpected error loading configuration from {config_path}")
        raise ReadError(
            message="Unexpected error loading configuration file",
            source=config_path,
            original_error=e,
        ) from e


def read_statement_configs_from_directory(
    directory_path: str,
) -> dict[str, dict[str, Any]]:
    """Reads all statement configs (JSON/YAML) from a directory.

    Args:
        directory_path: Path to the directory containing configuration files.

    Returns:
        A dictionary mapping statement identifiers (filename stem) to their
        parsed configuration data (dict).

    Raises:
        ReadError: If the directory doesn't exist or isn't accessible, or if
                   any individual file fails to read/parse (errors are logged
                   but reading continues for other files unless none succeed).
        FileNotFoundError: If the directory_path does not exist.
    """
    path = Path(directory_path)
    if not path.exists():
        raise FileNotFoundError(f"Configuration directory not found: {directory_path}")
    if not path.is_dir():
        raise ReadError(
            message="Provided path is not a directory",
            source=directory_path,
        )

    configs: dict[str, dict[str, Any]] = {}
    errors: list[str] = []

    config_files = list(path.glob("*.json")) + list(path.glob("*.y*ml"))

    if not config_files:
        logger.warning(f"No configuration files (.json, .yaml, .yml) found in {directory_path}")
        return {}

    for file_path in config_files:
        file_path_str = str(file_path)
        try:
            config_data = read_statement_config_from_path(file_path_str)
            statement_id = file_path.stem  # Use filename without extension as ID
            if statement_id in configs:
                logger.warning(
                    f"Duplicate statement ID '{statement_id}' found. Overwriting config from {file_path_str}."
                )
            configs[statement_id] = config_data
            logger.debug(
                f"Successfully loaded statement config '{statement_id}' from {file_path.name}"
            )
        except ReadError as e:
            logger.exception(f"Failed to read/parse config file {file_path.name}:")
            errors.append(f"{file_path.name}: {e.message}")
        except Exception as e:
            logger.exception(f"Unexpected error processing file {file_path.name}")
            errors.append(f"{file_path.name}: Unexpected error - {e!s}")

    # Decision: Raise error only if NO files could be read successfully?
    # Or just log errors and return what was successful?
    # Current approach: Log errors, return successful ones.
    # If no configs loaded AND errors occurred, maybe raise an aggregate error.
    if not configs and errors:
        raise ReadError(
            message=f"Failed to load any valid configurations from directory {directory_path}",
            source=directory_path,
            details="\n".join(errors),
        )
    elif errors:
        # Log that some files failed if others succeeded
        logger.warning(
            f"Encountered errors while loading configs from {directory_path}: {len(errors)} file(s) failed."
        )

    return configs


def _get_builtin_config_package() -> str:
    """Return the package path string for built-in statement configurations.

    Direct return, assuming a fixed location within the package structure.
    Environment variable override is removed as importlib.resources relies on package structure.
    """
    return "fin_statement_model.statements.config.mappings"


def list_available_builtin_configs() -> list[str]:
    """List the names of all built-in statement configuration mappings available.

    Returns:
        List[str]: List of mapping names (filename without extension).
    """
    package_path = _get_builtin_config_package()
    try:
        resource_path = importlib.resources.files(package_path)
        # Check if the resource exists and is a container (directory)
        if not resource_path.is_dir():
            logger.warning(f"Built-in config package path is not a directory: {package_path}")
            return []

        names = [
            res.name.split(".")[0]  # Get filename stem
            for res in resource_path.iterdir()
            if res.is_file() and res.suffix.lower() in (".yaml", ".yml", ".json")
        ]
        return sorted(names)
    except (
        ModuleNotFoundError,
        FileNotFoundError,
    ):  # Handle case where package/path doesn't exist
        logger.warning(f"Built-in config package path not found: {package_path}")
        return []


def read_builtin_statement_config(name: str) -> dict[str, Any]:
    """Reads and parses a built-in statement configuration by name.

    Searches for <name>.yaml, <name>.yml, or <name>.json in the built-in
    mappings directory.

    Args:
        name: The name of the built-in configuration (filename without extension).

    Returns:
        The parsed configuration data as a dictionary.

    Raises:
        ReadError: If no matching configuration file is found or if reading/parsing fails.
    """
    package_path = _get_builtin_config_package()
    found_resource_name: Optional[str] = None
    resource_content: Optional[str] = None
    file_extension: Optional[str] = None

    for ext in (".yaml", ".yml", ".json"):
        resource_name = f"{name}{ext}"
        try:
            resource_path = importlib.resources.files(package_path).joinpath(resource_name)
            if resource_path.is_file():
                resource_content = resource_path.read_text(encoding="utf-8")
                found_resource_name = resource_name
                file_extension = ext
                break
        except (FileNotFoundError, ModuleNotFoundError):
            continue  # Try next extension or handle package not found below
        except Exception as e:
            # Catch other potential errors during resource access
            logger.exception(f"Error accessing resource {resource_name} in {package_path}")
            raise ReadError(
                message=f"Error accessing built-in config resource '{name}'",
                source=f"{package_path}/{resource_name}",
                original_error=e,
            ) from e

    if (
        resource_content is not None
        and found_resource_name is not None
        and file_extension is not None
    ):
        logger.debug(
            f"Found and read built-in config '{name}' from resource: {package_path}/{found_resource_name}"
        )
        try:
            if file_extension == ".json":
                config_data = json.loads(resource_content)
            else:  # .yaml or .yml
                config_data = yaml.safe_load(resource_content)
            return config_data
        except json.JSONDecodeError as e:
            logger.exception(f"Error parsing JSON for built-in config '{name}'")
            raise ReadError(
                message="Invalid JSON format in built-in configuration",
                source=f"{package_path}/{found_resource_name}",
                original_error=e,
            ) from e
        except yaml.YAMLError as e:
            logger.exception(f"Error parsing YAML for built-in config '{name}'")
            raise ReadError(
                message="Invalid YAML format in built-in configuration",
                source=f"{package_path}/{found_resource_name}",
                original_error=e,
            ) from e
    else:
        logger.warning(f"Built-in statement config '{name}' not found in package {package_path}")
        raise ReadError(
            message=f"Built-in statement config '{name}' not found in package {package_path}",
            source=package_path,
        )


# ===== Statement Writing =====


def write_statement_to_excel(
    statement_df: pd.DataFrame,
    file_path: str,
    **kwargs: dict[str, object],
) -> None:
    """Write a statement DataFrame to an Excel file.

    Args:
        statement_df: The pandas DataFrame containing the formatted statement data.
        file_path: Path to save the Excel file.
        **kwargs: Additional arguments passed directly to pandas.DataFrame.to_excel
                 (e.g., sheet_name, index, header).

    Raises:
        WriteError: If writing the file fails.
    """
    try:
        # Default index=False is common for statement exports
        kwargs.setdefault("index", False)
        statement_df.to_excel(file_path, **kwargs)
    except Exception as e:
        # Removed StatementError handling as it's no longer relevant here
        raise WriteError(
            message="Failed to export statement DataFrame to Excel",
            target=file_path,
            format_type="excel",  # Corrected parameter name
            original_error=e,
        ) from e


def write_statement_to_json(
    statement_df: pd.DataFrame,
    file_path: str,
    orient: str = "columns",
    **kwargs: dict[str, object],
) -> None:
    """Write a statement DataFrame to a JSON file.

    Args:
        statement_df: The pandas DataFrame containing the formatted statement data.
        file_path: Path to save the JSON file.
        orient: JSON orientation format (passed to pandas.DataFrame.to_json).
        **kwargs: Additional arguments passed directly to pandas.DataFrame.to_json
                 (e.g., indent, date_format).

    Raises:
        WriteError: If writing the file fails.
    """
    try:
        statement_df.to_json(file_path, orient=orient, **kwargs)
    except Exception as e:
        # Removed StatementError handling
        raise WriteError(
            message="Failed to export statement DataFrame to JSON",
            target=file_path,
            format_type="json",  # Corrected parameter name
            original_error=e,
        ) from e


__all__ = [
    # Configuration reading
    "list_available_builtin_configs",
    "read_builtin_statement_config",
    "read_statement_config_from_path",
    "read_statement_configs_from_directory",
    # Statement writing
    "write_statement_to_excel",
    "write_statement_to_json",
]

# --- END FILE: fin_statement_model/io/specialized/statements.py ---

# --- START FILE: fin_statement_model/io/validation.py ---
"""Unified node name validation and standardization utilities.

This module provides a comprehensive validator that combines basic validation
with context-aware pattern recognition for financial statement nodes.
"""

import logging
import re
from typing import Optional, ClassVar, Any
from dataclasses import dataclass, field

from fin_statement_model.core.nodes import Node, standard_node_registry

logger = logging.getLogger(__name__)


@dataclass
class ValidationResult:
    """Result of a node name validation."""

    original_name: str
    standardized_name: str
    is_valid: bool
    message: str
    category: str
    confidence: float = 1.0
    suggestions: list[str] = field(default_factory=list)


class UnifiedNodeValidator:
    """Unified validator for node names with pattern recognition and standardization.

    This validator combines the functionality of NodeNameValidator and
    ContextAwareNodeValidator into a single, more efficient implementation.
    """

    # Common sub-node patterns
    SUBNODE_PATTERNS: ClassVar[list[tuple[str, str]]] = [
        (r"^(.+)_(q[1-4])$", "quarterly"),
        (r"^(.+)_(fy\d{4})$", "fiscal_year"),
        (r"^(.+)_(\d{4})$", "annual"),
        (r"^(.+)_(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)$", "monthly"),
        (r"^(.+)_(actual|budget|forecast)$", "scenario"),
    ]

    # Generic segment pattern - must be checked separately
    SEGMENT_PATTERN = r"^(.+)_([a-z_]+)$"

    # Formula patterns - check exact endings
    FORMULA_ENDINGS: ClassVar[list[str]] = ["_margin", "_ratio", "_growth", "_change", "_pct"]

    def __init__(
        self,
        strict_mode: bool = False,
        auto_standardize: bool = True,
        warn_on_non_standard: bool = True,
        enable_patterns: bool = True,
    ):
        """Initialize the unified validator.

        Args:
            strict_mode: If True, only standard names are allowed.
            auto_standardize: If True, convert alternate names to standard.
            warn_on_non_standard: If True, log warnings for non-standard names.
            enable_patterns: If True, recognize sub-node and formula patterns.
        """
        self.strict_mode = strict_mode
        self.auto_standardize = auto_standardize
        self.warn_on_non_standard = warn_on_non_standard
        self.enable_patterns = enable_patterns
        self._validation_cache: dict[str, ValidationResult] = {}

    def validate(
        self,
        name: str,
        node_type: Optional[str] = None,
        parent_nodes: Optional[list[str]] = None,
        use_cache: bool = True,
    ) -> ValidationResult:
        """Validate a node name with full context awareness.

        Args:
            name: The node name to validate.
            node_type: Optional node type hint.
            parent_nodes: Optional list of parent node names.
            use_cache: Whether to use cached results.

        Returns:
            ValidationResult with all validation details.
        """
        # Check cache first
        cache_key = f"{name}:{node_type}:{','.join(parent_nodes or [])}"
        if use_cache and cache_key in self._validation_cache:
            return self._validation_cache[cache_key]

        # Start validation
        result = self._perform_validation(name, node_type, parent_nodes)

        # Cache result
        if use_cache:
            self._validation_cache[cache_key] = result

        # Log warnings if configured
        if self.warn_on_non_standard and result.category in ["custom", "invalid"]:
            logger.warning(f"{result.message}")
            if result.suggestions:
                logger.info(f"Suggestions for '{name}': {'; '.join(result.suggestions)}")

        return result

    def _perform_validation(
        self,
        name: str,
        node_type: Optional[str],
        parent_nodes: Optional[list[str]],
    ) -> ValidationResult:
        """Perform the actual validation logic."""
        # Check standard names first
        if standard_node_registry.is_standard_name(name):
            return ValidationResult(
                original_name=name,
                standardized_name=name,
                is_valid=True,
                message=f"Standard node: {name}",
                category="standard",
                confidence=1.0,
            )

        # Check alternate names
        if standard_node_registry.is_alternate_name(name):
            standard_name = standard_node_registry.get_standard_name(name)
            return ValidationResult(
                original_name=name,
                standardized_name=standard_name if self.auto_standardize else name,
                is_valid=True,
                message=f"{'Standardized' if self.auto_standardize else 'Alternate name for'} '{standard_name}'",
                category="alternate",
                confidence=1.0,
            )

        # Pattern recognition if enabled
        if self.enable_patterns:
            pattern_result = self._check_pattern_validations(name, node_type, parent_nodes)
            if pattern_result:
                return pattern_result

        # Generate suggestions for unrecognized names
        suggestions = self._generate_suggestions(name)

        # Default to custom/invalid
        return ValidationResult(
            original_name=name,
            standardized_name=name,
            is_valid=not self.strict_mode,
            message=f"{'Non-standard' if self.strict_mode else 'Custom'} node: '{name}'",
            category="invalid" if self.strict_mode else "custom",
            confidence=0.5,
            suggestions=suggestions,
        )

    def _check_pattern_validations(
        self,
        name: str,
        node_type: Optional[str],
        parent_nodes: Optional[list[str]],
    ) -> Optional[ValidationResult]:
        """Check all pattern-based validations."""
        # Check formula patterns first (more specific)
        if node_type in ["calculation", "formula", None]:
            pattern_result = self._check_formula_ending(name)
            if pattern_result:
                base_name, formula_type = pattern_result

                return ValidationResult(
                    original_name=name,
                    standardized_name=name,
                    is_valid=True,
                    message=f"Formula node: {formula_type} of '{base_name}'",
                    category="formula",
                    confidence=0.85,
                )

        # Check parent relationships before sub-nodes
        if parent_nodes and self._check_parent_relationship(name, parent_nodes):
            return ValidationResult(
                original_name=name,
                standardized_name=name,
                is_valid=True,
                message="Derived from parent nodes",
                category="derived",
                confidence=0.8,
            )

        # Check specific sub-node patterns
        pattern_result = self._check_patterns(name, self.SUBNODE_PATTERNS, "subnode")
        if pattern_result:
            base_name, suffix, pattern_type = pattern_result
            is_base_standard = standard_node_registry.is_recognized_name(base_name)

            return ValidationResult(
                original_name=name,
                standardized_name=name,
                is_valid=not self.strict_mode or is_base_standard,
                message=f"{'Valid' if is_base_standard else 'Non-standard'} sub-node of '{base_name}' ({pattern_type})",
                category="subnode" if is_base_standard else "subnode_nonstandard",
                confidence=0.9 if is_base_standard else 0.7,
            )

        # Check generic segment pattern last
        match = re.match(self.SEGMENT_PATTERN, name.lower())
        if match and "_" in name:
            base_name = match.group(1)
            suffix = match.group(2)

            # Only treat as segment if it doesn't match other patterns
            # and has a reasonable structure (geographic/business segment)
            segment_keywords = [
                "america",
                "europe",
                "asia",
                "pacific",
                "africa",
                "region",
                "domestic",
                "international",
                "global",
                "local",
                "retail",
                "wholesale",
                "online",
                "digital",
                "services",
                "products",
                "solutions",
                "segment",
                "division",
                "unit",
            ]

            if len(suffix) > 2 and any(keyword in suffix.lower() for keyword in segment_keywords):
                is_base_standard = standard_node_registry.is_recognized_name(base_name)

                return ValidationResult(
                    original_name=name,
                    standardized_name=name,
                    is_valid=not self.strict_mode or is_base_standard,
                    message=f"{'Valid' if is_base_standard else 'Non-standard'} sub-node of '{base_name}' (segment)",
                    category="subnode" if is_base_standard else "subnode_nonstandard",
                    confidence=0.85 if is_base_standard else 0.65,
                )

        return None

    def _check_formula_ending(self, name: str) -> Optional[tuple[str, str]]:
        """Check if name ends with a formula pattern."""
        name_lower = name.lower()

        for ending in self.FORMULA_ENDINGS:
            if name_lower.endswith(ending):
                base_name = name[: -len(ending)]
                formula_type = ending[1:]  # Remove underscore
                return base_name, formula_type

        return None

    def _check_patterns(
        self,
        name: str,
        patterns: list[tuple[str, str]],
        pattern_category: str,
    ) -> Optional[tuple[str, str, str]]:
        """Check if name matches any pattern in the list."""
        name_lower = name.lower()

        for pattern, pattern_type in patterns:
            match = re.match(pattern, name_lower)
            if match:
                base_name = match.group(1)
                suffix = match.group(2) if match.lastindex > 1 else ""
                return base_name, suffix, pattern_type

        return None

    def _check_parent_relationship(self, name: str, parent_nodes: list[str]) -> bool:
        """Check if node name is related to its parents."""
        name_lower = name.lower()

        # Check if we have enough parents to establish a relationship
        if len(parent_nodes) < 2:
            return False

        # Check if any parent name is contained in this node name
        parent_match_count = 0
        for parent in parent_nodes:
            parent_lower = parent.lower()

            # Direct containment or similarity
            if parent_lower in name_lower or self._is_similar(
                name_lower, parent_lower, threshold=0.5
            ):
                parent_match_count += 1

            # Check standard name relationships
            if standard_node_registry.is_standard_name(parent):
                definition = standard_node_registry.get_definition(parent)
                if definition:
                    for alt in definition.alternate_names:
                        if alt.lower() in name_lower:
                            parent_match_count += 1
                            break

        # Consider it derived if it matches at least one parent
        return parent_match_count >= 1

    def _generate_suggestions(self, name: str) -> list[str]:
        """Generate improvement suggestions for non-standard names."""
        suggestions_with_scores = []
        name_lower = name.lower()

        # Find similar standard names
        for std_name in standard_node_registry.list_standard_names():
            std_lower = std_name.lower()

            # Calculate similarity score
            score = 0.0

            # Exact prefix match gets highest score
            if std_lower.startswith(name_lower) or name_lower.startswith(std_lower):
                score = 0.9
            # Check character overlap
            elif self._is_similar(name_lower, std_lower):
                overlap = len(set(name_lower) & set(std_lower))
                min_len = min(len(name_lower), len(std_lower))
                score = overlap / min_len * 0.8

            if score > 0:
                suggestions_with_scores.append(
                    (score, f"Consider using standard name: '{std_name}'")
                )

        # Sort by score (highest first) and take top suggestions
        suggestions_with_scores.sort(key=lambda x: x[0], reverse=True)
        suggestions = [msg for _, msg in suggestions_with_scores[:3]]

        # Check for pattern improvements
        if "_" in name and len(suggestions) < 3:
            parts = name.split("_", 1)
            base = parts[0]

            # Suggest standardizing the base
            for std_name in standard_node_registry.list_standard_names():
                if self._is_similar(base.lower(), std_name.lower()):
                    suggestions.append(f"Consider using '{std_name}_{parts[1]}' for consistency")
                    break

        # Generic suggestions if nothing specific found
        if not suggestions:
            if any(suffix in name for suffix in ["_margin", "_ratio", "_growth", "_pct"]):
                suggestions.append(
                    "Formula node detected - ensure base name follows standard conventions"
                )
            else:
                suggestions.append(
                    "Consider using a standard node name for better metric compatibility"
                )

        return suggestions[:3]  # Return top 3 suggestions

    def _is_similar(self, str1: str, str2: str, threshold: float = 0.6) -> bool:
        """Check if two strings are similar enough."""
        if len(str1) < 3 or len(str2) < 3:
            return False

        # Check if one is a prefix of the other
        if str1.startswith(str2) or str2.startswith(str1):
            return True

        # Check containment
        if str1 in str2 or str2 in str1:
            return True

        # Check character overlap
        overlap = len(set(str1) & set(str2))
        min_len = min(len(str1), len(str2))

        return overlap / min_len >= threshold

    def validate_batch(
        self,
        names: list[str],
        node_types: Optional[dict[str, str]] = None,
        parent_map: Optional[dict[str, list[str]]] = None,
    ) -> dict[str, ValidationResult]:
        """Validate multiple node names efficiently.

        Args:
            names: List of node names to validate.
            node_types: Optional mapping of names to node types.
            parent_map: Optional mapping of names to parent node lists.

        Returns:
            Dictionary mapping names to ValidationResults.
        """
        results = {}
        node_types = node_types or {}
        parent_map = parent_map or {}

        for name in names:
            result = self.validate(
                name,
                node_type=node_types.get(name),
                parent_nodes=parent_map.get(name),
            )
            results[name] = result

        return results

    def validate_graph(self, nodes: list[Node]) -> dict[str, Any]:
        """Validate all nodes in a graph with full context.

        Args:
            nodes: List of Node objects from the graph.

        Returns:
            Comprehensive validation report.
        """
        # Build context maps
        node_types = {}
        parent_map = {}

        for node in nodes:
            # Determine node type
            class_name = node.__class__.__name__
            if "Formula" in class_name:
                node_types[node.name] = "formula"
            elif "Calculation" in class_name:
                node_types[node.name] = "calculation"
            elif "Forecast" in class_name:
                node_types[node.name] = "forecast"
            else:
                node_types[node.name] = "data"

            # Extract parent nodes
            if hasattr(node, "inputs"):
                if isinstance(node.inputs, dict):
                    parent_map[node.name] = [p.name for p in node.inputs.values()]
                elif isinstance(node.inputs, list):
                    parent_map[node.name] = [p.name for p in node.inputs]

        # Validate all nodes
        node_names = [node.name for node in nodes]
        results = self.validate_batch(node_names, node_types, parent_map)

        # Categorize results
        report = {
            "total": len(results),
            "by_category": {},
            "by_validity": {"valid": 0, "invalid": 0},
            "suggestions": {},
            "details": results,
        }

        for name, result in results.items():
            # Count by category
            category = result.category
            if category not in report["by_category"]:
                report["by_category"][category] = []
            report["by_category"][category].append(name)

            # Count by validity
            if result.is_valid:
                report["by_validity"]["valid"] += 1
            else:
                report["by_validity"]["invalid"] += 1

            # Collect suggestions
            if result.suggestions:
                report["suggestions"][name] = result.suggestions

        return report

    def clear_cache(self) -> None:
        """Clear the validation cache."""
        self._validation_cache.clear()


# Convenience functions for backward compatibility
def create_validator(**kwargs: Any) -> UnifiedNodeValidator:
    """Create a validator instance with the given configuration."""
    return UnifiedNodeValidator(**kwargs)


def validate_node_name(
    name: str,
    strict: bool = False,
    auto_standardize: bool = True,
) -> tuple[str, bool, str]:
    """Simple validation function for backward compatibility.

    Args:
        name: Node name to validate.
        strict: Whether to use strict mode.
        auto_standardize: Whether to auto-standardize names.

    Returns:
        Tuple of (standardized_name, is_valid, message).
    """
    validator = UnifiedNodeValidator(
        strict_mode=strict,
        auto_standardize=auto_standardize,
        warn_on_non_standard=False,
    )

    result = validator.validate(name)
    return result.standardized_name, result.is_valid, result.message

# --- END FILE: fin_statement_model/io/validation.py ---

# --- START FILE: fin_statement_model/logging_config.py ---
"""Centralized logging configuration for the fin_statement_model library."""

import logging

# Attach a NullHandler to the base fin_statement_model logger so that
# all child loggers inherit it and avoid 'No handler' warnings by default.
logging.getLogger("fin_statement_model").addHandler(logging.NullHandler())

# --- END FILE: fin_statement_model/logging_config.py ---

# --- START FILE: fin_statement_model/preprocessing/__init__.py ---
"""Export DataTransformer, CompositeTransformer, and TransformerFactory for preprocessing.

This module exposes core transformer interfaces and factory for the preprocessing layer.
"""

from .base_transformer import DataTransformer, CompositeTransformer
from .transformer_factory import TransformerFactory
from .transformation_service import TransformationService

## Trigger transformer discovery on package import
TransformerFactory.discover_transformers("fin_statement_model.preprocessing.transformers")

__all__ = [
    "CompositeTransformer",
    "DataTransformer",
    "TransformationService",
    "TransformerFactory",
]

# --- END FILE: fin_statement_model/preprocessing/__init__.py ---

# --- START FILE: fin_statement_model/preprocessing/base_transformer.py ---
"""Define base DataTransformer interface for preprocessing layer.

This module provides the DataTransformer abstract base class and CompositeTransformer.
"""

from abc import ABC, abstractmethod
import pandas as pd
from typing import Optional
import logging

logger = logging.getLogger(__name__)


class DataTransformer(ABC):
    """Define base class for data transformers.

    Data transformers convert data between formats and apply business rules.

    This separation follows the Single Responsibility Principle for maintainability.
    """

    def __init__(self, config: Optional[dict[str, object]] = None):
        """Initialize the transformer with optional configuration.

        Args:
            config: Optional configuration dictionary for the transformer
        """
        self.config = config or {}
        logger.debug(f"Initialized {self.__class__.__name__} with config: {self.config}")

    @abstractmethod
    def transform(self, data: object) -> object:
        """Transform the input data.

        Args:
            data: The input data to transform

        Returns:
            Transformed data

        Raises:
            ValueError: If the data cannot be transformed
        """

    def validate_input(self, data: object) -> bool:
        """Validate that the input data is a pandas DataFrame by default.

        This performs a basic DataFrame type check and can be overridden by subclasses with more specific validation logic.

        Args:
            data (object): The input data to validate.

        Returns:
            bool: True if data is a pandas.DataFrame, False otherwise.
        """
        return isinstance(data, pd.DataFrame)

    def _pre_transform_hook(self, data: object) -> object:
        """Hook method called before transformation.

        Args:
            data: The input data

        Returns:
            Processed data to be passed to the transform method

        This method can be overridden by subclasses to add pre-processing steps.
        """
        return data

    def _post_transform_hook(self, data: object) -> object:
        """Hook method called after transformation.

        Args:
            data: The transformed data

        Returns:
            Final processed data

        This method can be overridden by subclasses to add post-processing steps.
        """
        return data

    def execute(self, data: object) -> object:
        """Execute the complete transformation pipeline.

        Args:
            data: The input data to transform

        Returns:
            Transformed data

        Raises:
            ValueError: If the data is invalid or cannot be transformed
        """
        if not self.validate_input(data):
            raise ValueError(f"Invalid input data for {self.__class__.__name__}")

        try:
            # Apply pre-transform hook
            processed_data = self._pre_transform_hook(data)

            # Perform transformation
            result = self.transform(processed_data)
            result = self._post_transform_hook(result)
            logger.debug(f"Successfully transformed data with {self.__class__.__name__}")
        except Exception as e:
            logger.exception(f"Error transforming data with {self.__class__.__name__}")
            raise ValueError("Error transforming data") from e
        else:
            return result


class CompositeTransformer(DataTransformer):
    """Compose multiple transformers into a pipeline.

    This allows building complex transformation chains from simple steps.
    """

    def __init__(self, transformers: list[DataTransformer], config: Optional[dict] = None):
        """Initialize with a list of transformers.

        Args:
            transformers: List of transformers to apply in sequence
            config: Optional configuration dictionary
        """
        super().__init__(config)
        self.transformers = transformers

    def transform(self, data: object) -> object:
        """Apply each transformer in sequence.

        Args:
            data: The input data to transform

        Returns:
            Data transformed by the pipeline
        """
        result = data
        for transformer in self.transformers:
            result = transformer.execute(result)
        return result

    def add_transformer(self, transformer: DataTransformer) -> None:
        """Add a transformer to the pipeline.

        Args:
            transformer: The transformer to add
        """
        self.transformers.append(transformer)

    def remove_transformer(self, index: int) -> Optional[DataTransformer]:
        """Remove a transformer from the pipeline.

        Args:
            index: Index of the transformer to remove

        Returns:
            The removed transformer or None if index is invalid
        """
        if 0 <= index < len(self.transformers):
            return self.transformers.pop(index)
        return None

    def validate_input(self, data: object) -> bool:
        """Validate input for the composite transformer.

        If the pipeline is empty, accepts any data; otherwise, delegates validation to the first transformer.

        Args:
            data (object): Input data to validate.

        Returns:
            bool: True if input is valid for the pipeline.
        """
        if not hasattr(self, "transformers") or not self.transformers:
            return True
        return self.transformers[0].validate_input(data)

# --- END FILE: fin_statement_model/preprocessing/base_transformer.py ---

# --- START FILE: fin_statement_model/preprocessing/config/__init__.py ---
"""Configuration models for preprocessing transformers."""

from .models import (
    NormalizationConfig,
    TimeSeriesConfig,
    PeriodConversionConfig,
    StatementFormattingConfig,
)
from .enums import (
    NormalizationType,
    TransformationType,
    ConversionType,
    StatementType,
)

__all__ = [
    # Config models
    "NormalizationConfig",
    "PeriodConversionConfig",
    "StatementFormattingConfig",
    "TimeSeriesConfig",
    # Enums
    "NormalizationType",
    "TransformationType",
    "ConversionType",
    "StatementType",
]

# --- END FILE: fin_statement_model/preprocessing/config/__init__.py ---

# --- START FILE: fin_statement_model/preprocessing/config/enums.py ---
"""Define Enum classes for preprocessing transformer types.

Centralize transformer type constants as Enums for clarity.
"""

from enum import Enum


class NormalizationType(Enum):
    """Available normalization types for NormalizationTransformer."""

    PERCENT_OF = "percent_of"
    MINMAX = "minmax"
    STANDARD = "standard"
    SCALE_BY = "scale_by"


class TransformationType(Enum):
    """Available transformation types for TimeSeriesTransformer."""

    GROWTH_RATE = "growth_rate"
    MOVING_AVG = "moving_avg"
    CAGR = "cagr"
    YOY = "yoy"
    QOQ = "qoq"


class ConversionType(Enum):
    """Available conversion types for PeriodConversionTransformer.

    - QUARTERLY_TO_ANNUAL: Aggregate quarterly data to annual
    - MONTHLY_TO_QUARTERLY: Aggregate monthly data to quarterly
    - MONTHLY_TO_ANNUAL: Aggregate monthly data to annual
    - QUARTERLY_TO_TTM: Convert quarterly data to trailing twelve months (TTM)
    """

    QUARTERLY_TO_ANNUAL = "quarterly_to_annual"
    MONTHLY_TO_QUARTERLY = "monthly_to_quarterly"
    MONTHLY_TO_ANNUAL = "monthly_to_annual"
    QUARTERLY_TO_TTM = "quarterly_to_ttm"


class StatementType(Enum):
    """Available statement types for StatementFormattingTransformer."""

    INCOME_STATEMENT = "income_statement"
    BALANCE_SHEET = "balance_sheet"
    CASH_FLOW = "cash_flow"

# --- END FILE: fin_statement_model/preprocessing/config/enums.py ---

# --- START FILE: fin_statement_model/preprocessing/config/models.py ---
"""Configuration models for preprocessing transformers.

This module contains Pydantic models for configuring various preprocessing transformations.
"""

from typing import Optional
from pydantic import BaseModel


class NormalizationConfig(BaseModel):
    """Configuration for normalization transformations.

    Attributes:
        normalization_type: 'percent_of', 'minmax', 'standard', or 'scale_by'
        reference: reference field name for 'percent_of' normalization
        scale_factor: factor to apply for 'scale_by' normalization
    """

    normalization_type: Optional[str] = None
    reference: Optional[str] = None
    scale_factor: Optional[float] = None


class TimeSeriesConfig(BaseModel):
    """Configuration for time series transformations.

    Attributes:
        transformation_type: 'growth_rate', 'moving_avg', 'cagr', 'yoy', or 'qoq'
        periods: number of periods for percentage change or other transformations
        window_size: window size for rolling calculations
    """

    transformation_type: Optional[str] = None
    periods: Optional[int] = None
    window_size: Optional[int] = None


class PeriodConversionConfig(BaseModel):
    """Configuration for period conversion transformations.

    Attributes:
        conversion_type: 'quarterly_to_annual', 'monthly_to_quarterly', etc.
        aggregation: aggregation method: 'sum', 'mean', 'last', etc.
    """

    conversion_type: Optional[str] = None
    aggregation: Optional[str] = None


class StatementFormattingConfig(BaseModel):
    """Configuration for formatting statement output.

    Attributes:
        statement_type: 'income_statement', 'balance_sheet', 'cash_flow'
        add_subtotals: whether to insert computed subtotals
        apply_sign_convention: whether to apply sign rules to values
    """

    statement_type: Optional[str] = None
    add_subtotals: Optional[bool] = None
    apply_sign_convention: Optional[bool] = None

# --- END FILE: fin_statement_model/preprocessing/config/models.py ---

# --- START FILE: fin_statement_model/preprocessing/transformation_service.py ---
"""Transformation Service for the Financial Statement Model.

This module provides a high-level service for managing and applying data transformations.
"""

from typing import Optional, Union

import pandas as pd
import logging

from .base_transformer import DataTransformer, CompositeTransformer
from .transformer_factory import TransformerFactory

logger = logging.getLogger(__name__)


class TransformationService:
    """Service for managing and applying data transformations.

    This service separates data transformation logic from data processing,
    making it easier to maintain, test, and extend the codebase.

    It provides methods for common financial data transformations and allows
    for composing multiple transformations into pipelines.
    """

    def __init__(self):
        """Initialize the transformation service."""
        logger.info("TransformationService initialized")

    def normalize_data(
        self,
        data: Union[pd.DataFrame, dict],
        normalization_type: str = "percent_of",
        reference: Optional[str] = None,
        scale_factor: Optional[float] = None,
    ) -> Union[pd.DataFrame, dict]:
        """Normalize financial data.

        Args:
            data: The data to normalize (DataFrame or Dict)
            normalization_type: Type of normalization
            reference: Reference field for percent_of normalization
            scale_factor: Scale factor for scale_by normalization

        Returns:
            Normalized data
        """
        transformer = TransformerFactory.create_transformer(
            "normalization",
            normalization_type=normalization_type,
            reference=reference,
            scale_factor=scale_factor,
        )

        return transformer.execute(data)

    def transform_time_series(
        self,
        data: Union[pd.DataFrame, dict],
        transformation_type: str = "growth_rate",
        periods: int = 1,
        window_size: int = 3,
    ) -> Union[pd.DataFrame, dict]:
        """Apply time series transformations to financial data.

        Args:
            data: The time series data to transform
            transformation_type: Type of transformation
            periods: Number of periods for calculations
            window_size: Window size for moving averages

        Returns:
            Transformed data
        """
        transformer = TransformerFactory.create_transformer(
            "time_series",
            transformation_type=transformation_type,
            periods=periods,
            window_size=window_size,
        )

        return transformer.execute(data)

    def convert_periods(
        self, data: pd.DataFrame, conversion_type: str, aggregation: str = "sum"
    ) -> pd.DataFrame:
        """Convert data between different period types.

        Args:
            data: DataFrame with time periods
            conversion_type: Type of period conversion
            aggregation: Aggregation method

        Returns:
            Transformed DataFrame with converted periods
        """
        transformer = TransformerFactory.create_transformer(
            "period_conversion",
            conversion_type=conversion_type,
            aggregation=aggregation,
        )

        return transformer.execute(data)

    def format_statement(
        self,
        data: pd.DataFrame,
        statement_type: str = "income_statement",
        add_subtotals: bool = True,
        apply_sign_convention: bool = True,
    ) -> pd.DataFrame:
        """Format a financial statement DataFrame.

        Args:
            data: Financial statement data
            statement_type: Type of statement
            add_subtotals: Whether to add standard subtotals
            apply_sign_convention: Whether to apply sign conventions

        Returns:
            Formatted financial statement
        """
        transformer = TransformerFactory.create_transformer(
            "statement_formatting",
            statement_type=statement_type,
            add_subtotals=add_subtotals,
            apply_sign_convention=apply_sign_convention,
        )

        return transformer.execute(data)

    def create_transformation_pipeline(
        self, transformers_config: list[dict[str, object]]
    ) -> DataTransformer:
        """Create a composite transformer from a list of transformer configurations.

        Args:
            transformers_config: List of dicts with transformer configurations
                Each dict should have:
                - 'name': Name of the transformer
                - Additional configuration parameters for that transformer

        Returns:
            A composite transformer with the configured pipeline

        Example:
            config = [
                {'name': 'period_conversion', 'conversion_type': 'quarterly_to_annual'},
                {'name': 'normalization', 'normalization_type': 'percent_of', 'reference': 'revenue'}
            ]
            pipeline = service.create_transformation_pipeline(config)
            transformed_data = pipeline.execute(data)
        """
        transformers = []

        for config in transformers_config:
            if "name" not in config:
                raise ValueError("Each transformer configuration must have a 'name' field")

            name = config.pop("name")
            transformer = TransformerFactory.create_transformer(name, **config)
            transformers.append(transformer)

        return CompositeTransformer(transformers)

    def apply_transformation_pipeline(
        self, data: object, transformers_config: list[dict[str, object]]
    ) -> object:
        """Apply a transformation pipeline to data.

        Args:
            data: The data to transform
            transformers_config: List of transformer configurations

        Returns:
            Transformed data
        """
        pipeline = self.create_transformation_pipeline(transformers_config)
        return pipeline.execute(data)

    def register_custom_transformer(
        self, name: str, transformer_class: type[DataTransformer]
    ) -> None:
        """Register a custom transformer with the factory.

        Args:
            name: Name for the transformer
            transformer_class: The transformer class to register
        """
        TransformerFactory.register_transformer(name, transformer_class)
        logger.info(f"Registered custom transformer: {name}")

    def list_available_transformers(self) -> list[str]:
        """List all available transformer types.

        Returns:
            List of transformer names
        """
        return TransformerFactory.list_transformers()

# --- END FILE: fin_statement_model/preprocessing/transformation_service.py ---

# --- START FILE: fin_statement_model/preprocessing/transformer_factory.py ---
"""Provide TransformerFactory to create and manage data transformers.

This module implements a factory for registering and instantiating transformers.
"""

import importlib
import re
import inspect
import pkgutil
import logging
from typing import ClassVar, Any

from .base_transformer import DataTransformer

logger = logging.getLogger(__name__)


class TransformerFactory:
    """Create and manage transformer instances.

    Centralizes transformer registration, discovery, and instantiation.
    """

    # Registry of transformer types
    _transformers: ClassVar[dict[str, type[DataTransformer]]] = {}

    @classmethod
    def register_transformer(cls, name: str, transformer_class: type[DataTransformer]) -> None:
        """Register a transformer class with the factory.

        Args:
            name: Name to register the transformer under
            transformer_class: The transformer class to register

        Raises:
            ValueError: If the name is already registered
            TypeError: If transformer_class is not a subclass of DataTransformer
        """
        if name in cls._transformers:
            raise ValueError(f"Transformer name '{name}' is already registered")

        if not issubclass(transformer_class, DataTransformer):
            raise TypeError("Transformer class must be a subclass of DataTransformer")

        cls._transformers[name] = transformer_class
        logger.info(f"Registered transformer '{name}'")

    @classmethod
    def create_transformer(cls, name: str, **kwargs: dict[str, Any]) -> DataTransformer:
        """Create a transformer instance by name.

        Args:
            name: Name of the registered transformer
            **kwargs: Arguments to pass to the transformer constructor

        Returns:
            DataTransformer: An instance of the requested transformer

        Raises:
            ValueError: If no transformer is registered with the given name
        """
        if name not in cls._transformers:
            raise ValueError(f"No transformer registered with name '{name}'")

        transformer_class = cls._transformers[name]
        transformer = transformer_class(**kwargs)
        logger.debug(f"Created transformer '{name}'")
        return transformer

    @classmethod
    def list_transformers(cls) -> list[str]:
        """List all registered transformer names.

        Returns:
            List[str]: List of registered transformer names
        """
        return list(cls._transformers.keys())

    @classmethod
    def get_transformer_class(cls, name: str) -> type[DataTransformer]:
        """Get a transformer class by name.

        Args:
            name: Name of the registered transformer

        Returns:
            Type[DataTransformer]: The requested transformer class

        Raises:
            ValueError: If no transformer is registered with the given name
        """
        if name not in cls._transformers:
            raise ValueError(f"No transformer registered with name '{name}'")

        return cls._transformers[name]

    @classmethod
    def discover_transformers(cls, package_name: str) -> None:
        """Discover and register all transformers in a package.

        This method imports all modules in the specified package and
        registers any DataTransformer subclasses found.

        Args:
            package_name: Name of the package to search
        """
        try:
            package = importlib.import_module(package_name)
            package_path = package.__path__

            # Import all modules in the package
            for _, module_name, _ in pkgutil.iter_modules(package_path):
                full_module_name = f"{package_name}.{module_name}"
                module = importlib.import_module(full_module_name)

                # Find all DataTransformer subclasses in the module
                for name, obj in inspect.getmembers(module):
                    if (
                        inspect.isclass(obj)
                        and issubclass(obj, DataTransformer)
                        and obj != DataTransformer
                    ):
                        # Register the transformer with its class name
                        cls.register_transformer(name, obj)
                        # Register snake_case alias without '_transformer'
                        snake = re.sub(r"(.)([A-Z][a-z]+)", r"\1_\2", name)
                        snake = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", snake).lower()
                        alias = snake.replace("_transformer", "")
                        if alias not in cls._transformers:
                            cls.register_transformer(alias, obj)

            logger.info(f"Discovered transformers from package '{package_name}'")

        except ImportError:
            logger.exception(f"Error discovering transformers from package '{package_name}'")

    @classmethod
    def create_composite_transformer(
        cls, transformer_names: list[str], **kwargs: dict[str, Any]
    ) -> DataTransformer:
        """Create a composite transformer from a list of transformer names.

        Args:
            transformer_names: List of registered transformer names to include in the pipeline
            **kwargs: Additional arguments to pass to individual transformers

        Returns:
            DataTransformer: A composite transformer containing the specified transformers

        Raises:
            ValueError: If any transformer name is not registered
        """
        from .base_transformer import CompositeTransformer

        # Use list comprehension for PERF401
        transformers = [cls.create_transformer(name, **kwargs) for name in transformer_names]

        return CompositeTransformer(transformers)

# --- END FILE: fin_statement_model/preprocessing/transformer_factory.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/__init__.py ---
"""Package for preprocessing transformers.

This package exports built-in data transformer classes for the preprocessing layer.
"""

from .normalization import NormalizationTransformer
from .time_series import TimeSeriesTransformer
from .period_conversion import PeriodConversionTransformer

__all__ = [
    "NormalizationTransformer",
    "PeriodConversionTransformer",
    "TimeSeriesTransformer",
]

# --- END FILE: fin_statement_model/preprocessing/transformers/__init__.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/normalization.py ---
"""Provide a NormalizationTransformer to normalize financial data.

Transforms data by percent_of, minmax, standard, or scale_by methods.

This module implements the NormalizationTransformer for the preprocessing layer.
"""

from typing import Optional, Union, ClassVar
import logging

import numpy as np
import pandas as pd

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.config.enums import NormalizationType
from fin_statement_model.preprocessing.config.models import NormalizationConfig

logger = logging.getLogger(__name__)


class NormalizationTransformer(DataTransformer):
    """Transformer that normalizes financial data using various methods.

    This transformer provides multiple normalization strategies commonly used in
    financial analysis to make data comparable across different scales or to
    express values as percentages of a reference metric.

    Supported normalization types:
        - **percent_of**: Express values as percentages of a reference column
          (e.g., all items as % of revenue)
        - **minmax**: Scale values to [0, 1] range based on min/max values
        - **standard**: Standardize using (x - mean) / std deviation
        - **scale_by**: Multiply all values by a fixed scale factor

    Examples:
        Express all income statement items as percentage of revenue:

        >>> import pandas as pd
        >>> from fin_statement_model.preprocessing.transformers import NormalizationTransformer
        >>>
        >>> # Sample income statement data
        >>> data = pd.DataFrame({
        ...     'revenue': [1000, 1100, 1200],
        ...     'cogs': [600, 650, 700],
        ...     'operating_expenses': [200, 220, 250]
        ... }, index=['2021', '2022', '2023'])
        >>>
        >>> # Create transformer to express as % of revenue
        >>> normalizer = NormalizationTransformer(
        ...     normalization_type='percent_of',
        ...     reference='revenue'
        ... )
        >>>
        >>> # Transform the data
        >>> normalized = normalizer.transform(data)
        >>> print(normalized)
        #       revenue  cogs  operating_expenses
        # 2021    100.0  60.0               20.0
        # 2022    100.0  59.1               20.0
        # 2023    100.0  58.3               20.8

        Scale financial data to millions:

        >>> # Scale values to millions (divide by 1,000,000)
        >>> scaler = NormalizationTransformer(
        ...     normalization_type='scale_by',
        ...     scale_factor=0.000001
        ... )
        >>> scaled = scaler.transform(data)

    Note:
        For 'percent_of' normalization, if a reference value is 0 or NaN,
        the corresponding output for that row will be NaN to avoid division
        by zero errors.
    """

    NORMALIZATION_TYPES: ClassVar[list[str]] = [t.value for t in NormalizationType]

    def __init__(
        self,
        normalization_type: Union[str, NormalizationType] = NormalizationType.PERCENT_OF,
        reference: Optional[str] = None,
        scale_factor: Optional[float] = None,
        config: Optional[NormalizationConfig] = None,
    ):
        """Initialize the normalizer with specified parameters.

        Args:
            normalization_type: Type of normalization to apply. Can be either
                a string or NormalizationType enum value:
                - 'percent_of': Express values as percentage of reference column
                - 'minmax': Scale to [0,1] range
                - 'standard': Apply z-score normalization
                - 'scale_by': Multiply by scale_factor
            reference: Name of the reference column for 'percent_of' normalization.
                Required when normalization_type is 'percent_of'.
            scale_factor: Multiplication factor for 'scale_by' normalization.
                Required when normalization_type is 'scale_by'.
                Common values: 0.001 (to thousands), 0.000001 (to millions)
            config: Optional NormalizationConfig object containing configuration.
                If provided, overrides other parameters.

        Raises:
            ValueError: If normalization_type is invalid, or if required
                parameters are missing for the selected normalization type.
        """
        super().__init__(config.model_dump() if config else None)
        # Normalize enum to string
        if isinstance(normalization_type, NormalizationType):
            norm_type = normalization_type.value
        else:
            norm_type = normalization_type
        if norm_type not in self.NORMALIZATION_TYPES:
            raise ValueError(
                f"Invalid normalization type: {norm_type}. "
                f"Must be one of {self.NORMALIZATION_TYPES}"
            )
        self.normalization_type = norm_type

        self.reference = reference
        self.scale_factor = scale_factor

        # Validation
        if self.normalization_type == NormalizationType.PERCENT_OF.value and not reference:
            raise ValueError("Reference field must be provided for percent_of normalization")

        if self.normalization_type == NormalizationType.SCALE_BY.value and scale_factor is None:
            raise ValueError("Scale factor must be provided for scale_by normalization")

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Normalize the data based on the configured normalization type.

        Args:
            data: DataFrame containing financial data to normalize.
                All columns will be normalized except the reference column
                in 'percent_of' normalization.

        Returns:
            DataFrame with normalized values. Original column names are preserved
            for all normalization types.

        Raises:
            TypeError: If data is not a pandas DataFrame.
            ValueError: If reference column is not found in DataFrame
                (for 'percent_of' normalization).

        Examples:
            >>> df = pd.DataFrame({'revenue': [100, 200], 'costs': [60, 120]})
            >>> normalizer = NormalizationTransformer('percent_of', reference='revenue')
            >>> result = normalizer.transform(df)
            >>> print(result)
            #    revenue  costs
            # 0    100.0   60.0
            # 1    100.0   60.0
        """
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f"Unsupported data type: {type(data)}. Expected pandas.DataFrame")
        return self._transform_dataframe(data)

    def _transform_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform a DataFrame with the selected normalization method.

        For 'percent_of' normalization, if a reference value is 0 or NaN,
        the corresponding output for that row will be NaN.
        """
        result = df.copy()

        if self.normalization_type == NormalizationType.PERCENT_OF.value:
            if self.reference not in df.columns:
                raise ValueError(f"Reference column '{self.reference}' not found in DataFrame")

            for col in df.columns:
                if col != self.reference:
                    # Replace 0 with NaN in the denominator to ensure division by zero results in NaN
                    reference_series = df[self.reference].replace(0, np.nan)
                    if (
                        reference_series.isnull().all()
                    ):  # If all reference values are NaN (or were 0)
                        result[col] = np.nan
                        logger.warning(
                            f"All reference values for '{self.reference}' are zero or NaN. '{col}' will be NaN."
                        )
                    else:
                        result[col] = (df[col] / reference_series) * 100

        elif self.normalization_type == NormalizationType.MINMAX.value:  # pragma: no cover
            for col in df.columns:
                min_val = df[col].min()
                max_val = df[col].max()

                if max_val > min_val:
                    result[col] = (df[col] - min_val) / (max_val - min_val)  # pragma: no cover
                elif max_val == min_val:  # Handles constant columns
                    result[col] = (
                        0.0  # Or np.nan, depending on desired behavior for constant series
                    )
                # else: max_val < min_val (should not happen with .min()/.max())

        elif self.normalization_type == NormalizationType.STANDARD.value:
            for col in df.columns:
                mean = df[col].mean()
                std = df[col].std()

                if std > 0:
                    result[col] = (df[col] - mean) / std
                elif std == 0:  # Handles constant columns
                    result[col] = 0.0  # Or np.nan, depending on desired behavior
                # else: std < 0 (not possible)

        elif self.normalization_type == NormalizationType.SCALE_BY.value:
            for col in df.columns:
                result[col] = df[col] * self.scale_factor

        return result

# --- END FILE: fin_statement_model/preprocessing/transformers/normalization.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/period_conversion.py ---
"""Financial data transformers for the Financial Statement Model.

This module provides the PeriodConversionTransformer for converting between period types:
quarterly_to_annual, monthly_to_quarterly, monthly_to_annual, and quarterly_to_ttm.
"""

import logging
import pandas as pd
from typing import Optional, Union, ClassVar

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.config.models import PeriodConversionConfig
from fin_statement_model.preprocessing.config.enums import ConversionType

# Configure logging
logger = logging.getLogger(__name__)


class PeriodConversionTransformer(DataTransformer):
    """Transformer for converting between different financial reporting periods.

    This transformer aggregates financial data from higher-frequency periods
    (e.g., monthly, quarterly) to lower-frequency periods (e.g., quarterly, annual)
    or calculates trailing metrics like TTM (Trailing Twelve Months).

    Supported conversion types:
        - **quarterly_to_annual**: Aggregate 4 quarters into annual data
        - **monthly_to_quarterly**: Aggregate 3 months into quarterly data
        - **monthly_to_annual**: Aggregate 12 months into annual data
        - **quarterly_to_ttm**: Calculate trailing twelve months from quarterly data

    Input Data Requirements:
        - Data must have a DatetimeIndex or an index convertible to datetime
        - The index should represent the period-end dates
        - Data frequency should match the conversion type (e.g., quarterly data
          for quarterly_to_annual conversion)

    Aggregation Methods:
        - **sum**: Total values (default) - use for flow items like revenue, expenses
        - **mean**: Average values - use for rates, ratios, or average balances
        - **last**: Take last value - use for balance sheet items (point-in-time)
        - **first**: Take first value - use for opening balances
        - **max/min**: Maximum/minimum values - use for peak/trough analysis

    Examples:
        Convert quarterly revenue to annual totals:

        >>> import pandas as pd
        >>> from fin_statement_model.preprocessing.transformers import PeriodConversionTransformer
        >>>
        >>> # Quarterly revenue and expense data
        >>> quarterly_data = pd.DataFrame({
        ...     'revenue': [100, 110, 120, 130, 140, 150, 160, 170],
        ...     'expenses': [80, 85, 90, 95, 100, 105, 110, 115]
        ... }, index=pd.date_range('2022-03-31', periods=8, freq='Q'))
        >>>
        >>> # Convert to annual data (sum 4 quarters)
        >>> annual_converter = PeriodConversionTransformer(
        ...     conversion_type='quarterly_to_annual',
        ...     aggregation='sum'
        ... )
        >>> annual_data = annual_converter.transform(quarterly_data)
        >>> print(annual_data)
        #       revenue  expenses
        # 2022      460       350
        # 2023      620       430

        Convert monthly balance sheet to quarterly (taking last value):

        >>> # Monthly balance sheet data
        >>> monthly_bs = pd.DataFrame({
        ...     'total_assets': [1000, 1020, 1050, 1080, 1100, 1150],
        ...     'total_equity': [600, 610, 620, 630, 640, 650]
        ... }, index=pd.date_range('2023-01-31', periods=6, freq='M'))
        >>>
        >>> # Convert to quarterly, taking last month's value
        >>> quarterly_converter = PeriodConversionTransformer(
        ...     conversion_type='monthly_to_quarterly',
        ...     aggregation='last'
        ... )
        >>> quarterly_bs = quarterly_converter.transform(monthly_bs)
        >>> print(quarterly_bs)
        #                  total_assets  total_equity
        # (2023, 1)              1050           620
        # (2023, 2)              1150           650

        Calculate trailing twelve months (TTM) from quarterly data:

        >>> # Quarterly earnings data
        >>> quarterly_earnings = pd.DataFrame({
        ...     'net_income': [25, 30, 35, 40, 45, 50, 55, 60]
        ... }, index=pd.date_range('2022-03-31', periods=8, freq='Q'))
        >>>
        >>> # Calculate TTM (rolling 4-quarter sum)
        >>> ttm_converter = PeriodConversionTransformer(
        ...     conversion_type='quarterly_to_ttm',
        ...     aggregation='sum'
        ... )
        >>> ttm_data = ttm_converter.transform(quarterly_earnings)
        >>> print(ttm_data.iloc[3:])  # First 3 periods will be NaN
        #             net_income
        # 2023-03-31       130.0
        # 2023-06-30       150.0
        # 2023-09-30       170.0
        # 2023-12-31       190.0
        # 2024-03-31       210.0

    Note:
        - The resulting index format depends on the conversion type
        - Annual conversions group by year (integer index)
        - Quarterly conversions group by (year, quarter) tuple
        - TTM conversions maintain the original datetime index
        - Ensure your aggregation method matches the financial item type
    """

    # All valid conversion types
    CONVERSION_TYPES: ClassVar[list[str]] = [t.value for t in ConversionType]

    def __init__(
        self,
        conversion_type: Union[str, ConversionType] = ConversionType.QUARTERLY_TO_ANNUAL,
        aggregation: str = "sum",
        config: Optional[PeriodConversionConfig] = None,
    ):
        """Initialize the period conversion transformer.

        Args:
            conversion_type: Type of period conversion to apply. Can be either
                a string or ConversionType enum value:
                - 'quarterly_to_annual': Convert 4 quarters to 1 year
                - 'monthly_to_quarterly': Convert 3 months to 1 quarter
                - 'monthly_to_annual': Convert 12 months to 1 year
                - 'quarterly_to_ttm': Calculate trailing twelve months
            aggregation: How to aggregate data within each period:
                - 'sum': Add up all values (default) - for flow items
                - 'mean': Calculate average - for rates/ratios
                - 'last': Take last value - for balance sheet items
                - 'first': Take first value - for opening balances
                - 'max': Take maximum value
                - 'min': Take minimum value
                - 'std': Calculate standard deviation
                - 'count': Count non-null values
            config: Optional PeriodConversionConfig object containing configuration.
                If provided, overrides other parameters.

        Raises:
            ValueError: If conversion_type is invalid.

        Examples:
            >>> # Annual totals from quarterly data
            >>> converter = PeriodConversionTransformer('quarterly_to_annual', 'sum')
            >>>
            >>> # Quarter-end balances from monthly data
            >>> converter = PeriodConversionTransformer('monthly_to_quarterly', 'last')
            >>>
            >>> # TTM revenue from quarterly data
            >>> converter = PeriodConversionTransformer('quarterly_to_ttm', 'sum')
        """
        super().__init__(config.model_dump() if config else None)
        # Normalize enum to string
        if isinstance(conversion_type, ConversionType):
            ctype = conversion_type.value
        else:
            ctype = conversion_type
        if ctype not in self.CONVERSION_TYPES:
            raise ValueError(
                f"Invalid conversion type: {ctype}. Must be one of {self.CONVERSION_TYPES}"
            )
        self.conversion_type = ctype
        self.aggregation = aggregation

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform data by converting between period types.

        Args:
            data: DataFrame with time-based data to convert. Must have either:
                - A DatetimeIndex
                - An index containing date/time strings parsable by pd.to_datetime
                - Period labels that can be converted to datetime

                The data frequency should match the source period type (e.g.,
                quarterly data for 'quarterly_to_annual' conversion).

        Returns:
            DataFrame with converted periods:
            - For annual conversions: Index will be years (integers)
            - For quarterly conversions: Index will be (year, quarter) tuples
            - For TTM conversions: Original datetime index is preserved

            All columns are aggregated according to the specified method.

        Raises:
            TypeError: If data is not a pandas DataFrame.
            ValueError: If index cannot be converted to datetime or if
                aggregation='sum' is used with 'quarterly_to_ttm' and a
                different aggregation method is specified.

        Examples:
            >>> # Convert quarterly data to annual
            >>> df = pd.DataFrame({
            ...     'revenue': [100, 110, 120, 130],
            ...     'costs': [60, 65, 70, 75]
            ... }, index=['2023-Q1', '2023-Q2', '2023-Q3', '2023-Q4'])
            >>>
            >>> converter = PeriodConversionTransformer('quarterly_to_annual')
            >>> annual = converter.transform(df)
            >>> print(annual)
            #       revenue  costs
            # 2023      460    270
        """
        # Ensure we have a DataFrame
        if not isinstance(data, pd.DataFrame):
            raise TypeError("Period conversion requires a pandas DataFrame")

        df_copy = data.copy()

        # Try to convert index to datetime if it's not already
        if not isinstance(df_copy.index, pd.DatetimeIndex):
            try:
                df_copy.index = pd.to_datetime(df_copy.index)
                logger.debug("Successfully converted DataFrame index to DatetimeIndex.")
            except Exception as e:
                logger.exception(
                    "Failed to convert DataFrame index to DatetimeIndex. Ensure index contains standard date/time strings or is already a DatetimeIndex."
                )
                raise ValueError(
                    f"Index must be convertible to datetime for period conversion: {e}"
                )

        if self.conversion_type == ConversionType.QUARTERLY_TO_ANNUAL.value:
            # Group by year and aggregate
            return df_copy.groupby(df_copy.index.year).agg(self.aggregation)

        elif self.conversion_type == ConversionType.MONTHLY_TO_QUARTERLY.value:
            # Group by year and quarter
            return df_copy.groupby([df_copy.index.year, df_copy.index.quarter]).agg(
                self.aggregation
            )

        elif self.conversion_type == ConversionType.MONTHLY_TO_ANNUAL.value:
            # Group by year
            return df_copy.groupby(df_copy.index.year).agg(self.aggregation)

        elif self.conversion_type == ConversionType.QUARTERLY_TO_TTM.value:
            # Implement TTM as rolling sum with window=4 for quarterly data
            if self.aggregation == "sum":
                return df_copy.rolling(window=4, min_periods=4).sum()
            else:
                # For other aggregation methods, we need custom logic
                raise ValueError(
                    "QUARTERLY_TO_TTM conversion currently only supports 'sum' aggregation for TTM. "
                    "TTM typically represents the sum of the last 4 quarters for flow items like revenue."
                )
        else:
            raise NotImplementedError(
                f"Conversion type '{self.conversion_type}' is defined in ConversionType enum but not implemented in PeriodConversionTransformer."
            )

# --- END FILE: fin_statement_model/preprocessing/transformers/period_conversion.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/time_series.py ---
"""Financial data transformers for the Financial Statement Model.

This module provides the TimeSeriesTransformer which applies growth rates,
moving averages, CAGR, year-over-year, and quarter-over-quarter conversions.
"""

import logging
import numpy as np
import pandas as pd
from typing import Union, Optional, ClassVar

from fin_statement_model.preprocessing.config.models import TimeSeriesConfig
from fin_statement_model.preprocessing.config.enums import TransformationType
from fin_statement_model.preprocessing.base_transformer import DataTransformer

logger = logging.getLogger(__name__)


class TimeSeriesTransformer(DataTransformer):
    """Transformer for time series financial data analysis.

    This transformer provides common time series transformations used in financial
    analysis to identify trends, growth patterns, and period-over-period changes.

    Supported transformation types:
        - **growth_rate**: Calculate period-to-period growth rates (%)
        - **moving_avg**: Calculate moving averages over specified window
        - **cagr**: Compute compound annual growth rate
        - **yoy**: Year-over-year comparison (%)
        - **qoq**: Quarter-over-quarter comparison (%)

    Data Frequency Assumptions:
        The transformer makes no assumptions about the frequency of your data.
        You must specify the appropriate 'periods' parameter based on your data:

        - For **monthly data**:
            - YoY: use periods=12 (compare to same month last year)
            - QoQ: use periods=3 (compare to same month last quarter)

        - For **quarterly data**:
            - YoY: use periods=4 (compare to same quarter last year)
            - QoQ: use periods=1 (compare to previous quarter)

        - For **annual data**:
            - YoY: use periods=1 (compare to previous year)

    Examples:
        Calculate year-over-year growth for quarterly revenue data:

        >>> import pandas as pd
        >>> from fin_statement_model.preprocessing.transformers import TimeSeriesTransformer
        >>>
        >>> # Quarterly revenue data
        >>> data = pd.DataFrame({
        ...     'revenue': [100, 105, 110, 115, 120, 125, 130, 135],
        ...     'costs': [60, 62, 65, 68, 70, 73, 75, 78]
        ... }, index=pd.date_range('2022-Q1', periods=8, freq='Q'))
        >>>
        >>> # Calculate YoY growth (comparing to same quarter previous year)
        >>> yoy_transformer = TimeSeriesTransformer(
        ...     transformation_type='yoy',
        ...     periods=4  # 4 quarters back for quarterly data
        ... )
        >>> yoy_growth = yoy_transformer.transform(data)
        >>> print(yoy_growth[['revenue_yoy', 'costs_yoy']].iloc[4:])  # First 4 periods will be NaN
        #             revenue_yoy  costs_yoy
        # 2023-Q1           20.0      16.67
        # 2023-Q2           19.05     17.74
        # 2023-Q3           18.18     15.38
        # 2023-Q4           17.39     14.71

        Calculate 3-month moving average for monthly data:

        >>> # Monthly sales data
        >>> monthly_data = pd.DataFrame({
        ...     'sales': [100, 95, 105, 110, 108, 115, 120, 118, 125]
        ... }, index=pd.date_range('2023-01', periods=9, freq='M'))
        >>>
        >>> # Calculate 3-month moving average
        >>> ma_transformer = TimeSeriesTransformer(
        ...     transformation_type='moving_avg',
        ...     window_size=3
        ... )
        >>> ma_result = ma_transformer.transform(monthly_data)
        >>> print(ma_result['sales_ma3'].round(2))
        # 2023-01-31       NaN
        # 2023-02-28       NaN
        # 2023-03-31    100.00
        # 2023-04-30    103.33
        # 2023-05-31    107.67
        # 2023-06-30    111.00
        # 2023-07-31    114.33
        # 2023-08-31    117.67
        # 2023-09-30    121.00

    Note:
        - Growth rate calculations will return NaN for periods without valid
          comparison data (e.g., first 4 periods for YoY with quarterly data)
        - CAGR requires at least 2 data points and positive starting values
        - Moving averages will have NaN values for the first (window_size - 1) periods
    """

    TRANSFORMATION_TYPES: ClassVar[list[str]] = [t.value for t in TransformationType]

    def __init__(
        self,
        transformation_type: Union[str, TransformationType] = TransformationType.GROWTH_RATE,
        periods: int = 1,
        window_size: int = 3,
        config: Optional[TimeSeriesConfig] = None,
    ):
        """Initialize the time series transformer.

        Args:
            transformation_type: Type of transformation to apply. Can be either
                a string or TransformationType enum value:
                - 'growth_rate': Period-to-period growth rate
                - 'moving_avg': Rolling window average
                - 'cagr': Compound annual growth rate
                - 'yoy': Year-over-year growth rate
                - 'qoq': Quarter-over-quarter growth rate
            periods: Number of periods for lag calculations. Critical for YoY/QoQ:
                - For YoY with quarterly data: use periods=4
                - For YoY with monthly data: use periods=12
                - For QoQ with quarterly data: use periods=1
                - For QoQ with monthly data: use periods=3
                - For growth_rate: use periods=1 for consecutive period growth
            window_size: Size of the moving average window (only used for 'moving_avg').
                Default is 3.
            config: Optional TimeSeriesConfig object containing configuration.
                If provided, overrides other parameters.

        Raises:
            ValueError: If transformation_type is invalid.

        Examples:
            >>> # YoY for quarterly data
            >>> transformer = TimeSeriesTransformer('yoy', periods=4)
            >>>
            >>> # 3-month moving average
            >>> transformer = TimeSeriesTransformer('moving_avg', window_size=3)
            >>>
            >>> # Quarter-over-quarter for monthly data
            >>> transformer = TimeSeriesTransformer('qoq', periods=3)
        """
        super().__init__(config.model_dump() if config else None)
        # Normalize to string
        if isinstance(transformation_type, TransformationType):
            ttype = transformation_type.value
        else:
            ttype = transformation_type
        if ttype not in self.TRANSFORMATION_TYPES:
            raise ValueError(
                f"Invalid transformation type: {ttype}. Must be one of {self.TRANSFORMATION_TYPES}"
            )
        self.transformation_type = ttype

        self.periods = periods
        self.window_size = window_size

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform time series data based on the configured transformation type.

        Args:
            data: DataFrame containing time series financial data. The DataFrame
                should have a time-based index (DatetimeIndex, PeriodIndex, or
                sequential numeric index) for meaningful time series analysis.

        Returns:
            DataFrame with new columns containing transformed values:
            - For 'growth_rate': adds '{column}_growth' columns
            - For 'moving_avg': adds '{column}_ma{window_size}' columns
            - For 'cagr': adds '{column}_cagr' columns (single value repeated)
            - For 'yoy': adds '{column}_yoy' columns
            - For 'qoq': adds '{column}_qoq' columns

            Original columns are preserved in all cases.

        Raises:
            TypeError: If data is not a pandas DataFrame.

        Examples:
            >>> df = pd.DataFrame({'revenue': [100, 110, 120, 130]})
            >>> transformer = TimeSeriesTransformer('growth_rate')
            >>> result = transformer.transform(df)
            >>> print(result)
            #    revenue  revenue_growth
            # 0      100             NaN
            # 1      110            10.0
            # 2      120            9.09
            # 3      130            8.33
        """
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f"Unsupported data type: {type(data)}. Expected pandas.DataFrame")
        return self._transform_dataframe(data)

    def _transform_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform a DataFrame with time series data.

        Internal method that performs the actual transformation based on
        the configured transformation type.
        """
        result = df.copy()

        if self.transformation_type == "growth_rate":
            for col in df.columns:
                result[f"{col}_growth"] = df[col].pct_change(periods=self.periods) * 100

        elif self.transformation_type == "moving_avg":
            for col in df.columns:
                result[f"{col}_ma{self.window_size}"] = (
                    df[col].rolling(window=self.window_size).mean()
                )

        elif self.transformation_type == "cagr":
            # Assuming the index represents time periods
            n_periods_for_cagr = len(df) - 1

            if n_periods_for_cagr < 1:
                logger.warning("CAGR requires at least 2 periods. Returning NaN for all columns.")
                for col in df.columns:
                    result[f"{col}_cagr"] = pd.NA
            else:
                for col in df.columns:
                    start_val = df[col].iloc[0]
                    end_val = df[col].iloc[-1]

                    if pd.isna(start_val) or pd.isna(end_val) or start_val == 0:
                        result[f"{col}_cagr"] = pd.NA
                        continue

                    ratio = end_val / start_val
                    # Check for negative base with fractional exponent leading to complex numbers
                    if ratio < 0 and (1 / n_periods_for_cagr) % 1 != 0:
                        result[f"{col}_cagr"] = pd.NA
                    else:
                        try:
                            # Ensure result is float, np.power can handle negative base if exponent is integer
                            power_val = np.power(ratio, (1 / n_periods_for_cagr))
                            if np.iscomplex(power_val):  # Should be caught by above, but defensive
                                result[f"{col}_cagr"] = pd.NA
                            else:
                                result[f"{col}_cagr"] = (float(power_val) - 1) * 100
                        except (
                            ValueError,
                            TypeError,
                            ZeroDivisionError,
                        ):  # Catch any math errors
                            result[f"{col}_cagr"] = pd.NA

        elif self.transformation_type == "yoy":
            if self.periods not in [
                4,
                12,
            ]:  # Assuming YoY is typically for quarterly (lag 4) or monthly (lag 12)
                logger.warning(
                    f"For YoY transformation, 'periods' parameter is {self.periods}. "
                    f"This will calculate change over {self.periods} periods. "
                    f"Commonly, 4 (for quarterly data) or 12 (for monthly data) is used for YoY."
                )
            for col in df.columns:
                result[f"{col}_yoy"] = df[col].pct_change(periods=self.periods) * 100

        elif self.transformation_type == "qoq":
            if self.periods not in [
                1,
                3,
            ]:  # Assuming QoQ is typically for quarterly (lag 1) or monthly (lag 3)
                logger.warning(
                    f"For QoQ transformation, 'periods' parameter is {self.periods}. "
                    f"This will calculate change over {self.periods} periods. "
                    f"Commonly, 1 (for quarterly data) or 3 (for monthly data) is used for QoQ."
                )
            for col in df.columns:
                result[f"{col}_qoq"] = df[col].pct_change(periods=self.periods) * 100

        else:
            # This case should ideally be caught by the __init__ validation,
            # but as a safeguard during development:
            raise NotImplementedError(
                f"Transformation type '{self.transformation_type}' is defined in TransformationType enum but not implemented in TimeSeriesTransformer."
            )

        return result

# --- END FILE: fin_statement_model/preprocessing/transformers/time_series.py ---

# --- START FILE: fin_statement_model/statements/__init__.py ---
"""Financial Statements Layer (`fin_statement_model.statements`).

This package provides domain-specific abstractions for defining, building,
managing, and presenting financial statements (like Income Statement,
Balance Sheet, Cash Flow Statement) based on underlying configurations.

It sits above the `core` layer and orchestrates the use of core components
(like `Graph`, `Node`) within the context of financial statement structures.
It utilizes configurations (often YAML files) to define the layout, items,
and calculations of a statement.

Key functionalities include:
  - Defining statement structure (`StatementStructure`, `Section`, `LineItem` etc.)
  - Loading and validating statement configurations (`StatementConfig`).
  - Building `StatementStructure` objects from configurations
    (`StatementStructureBuilder`).
  - Managing multiple loaded statements (`StatementRegistry`).
  - Populating a `core.graph.Graph` with calculation nodes based on statement
    definitions (`populate_graph_from_statement`).
  - Formatting statement data retrieved from a graph into user-friendly formats,
    primarily pandas DataFrames (`StatementFormatter`).
  - High-level functions to streamline common workflows like generating a
    statement DataFrame or exporting statements to files (`create_statement_dataframe`,
    `export_statements_to_excel`).
  - Centralizing ID resolution logic between statement items and graph nodes
    (`IDResolver`).

This package imports from `core` and `io` (indirectly via `factory`), but should
not be imported by `core`.
"""

# Core statement structure components
from .structure import (
    StatementStructure,
    Section,
    LineItem,
    CalculatedLineItem,
    SubtotalLineItem,
    StatementItemType,
    StatementItem,  # Added base item type if needed
)

# Configuration related classes
from .configs.validator import StatementConfig

# Building
from .structure.builder import StatementStructureBuilder

# Registry
from .registry import StatementRegistry

# ID Resolution
from .population.id_resolver import IDResolver

# Data Fetching
from .formatting.data_fetcher import DataFetcher, FetchResult, NodeData

# Item Processors
from .population.item_processors import (
    ProcessorResult,
    ItemProcessor,
    MetricItemProcessor,
    CalculatedItemProcessor,
    SubtotalItemProcessor,
    ItemProcessorManager,
)

# Result Types for Error Handling
from .utilities.result_types import (
    Result,
    Success,
    Failure,
    ErrorDetail,
    ErrorSeverity,
    ErrorCollector,
    OperationResult,
    ValidationResult,
    ProcessingResult,
    combine_results,
)

# Retry Handler
from .utilities.retry_handler import (
    RetryHandler,
    RetryConfig,
    RetryStrategy,
    RetryResult,
    BackoffStrategy,
    ExponentialBackoff,
    LinearBackoff,
    ConstantBackoff,
    retry_with_exponential_backoff,
    retry_on_specific_errors,
)

# Populator
from .population.populator import populate_graph_from_statement

# Formatting
from .formatting.formatter import StatementFormatter

# High-level Orchestration functions (previously Factory)
from .orchestration.factory import (
    create_statement_dataframe,
    export_statements_to_excel,
    export_statements_to_json,
)

# Errors specific to statements
from .errors import StatementError, ConfigurationError

# Public API definition
__all__ = [
    "BackoffStrategy",
    "CalculatedItemProcessor",
    "CalculatedLineItem",
    "ConfigurationError",
    "ConstantBackoff",
    # Data Fetching
    "DataFetcher",
    "ErrorCollector",
    "ErrorDetail",
    "ErrorSeverity",
    "ExponentialBackoff",
    "Failure",
    "FetchResult",
    "IDResolver",
    "ItemProcessor",
    "ItemProcessorManager",
    "LineItem",
    "LinearBackoff",
    "MetricItemProcessor",
    "MetricLineItem",
    "NodeData",
    "OperationResult",
    "ProcessingResult",
    # Item Processors
    "ProcessorResult",
    # Result Types
    "Result",
    "RetryConfig",
    # Retry Handler
    "RetryHandler",
    "RetryResult",
    "RetryStrategy",
    "Section",
    "StatementConfig",
    "StatementError",
    "StatementFormatter",
    "StatementItem",
    "StatementItemType",
    "StatementRegistry",
    "StatementStructure",
    "StatementStructureBuilder",
    "SubtotalItemProcessor",
    "SubtotalLineItem",
    "Success",
    "ValidationResult",
    "combine_results",
    "create_statement_dataframe",
    "export_statements_to_excel",
    "export_statements_to_json",
    "populate_graph_from_statement",
    "retry_on_specific_errors",
    "retry_with_exponential_backoff",
    # --- Removed --- #
    # "FinancialStatementGraph", (unless reintroduced)
    # "StatementFactory", (class)
    # "StatementManager",
]

# Note: FinancialStatementGraph removed as part of refactor, assuming its
# responsibilities are covered by core Graph and statement-specific components.

# --- END FILE: fin_statement_model/statements/__init__.py ---

# --- START FILE: fin_statement_model/statements/configs/__init__.py ---
"""Configuration handling for financial statements.

This package provides:
- Pydantic models for configuration validation
- Configuration file loading utilities
- StatementConfig class for managing configurations
"""

from .loader import load_config_file, load_config_directory
from .models import (
    BaseItemModel,
    CalculatedItemModel,
    CalculationSpec,
    LineItemModel,
    MetricItemModel,
    SectionModel,
    StatementModel,
    SubtotalModel,
)
from .validator import StatementConfig

__all__ = [
    # Models
    "BaseItemModel",
    "CalculatedItemModel",
    "CalculationSpec",
    "LineItemModel",
    "MetricItemModel",
    "SectionModel",
    # Validator
    "StatementConfig",
    "StatementModel",
    "SubtotalModel",
    "load_config_directory",
    # Loader functions
    "load_config_file",
]

# --- END FILE: fin_statement_model/statements/configs/__init__.py ---

# --- START FILE: fin_statement_model/statements/configs/loader.py ---
"""Configuration file loader for statement configurations.

This module handles reading statement configuration files from disk,
delegating to the IO layer for actual file operations.
"""

import logging
from typing import Any

from fin_statement_model.io import (
    read_statement_config_from_path,
    read_statement_configs_from_directory,
)

logger = logging.getLogger(__name__)

__all__ = ["load_config_directory", "load_config_file"]


def load_config_file(config_path: str) -> dict[str, Any]:
    """Load a single statement configuration file.

    Args:
        config_path: Path to the configuration file.

    Returns:
        Dictionary containing the configuration data.

    Raises:
        ReadError: If the file cannot be read.
        FileNotFoundError: If the file doesn't exist.
    """
    try:
        return read_statement_config_from_path(config_path)
    except:
        logger.exception(f"Failed to load config file {config_path}")
        raise


def load_config_directory(config_dir: str) -> dict[str, dict[str, Any]]:
    """Load all statement configuration files from a directory.

    Args:
        config_dir: Path to the directory containing config files.

    Returns:
        Dictionary mapping config names to configuration data.

    Raises:
        ReadError: If any file cannot be read.
        FileNotFoundError: If the directory doesn't exist.
    """
    try:
        return read_statement_configs_from_directory(config_dir)
    except:
        logger.exception(f"Failed to load configs from directory {config_dir}")
        raise

# --- END FILE: fin_statement_model/statements/configs/loader.py ---

# --- START FILE: fin_statement_model/statements/configs/models.py ---
"""Define Pydantic models for statement configuration.

This module defines Pydantic models for validating statement configuration data,
including statements, sections, line items, calculations, and subtotals.
"""

from __future__ import annotations

from typing import Any, Optional, Union, Literal

from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator


class CalculationSpec(BaseModel):
    """Define a calculation specification.

    Args:
        type: Type identifier for the calculation (e.g., 'addition', 'subtraction').
        inputs: List of input node or line item IDs referenced by this calculation.
    """

    type: str = Field(
        ...,
        description="Type identifier for the calculation (e.g., 'addition', 'subtraction').",
    )
    inputs: list[str] = Field(
        ...,
        description="List of input node or line item IDs referenced by this calculation.",
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class BaseItemModel(BaseModel):
    """Define common fields for all statement items.

    Args:
        id: Unique identifier for the item. Must not contain spaces.
        name: Human-readable name of the item.
        description: Optional description for the item.
        metadata: Optional metadata dictionary for the item.
        sign_convention: Sign convention for the item (1 or -1).
    """

    id: str = Field(
        ...,
        description="Unique identifier for the item. Must not contain spaces.",
    )
    name: str = Field(..., description="Human-readable name of the item.")
    description: Optional[str] = Field(
        "", description="Optional description for the item."
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Optional metadata for the item."
    )
    sign_convention: int = Field(
        1, description="Sign convention for the item (1 or -1)."
    )

    @field_validator("id", mode="before")
    def id_must_not_contain_spaces(cls, value: str) -> str:
        """Ensure that 'id' does not contain spaces."""
        if " " in value:
            raise ValueError("must not contain spaces")
        return value

    model_config = ConfigDict(extra="forbid", frozen=True)


class LineItemModel(BaseItemModel):
    """Define a basic line item configuration model.

    Args:
        type: Must be 'line_item' for this model.
        node_id: ID of the core node this line item maps to.
    """

    type: Literal["line_item"] = Field(
        "line_item", description="Discriminator for basic line items."
    )
    node_id: str = Field(..., description="ID of the core node this line item maps to.")

    model_config = ConfigDict(extra="forbid", frozen=True)


class MetricItemModel(BaseItemModel):
    """Define a metric-based line item configuration model.

    Args:
        type: Must be 'metric' for this model.
        metric_id: ID of the metric in the core registry.
        inputs: Mapping of metric input names to statement item IDs.
    """

    type: Literal["metric"] = Field(
        "metric", description="Discriminator for metric-based items."
    )
    metric_id: str = Field(
        ..., description="ID of the metric in the core.metrics.registry."
    )
    inputs: dict[str, str] = Field(
        ..., description="Mapping of metric input names to statement item IDs."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class CalculatedItemModel(BaseItemModel):
    """Define a calculated line item configuration model.

    Args:
        type: Must be 'calculated' for this model.
        calculation: Calculation specification for the calculated item.
    """

    type: Literal["calculated"] = Field(
        "calculated", description="Discriminator for calculated items."
    )
    calculation: CalculationSpec = Field(
        ..., description="Calculation specification for the calculated item."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class SubtotalModel(BaseItemModel):
    """Define a subtotal configuration model.

    Args:
        type: Must be 'subtotal' for this model.
        calculation: Optional calculation specification for the subtotal.
        items_to_sum: Optional list of item IDs to sum for the subtotal.
    """

    type: Literal["subtotal"] = Field(
        "subtotal", description="Discriminator for subtotal items."
    )
    calculation: Optional[CalculationSpec] = Field(
        None, description="Calculation specification for the subtotal."
    )
    items_to_sum: Optional[list[str]] = Field(
        None, description="List of item IDs to sum for the subtotal."
    )

    @model_validator(mode="before")
    def exactly_one_of_calculation_or_items(
        cls, values: dict[str, Any]
    ) -> dict[str, Any]:
        """Ensure exactly one of 'calculation' or 'items_to_sum' is provided."""
        calc, items = values.get("calculation"), values.get("items_to_sum")
        if bool(calc) == bool(items):
            raise ValueError(
                "must provide exactly one of 'calculation' or 'items_to_sum'"
            )
        return values

    model_config = ConfigDict(extra="forbid", frozen=True)


class SectionModel(BaseItemModel):
    """Define a nested section within the statement configuration.

    Args:
        type: Must be 'section' for this model.
        items: List of line items, calculated items, subtotals, or nested sections.
        subsections: List of nested sections.
        subtotal: Optional subtotal configuration for this section.
    """

    type: Literal["section"] = Field(
        "section", description="Discriminator for nested sections."
    )
    items: list[
        Union[
            LineItemModel,
            CalculatedItemModel,
            MetricItemModel,
            SubtotalModel,
            SectionModel,
        ]
    ] = Field(
        default_factory=list,
        description=(
            "List of line items, calculated items, subtotals, or nested sections."
        ),
    )
    subsections: list[SectionModel] = Field(
        default_factory=list,
        description="List of nested sections.",
    )
    subtotal: Optional[SubtotalModel] = Field(
        None, description="Optional subtotal configuration for this section."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)

    @model_validator(mode="after")
    def check_unique_item_ids(cls, section: SectionModel) -> SectionModel:
        """Ensure that item and subsection IDs within a section are unique and subtotal refs valid."""
        ids = [item.id for item in section.items] + [
            sub.id for sub in section.subsections
        ]
        duplicates = {item_id for item_id in ids if ids.count(item_id) > 1}
        if duplicates:
            raise ValueError(
                f"Duplicate item id(s) in section '{section.id}': {', '.join(duplicates)}"
            )
        if section.subtotal and section.subtotal.items_to_sum is not None:
            valid_ids = [item.id for item in section.items]
            missing = [i for i in section.subtotal.items_to_sum if i not in valid_ids]
            if missing:
                raise ValueError(
                    f"Section '{section.id}' subtotal references undefined ids: {', '.join(missing)}"
                )
        return section


SectionModel.model_rebuild(force=True)


class StatementModel(BaseModel):
    """Define the top-level statement configuration model.

    Args:
        id: Unique identifier for the statement. Must not contain spaces.
        name: Human-readable name of the statement.
        description: Optional description of the statement.
        metadata: Optional metadata dictionary.
        sections: List of top-level sections in the statement.
    """

    id: str = Field(
        ..., description="Unique statement identifier. Must not contain spaces."
    )
    name: str = Field(..., description="Human-readable statement name.")
    description: Optional[str] = Field(
        "", description="Optional statement description."
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Optional metadata dictionary."
    )
    sections: list[SectionModel] = Field(
        ..., description="List of top-level sections in the statement."
    )

    @field_validator("id", mode="before")
    def id_must_not_contain_spaces(cls, value: str) -> str:
        """Ensure that statement 'id' does not contain spaces."""
        if " " in value:
            raise ValueError("must not contain spaces")
        return value

    model_config = ConfigDict(extra="forbid", frozen=True)

    @model_validator(mode="after")
    def check_unique_section_ids(cls, model: StatementModel) -> StatementModel:
        """Ensure that top-level section IDs are unique."""
        ids = [section.id for section in model.sections]
        duplicates = {sec_id for sec_id in ids if ids.count(sec_id) > 1}
        if duplicates:
            raise ValueError(f"Duplicate section id(s): {', '.join(duplicates)}")
        return model

# --- END FILE: fin_statement_model/statements/configs/models.py ---

# --- START FILE: fin_statement_model/statements/configs/validator.py ---
"""Statement configuration handling for Financial Statement Model.

This module provides utilities for parsing and validating statement configuration data
(provided as a dictionary) and building StatementStructure objects.
"""

# Removed json, yaml, Path imports as file loading moved to IO
import logging
from typing import Any, Optional

# Use absolute imports
# Import Pydantic models for building from validated configuration
from fin_statement_model.statements.configs.models import (
    StatementModel,
)
from pydantic import ValidationError  # Import directly

# Configure logging
logger = logging.getLogger(__name__)


class StatementConfig:
    """Manages configuration parsing and building for financial statement structures.

    This class handles validating statement configuration data (provided as a dictionary)
    and building StatementStructure objects from these configurations.
    It does NOT handle file loading.
    """

    def __init__(self, config_data: dict[str, Any]):
        """Initialize a statement configuration processor.

        Args:
            config_data: Dictionary containing the raw configuration data.

        Raises:
            ValueError: If config_data is not a non-empty dictionary.
        """
        if not config_data or not isinstance(config_data, dict):
            raise ValueError("config_data must be a non-empty dictionary.")
        self.config_data = config_data
        # Remove config_path attribute
        # self.config_path = None # No longer needed
        self.model: Optional[StatementModel] = None  # Store validated model

    # Removed load_config method
    # def load_config(self, config_path: str) -> None:
    #     ...

    def validate_config(self) -> list[str]:
        """Validate the configuration data using Pydantic models.

        Returns:
            list[str]: List of validation errors, or empty list if valid.
                     Stores the validated model in self.model on success.
        """
        try:
            # Validate against Pydantic StatementModel
            # Removed redundant import from inside method
            self.model = StatementModel.model_validate(self.config_data)
            return []
        except ValidationError as ve:
            # Convert Pydantic errors to list of strings
            errors: list[str] = []
            for err in ve.errors():
                loc = ".".join(str(x) for x in err.get("loc", []))
                msg = err.get("msg", "")
                errors.append(f"{loc}: {msg}")
            self.model = None  # Ensure model is not set on validation error
            return errors
        except Exception as e:
            # Catch other potential validation issues
            logger.exception("Unexpected error during configuration validation")
            self.model = None
            return [f"Unexpected validation error: {e}"]

    # Removed build_statement_structure and helper methods (_build_section_model, etc.)
    # This logic is now moved to StatementStructureBuilder

    # def build_statement_structure(self) -> StatementStructure:
    #     ...
    #
    # def _build_section_model(self, section_model: SectionModel) -> Section:
    #     ...
    #
    # def _build_item_model(...):
    #     ...
    #
    # def _build_subtotal_model(...):
    #     ...

# --- END FILE: fin_statement_model/statements/configs/validator.py ---

# --- START FILE: fin_statement_model/statements/errors.py ---
"""Custom Exception classes for the `fin_statement_model.statements` package.

These exceptions provide more specific error information related to statement
definition, configuration, building, and processing, inheriting from the base
`FinancialModelError` defined in `fin_statement_model.core.errors`.
"""

from typing import Optional
from fin_statement_model.core.errors import FinancialModelError

__all__ = ["ConfigurationError", "StatementError"]


class StatementError(FinancialModelError):
    """Base exception for errors specific to the statements package.

    Indicates a general issue related to statement structure, processing, or
    management (e.g., duplicate registration, invalid item type).
    """


class ConfigurationError(StatementError):
    """Exception raised for errors during statement configuration processing.

    This includes errors encountered while loading, parsing, validating, or
    building statement structures from configuration files (e.g., YAML).

    Attributes:
        message (str): The main error message summarizing the issue.
        config_path (Optional[str]): The path to the configuration file that
            caused the error, if applicable.
        errors (List[str]): A list of specific validation errors or details
            related to the configuration issue.
    """

    def __init__(
        self,
        message: str,
        config_path: Optional[str] = None,
        errors: Optional[list[str]] = None,
    ):
        """Initialize a ConfigurationError.

        Args:
            message: The primary error message describing the configuration issue.
            config_path: Optional path to the configuration file involved.
            errors: Optional list of specific validation errors or related details.
        """
        self.message = message
        self.config_path = config_path
        self.errors = errors or []

        # Build detailed error message
        details = []
        if config_path:
            details.append(f"Config file: {config_path}")
        if errors:
            details.append("Validation errors:")
            details.extend([f"  - {error}" for error in errors])

        full_message = message
        if details:
            full_message = f"{message}\n" + "\n".join(details)

        super().__init__(full_message)

# --- END FILE: fin_statement_model/statements/errors.py ---

# --- START FILE: fin_statement_model/statements/formatting/__init__.py ---
"""Formatting and data fetching for financial statements.

This package provides tools for:
- Fetching data from graphs for statement display
- Formatting statements as DataFrames
- Applying formatting rules and conventions
"""

from .data_fetcher import DataFetcher, FetchResult, NodeData
from .formatter import StatementFormatter

__all__ = [
    # Data Fetching
    "DataFetcher",
    "FetchResult",
    "NodeData",
    # Formatting
    "StatementFormatter",
]

# --- END FILE: fin_statement_model/statements/formatting/__init__.py ---

# --- START FILE: fin_statement_model/statements/formatting/_formatting_utils.py ---
"""Utility functions for formatting statement DataFrames."""

import pandas as pd
from typing import Optional, Any  # Keep necessary imports
from pandas.api.types import is_numeric_dtype


def apply_sign_convention(df: pd.DataFrame, period_columns: list[str]) -> pd.DataFrame:
    """Apply sign conventions to the statement values across periods."""
    result = df.copy()
    if "sign_convention" in result.columns:
        for col in period_columns:
            if col in result.columns and is_numeric_dtype(result[col]):
                mask = result[col].notna()
                # Ensure sign_convention is treated as numeric if needed
                sign_col = pd.to_numeric(
                    result.loc[mask, "sign_convention"], errors="coerce"
                ).fillna(1)
                result.loc[mask, col] = result.loc[mask, col] * sign_col
    return result


def format_numbers(
    df: pd.DataFrame,
    default_formats: dict[str, Any],  # Pass defaults needed
    number_format: Optional[str] = None,
    period_columns: Optional[list[str]] = None,
) -> pd.DataFrame:
    """Format numeric values in the statement.

    Args:
        df: DataFrame to format numbers in
        default_formats: Dictionary containing default formatting options
                         (e.g., 'precision', 'use_thousands_separator').
        number_format: Optional format string
        period_columns: List of columns containing period data to format.
                        If None, attempts to format all numeric columns
                        except metadata/indicators.

    Returns:
        pd.DataFrame: DataFrame with formatted numbers
    """
    result = df.copy()

    if period_columns:
        numeric_cols = [
            col
            for col in period_columns
            if col in result.columns and is_numeric_dtype(result[col])
        ]
    else:
        # Original logic if period_columns not specified
        numeric_cols = [
            col
            for col in result.columns
            if is_numeric_dtype(result[col])
            and col not in ("sign_convention", "depth", "ID")  # Added ID
            and not col.startswith("meta_")
            and col != "Line Item"  # Ensure Line Item name is not formatted
        ]

    # Get defaults from the passed dictionary
    precision = default_formats.get("precision", 2)  # Provide fallback default
    use_thousands = default_formats.get("use_thousands_separator", True)

    if number_format:
        # Use provided format string
        for col in numeric_cols:
            # Check if column exists before applying format
            if col in result.columns:
                result[col] = result[col].apply(
                    lambda x: f"{x:{number_format}}" if pd.notna(x) else ""
                )
    else:
        # Use default formatting based on passed defaults
        for col in numeric_cols:
            # Check if column exists before applying format
            if col in result.columns:
                result[col] = result[col].apply(
                    lambda x: (
                        (f"{x:,.{precision}f}" if pd.notna(x) else "")
                        if use_thousands
                        else (f"{x:.{precision}f}" if pd.notna(x) else "")
                    )
                )

    return result

# --- END FILE: fin_statement_model/statements/formatting/_formatting_utils.py ---

# --- START FILE: fin_statement_model/statements/formatting/data_fetcher.py ---
"""Data fetching functionality for financial statements.

This module provides the DataFetcher class that handles retrieving data from
the graph for statement formatting. It encapsulates the logic for resolving
item IDs to node IDs and fetching values with proper error handling.
"""

import logging
from dataclasses import dataclass
from typing import Optional

import numpy as np
import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import NodeError, CalculationError
from fin_statement_model.core.adjustments.models import AdjustmentFilterInput
from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.statements.population.id_resolver import IDResolver
from fin_statement_model.statements.utilities.result_types import (
    Result,
    Success,
    Failure,
    ErrorDetail,
    ErrorSeverity,
    ErrorCollector,
)

logger = logging.getLogger(__name__)

__all__ = ["DataFetcher", "FetchResult", "NodeData"]


@dataclass
class NodeData:
    """Data for a single node across all periods.

    Attributes:
        node_id: The graph node ID
        values: Dict mapping period to value
        is_adjusted: Dict mapping period to bool indicating if adjusted
        errors: Any errors encountered during fetching
    """

    node_id: str
    values: dict[str, float]
    is_adjusted: dict[str, bool]
    errors: list[ErrorDetail]

    @property
    def has_data(self) -> bool:
        """Check if any non-NaN values exist."""
        return any(pd.notna(v) for v in self.values.values())


@dataclass
class FetchResult:
    """Result of fetching data for a statement.

    Attributes:
        data: Dict mapping node_id to period values
        errors: ErrorCollector with any errors/warnings
        node_count: Number of nodes successfully fetched
        missing_nodes: List of node IDs that couldn't be found
    """

    data: dict[str, dict[str, float]]
    errors: ErrorCollector
    node_count: int
    missing_nodes: list[str]

    def to_result(self) -> Result[dict[str, dict[str, float]]]:
        """Convert to Result type."""
        if self.errors.has_errors():
            return Failure(errors=self.errors.get_errors())
        return Success(value=self.data)


class DataFetcher:
    """Fetches data from graph for statement formatting.

    This class encapsulates the logic for:
    - Resolving statement item IDs to graph node IDs
    - Fetching values from the graph with error handling
    - Applying adjustment filters if specified
    - Collecting errors and warnings during the process
    """

    def __init__(self, statement: StatementStructure, graph: Graph):
        """Initialize the data fetcher.

        Args:
            statement: The statement structure to fetch data for
            graph: The graph containing the data
        """
        self.statement = statement
        self.graph = graph
        self.id_resolver = IDResolver(statement)

    def fetch_all_data(
        self,
        adjustment_filter: Optional[AdjustmentFilterInput] = None,
        include_missing: bool = False,
    ) -> FetchResult:
        """Fetch data for all items in the statement.

        Args:
            adjustment_filter: Optional filter for adjustments
            include_missing: If True, include nodes that don't exist in graph
                           with NaN values

        Returns:
            FetchResult containing the fetched data and any errors
        """
        error_collector = ErrorCollector()
        data: dict[str, dict[str, float]] = {}
        missing_nodes: list[str] = []

        # Check if graph has periods
        periods = self.graph.periods
        if not periods:
            error_collector.add_error(
                code="no_periods",
                message=f"Graph has no periods defined for statement '{self.statement.id}'",
                source=self.statement.id,
            )
            return FetchResult(
                data={}, errors=error_collector, node_count=0, missing_nodes=[]
            )

        logger.debug(
            f"Fetching data for statement '{self.statement.id}' across {len(periods)} periods"
        )

        # Get all items and resolve their node IDs
        all_items = self.statement.get_all_items()
        processed_node_ids = set()

        for item in all_items:
            # Resolve item ID to node ID
            node_id = self.id_resolver.resolve(item.id, self.graph)

            if not node_id:
                error_collector.add_warning(
                    code="unresolvable_item",
                    message=f"Cannot resolve item '{item.id}' to a node ID",
                    source=item.id,
                    context="IDResolver.resolve",
                )
                continue

            if node_id in processed_node_ids:
                continue  # Skip already processed nodes

            processed_node_ids.add(node_id)

            # Fetch data for this node
            node_result = self._fetch_node_data(
                node_id, periods, adjustment_filter, item_id=item.id
            )

            if node_result.is_success():
                node_data = node_result.get_value()
                if node_data.has_data or include_missing:
                    data[node_id] = node_data.values

                # Add any warnings from node fetching
                for error in node_data.errors:
                    if error.severity == ErrorSeverity.WARNING:
                        error_collector.add_warning(
                            error.code,
                            error.message,
                            error.context,
                            error.source or item.id,
                        )
            else:
                # Node doesn't exist in graph
                missing_nodes.append(node_id)
                if include_missing:
                    # Fill with NaN values
                    data[node_id] = {period: np.nan for period in periods}

                error_collector.add_from_result(node_result, source=item.id)

        logger.info(
            f"Fetched data for {len(data)} nodes from statement '{self.statement.id}'. "
            f"Missing: {len(missing_nodes)}, Warnings: {len(error_collector.get_warnings())}"
        )

        return FetchResult(
            data=data,
            errors=error_collector,
            node_count=len(data),
            missing_nodes=missing_nodes,
        )

    def _fetch_node_data(
        self,
        node_id: str,
        periods: list[str],
        adjustment_filter: Optional[AdjustmentFilterInput],
        item_id: Optional[str] = None,
    ) -> Result[NodeData]:
        """Fetch data for a single node across all periods.

        Args:
            node_id: The graph node ID to fetch
            periods: List of periods to fetch
            adjustment_filter: Optional adjustment filter
            item_id: Optional statement item ID for error context

        Returns:
            Result containing NodeData or error details
        """
        # Check if node exists
        if not self.graph.has_node(node_id):
            return Failure(
                [
                    ErrorDetail(
                        code="node_not_found",
                        message=f"Node '{node_id}' not found in graph",
                        source=item_id or node_id,
                        severity=ErrorSeverity.WARNING,
                    )
                ]
            )

        values = {}
        is_adjusted = {}
        errors = []

        for period in periods:
            try:
                # Fetch value with optional adjustments
                value = self.graph.get_adjusted_value(
                    node_id,
                    period,
                    filter_input=adjustment_filter,
                    return_flag=False,  # Only need the value
                )
                # Ensure value is float or NaN
                values[period] = float(value) if pd.notna(value) else np.nan
                is_adjusted[period] = bool(value)

            except (NodeError, CalculationError) as e:
                # Expected errors - log as warning
                logger.warning(
                    f"Error calculating node '{node_id}' for period '{period}': {e}"
                )
                values[period] = np.nan
                is_adjusted[period] = False
                errors.append(
                    ErrorDetail(
                        code="calculation_error",
                        message=f"Failed to calculate value: {e}",
                        context=f"period={period}",
                        severity=ErrorSeverity.WARNING,
                        source=item_id or node_id,
                    )
                )

            except TypeError as e:
                # Filter/adjustment errors
                logger.warning(
                    f"Type error for node '{node_id}', period '{period}': {e}"
                )
                values[period] = np.nan
                is_adjusted[period] = False
                errors.append(
                    ErrorDetail(
                        code="filter_error",
                        message=f"Invalid adjustment filter: {e}",
                        context=f"period={period}",
                        severity=ErrorSeverity.WARNING,
                        source=item_id or node_id,
                    )
                )

            except Exception as e:
                # Unexpected errors - log as error
                logger.error(
                    f"Unexpected error for node '{node_id}', period '{period}': {e}",
                    exc_info=True,
                )
                values[period] = np.nan
                is_adjusted[period] = False
                errors.append(
                    ErrorDetail(
                        code="unexpected_error",
                        message=f"Unexpected error: {e}",
                        context=f"period={period}, error_type={type(e).__name__}",
                        severity=ErrorSeverity.ERROR,
                        source=item_id or node_id,
                    )
                )

        return Success(
            NodeData(
                node_id=node_id, values=values, is_adjusted=is_adjusted, errors=errors
            )
        )

    def check_adjustments(
        self,
        node_ids: list[str],
        periods: list[str],
        adjustment_filter: Optional[AdjustmentFilterInput] = None,
    ) -> dict[str, dict[str, bool]]:
        """Check which node/period combinations have adjustments.

        Args:
            node_ids: List of node IDs to check
            periods: List of periods to check
            adjustment_filter: Filter to check adjustments against

        Returns:
            Dict mapping node_id -> period -> was_adjusted boolean
        """
        results = {}

        for node_id in node_ids:
            if not self.graph.has_node(node_id):
                results[node_id] = {period: False for period in periods}
                continue

            period_results = {}
            for period in periods:
                try:
                    was_adjusted = self.graph.was_adjusted(
                        node_id, period, adjustment_filter
                    )
                    period_results[period] = bool(was_adjusted)
                except Exception as e:
                    logger.warning(
                        f"Error checking adjustments for {node_id}/{period}: {e}"
                    )
                    period_results[period] = False

            results[node_id] = period_results

        return results

# --- END FILE: fin_statement_model/statements/formatting/data_fetcher.py ---

# --- START FILE: fin_statement_model/statements/formatting/formatter.py ---
"""Formatter for financial statements.

This module provides functionality for formatting financial statements
for display or reporting, including applying formatting rules, adding subtotals,
and applying sign conventions.
"""

import pandas as pd
import numpy as np  # Added numpy for NaN handling
from typing import Optional, Any, Union
import logging

from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.statements.structure import (
    Section,
    CalculatedLineItem,
    SubtotalLineItem,
    StatementItem,
)

# Add core Graph and errors
from fin_statement_model.core.graph import Graph

# Import adjustment types for filtering
from fin_statement_model.core.adjustments.models import AdjustmentFilterInput

# Import the ID resolver
from fin_statement_model.statements.population.id_resolver import IDResolver

# Import the data fetcher
from fin_statement_model.statements.formatting.data_fetcher import DataFetcher

# Import the new formatting utils
from ._formatting_utils import format_numbers
from ._formatting_utils import apply_sign_convention as apply_sign_convention_func

# Configure logging
logger = logging.getLogger(__name__)


class StatementFormatter:
    """Formats financial statements for display or reporting.

    This class provides methods to transform raw financial data into
    formatted financial statements with proper headers, indentation,
    subtotals, and sign conventions.
    """

    def __init__(self, statement: StatementStructure):
        """Initialize a statement formatter.

        Args:
            statement: The statement structure to format
        """
        self.statement = statement
        self.config = {}  # TODO: Consider how config is passed/used
        self.default_formats = {
            "precision": 2,
            "use_thousands_separator": True,
            "show_zero_values": True,  # TODO: Check if used
            "show_negative_sign": True,  # TODO: Check if used
            "indent_character": "  ",
            "subtotal_style": "bold",  # TODO: Check if used
            "total_style": "bold",  # TODO: Check if used
            "header_style": "bold",  # TODO: Check if used
        }

    def generate_dataframe(
        self,
        graph: Graph,
        should_apply_signs: bool = True,  # Renamed arg
        include_empty_items: bool = False,
        number_format: Optional[str] = None,
        include_metadata_cols: bool = False,
        # --- Adjustment Integration ---
        adjustment_filter: AdjustmentFilterInput = None,
        add_is_adjusted_column: bool = False,
        # --- End Adjustment Integration ---
    ) -> pd.DataFrame:
        """Generate a formatted DataFrame of the statement including subtotals.

        Queries the graph for data based on the statement structure,
        calculates subtotals, and formats the result.

        Args:
            graph: The core.graph.Graph instance containing the data.
            should_apply_signs: Whether to apply sign conventions after calculation.
            include_empty_items: Whether to include items with no data rows.
            number_format: Optional Python format string for numbers (e.g., ',.2f').
            include_metadata_cols: If True, includes hidden metadata columns
                                   (like sign_convention, node_id) in the output.
            adjustment_filter: Optional filter for applying adjustments during data fetch.
            add_is_adjusted_column: If True, adds a boolean column indicating if the
                                    value for a node/period was adjusted.

        Returns:
            pd.DataFrame: Formatted statement DataFrame with subtotals.
        """
        # Use DataFetcher to get data
        data_fetcher = DataFetcher(self.statement, graph)
        fetch_result = data_fetcher.fetch_all_data(
            adjustment_filter=adjustment_filter,
            include_missing=include_empty_items,
        )

        # Log any warnings/errors from fetching
        if fetch_result.errors.has_warnings() or fetch_result.errors.has_errors():
            fetch_result.errors.log_all(
                prefix=f"Statement '{self.statement.id}' data fetch: "
            )

        data = fetch_result.data
        all_periods = graph.periods

        # Initialize ID resolver for consistent node ID resolution
        id_resolver = IDResolver(self.statement)

        # --- Build Rows Recursively --- #
        rows: list[dict[str, Any]] = []
        indent_char = self.default_formats["indent_character"]

        def process_recursive(
            items_or_sections: list[Union[Section, StatementItem]], current_depth: int
        ) -> None:
            for item in items_or_sections:
                if isinstance(item, Section):
                    process_recursive(item.items, current_depth + 1)
                    if hasattr(item, "subtotal") and item.subtotal:
                        process_recursive(
                            [item.subtotal], current_depth + 1
                        )  # Process subtotal like other items
                elif isinstance(item, StatementItem):
                    # Use ID resolver to get the correct node ID
                    node_id = id_resolver.resolve(item.id, graph)
                    if node_id:
                        item_data = data.get(node_id, {})
                        row_values = {p: item_data.get(p, np.nan) for p in all_periods}

                        if include_empty_items or any(
                            pd.notna(v) for v in row_values.values()
                        ):
                            row = {
                                "Line Item": indent_char * current_depth + item.name,
                                "ID": item.id,
                                **row_values,
                                # Metadata
                                "line_type": self._get_item_type(item),
                                "node_id": node_id,
                                "sign_convention": getattr(item, "sign_convention", 1),
                                "is_subtotal": isinstance(item, SubtotalLineItem),
                                "is_calculated": isinstance(item, CalculatedLineItem),
                            }
                            rows.append(row)

        process_recursive(self.statement.sections, 0)
        # --- End Build Rows --- #

        if not rows:
            return pd.DataFrame(columns=["Line Item", "ID", *all_periods])

        df = pd.DataFrame(rows)

        base_cols = ["Line Item", "ID"]
        metadata_cols = [
            "line_type",
            "node_id",
            "sign_convention",
            "is_subtotal",
            "is_calculated",
        ]

        # --- Adjustment Integration: Add 'is_adjusted' column if requested ---
        adjusted_flag_cols = []
        if add_is_adjusted_column and all_periods:
            # Get node IDs from the dataframe
            node_ids_to_check = []
            for _, row in df.iterrows():
                node_id = row.get("node_id")
                is_calc_or_subtotal = row.get("is_calculated", False) or row.get(
                    "is_subtotal", False
                )
                if node_id and not is_calc_or_subtotal:
                    node_ids_to_check.append(node_id)

            # Use DataFetcher to check adjustments
            if node_ids_to_check:
                adjustment_status = data_fetcher.check_adjustments(
                    node_ids_to_check, all_periods, adjustment_filter
                )
            else:
                adjustment_status = {}

            # Build adjustment columns
            is_adjusted_data = []
            for _, row in df.iterrows():
                node_id = row.get("node_id")
                is_calc_or_subtotal = row.get("is_calculated", False) or row.get(
                    "is_subtotal", False
                )

                if node_id and not is_calc_or_subtotal and node_id in adjustment_status:
                    row_adj_flags = {
                        f"{period}_is_adjusted": adjustment_status[node_id].get(
                            period, False
                        )
                        for period in all_periods
                    }
                else:
                    # For calculated/subtotal items or missing nodes, flags are False
                    row_adj_flags = {
                        f"{period}_is_adjusted": False for period in all_periods
                    }
                is_adjusted_data.append(row_adj_flags)

            if is_adjusted_data:
                adj_df = pd.DataFrame(is_adjusted_data, index=df.index)
                df = pd.concat([df, adj_df], axis=1)
                adjusted_flag_cols = list(adj_df.columns)
        # --- End Adjustment Integration ---

        final_cols = base_cols + all_periods
        if add_is_adjusted_column:
            final_cols += adjusted_flag_cols  # Add adjustment flag columns here
        if include_metadata_cols:
            # Add metadata cols (excluding adjustment flags if they are already added)
            final_cols += [
                m_col for m_col in metadata_cols if m_col not in adjusted_flag_cols
            ]

        for col in final_cols:
            if col not in df.columns:
                df[col] = (
                    np.nan
                    if col in all_periods
                    else ("" if col == "Line Item" else None)
                )

        df = df[final_cols]

        # Use imported function and renamed argument
        if should_apply_signs:  # Check the renamed argument
            # Call the imported function
            df = apply_sign_convention_func(df, all_periods)

        # Use imported function for number formatting
        df = format_numbers(
            df,
            default_formats=self.default_formats,
            number_format=number_format,
            period_columns=all_periods,
        )

        return df

    def _get_item_type(self, item: StatementItem) -> str:
        """Get the type of a statement item.

        Args:
            item: Statement item to get type for

        Returns:
            str: Item type identifier
        """
        if isinstance(item, Section):
            return "section"
        elif isinstance(item, SubtotalLineItem):
            return "subtotal"
        elif isinstance(item, CalculatedLineItem):
            return "calculated"
        else:
            return "item"

    def format_html(
        self,
        graph: Graph,
        should_apply_signs: bool = True,  # Use consistent arg name
        include_empty_items: bool = False,
        css_styles: Optional[dict[str, str]] = None,
    ) -> str:
        """Format the statement data as HTML.

        Args:
            graph: The core.graph.Graph instance containing the data.
            should_apply_signs: Whether to apply sign conventions.
            include_empty_items: Whether to include items with no data.
            css_styles: Optional dict of CSS styles for the HTML.

        Returns:
            str: HTML string representing the statement.
        """
        df = self.generate_dataframe(
            graph=graph,
            should_apply_signs=should_apply_signs,
            include_empty_items=include_empty_items,
            # number_format is applied internally by generate_dataframe
        )
        html = df.to_html(index=False)
        if css_styles:
            style_str = "<style>\n"
            for selector, style in css_styles.items():
                style_str += f"{selector} {{ {style} }}\n"
            style_str += "</style>\n"
            html = style_str + html
        return html

# --- END FILE: fin_statement_model/statements/formatting/formatter.py ---

# --- START FILE: fin_statement_model/statements/loader.py ---
"""Statement loading and building functionality.

This module handles the loading of statement configurations from files or directories,
validates them, builds statement structures, and registers them with the registry.
"""

import logging
from pathlib import Path

from fin_statement_model.core.errors import ConfigurationError, StatementError
from fin_statement_model.io import (
    read_statement_config_from_path,
    read_statement_configs_from_directory,
)
from fin_statement_model.io.exceptions import ReadError

from fin_statement_model.statements.structure.builder import StatementStructureBuilder
from fin_statement_model.statements.configs.validator import StatementConfig
from fin_statement_model.statements.registry import StatementRegistry

logger = logging.getLogger(__name__)

__all__ = ["load_build_register_statements"]


def load_build_register_statements(
    config_path_or_dir: str,
    registry: StatementRegistry,
    builder: StatementStructureBuilder,
) -> list[str]:
    """Load, validate, build, and register statement structures from configs.

    This function orchestrates the first part of the statement processing pipeline.
    It reads configurations, validates them using StatementConfig, builds the
    structure using StatementStructureBuilder, and registers them with the
    provided StatementRegistry.

    Args:
        config_path_or_dir: Path to a single statement config file (e.g.,
            'income_statement.yaml') or a directory containing multiple
            config files.
        registry: The StatementRegistry instance to register loaded statements.
        builder: The StatementStructureBuilder instance used to construct
            statement objects from validated configurations.

    Returns:
        A list of statement IDs that were successfully loaded and registered.

    Raises:
        ConfigurationError: If reading or validation of any configuration fails.
        FileNotFoundError: If the `config_path_or_dir` does not exist.
        StatementError: If registration fails (e.g., duplicate ID).
    """
    loaded_statement_ids = []
    errors = []

    try:
        if Path(config_path_or_dir).is_dir():
            raw_configs = read_statement_configs_from_directory(config_path_or_dir)
        elif Path(config_path_or_dir).is_file():
            stmt_id = Path(config_path_or_dir).stem
            raw_config = read_statement_config_from_path(config_path_or_dir)
            raw_configs = {stmt_id: raw_config}
        else:
            raise FileNotFoundError(
                f"Config path is not a valid file or directory: {config_path_or_dir}"
            )

    except (ReadError, FileNotFoundError) as e:
        logger.exception(f"Failed to read configuration from {config_path_or_dir}:")
        raise ConfigurationError(
            message=f"Failed to read config: {e}", config_path=config_path_or_dir
        ) from e

    if not raw_configs:
        logger.warning(f"No statement configurations found at {config_path_or_dir}")
        return []

    for stmt_id, raw_data in raw_configs.items():
        try:
            config = StatementConfig(raw_data)
            validation_errors = config.validate_config()
            if validation_errors:
                raise ConfigurationError(
                    f"Invalid configuration for statement '{stmt_id}'",
                    config_path=f"{config_path_or_dir}/{stmt_id}.ext",  # Placeholder path
                    errors=validation_errors,
                )

            statement = builder.build(config)
            registry.register(statement)  # Raises StatementError on conflict
            loaded_statement_ids.append(statement.id)

        except (ConfigurationError, StatementError, ValueError) as e:
            logger.exception(f"Failed to process/register statement '{stmt_id}':")
            errors.append((stmt_id, str(e)))
        except Exception as e:
            logger.exception(
                f"Unexpected error processing statement '{stmt_id}' from {config_path_or_dir}"
            )
            errors.append((stmt_id, f"Unexpected error: {e!s}"))

    # Handle errors - maybe raise an aggregate error if any occurred?
    if errors:
        # For now, just log a warning, processing continues with successfully
        # loaded statements
        error_details = "; ".join([f"{sid}: {msg}" for sid, msg in errors])
        logger.warning(
            f"Encountered {len(errors)} errors during statement loading/building "
            f"from {config_path_or_dir}: {error_details}"
        )
        # Consider raising an aggregated error if needed for stricter handling

    return loaded_statement_ids

# --- END FILE: fin_statement_model/statements/loader.py ---

# --- START FILE: fin_statement_model/statements/orchestration/__init__.py ---
"""High-level orchestration functions for statement processing.

This package provides the main public API for:
- Creating statement DataFrames from configurations
- Exporting statements to various formats
- Coordinating the overall workflow
"""

from .exporter import export_statements_to_excel, export_statements_to_json
from .factory import create_statement_dataframe
from .orchestrator import populate_graph

__all__ = [
    # Main API
    "create_statement_dataframe",
    "export_statements_to_excel",
    "export_statements_to_json",
    # Internal helpers
    "populate_graph",
]

# --- END FILE: fin_statement_model/statements/orchestration/__init__.py ---

# --- START FILE: fin_statement_model/statements/orchestration/exporter.py ---
"""Statement export functionality.

This module handles exporting financial statements to various file formats
including Excel and JSON. It provides both internal helper functions and
public API functions for exporting statements.
"""

import logging
from pathlib import Path
from typing import Any, Optional
from collections.abc import Callable

import pandas as pd

from fin_statement_model.core.errors import FinancialModelError
from fin_statement_model.core.graph import Graph
from fin_statement_model.io import write_statement_to_excel, write_statement_to_json
from fin_statement_model.io.exceptions import WriteError

logger = logging.getLogger(__name__)

__all__ = [
    "export_statements",
    "export_statements_to_excel",
    "export_statements_to_json",
]


def export_statements(
    graph: Graph,
    config_path_or_dir: str,
    output_dir: str,
    format_kwargs: Optional[dict[str, Any]],
    writer_func: Callable,
    writer_kwargs: Optional[dict[str, Any]],
    file_suffix: str,
) -> None:
    """Generate and export statement DataFrames using a specific writer function.

    Internal helper function that takes generated DataFrames (or generates them
    if needed via `create_statement_dataframe`) and uses the provided
    `writer_func` to save them to disk.

    Args:
        graph: The core.graph.Graph instance.
        config_path_or_dir: Path to config file or directory.
        output_dir: Directory where output files will be saved.
        format_kwargs: Optional arguments for `create_statement_dataframe`.
        writer_func: The function responsible for writing a DataFrame to a file
            (e.g., `write_statement_to_excel`).
        writer_kwargs: Optional arguments passed directly to the `writer_func`.
        file_suffix: The file extension to use for output files (e.g., ".xlsx").

    Raises:
        WriteError: If any errors occur during the file writing process.
        FinancialModelError: If errors occur during DataFrame generation.
        FileNotFoundError: If config path doesn't exist.
    """
    # Import here to avoid circular dependency
    from fin_statement_model.statements.orchestration.orchestrator import (
        create_statement_dataframe,
    )

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    try:
        dfs = create_statement_dataframe(graph, config_path_or_dir, format_kwargs)
    except FinancialModelError:
        logger.exception("Failed to generate statement dataframes for export:")
        raise  # Re-raise critical errors from generation step

    if not dfs:
        logger.warning(
            f"No DataFrames generated, nothing to export to {file_suffix} files."
        )
        return

    # Standardize to dictionary format
    if isinstance(dfs, pd.DataFrame):
        # Try to get a meaningful name if it was a single file
        stmt_id = (
            Path(config_path_or_dir).stem
            if Path(config_path_or_dir).is_file()
            else "statement"
        )
        dfs_dict = {stmt_id: dfs}
    else:
        dfs_dict = dfs

    export_errors = []
    for stmt_id, df in dfs_dict.items():
        # Ensure stmt_id is filename-safe (basic replacement)
        safe_stmt_id = stmt_id.replace("/", "_").replace("\\", "_")
        file_path = output_path / f"{safe_stmt_id}{file_suffix}"
        try:
            writer_func(df, str(file_path), **writer_kwargs)
            logger.info(f"Successfully exported statement '{stmt_id}' to {file_path}")
        except WriteError as e:
            logger.exception(
                f"Failed to write {file_suffix} file for statement '{stmt_id}':"
            )
            export_errors.append((stmt_id, str(e)))
        except Exception as e:
            logger.exception(
                f"Unexpected error exporting statement '{stmt_id}' to {file_suffix}."
            )
            export_errors.append((stmt_id, f"Unexpected export error: {e!s}"))

    if export_errors:
        error_summary = "; ".join([f"{sid}: {err}" for sid, err in export_errors])
        raise WriteError(
            f"Encountered {len(export_errors)} errors during {file_suffix} export: {error_summary}"
        )


def export_statements_to_excel(
    graph: Graph,
    config_path_or_dir: str,
    output_dir: str,
    format_kwargs: Optional[dict[str, Any]] = None,
    writer_kwargs: Optional[dict[str, Any]] = None,
) -> None:
    """Generate statement DataFrames and export them to individual Excel files.

    Loads configurations, builds statements, populates the graph (if necessary),
    generates DataFrames for each statement, and saves each DataFrame to a
    separate `.xlsx` file in the specified `output_dir`.

    Args:
        graph: The core.graph.Graph instance containing necessary data.
        config_path_or_dir: Path to a single statement config file or a
            directory containing multiple config files.
        output_dir: The directory where the resulting Excel files will be saved.
            File names will be derived from the statement IDs (e.g.,
            'income_statement.xlsx').
        format_kwargs: Optional dictionary of arguments passed to
            `StatementFormatter.generate_dataframe` when creating the DataFrames.
        writer_kwargs: Optional dictionary of arguments passed to the underlying
            Excel writer (`write_statement_to_excel`), such as
            `sheet_name` or engine options.

    Raises:
        ConfigurationError: If loading or validating configurations fails.
        StatementError: If processing statements fails critically.
        FileNotFoundError: If `config_path_or_dir` is not found.
        WriteError: If writing any of the Excel files fails.
        FinancialModelError: Potentially other errors from graph operations.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> # Assume 'my_graph' is a pre-populated Graph instance
        >>> # Assume configs exist in './statement_configs/'
        >>> try:
        ...     export_statements_to_excel(
        ...         graph=my_graph,
        ...         config_path_or_dir='./statement_configs/',
        ...         output_dir='./output_excel/',
        ...         writer_kwargs={'freeze_panes': (1, 1)} # Freeze header row/col
        ...     )
        ...     # Use logger.info
        ...     logger.info("Statements exported to ./output_excel/")
        ... except (FileNotFoundError, ConfigurationError, StatementError, WriteError) as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Export failed: {e}")
    """
    export_statements(
        graph=graph,
        config_path_or_dir=config_path_or_dir,
        output_dir=output_dir,
        format_kwargs=format_kwargs or {},
        writer_func=write_statement_to_excel,
        writer_kwargs=writer_kwargs or {},
        file_suffix=".xlsx",
    )


def export_statements_to_json(
    graph: Graph,
    config_path_or_dir: str,
    output_dir: str,
    format_kwargs: Optional[dict[str, Any]] = None,
    writer_kwargs: Optional[dict[str, Any]] = None,
) -> None:
    """Generate statement DataFrames and export them to individual JSON files.

    Loads configurations, builds statements, populates the graph (if necessary),
    generates DataFrames for each statement, and saves each DataFrame to a
    separate `.json` file in the specified `output_dir`.

    Args:
        graph: The core.graph.Graph instance containing necessary data.
        config_path_or_dir: Path to a single statement config file or a
            directory containing multiple config files.
        output_dir: The directory where the resulting JSON files will be saved.
            File names will be derived from the statement IDs (e.g.,
            'balance_sheet.json').
        format_kwargs: Optional dictionary of arguments passed to
            `StatementFormatter.generate_dataframe` when creating the DataFrames.
        writer_kwargs: Optional dictionary of arguments passed to the underlying
            JSON writer (`write_statement_to_json`). Common options
            include `orient` (e.g., 'records', 'columns', 'split') and `indent`.
            Defaults to 'records' orient and indent=2 if not provided.

    Raises:
        ConfigurationError: If loading or validating configurations fails.
        StatementError: If processing statements fails critically.
        FileNotFoundError: If `config_path_or_dir` is not found.
        WriteError: If writing any of the JSON files fails.
        FinancialModelError: Potentially other errors from graph operations.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> # Assume 'my_graph' is a pre-populated Graph instance
        >>> # Assume configs exist in './statement_configs/'
        >>> try:
        ...     export_statements_to_json(
        ...         graph=my_graph,
        ...         config_path_or_dir='./statement_configs/',
        ...         output_dir='./output_json/',
        ...         writer_kwargs={'orient': 'split', 'indent': 4}
        ...     )
        ...     # Use logger.info
        ...     logger.info("Statements exported to ./output_json/")
        ... except (FileNotFoundError, ConfigurationError, StatementError, WriteError) as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Export failed: {e}")
    """
    final_writer_kwargs = writer_kwargs or {}
    # Set JSON specific defaults if not provided
    final_writer_kwargs.setdefault("orient", "records")
    final_writer_kwargs.setdefault("indent", 2)

    export_statements(
        graph=graph,
        config_path_or_dir=config_path_or_dir,
        output_dir=output_dir,
        format_kwargs=format_kwargs or {},
        writer_func=write_statement_to_json,
        writer_kwargs=final_writer_kwargs,
        file_suffix=".json",
    )

# --- END FILE: fin_statement_model/statements/orchestration/exporter.py ---

# --- START FILE: fin_statement_model/statements/orchestration/factory.py ---
"""Statement processing facade.

This module provides the main public API for statement processing,
delegating to specialized modules for specific functionality.

The factory module serves as a convenient entry point that maintains
backward compatibility while the actual implementation is split across:
- orchestrator.py: Main workflow coordination
- loader.py: Configuration loading and validation
- exporter.py: Export functionality
"""

from fin_statement_model.statements.orchestration.exporter import (
    export_statements_to_excel,
    export_statements_to_json,
)
from fin_statement_model.statements.orchestration.orchestrator import (
    create_statement_dataframe,
)

__all__ = [
    "create_statement_dataframe",
    "export_statements_to_excel",
    "export_statements_to_json",
]

# --- END FILE: fin_statement_model/statements/orchestration/factory.py ---

# --- START FILE: fin_statement_model/statements/orchestration/loader.py ---
"""Statement loading and building functionality.

This module handles the loading of statement configurations from files or directories,
validates them, builds statement structures, and registers them with the registry.
"""

import logging
from pathlib import Path

from fin_statement_model.core.errors import ConfigurationError, StatementError
from fin_statement_model.io import (
    read_statement_config_from_path,
    read_statement_configs_from_directory,
)
from fin_statement_model.io.exceptions import ReadError

from fin_statement_model.statements.structure.builder import StatementStructureBuilder
from fin_statement_model.statements.configs.validator import StatementConfig
from fin_statement_model.statements.registry import StatementRegistry

logger = logging.getLogger(__name__)

__all__ = ["load_build_register_statements"]


def load_build_register_statements(
    config_path_or_dir: str,
    registry: StatementRegistry,
    builder: StatementStructureBuilder,
) -> list[str]:
    """Load, validate, build, and register statement structures from configs.

    This function orchestrates the first part of the statement processing pipeline.
    It reads configurations, validates them using StatementConfig, builds the
    structure using StatementStructureBuilder, and registers them with the
    provided StatementRegistry.

    Args:
        config_path_or_dir: Path to a single statement config file (e.g.,
            'income_statement.yaml') or a directory containing multiple
            config files.
        registry: The StatementRegistry instance to register loaded statements.
        builder: The StatementStructureBuilder instance used to construct
            statement objects from validated configurations.

    Returns:
        A list of statement IDs that were successfully loaded and registered.

    Raises:
        ConfigurationError: If reading or validation of any configuration fails.
        FileNotFoundError: If the `config_path_or_dir` does not exist.
        StatementError: If registration fails (e.g., duplicate ID).
    """
    loaded_statement_ids = []
    errors = []

    try:
        if Path(config_path_or_dir).is_dir():
            raw_configs = read_statement_configs_from_directory(config_path_or_dir)
        elif Path(config_path_or_dir).is_file():
            stmt_id = Path(config_path_or_dir).stem
            raw_config = read_statement_config_from_path(config_path_or_dir)
            raw_configs = {stmt_id: raw_config}
        else:
            raise FileNotFoundError(
                f"Config path is not a valid file or directory: {config_path_or_dir}"
            )

    except (ReadError, FileNotFoundError) as e:
        logger.exception(f"Failed to read configuration from {config_path_or_dir}:")
        raise ConfigurationError(
            message=f"Failed to read config: {e}", config_path=config_path_or_dir
        ) from e

    if not raw_configs:
        logger.warning(f"No statement configurations found at {config_path_or_dir}")
        return []

    for stmt_id, raw_data in raw_configs.items():
        try:
            config = StatementConfig(raw_data)
            validation_errors = config.validate_config()
            if validation_errors:
                raise ConfigurationError(
                    f"Invalid configuration for statement '{stmt_id}'",
                    config_path=f"{config_path_or_dir}/{stmt_id}.ext",  # Placeholder path
                    errors=validation_errors,
                )

            statement = builder.build(config)
            registry.register(statement)  # Raises StatementError on conflict
            loaded_statement_ids.append(statement.id)

        except (ConfigurationError, StatementError, ValueError) as e:
            logger.exception(f"Failed to process/register statement '{stmt_id}':")
            errors.append((stmt_id, str(e)))
        except Exception as e:
            logger.exception(
                f"Unexpected error processing statement '{stmt_id}' from {config_path_or_dir}"
            )
            errors.append((stmt_id, f"Unexpected error: {e!s}"))

    # Handle errors - maybe raise an aggregate error if any occurred?
    if errors:
        # For now, just log a warning, processing continues with successfully
        # loaded statements
        error_details = "; ".join([f"{sid}: {msg}" for sid, msg in errors])
        logger.warning(
            f"Encountered {len(errors)} errors during statement loading/building "
            f"from {config_path_or_dir}: {error_details}"
        )
        # Consider raising an aggregated error if needed for stricter handling

    return loaded_statement_ids

# --- END FILE: fin_statement_model/statements/orchestration/loader.py ---

# --- START FILE: fin_statement_model/statements/orchestration/orchestrator.py ---
"""Main orchestration for statement processing.

This module coordinates the overall workflow of loading statements, populating
graphs, and generating DataFrames. It provides the main public API function
for creating statement DataFrames.
"""

import logging
from pathlib import Path
from typing import Any, Optional, Union

import pandas as pd

from fin_statement_model.core.errors import StatementError
from fin_statement_model.core.graph import Graph

from fin_statement_model.statements.structure.builder import StatementStructureBuilder
from fin_statement_model.statements.formatting.formatter import StatementFormatter
from fin_statement_model.statements.orchestration.loader import (
    load_build_register_statements,
)
from fin_statement_model.statements.population.populator import (
    populate_graph_from_statement,
)
from fin_statement_model.statements.registry import StatementRegistry

logger = logging.getLogger(__name__)

__all__ = ["create_statement_dataframe", "populate_graph"]


def populate_graph(registry: StatementRegistry, graph: Graph) -> list[tuple[str, str]]:
    """Populate the graph with nodes based on registered statements.

    Internal helper function that iterates through all statements registered
    in the `registry` and uses `populate_graph_from_statement` to add the
    corresponding nodes and relationships to the `graph`.

    Args:
        registry: The StatementRegistry containing the statements to process.
        graph: The Graph instance to be populated.

    Returns:
        A list of tuples, where each tuple contains (item_id, error_message)
        for any items that failed during population. Returns an empty list if
        population was successful for all items.
    """
    all_populator_errors = []
    statements = registry.get_all_statements()
    if not statements:
        logger.warning("No statements registered to populate the graph.")
        return []

    for statement in statements:
        populator_errors = populate_graph_from_statement(statement, graph)
        if populator_errors:
            all_populator_errors.extend(
                [(statement.id, item_id, msg) for item_id, msg in populator_errors]
            )

    if all_populator_errors:
        logger.warning(
            f"Encountered {len(all_populator_errors)} errors during graph population."
        )
        # Log details if needed: logger.warning(f"Population errors: {all_populator_errors}")

    return [
        (item_id, msg) for stmt_id, item_id, msg in all_populator_errors
    ]  # Return simplified list


def create_statement_dataframe(
    graph: Graph,
    config_path_or_dir: str,
    format_kwargs: Optional[dict[str, Any]] = None,
) -> Union[pd.DataFrame, dict[str, pd.DataFrame]]:
    r"""Load config(s), build structure(s), populate graph, format as DataFrame(s).

    This function orchestrates the entire process of turning statement
    configuration files into pandas DataFrames containing the calculated or
    retrieved financial data.

    It performs the following steps:
    1. Loads configuration(s) from the specified path or directory.
    2. Validates the configuration(s).
    3. Builds the internal statement structure(s).
    4. Registers the structure(s).
    5. Populates the provided `graph` with nodes based on the statement(s).
       (Assumes the graph might already contain necessary data nodes or will
       fetch them).
    6. Formats the statement data from the graph into pandas DataFrame(s).

    Args:
        graph: The core.graph.Graph instance to use and populate. This graph
            should ideally contain the necessary base data nodes (e.g.,
            actuals) before calling this function, or nodes should be capable
            of fetching their data.
        config_path_or_dir: Path to a single statement config file (e.g.,
            './configs/income_statement.yaml') or a directory containing
            multiple config files (e.g., './configs/').
        format_kwargs: Optional dictionary of keyword arguments passed directly
            to the `StatementFormatter.generate_dataframe` method. This can
            be used to control aspects like date ranges, periods, or number
            formatting. See `StatementFormatter` documentation for details.

    Returns:
        If `config_path_or_dir` points to a single file, returns a single
        pandas DataFrame representing that statement.
        If `config_path_or_dir` points to a directory, returns a dictionary
        mapping statement IDs (derived from filenames) to their corresponding
        pandas DataFrames.

    Raises:
        ConfigurationError: If loading or validating configurations fails.
        StatementError: If registering statements fails or if no valid
            statements can be processed.
        FileNotFoundError: If `config_path_or_dir` does not exist or is not a
            valid file or directory.
        FinancialModelError: Potentially other errors from graph operations
            during population or formatting.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> # Assume 'my_graph' is a pre-populated Graph instance
        >>> # Assume 'configs/income_stmt.yaml' defines an income statement
        >>> try:
        ...     income_df = create_statement_dataframe(
        ...         graph=my_graph,
        ...         config_path_or_dir='configs/income_stmt.yaml',
        ...         format_kwargs={'periods': ['2023Q1', '2023Q2']}
        ...     )
        ...     # In real code, use logger.debug or logger.info
        ...     logger.debug(f"Income DataFrame head:\n{income_df.head()}")
        ... except FileNotFoundError:
        ...     # Use logger.error or logger.warning
        ...     logger.error("Config file not found.")
        ... except (ConfigurationError, StatementError) as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Error processing statement: {e}")

        >>> # Process all configs in a directory
        >>> try:
        ...     all_statements = create_statement_dataframe(
        ...         graph=my_graph,
        ...         config_path_or_dir='configs/'
        ...     )
        ...     balance_sheet_df = all_statements.get('balance_sheet')
        ...     if balance_sheet_df is not None:
        ...         # Use logger.info
        ...         logger.info("Balance Sheet DataFrame created.")
        ... except FileNotFoundError:
        ...     # Use logger.error or logger.warning
        ...     logger.error("Config directory not found.")
        ... except StatementError as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Error processing statements: {e}")
    """
    registry = StatementRegistry()
    builder = StatementStructureBuilder()
    format_kwargs = format_kwargs or {}

    # Step 1: Load, Build, Register
    loaded_ids = load_build_register_statements(config_path_or_dir, registry, builder)
    if not loaded_ids:
        raise StatementError(
            f"No valid statements could be loaded from {config_path_or_dir}"
        )

    # Step 2: Populate Graph (handles errors internally, logs warnings)
    populate_graph(registry, graph)

    # Step 3: Format results
    results: dict[str, pd.DataFrame] = {}
    formatting_errors = []
    for stmt_id in loaded_ids:
        statement = registry.get(stmt_id)
        if not statement:
            logger.error(
                f"Internal error: Statement '{stmt_id}' was loaded but not found in registry."
            )
            formatting_errors.append(
                (stmt_id, "Statement not found in registry after loading")
            )
            continue
        try:
            formatter = StatementFormatter(statement)
            df = formatter.generate_dataframe(graph, **format_kwargs)
            results[stmt_id] = df
        except Exception as e:
            logger.exception(f"Failed to format statement '{stmt_id}'")
            formatting_errors.append((stmt_id, f"Formatting error: {e!s}"))

    if formatting_errors:
        # Decide policy: raise error, or return partial results?
        # For now, log warning and return what succeeded.
        logger.warning(
            f"Encountered {len(formatting_errors)} errors during formatting."
        )

    # Return single DF or Dict based on input type
    is_single_file = Path(config_path_or_dir).is_file()
    if is_single_file and len(results) == 1:
        return next(iter(results.values()))
    elif is_single_file and not results:
        raise StatementError(
            f"Failed to generate DataFrame for statement from file: {config_path_or_dir}"
        )
    else:
        # Return dict for directory input, or if multiple results came from single file (unexpected)
        return results

# --- END FILE: fin_statement_model/statements/orchestration/orchestrator.py ---

# --- START FILE: fin_statement_model/statements/population/__init__.py ---
"""Graph population functionality for financial statements.

This package handles the conversion of statement structures into graph nodes:
- ID resolution between statement items and graph nodes
- Processing different item types (metrics, calculations, subtotals)
- Managing dependencies and retry logic
"""

from .id_resolver import IDResolver
from .item_processors import (
    CalculatedItemProcessor,
    ItemProcessor,
    ItemProcessorManager,
    MetricItemProcessor,
    ProcessorResult,
    SubtotalItemProcessor,
)
from .populator import populate_graph_from_statement

__all__ = [
    "CalculatedItemProcessor",
    # ID Resolution
    "IDResolver",
    # Item Processors
    "ItemProcessor",
    "ItemProcessorManager",
    "MetricItemProcessor",
    "ProcessorResult",
    "SubtotalItemProcessor",
    # Main Function
    "populate_graph_from_statement",
]

# --- END FILE: fin_statement_model/statements/population/__init__.py ---

# --- START FILE: fin_statement_model/statements/population/id_resolver.py ---
"""ID resolution for statement items to graph nodes.

This module provides centralized logic for resolving statement item IDs to their
corresponding graph node IDs, handling the complexity of different item types
having different ID mapping rules.
"""

import logging
from typing import Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.statements.structure import (
    StatementStructure,
    LineItem,
)

logger = logging.getLogger(__name__)

__all__ = ["IDResolver"]


class IDResolver:
    """Centralizes ID resolution from statement items to graph nodes.

    This class handles the complexity of mapping statement item IDs to graph
    node IDs, accounting for the fact that:
    - LineItems have a separate node_id property that differs from their ID
    - Other items (CalculatedLineItem, SubtotalLineItem, MetricLineItem) use
      their ID directly as the node ID
    - Some nodes may exist directly in the graph without being statement items

    The resolver caches mappings for performance and provides both single and
    batch resolution methods.
    """

    def __init__(self, statement: StatementStructure):
        """Initialize the resolver with a statement structure.

        Args:
            statement: The statement structure containing items to resolve.
        """
        self.statement = statement
        self._item_to_node_cache: dict[str, str] = {}
        self._node_to_items_cache: dict[str, list[str]] = {}
        self._build_cache()

    def _build_cache(self) -> None:
        """Pre-build ID mappings for all items in the statement."""
        logger.debug(f"Building ID cache for statement '{self.statement.id}'")

        for item in self.statement.get_all_items():
            if isinstance(item, LineItem):
                # LineItems map their ID to their node_id property
                self._item_to_node_cache[item.id] = item.node_id
                self._node_to_items_cache.setdefault(item.node_id, []).append(item.id)
            else:
                # Other items use their ID directly as the node ID
                self._item_to_node_cache[item.id] = item.id
                self._node_to_items_cache.setdefault(item.id, []).append(item.id)

        logger.debug(
            f"ID cache built: {len(self._item_to_node_cache)} item->node mappings, "
            f"{len(self._node_to_items_cache)} unique nodes"
        )

    def resolve(self, item_id: str, graph: Optional[Graph] = None) -> Optional[str]:
        """Resolve a statement item ID to its graph node ID.

        Resolution process:
        1. Check the pre-built cache for the item ID
        2. If not found and a graph is provided, check if the ID exists
           directly as a node in the graph
        3. Return None if not found anywhere

        Args:
            item_id: The statement item ID to resolve.
            graph: Optional graph to check for direct node existence.

        Returns:
            The resolved graph node ID if found, None otherwise.
        """
        # Rebuild cache if it's empty (e.g., after invalidation)
        if not self._item_to_node_cache:
            self._build_cache()

        # Check cache first
        if item_id in self._item_to_node_cache:
            return self._item_to_node_cache[item_id]

        # Check if it exists directly in graph
        if graph and graph.has_node(item_id):
            # Cache this discovery for future lookups
            self._item_to_node_cache[item_id] = item_id
            self._node_to_items_cache.setdefault(item_id, []).append(item_id)
            return item_id

        return None

    def resolve_multiple(
        self, item_ids: list[str], graph: Optional[Graph] = None
    ) -> dict[str, Optional[str]]:
        """Resolve multiple item IDs at once.

        Args:
            item_ids: List of statement item IDs to resolve.
            graph: Optional graph to check for direct node existence.

        Returns:
            Dictionary mapping each item ID to its resolved node ID (or None).
        """
        return {item_id: self.resolve(item_id, graph) for item_id in item_ids}

    def get_items_for_node(self, node_id: str) -> list[str]:
        """Get all statement item IDs that map to a given node ID.

        This reverse lookup can be useful for debugging and understanding
        which statement items contribute to a particular graph node.

        Args:
            node_id: The graph node ID to look up.

        Returns:
            List of statement item IDs that map to this node (may be empty).
        """
        # Rebuild cache if it's empty
        if not self._node_to_items_cache:
            self._build_cache()
        return self._node_to_items_cache.get(node_id, [])

    def get_all_mappings(self) -> dict[str, str]:
        """Get all item ID to node ID mappings.

        Returns:
            Dictionary of all cached mappings.
        """
        # Rebuild cache if it's empty
        if not self._item_to_node_cache:
            self._build_cache()
        return self._item_to_node_cache.copy()

    def invalidate_cache(self) -> None:
        """Clear the cache, forcing a rebuild on next resolution.

        This should be called if the statement structure changes after
        the resolver was created.
        """
        self._item_to_node_cache.clear()
        self._node_to_items_cache.clear()
        logger.debug(f"ID cache invalidated for statement '{self.statement.id}'")

    def refresh_cache(self) -> None:
        """Rebuild the cache from the current statement structure."""
        self.invalidate_cache()
        self._build_cache()

# --- END FILE: fin_statement_model/statements/population/id_resolver.py ---

# --- START FILE: fin_statement_model/statements/population/item_processors.py ---
"""Item processors for converting statement items into graph nodes.

This module provides a processor hierarchy that handles the conversion of different
statement item types (MetricLineItem, CalculatedLineItem, SubtotalLineItem) into
graph nodes. Each processor encapsulates the logic for its specific item type,
reducing complexity and improving testability.
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import (
    NodeError,
    CircularDependencyError,
    CalculationError,
    ConfigurationError,
    MetricError,
)
from fin_statement_model.core.metrics import metric_registry
from fin_statement_model.statements.structure import (
    StatementStructure,
    StatementItem,
    MetricLineItem,
    CalculatedLineItem,
    SubtotalLineItem,
)
from fin_statement_model.statements.population.id_resolver import IDResolver
from fin_statement_model.statements.utilities.result_types import (
    Result,
    Success,
    Failure,
    ErrorDetail,
    ErrorSeverity,
)

logger = logging.getLogger(__name__)

__all__ = [
    "CalculatedItemProcessor",
    "ItemProcessor",
    "ItemProcessorManager",
    "MetricItemProcessor",
    "ProcessorResult",
    "SubtotalItemProcessor",
]


@dataclass
class ProcessorResult:
    """Result of processing a statement item.

    Attributes:
        success: Whether the processing was successful.
        node_added: Whether a new node was added to the graph.
        error_message: Error message if processing failed.
        missing_inputs: List of missing input details (item_id, resolved_node_id).
    """

    success: bool
    node_added: bool = False
    error_message: Optional[str] = None
    missing_inputs: Optional[list[tuple[str, Optional[str]]]] = None

    def to_result(self) -> Result[bool]:
        """Convert to the new Result type."""
        if self.success:
            return Success(value=self.node_added)

        errors = []
        if self.error_message:
            errors.append(
                ErrorDetail(
                    code="processing_error",
                    message=self.error_message,
                    severity=ErrorSeverity.ERROR,
                )
            )

        if self.missing_inputs:
            for item_id, node_id in self.missing_inputs:
                msg = (
                    f"Missing input: item '{item_id}' needs node '{node_id}'"
                    if node_id
                    else f"Missing input: item '{item_id}' not found/mappable"
                )
                errors.append(
                    ErrorDetail(
                        code="missing_input",
                        message=msg,
                        context=f"item_id={item_id}, node_id={node_id}",
                        severity=ErrorSeverity.ERROR,
                    )
                )

        return Failure(errors=errors if errors else None)


class ItemProcessor(ABC):
    """Abstract base class for processing statement items into graph nodes.

    This base class provides common functionality for resolving input IDs
    and handling missing inputs across different item types.
    """

    def __init__(
        self, id_resolver: IDResolver, graph: Graph, statement: StatementStructure
    ):
        """Initialize the processor.

        Args:
            id_resolver: ID resolver for mapping statement IDs to graph node IDs.
            graph: The graph to add nodes to.
            statement: The statement structure being processed.
        """
        self.id_resolver = id_resolver
        self.graph = graph
        self.statement = statement

    @abstractmethod
    def can_process(self, item: StatementItem) -> bool:
        """Check if this processor can handle the given item type.

        Args:
            item: The statement item to check.

        Returns:
            True if this processor can handle the item type.
        """

    @abstractmethod
    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process the item and add it to the graph if needed.

        Args:
            item: The statement item to process.
            is_retry: Whether this is a retry attempt (affects error logging).

        Returns:
            ProcessorResult indicating success/failure and details.
        """

    def resolve_inputs(
        self, input_ids: list[str]
    ) -> tuple[list[str], list[tuple[str, Optional[str]]]]:
        """Resolve input IDs to graph node IDs.

        Args:
            input_ids: List of statement item IDs to resolve.

        Returns:
            Tuple of (resolved_node_ids, missing_details).
            missing_details contains tuples of (item_id, resolved_node_id_or_none).
        """
        resolved = []
        missing = []

        for input_id in input_ids:
            node_id = self.id_resolver.resolve(input_id, self.graph)
            if node_id and self.graph.has_node(node_id):
                resolved.append(node_id)
            else:
                missing.append((input_id, node_id))

        return resolved, missing

    def _handle_missing_inputs(
        self,
        item: StatementItem,
        missing: list[tuple[str, Optional[str]]],
        is_retry: bool,
    ) -> ProcessorResult:
        """Handle missing input nodes consistently across processors.

        Args:
            item: The item being processed.
            missing: List of missing input details.
            is_retry: Whether this is a retry attempt.

        Returns:
            ProcessorResult with appropriate error details.
        """
        missing_summary = [
            (
                f"item '{i_id}' needs node '{n_id}'"
                if n_id
                else f"item '{i_id}' not found/mappable"
            )
            for i_id, n_id in missing
        ]

        if is_retry:
            logger.error(
                f"Retry failed for {type(item).__name__} '{item.id}' in statement '{self.statement.id}': "
                f"missing required inputs: {'; '.join(missing_summary)}"
            )
            return ProcessorResult(
                success=False,
                error_message=f"Missing inputs on retry: {missing_summary}",
                missing_inputs=missing,
            )
        else:
            # Don't log on first attempt - allows dependency resolution
            return ProcessorResult(success=False, missing_inputs=missing)


class MetricItemProcessor(ItemProcessor):
    """Processor for MetricLineItem objects.

    Handles the creation of metric-based calculation nodes by:
    1. Looking up the metric in the registry
    2. Validating input mappings
    3. Resolving input IDs to graph nodes
    4. Adding the metric node to the graph
    """

    def can_process(self, item: StatementItem) -> bool:
        """Check if item is a MetricLineItem."""
        return isinstance(item, MetricLineItem)

    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process a MetricLineItem and add it to the graph."""
        # Early validation
        if not isinstance(item, MetricLineItem):
            return ProcessorResult(success=False, error_message="Invalid item type")

        # Check if node already exists
        if self.graph.has_node(item.id):
            return ProcessorResult(success=True, node_added=False)

        # Initialize result variables
        error_message = None
        node_added = False

        # Get metric from registry
        try:
            metric = metric_registry.get(item.metric_id)
        except MetricError as e:
            logger.exception(
                f"Cannot populate item '{item.id}': Metric '{item.metric_id}' not found in registry"
            )
            error_message = f"Metric '{item.metric_id}' not found: {e}"

        # Validate input mappings if no error yet
        if not error_message:
            error_message = self._validate_metric_inputs(metric, item)

        # Resolve metric inputs if no error yet
        if not error_message:
            resolved_map, missing = self._resolve_metric_inputs(metric, item)
            if missing:
                return self._handle_missing_inputs(item, missing, is_retry)

            # Add to graph
            try:
                self.graph.add_metric(
                    metric_name=item.metric_id,
                    node_name=item.id,
                    input_node_map=resolved_map,
                )
                node_added = True
            except Exception as e:
                logger.exception(f"Failed to add metric node '{item.id}'")
                error_message = f"Failed to add metric node: {e}"

        # Single exit point
        if error_message:
            return ProcessorResult(success=False, error_message=error_message)
        return ProcessorResult(success=True, node_added=node_added)

    def _validate_metric_inputs(
        self, metric: Any, item: MetricLineItem
    ) -> Optional[str]:
        """Validate that the item provides all required metric inputs."""
        provided_inputs = set(item.inputs.keys())
        required_inputs = set(metric.inputs)

        if provided_inputs != required_inputs:
            missing_req = required_inputs - provided_inputs
            extra_prov = provided_inputs - required_inputs
            error_msg = f"Input mapping mismatch for metric '{item.metric_id}' in item '{item.id}'."

            if missing_req:
                error_msg += f" Missing required metric inputs: {missing_req}."
            if extra_prov:
                error_msg += f" Unexpected inputs provided: {extra_prov}."

            logger.error(error_msg)
            return error_msg

        return None

    def _resolve_metric_inputs(
        self, metric: Any, item: MetricLineItem
    ) -> tuple[dict[str, str], list[tuple[str, Optional[str]]]]:
        """Resolve metric input mappings to graph node IDs."""
        resolved_map = {}
        missing = []

        for metric_input_name in metric.inputs:
            input_item_id = item.inputs[metric_input_name]
            node_id = self.id_resolver.resolve(input_item_id, self.graph)

            if node_id and self.graph.has_node(node_id):
                resolved_map[metric_input_name] = node_id
            else:
                missing.append((input_item_id, node_id))

        return resolved_map, missing


class CalculatedItemProcessor(ItemProcessor):
    """Processor for CalculatedLineItem objects.

    Handles the creation of calculation nodes with specific operations by:
    1. Resolving input IDs to graph nodes
    2. Adding the calculation node with the specified operation type
    """

    def can_process(self, item: StatementItem) -> bool:
        """Check if item is a CalculatedLineItem."""
        return isinstance(item, CalculatedLineItem)

    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process a CalculatedLineItem and add it to the graph."""
        if not isinstance(item, CalculatedLineItem):
            return ProcessorResult(success=False, error_message="Invalid item type")

        # Check if node already exists
        if self.graph.has_node(item.id):
            return ProcessorResult(success=True, node_added=False)

        # Resolve inputs
        resolved, missing = self.resolve_inputs(item.input_ids)
        if missing:
            return self._handle_missing_inputs(item, missing, is_retry)

        # Add calculation node
        error_message = None
        try:
            self.graph.add_calculation(
                name=item.id,
                input_names=resolved,
                operation_type=item.calculation_type,
                **item.parameters,
            )
        except (
            NodeError,
            CircularDependencyError,
            CalculationError,
            ConfigurationError,
        ) as e:
            error_msg = f"Failed to add calculation node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = str(e)
        except Exception as e:
            error_msg = f"Unexpected error adding calculation node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = f"Unexpected error: {e}"

        if error_message:
            return ProcessorResult(success=False, error_message=error_message)
        return ProcessorResult(success=True, node_added=True)


class SubtotalItemProcessor(ItemProcessor):
    """Processor for SubtotalLineItem objects.

    Handles the creation of subtotal (addition) nodes by:
    1. Resolving input IDs to graph nodes
    2. Adding an addition calculation node
    """

    def can_process(self, item: StatementItem) -> bool:
        """Check if item is a SubtotalLineItem."""
        return isinstance(item, SubtotalLineItem)

    def process(self, item: StatementItem, is_retry: bool = False) -> ProcessorResult:
        """Process a SubtotalLineItem and add it to the graph."""
        if not isinstance(item, SubtotalLineItem):
            return ProcessorResult(success=False, error_message="Invalid item type")

        # Check if node already exists
        if self.graph.has_node(item.id):
            return ProcessorResult(success=True, node_added=False)

        # Handle empty subtotals
        if not item.item_ids:
            logger.debug(f"Subtotal item '{item.id}' has no input items")
            return ProcessorResult(success=True, node_added=False)

        # Resolve inputs
        resolved, missing = self.resolve_inputs(item.item_ids)
        if missing:
            return self._handle_missing_inputs(item, missing, is_retry)

        # Add subtotal as addition calculation
        error_message = None
        try:
            self.graph.add_calculation(
                name=item.id, input_names=resolved, operation_type="addition"
            )
        except (
            NodeError,
            CircularDependencyError,
            CalculationError,
            ConfigurationError,
        ) as e:
            error_msg = f"Failed to add subtotal node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = str(e)
        except Exception as e:
            error_msg = f"Unexpected error adding subtotal node '{item.id}': {e}"
            logger.exception(error_msg)
            error_message = f"Unexpected error: {e}"

        if error_message:
            return ProcessorResult(success=False, error_message=error_message)
        return ProcessorResult(success=True, node_added=True)


class ItemProcessorManager:
    """Manages the collection of item processors.

    This class coordinates the processing of different statement item types
    by delegating to the appropriate processor based on the item type.
    """

    def __init__(
        self, id_resolver: IDResolver, graph: Graph, statement: StatementStructure
    ):
        """Initialize the processor manager with all available processors.

        Args:
            id_resolver: ID resolver for mapping statement IDs to graph node IDs.
            graph: The graph to add nodes to.
            statement: The statement structure being processed.
        """
        self.processors = [
            MetricItemProcessor(id_resolver, graph, statement),
            CalculatedItemProcessor(id_resolver, graph, statement),
            SubtotalItemProcessor(id_resolver, graph, statement),
        ]

    def process_item(
        self, item: StatementItem, is_retry: bool = False
    ) -> ProcessorResult:
        """Process a statement item using the appropriate processor.

        Args:
            item: The statement item to process.
            is_retry: Whether this is a retry attempt.

        Returns:
            ProcessorResult from the appropriate processor, or a success result
            if no processor handles the item type (e.g., for LineItem).
        """
        for processor in self.processors:
            if processor.can_process(item):
                return processor.process(item, is_retry)

        # No processor found - this is OK for non-calculation items like LineItem
        logger.debug(
            f"No processor for item type {type(item).__name__} with ID '{item.id}'. "
            "This is expected for non-calculation items."
        )
        return ProcessorResult(success=True, node_added=False)

# --- END FILE: fin_statement_model/statements/population/item_processors.py ---

# --- START FILE: fin_statement_model/statements/population/populator.py ---
"""Populates a `fin_statement_model.core.graph.Graph` with calculation nodes.

This module provides the function `populate_graph_from_statement`, which is
responsible for translating the calculation logic defined within a
`StatementStructure` (specifically `CalculatedLineItem`, `SubtotalLineItem`,
and `MetricLineItem`) into actual calculation nodes within a `Graph` instance.

Key Concepts:
- **ID Resolution**: Statement configurations use item IDs that must be resolved
  to graph node IDs. This is handled by the `IDResolver` class.
- **Dependency Ordering**: Items may depend on other items. The populator handles
  this by retrying failed items after successful ones, allowing dependencies to
  be satisfied.
- **Idempotency**: If a node already exists in the graph, it will be skipped.
"""

import logging

from fin_statement_model.core.graph import Graph
from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.statements.population.id_resolver import IDResolver
from fin_statement_model.statements.population.item_processors import (
    ItemProcessorManager,
)

logger = logging.getLogger(__name__)

__all__ = ["populate_graph_from_statement"]


def populate_graph_from_statement(
    statement: StatementStructure, graph: Graph
) -> list[tuple[str, str]]:
    """Add calculation nodes defined in a StatementStructure to a Graph.

    This function bridges the gap between static statement definitions and the
    dynamic calculation graph. It processes three types of items:

    1. **CalculatedLineItem**: Creates calculation nodes with specified operations
    2. **SubtotalLineItem**: Creates addition nodes that sum multiple items
    3. **MetricLineItem**: Creates metric-based calculation nodes

    ID Resolution Logic:
    - Input IDs in statement configurations are resolved to graph node IDs using
      the `IDResolver` class
    - This handles the mapping between statement item IDs and actual graph nodes
    - Resolution accounts for LineItem.node_id vs other items using their ID directly

    Dependency Handling:
    - Items may depend on other items that haven't been created yet
    - The function uses a retry mechanism: failed items are retried after
      successful ones, allowing dependencies to be resolved
    - Circular dependencies are detected and reported as errors

    Idempotency:
    - If a node already exists in the graph, it will be skipped
    - This allows the function to be called multiple times safely

    Args:
        statement: The `StatementStructure` object containing the definitions
            of calculated items, subtotals, and metrics.
        graph: The `core.graph.Graph` instance that will be populated with
            the calculation nodes.

    Returns:
        A list of tuples, where each tuple contains `(item_id, error_message)`
        for any items that could not be successfully added to the graph. An
        empty list indicates that all applicable items were added (or already
        existed) without critical errors.

    Raises:
        TypeError: If `statement` is not a `StatementStructure` or `graph` is
            not a `Graph` instance.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> from fin_statement_model.statements.structure import StatementStructure
        >>>
        >>> # Create graph with data nodes
        >>> graph = Graph()
        >>> graph.add_node('revenue_node', values={'2023': 1000})
        >>> graph.add_node('cogs_node', values={'2023': 600})
        >>>
        >>> # Create statement with calculations
        >>> statement = StatementStructure(id="IS", name="Income Statement")
        >>> # Add a LineItem that maps to 'revenue_node'
        >>> revenue_item = LineItem(id='revenue', name='Revenue', node_id='revenue_node')
        >>> # Add a CalculatedLineItem that references the LineItem
        >>> gross_profit = CalculatedLineItem(
        ...     id='gross_profit',
        ...     name='Gross Profit',
        ...     calculation_type='subtraction',
        ...     input_ids=['revenue', 'cogs']  # Uses LineItem IDs
        ... )
        >>>
        >>> errors = populate_graph_from_statement(statement, graph)
        >>> # The function will:
        >>> # 1. Resolve 'revenue' to 'revenue_node' via LineItem.node_id
        >>> # 2. Resolve 'cogs' to 'cogs_node' (if it exists in statement or graph)
        >>> # 3. Create a calculation node 'gross_profit' with the resolved inputs
    """
    # Validate inputs
    if not isinstance(statement, StatementStructure):
        raise TypeError("statement must be a StatementStructure instance")
    if not isinstance(graph, Graph):
        raise TypeError("graph must be a Graph instance")

    # Initialize components
    id_resolver = IDResolver(statement)
    processor_manager = ItemProcessorManager(id_resolver, graph, statement)

    # Get all items to process
    calculation_items = statement.get_calculation_items()
    metric_items = statement.get_metric_items()
    all_items_to_process = calculation_items + metric_items

    # Track results
    errors_encountered: list[tuple[str, str]] = []
    nodes_added_count = 0

    logger.info(
        f"Starting graph population for statement '{statement.id}'. "
        f"Processing {len(all_items_to_process)} calculation/metric items."
    )

    # Process items with retry mechanism
    items_to_process = list(all_items_to_process)
    processed_in_pass = -1  # Initialize to enter loop

    while items_to_process and processed_in_pass != 0:
        items_failed_this_pass = []
        processed_in_pass = 0

        logger.debug(f"Population loop: Processing {len(items_to_process)} items...")

        for item in items_to_process:
            # Determine if this is a retry (not the first overall pass)
            is_retry = len(items_to_process) < len(all_items_to_process)

            # Process the item
            result = processor_manager.process_item(item, is_retry)

            if result.success:
                processed_in_pass += 1
                if result.node_added:
                    nodes_added_count += 1
            else:
                items_failed_this_pass.append(item)
                # Only record errors on retry or for non-dependency errors
                if is_retry and result.error_message:
                    errors_encountered.append((item.id, result.error_message))

        # Prepare for next iteration
        items_to_process = items_failed_this_pass

        # Check for stalled progress
        if processed_in_pass == 0 and items_to_process:
            logger.warning(
                f"Population loop stalled. {len(items_to_process)} items could not be processed: "
                f"{[item.id for item in items_to_process]}"
            )
            # Add errors for items that couldn't be processed
            for item in items_to_process:
                if not any(err[0] == item.id for err in errors_encountered):
                    errors_encountered.append(
                        (
                            item.id,
                            "Failed to process due to unresolved dependencies or circular reference.",
                        )
                    )
            break

    # Log results
    if errors_encountered:
        logger.warning(
            f"Graph population for statement '{statement.id}' completed with "
            f"{len(errors_encountered)} persistent errors."
        )
    else:
        log_level = logging.INFO if nodes_added_count > 0 else logging.DEBUG
        logger.log(
            log_level,
            f"Graph population for statement '{statement.id}' completed. "
            f"Added {nodes_added_count} new nodes.",
        )

    return errors_encountered

# --- END FILE: fin_statement_model/statements/population/populator.py ---

# --- START FILE: fin_statement_model/statements/registry.py ---
"""Registry for managing loaded and validated financial statement structures.

This module provides the `StatementRegistry` class, which acts as a central
store for `StatementStructure` objects after they have been loaded from
configurations and built. It ensures uniqueness of statement IDs and provides
methods for retrieving registered statements.
"""

import logging
from typing import Optional

# Assuming StatementStructure is defined here or imported appropriately
# We might need to adjust this import based on the actual location
try:
    from .structure import StatementStructure
except ImportError:
    # Handle cases where structure might be in a different sub-package later if needed
    # For now, assume it's available via relative import
    from fin_statement_model.statements.structure import StatementStructure

from .errors import StatementError  # Assuming StatementError is in statements/errors.py

logger = logging.getLogger(__name__)

__all__ = ["StatementRegistry"]


class StatementRegistry:
    """Manages a collection of loaded financial statement structures.

    This registry holds instances of `StatementStructure`, keyed by their unique
    IDs. It prevents duplicate registrations and provides methods to access
    registered statements individually or collectively.

    Attributes:
        _statements: A dictionary mapping statement IDs (str) to their
                     corresponding `StatementStructure` objects.
    """

    def __init__(self):
        """Initialize an empty statement registry."""
        self._statements: dict[str, StatementStructure] = {}
        logger.debug("StatementRegistry initialized.")

    def register(self, statement: StatementStructure) -> None:
        """Register a statement structure with the registry.

        Ensures the provided object is a `StatementStructure` with a valid ID
        and that the ID is not already present in the registry.

        Args:
            statement: The `StatementStructure` instance to register.

        Raises:
            TypeError: If the `statement` argument is not an instance of
                `StatementStructure`.
            ValueError: If the `statement` has an invalid or empty ID.
            StatementError: If a statement with the same ID (`statement.id`) is
                already registered.
        """
        if not isinstance(statement, StatementStructure):
            raise TypeError("Only StatementStructure objects can be registered.")

        statement_id = statement.id
        if not statement_id:
            raise ValueError(
                "StatementStructure must have a valid non-empty id to be registered."
            )

        if statement_id in self._statements:
            # Policy: Raise error on conflict
            logger.error(
                f"Attempted to register duplicate statement ID: '{statement_id}'"
            )
            raise StatementError(
                message=f"Statement with ID '{statement_id}' is already registered.",
                # statement_id=statement_id # Add if StatementError accepts this arg
            )

        self._statements[statement_id] = statement
        logger.info(f"Registered statement '{statement.name}' with ID '{statement_id}'")

    def get(self, statement_id: str) -> Optional[StatementStructure]:
        """Get a registered statement by its ID.

        Returns:
            The `StatementStructure` instance associated with the given ID if
            it exists, otherwise returns `None`.

        Example:
            >>> registry = StatementRegistry()
            >>> # Assume 'income_statement' is a valid StatementStructure instance
            >>> # registry.register(income_statement)
            >>> retrieved_statement = registry.get("income_statement_id")
            >>> if retrieved_statement:
            ...     logger.info(f"Found: {retrieved_statement.name}")
            ... else:
            ...     logger.info("Statement not found.")
        """
        return self._statements.get(statement_id)

    def get_all_ids(self) -> list[str]:
        """Get the IDs of all registered statements.

        Returns:
            A list containing the unique IDs of all statements currently held
            in the registry.
        """
        return list(self._statements.keys())

    def get_all_statements(self) -> list[StatementStructure]:
        """Get all registered statement structure objects.

        Returns:
            A list containing all `StatementStructure` objects currently held
            in the registry.
        """
        return list(self._statements.values())

    def clear(self) -> None:
        """Remove all statement structures from the registry.

        Resets the registry to an empty state.
        """
        self._statements = {}
        logger.info("StatementRegistry cleared.")

    def __len__(self) -> int:
        """Return the number of registered statements."""
        return len(self._statements)

    def __contains__(self, statement_id: str) -> bool:
        """Check if a statement ID exists in the registry.

        Allows using the `in` operator with the registry.

        Args:
            statement_id: The statement ID to check for.

        Returns:
            `True` if a statement with the given ID is registered, `False` otherwise.

        Example:
            >>> registry = StatementRegistry()
            >>> # Assume 'income_statement' is registered with ID 'IS_2023'
            >>> # registry.register(income_statement)
            >>> print("IS_2023" in registry)  # Output: True
            >>> print("BS_2023" in registry)  # Output: False
        """
        return statement_id in self._statements

# --- END FILE: fin_statement_model/statements/registry.py ---

# --- START FILE: fin_statement_model/statements/structure/__init__.py ---
"""Statement structure package.

Re-export domain model classes from submodules.
"""

from .items import (
    StatementItem,
    StatementItemType,
    LineItem,
    CalculatedLineItem,
    MetricLineItem,
    SubtotalLineItem,
)
from .containers import Section, StatementStructure

__all__ = [
    "CalculatedLineItem",
    "LineItem",
    "MetricLineItem",
    "Section",
    "StatementItem",
    "StatementItemType",
    "StatementStructure",
    "SubtotalLineItem",
]

# --- END FILE: fin_statement_model/statements/structure/__init__.py ---

# --- START FILE: fin_statement_model/statements/structure/builder.py ---
"""Builds StatementStructure objects from validated StatementConfig models.

This module provides the `StatementStructureBuilder`, which translates the
deserialized and validated configuration (represented by `StatementConfig`
containing Pydantic models) into the hierarchical `StatementStructure` object
used internally for representing the layout and components of a financial
statement.
"""

import logging
from typing import Union

# Assuming config and structure modules are accessible
from fin_statement_model.statements.configs.validator import StatementConfig
from fin_statement_model.statements.configs.models import (
    SectionModel,
    BaseItemModel,
    LineItemModel,
    CalculatedItemModel,
    MetricItemModel,
    SubtotalModel,
)
from fin_statement_model.statements.structure import (
    StatementStructure,
    Section,
    LineItem,
    MetricLineItem,
    CalculatedLineItem,
    SubtotalLineItem,
)
from fin_statement_model.statements.errors import ConfigurationError

logger = logging.getLogger(__name__)

__all__ = ["StatementStructureBuilder"]


class StatementStructureBuilder:
    """Constructs a `StatementStructure` object from a validated configuration.

    Takes a `StatementConfig` instance (which should have successfully passed
    validation, populating its `.model` attribute) and recursively builds the
    corresponding `StatementStructure`, including its sections, line items,
    calculated items, subtotals, and nested sections.
    """

    def build(self, config: StatementConfig) -> StatementStructure:
        """Build a `StatementStructure` from a validated `StatementConfig`.

        This is the main public method of the builder. It orchestrates the
        conversion process, calling internal helper methods to build sections
        and items.

        Args:
            config: A `StatementConfig` instance whose `.validate_config()`
                method has been successfully called, populating `config.model`.

        Returns:
            The fully constructed `StatementStructure` object, ready to be
            registered or used.

        Raises:
            ValueError: If the provided `config` object has not been validated
                (i.e., `config.model` is `None`).
            ConfigurationError: If an unexpected error occurs during the building
                process, potentially indicating an issue not caught by the
                initial Pydantic validation or an internal inconsistency.
        """
        if config.model is None:
            # Ensure validation has run successfully before building
            raise ValueError(
                "StatementConfig must be validated (config.model must be set) "
                "before building the structure."
            )

        # Build from the validated Pydantic model stored in config.model
        try:
            stmt_model = config.model  # Use validated model from config
            statement = StatementStructure(
                id=stmt_model.id,
                name=stmt_model.name,
                description=stmt_model.description,
                metadata=stmt_model.metadata,
            )
            for sec_model in stmt_model.sections:
                section = self._build_section_model(sec_model)
                statement.add_section(section)
            logger.info(
                f"Successfully built StatementStructure for ID '{statement.id}'"
            )
            return statement
        except Exception as e:
            # Catch potential errors during the building process itself
            logger.exception(
                f"Error building statement structure from validated model for ID '{config.model.id}'"
            )
            raise ConfigurationError(
                message=f"Failed to build statement structure from validated config: {e}",
                errors=[str(e)],
            ) from e

    def _build_section_model(self, section_model: SectionModel) -> Section:
        """Build a `Section` object from a `SectionModel`.

        Recursively builds the items and subsections within this section.

        Args:
            section_model: The Pydantic model representing the section configuration.

        Returns:
            A `Section` instance corresponding to the model.
        """
        section = Section(
            id=section_model.id,
            name=section_model.name,
            description=section_model.description,
            metadata=section_model.metadata,
        )
        for item in section_model.items:
            section.add_item(self._build_item_model(item))
        for sub in section_model.subsections:
            # Recursively build subsections
            section.add_item(self._build_section_model(sub))
        if section_model.subtotal:
            section.subtotal = self._build_subtotal_model(section_model.subtotal)
        return section

    def _build_item_model(
        self, item_model: BaseItemModel
    ) -> Union[LineItem, CalculatedLineItem, MetricLineItem, SubtotalLineItem, Section]:
        """Build a statement item object from its corresponding Pydantic model.

        Dispatches the building process based on the specific type of the input
        model (`LineItemModel`, `CalculatedItemModel`, `MetricItemModel`,
        `SubtotalModel`, or `SectionModel` for nested sections).

        Args:
            item_model: The Pydantic model representing a line item, calculated
                item, metric item, subtotal, or nested section.

        Returns:
            The corresponding `StatementStructure` component (`LineItem`,
            `CalculatedLineItem`, `MetricLineItem`, `SubtotalLineItem`, or `Section`).

        Raises:
            ConfigurationError: If an unknown or unexpected model type is
                encountered.
        """
        # Dispatch by model instance type
        if isinstance(item_model, SectionModel):
            # Handle nested sections directly
            return self._build_section_model(item_model)
        if isinstance(item_model, LineItemModel):
            return LineItem(
                id=item_model.id,
                name=item_model.name,
                node_id=item_model.node_id,
                description=item_model.description,
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
            )
        if isinstance(item_model, CalculatedItemModel):
            # Pass the calculation model directly or its dict representation
            return CalculatedLineItem(
                id=item_model.id,
                name=item_model.name,
                # Pass the nested Pydantic model if structure expects dict
                calculation=item_model.calculation.model_dump(),  # Assuming structure expects dict
                description=item_model.description,
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
            )
        if isinstance(item_model, MetricItemModel):
            return MetricLineItem(
                id=item_model.id,
                name=item_model.name,
                metric_id=item_model.metric_id,
                inputs=item_model.inputs,
                description=item_model.description,
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
            )
        if isinstance(item_model, SubtotalModel):
            return self._build_subtotal_model(item_model)

        # Should be unreachable if Pydantic validation works
        logger.error(
            f"Encountered unknown item model type during build: {type(item_model).__name__}"
        )
        raise ConfigurationError(
            message=f"Unknown item model type: {type(item_model).__name__}",
            errors=[
                f"Item '{getattr(item_model, 'id', '<unknown>')}' has invalid model type."
            ],
        )

    def _build_subtotal_model(self, subtotal_model: SubtotalModel) -> SubtotalLineItem:
        """Build a `SubtotalLineItem` object from a `SubtotalModel`.

        Extracts the relevant item IDs to be summed, either from the explicit
        `items_to_sum` list or from the `calculation.inputs` if provided.

        Args:
            subtotal_model: The Pydantic model representing the subtotal configuration.

        Returns:
            A `SubtotalLineItem` instance.
        """
        # Consolidate logic for getting item IDs
        item_ids = (
            subtotal_model.calculation.inputs
            if subtotal_model.calculation and subtotal_model.calculation.inputs
            else subtotal_model.items_to_sum
        )
        if not item_ids:
            logger.warning(
                f"Subtotal '{subtotal_model.id}' has no items_to_sum or calculation inputs defined."
            )
            # Decide handling: error or allow empty subtotal?
            # Allowing for now, may need adjustment based on desired behavior.

        return SubtotalLineItem(
            id=subtotal_model.id,
            name=subtotal_model.name,
            item_ids=item_ids or [],  # Ensure it's a list
            description=subtotal_model.description,
            sign_convention=subtotal_model.sign_convention,
            metadata=subtotal_model.metadata,
        )

# --- END FILE: fin_statement_model/statements/structure/builder.py ---

# --- START FILE: fin_statement_model/statements/structure/containers.py ---
"""Container classes for defining hierarchical financial statement structures.

This module provides Section and StatementStructure, which organize
LineItem and CalculatedLineItem objects into nested groups.
"""

from typing import Any, Optional, Union

from fin_statement_model.core.errors import StatementError
from fin_statement_model.statements.structure.items import (
    StatementItem,
    StatementItemType,
    CalculatedLineItem,
    MetricLineItem,
    SubtotalLineItem,
)


__all__ = ["Section", "StatementStructure"]


class Section:
    """Represents a section in a financial statement.

    Sections group related items and subsections into a hierarchical container.
    """

    def __init__(
        self,
        id: str,
        name: str,
        description: str = "",
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a section.

        Args:
            id: Unique identifier for the section.
            name: Display name for the section.
            description: Optional description text.
            metadata: Optional additional metadata.

        Raises:
            StatementError: If id or name is invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(message=f"Invalid section ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(message=f"Invalid section name: {name} for ID: {id}")

        self._id = id
        self._name = name
        self._description = description
        self._metadata = metadata or {}
        self._items: list[Union[Section, StatementItem]] = []

    @property
    def id(self) -> str:
        """Get the section identifier."""
        return self._id

    @property
    def name(self) -> str:
        """Get the section display name."""
        return self._name

    @property
    def description(self) -> str:
        """Get the section description."""
        return self._description

    @property
    def metadata(self) -> dict[str, Any]:
        """Get the section metadata."""
        return self._metadata

    @property
    def items(self) -> list[Union["Section", StatementItem]]:
        """Get the child items and subsections."""
        return list(self._items)

    @property
    def item_type(self) -> StatementItemType:
        """Get the item type (SECTION)."""
        return StatementItemType.SECTION

    def add_item(self, item: Union["Section", StatementItem]) -> None:
        """Add a child item or subsection to this section.

        Args:
            item: The Section or StatementItem to add.

        Raises:
            StatementError: If an item with the same id already exists.
        """
        if any(existing.id == item.id for existing in self._items):
            raise StatementError(
                message=f"Duplicate item ID: {item.id} in section: {self.id}"
            )
        self._items.append(item)

    def find_item_by_id(
        self, item_id: str
    ) -> Optional[Union["Section", StatementItem]]:
        """Recursively find an item by its identifier within this section.

        Args:
            item_id: Identifier of the item to search for.

        Returns:
            The found Section or StatementItem, or None if not found.
        """
        if self.id == item_id:
            return self
        for child in self._items:
            if child.id == item_id:
                return child
            if isinstance(child, Section):
                found = child.find_item_by_id(item_id)
                if found:
                    return found
        if hasattr(self, "subtotal") and self.subtotal and self.subtotal.id == item_id:
            return self.subtotal
        return None


class StatementStructure:
    """Top-level container for a financial statement structure.

    Manages a hierarchy of Section objects.
    """

    def __init__(
        self,
        id: str,
        name: str,
        description: str = "",
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a statement structure.

        Args:
            id: Unique identifier for the statement.
            name: Display name for the statement.
            description: Optional description text.
            metadata: Optional additional metadata.

        Raises:
            StatementError: If id or name is invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(message=f"Invalid statement ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(message=f"Invalid statement name: {name} for ID: {id}")

        self._id = id
        self._name = name
        self._description = description
        self._metadata = metadata or {}
        self._sections: list[Section] = []

    @property
    def id(self) -> str:
        """Get the statement identifier."""
        return self._id

    @property
    def name(self) -> str:
        """Get the statement display name."""
        return self._name

    @property
    def description(self) -> str:
        """Get the statement description."""
        return self._description

    @property
    def metadata(self) -> dict[str, Any]:
        """Get the statement metadata."""
        return self._metadata

    @property
    def sections(self) -> list[Section]:
        """Get the top-level sections."""
        return list(self._sections)

    @property
    def items(self) -> list[Section]:
        """Alias for sections to ease iteration."""
        return self.sections

    def add_section(self, section: Section) -> None:
        """Add a section to the statement.

        Args:
            section: Section to add.

        Raises:
            StatementError: If a section with the same id already exists.
        """
        if any(s.id == section.id for s in self._sections):
            raise StatementError(
                message=f"Duplicate section ID: {section.id} in statement: {self.id}"
            )
        self._sections.append(section)

    def find_item_by_id(self, item_id: str) -> Optional[Union[Section, StatementItem]]:
        """Find an item by its ID in the statement structure.

        Args:
            item_id: The ID of the item to find.

        Returns:
            Optional[Union[Section, StatementItem]]: The found item or None if not found.
        """
        for section in self._sections:
            item = section.find_item_by_id(item_id)
            if item:
                return item
        return None

    def get_calculation_items(
        self,
    ) -> list[Union[CalculatedLineItem, SubtotalLineItem]]:
        """Get all calculation items from the statement structure.

        Returns:
            List[Union[CalculatedLineItem, SubtotalLineItem]]: List of calculation items.
        """
        calculation_items = []

        def collect_calculation_items(items: list[Union["Section", "StatementItem"]]):
            for item in items:
                if isinstance(item, CalculatedLineItem | SubtotalLineItem):
                    calculation_items.append(item)
                elif isinstance(item, Section):
                    collect_calculation_items(item.items)
                    if hasattr(item, "subtotal") and item.subtotal:
                        if isinstance(item.subtotal, SubtotalLineItem):
                            calculation_items.append(item.subtotal)
                        else:
                            pass

        collect_calculation_items(self._sections)
        return calculation_items

    def get_metric_items(self) -> list[MetricLineItem]:
        """Get all metric items from the statement structure.

        Returns:
            List[MetricLineItem]: List of metric items.
        """
        metric_items = []

        def collect_metric_items(items: list[Union["Section", "StatementItem"]]):
            for item in items:
                if isinstance(item, MetricLineItem):
                    metric_items.append(item)
                elif isinstance(item, Section):
                    collect_metric_items(item.items)
                    # Subtotals are handled by get_calculation_items, not relevant here

        collect_metric_items(self._sections)
        return metric_items

    def get_all_items(self) -> list[StatementItem]:
        """Get all StatementItem instances recursively from the structure.

        Traverses all sections and nested sections, collecting only objects that
        are subclasses of StatementItem (e.g., LineItem, CalculatedLineItem),
        excluding Section objects themselves.

        Returns:
            List[StatementItem]: A flat list of all statement items found.
        """
        all_statement_items: list[StatementItem] = []

        def _collect_items_recursive(
            items_or_sections: list[Union[Section, StatementItem]],
        ) -> None:
            for item in items_or_sections:
                if isinstance(item, Section):
                    _collect_items_recursive(item.items)
                    # Also collect the section's subtotal if it exists and is a StatementItem
                    if hasattr(item, "subtotal") and isinstance(
                        item.subtotal, StatementItem
                    ):
                        all_statement_items.append(item.subtotal)
                elif isinstance(item, StatementItem):
                    all_statement_items.append(item)

        _collect_items_recursive(self._sections)

        return all_statement_items

# --- END FILE: fin_statement_model/statements/structure/containers.py ---

# --- START FILE: fin_statement_model/statements/structure/items.py ---
"""Statement structure items module defining line items, calculated items, and subtotals."""

from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Optional

from fin_statement_model.core.errors import StatementError

__all__ = [
    "CalculatedLineItem",
    "LineItem",
    "MetricLineItem",
    "StatementItem",
    "StatementItemType",
    "SubtotalLineItem",
]


class StatementItemType(Enum):
    """Types of statement structure items.

    Attributes:
      SECTION: Section container
      LINE_ITEM: Basic financial line item
      SUBTOTAL: Subtotal of multiple items
      CALCULATED: Derived calculation item
      METRIC: Derived metric item from registry
    """

    SECTION = "section"
    LINE_ITEM = "line_item"
    SUBTOTAL = "subtotal"
    CALCULATED = "calculated"
    METRIC = "metric"


class StatementItem(ABC):
    """Abstract base class for all statement structure items.

    Defines a common interface: id, name, and item_type.
    """

    @property
    @abstractmethod
    def id(self) -> str:
        """Get the unique identifier of the item."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Get the display name of the item."""

    @property
    @abstractmethod
    def item_type(self) -> StatementItemType:
        """Get the type of this statement item."""


class LineItem(StatementItem):
    """Represents a basic line item in a financial statement.

    Args:
      id: Unique ID for the line item
      name: Display name for the line item
      node_id: ID of the core graph node that holds values
      description: Optional explanatory text
      sign_convention: 1 for normal values, -1 for inverted
      metadata: Optional additional attributes

    Raises:
      StatementError: If inputs are invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        node_id: str,
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a basic LineItem.

        Args:
            id: Unique ID for the line item.
            name: Display name for the line item.
            node_id: ID of the core graph node holding values.
            description: Optional explanatory text.
            sign_convention: Sign convention (1 for positive, -1 for negative).
            metadata: Optional additional attributes.

        Raises:
            StatementError: If inputs are invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(message=f"Invalid line item ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(message=f"Invalid line item name: {name} for ID: {id}")
        if not node_id or not isinstance(node_id, str):
            raise StatementError(message=f"Invalid node ID for line item: {id}")
        if sign_convention not in (1, -1):
            raise StatementError(
                message=f"Invalid sign convention {sign_convention} for item: {id}"
            )
        self._id = id
        self._name = name
        self._node_id = node_id
        self._description = description
        self._sign_convention = sign_convention
        self._metadata = metadata or {}

    @property
    def id(self) -> str:
        """Get the unique identifier of the line item."""
        return self._id

    @property
    def name(self) -> str:
        """Get the display name of the line item."""
        return self._name

    @property
    def node_id(self) -> str:
        """Get the core graph node ID for this line item."""
        return self._node_id

    @property
    def description(self) -> str:
        """Get the description for this line item."""
        return self._description

    @property
    def sign_convention(self) -> int:
        """Get the sign convention (1 or -1)."""
        return self._sign_convention

    @property
    def metadata(self) -> dict[str, Any]:
        """Get custom metadata associated with this item."""
        return self._metadata

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (LINE_ITEM)."""
        return StatementItemType.LINE_ITEM


class MetricLineItem(LineItem):
    """Represents a line item whose calculation is defined by a core metric.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      metric_id: ID of the metric in the core.metrics.registry
      inputs: Dict mapping metric input names to statement item IDs
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata

    Raises:
      StatementError: If metric_id or inputs are invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        metric_id: str,
        inputs: dict[str, str],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a MetricLineItem referencing a registered metric.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            metric_id: ID of the metric in the core.metrics.registry.
            inputs: Dict mapping metric input names to statement item IDs.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.

        Raises:
            StatementError: If metric_id or inputs are invalid.
        """
        super().__init__(
            id=id,
            name=name,
            node_id=id,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
        )
        if not metric_id or not isinstance(metric_id, str):
            raise StatementError(
                message=f"Invalid metric_id '{metric_id}' for item: {id}"
            )
        if not isinstance(inputs, dict) or not inputs:
            raise StatementError(
                message=f"Metric inputs must be a non-empty dictionary for item: {id}"
            )
        if not all(
            isinstance(k, str) and isinstance(v, str) for k, v in inputs.items()
        ):
            raise StatementError(
                message=f"Metric input keys and values must be strings for item: {id}"
            )

        self._metric_id = metric_id
        self._inputs = inputs

    @property
    def metric_id(self) -> str:
        """Get the ID of the metric referenced from the core registry."""
        return self._metric_id

    @property
    def inputs(self) -> dict[str, str]:
        """Get the mapping from metric input names to statement item IDs."""
        return self._inputs

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (METRIC)."""
        return StatementItemType.METRIC


class CalculatedLineItem(LineItem):
    """Represents a calculated line item whose values come from graph calculations.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      calculation: Dict with 'type', 'inputs', optional 'parameters'
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata

    Raises:
      StatementError: If calculation dictionary is invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        calculation: dict[str, Any],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a CalculatedLineItem based on calculation specification.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            calculation: Calculation spec dict with 'type', 'inputs', optional 'parameters'.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.

        Raises:
            StatementError: If calculation dictionary is invalid.
        """
        super().__init__(
            id=id,
            name=name,
            node_id=id,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
        )
        if not isinstance(calculation, dict):
            raise StatementError(message=f"Invalid calculation spec for item: {id}")
        if "type" not in calculation:
            raise StatementError(message=f"Missing calculation type for item: {id}")
        inputs = calculation.get("inputs")
        if not isinstance(inputs, list) or not inputs:
            raise StatementError(
                message=f"Calculation inputs must be a non-empty list for item: {id}"
            )
        self._calculation = calculation

    @property
    def calculation_type(self) -> str:
        """Get the calculation operation type (e.g., 'addition')."""
        return self._calculation["type"]

    @property
    def input_ids(self) -> list[str]:
        """Get the list of input item IDs for this calculation."""
        return self._calculation["inputs"]

    @property
    def parameters(self) -> dict[str, Any]:
        """Get optional parameters for the calculation."""
        return self._calculation.get("parameters", {})

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (CALCULATED)."""
        return StatementItemType.CALCULATED


class SubtotalLineItem(CalculatedLineItem):
    """Represents a subtotal line item summing multiple other items.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      item_ids: List of IDs to sum
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata

    Raises:
      StatementError: If item_ids is empty or not a list
    """

    def __init__(
        self,
        id: str,
        name: str,
        item_ids: list[str],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a SubtotalLineItem summing multiple items.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            item_ids: List of IDs to sum.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.

        Raises:
            StatementError: If item_ids is empty or not a list.
        """
        if not isinstance(item_ids, list) or not item_ids:
            raise StatementError(
                message=f"Invalid or empty item IDs for subtotal: {id}"
            )
        calculation = {"type": "addition", "inputs": item_ids, "parameters": {}}
        super().__init__(
            id=id,
            name=name,
            calculation=calculation,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
        )
        self._item_ids = item_ids

    @property
    def item_ids(self) -> list[str]:
        """Get the IDs of items summed by this subtotal."""
        return self._item_ids

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (SUBTOTAL)."""
        return StatementItemType.SUBTOTAL

# --- END FILE: fin_statement_model/statements/structure/items.py ---

# --- START FILE: fin_statement_model/statements/utilities/__init__.py ---
"""Cross-cutting utilities for the statements package.

This package provides reusable components:
- Result types for functional error handling
- Retry mechanisms for transient failures
- Common error codes and handling patterns
"""

from .result_types import (
    ErrorCollector,
    ErrorDetail,
    ErrorSeverity,
    Failure,
    OperationResult,
    ProcessingResult,
    Result,
    Success,
    ValidationResult,
    combine_results,
)
from .retry_handler import (
    BackoffStrategy,
    ConstantBackoff,
    ExponentialBackoff,
    LinearBackoff,
    RetryConfig,
    RetryHandler,
    RetryResult,
    RetryStrategy,
    retry_on_specific_errors,
    retry_with_exponential_backoff,
)

__all__ = [
    "BackoffStrategy",
    "ConstantBackoff",
    "ErrorCollector",
    "ErrorDetail",
    "ErrorSeverity",
    "ExponentialBackoff",
    "Failure",
    "LinearBackoff",
    "OperationResult",
    "ProcessingResult",
    # Result Types
    "Result",
    "RetryConfig",
    # Retry Handler
    "RetryHandler",
    "RetryResult",
    "RetryStrategy",
    "Success",
    "ValidationResult",
    "combine_results",
    "retry_on_specific_errors",
    "retry_with_exponential_backoff",
]

# --- END FILE: fin_statement_model/statements/utilities/__init__.py ---

# --- START FILE: fin_statement_model/statements/utilities/result_types.py ---
"""Common result types for standardized error handling in the statements module.

This module provides consistent result types and error collection utilities
to standardize error handling across the statements package. These types
enable functional error handling without exceptions for operations that
can fail in expected ways.
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Generic, Optional, TypeVar

logger = logging.getLogger(__name__)

__all__ = [
    "ErrorCollector",
    "ErrorDetail",
    "ErrorSeverity",
    "Failure",
    "OperationResult",
    "ProcessingResult",
    "Result",
    "Success",
    "ValidationResult",
]

T = TypeVar("T")


class ErrorSeverity(Enum):
    """Severity levels for errors."""

    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass(frozen=True)
class ErrorDetail:
    """Detailed information about an error.

    Attributes:
        code: Error code for programmatic handling
        message: Human-readable error message
        context: Optional context about where/what caused the error
        severity: Severity level of the error
        source: Optional source identifier (e.g., item ID, file path)
    """

    code: str
    message: str
    context: Optional[str] = None
    severity: ErrorSeverity = ErrorSeverity.ERROR
    source: Optional[str] = None

    def __str__(self) -> str:
        """Format error as string."""
        parts = [f"[{self.severity.value.upper()}]"]
        if self.source:
            parts.append(f"{self.source}:")
        parts.append(self.message)
        if self.context:
            parts.append(f"({self.context})")
        return " ".join(parts)


class Result(ABC, Generic[T]):
    """Abstract base class for operation results.

    Provides a functional approach to error handling, allowing
    operations to return either success or failure without exceptions.
    """

    @abstractmethod
    def is_success(self) -> bool:
        """Check if the result represents success."""

    @abstractmethod
    def is_failure(self) -> bool:
        """Check if the result represents failure."""

    @abstractmethod
    def get_value(self) -> Optional[T]:
        """Get the success value if available."""

    @abstractmethod
    def get_errors(self) -> list[ErrorDetail]:
        """Get error details if this is a failure."""

    def unwrap(self) -> T:
        """Get the value or raise an exception if failed.

        Raises:
            ValueError: If the result is a failure.
        """
        if self.is_failure():
            errors_str = "\n".join(str(e) for e in self.get_errors())
            raise ValueError(f"Cannot unwrap failed result:\n{errors_str}")
        return self.get_value()  # type: ignore

    def unwrap_or(self, default: T) -> T:
        """Get the value or return a default if failed."""
        return self.get_value() if self.is_success() else default


@dataclass(frozen=True)
class Success(Result[T]):
    """Represents a successful operation result."""

    value: T

    def is_success(self) -> bool:
        """Always returns True for Success."""
        return True

    def is_failure(self) -> bool:
        """Always returns False for Success."""
        return False

    def get_value(self) -> Optional[T]:
        """Return the success value."""
        return self.value

    def get_errors(self) -> list[ErrorDetail]:
        """Return empty list for Success."""
        return []


@dataclass(frozen=True)
class Failure(Result[T]):
    """Represents a failed operation result."""

    errors: list[ErrorDetail] = field(default_factory=list)

    def __post_init__(self):
        """Ensure at least one error is present."""
        if not self.errors:
            # Add a default error if none provided
            object.__setattr__(
                self,
                "errors",
                [
                    ErrorDetail(
                        code="unknown",
                        message="Operation failed with no specific error",
                    )
                ],
            )

    def is_success(self) -> bool:
        """Always returns False for Failure."""
        return False

    def is_failure(self) -> bool:
        """Always returns True for Failure."""
        return True

    def get_value(self) -> Optional[T]:
        """Always returns None for Failure."""
        return None

    def get_errors(self) -> list[ErrorDetail]:
        """Return the error details."""
        return self.errors

    @classmethod
    def from_exception(cls, exc: Exception, code: str = "exception") -> "Failure[T]":
        """Create a Failure from an exception."""
        return cls(
            errors=[
                ErrorDetail(
                    code=code,
                    message=str(exc),
                    context=type(exc).__name__,
                    severity=ErrorSeverity.ERROR,
                )
            ]
        )


class ErrorCollector:
    """Collects errors during multi-step operations.

    Useful for operations that should continue collecting errors
    rather than failing fast on the first error.
    """

    def __init__(self):
        """Initialize an empty error collector."""
        self._errors: list[ErrorDetail] = []
        self._warnings: list[ErrorDetail] = []

    def add_error(
        self,
        code: str,
        message: str,
        context: Optional[str] = None,
        source: Optional[str] = None,
    ) -> None:
        """Add an error to the collector."""
        self._errors.append(
            ErrorDetail(
                code=code,
                message=message,
                context=context,
                severity=ErrorSeverity.ERROR,
                source=source,
            )
        )

    def add_warning(
        self,
        code: str,
        message: str,
        context: Optional[str] = None,
        source: Optional[str] = None,
    ) -> None:
        """Add a warning to the collector."""
        self._warnings.append(
            ErrorDetail(
                code=code,
                message=message,
                context=context,
                severity=ErrorSeverity.WARNING,
                source=source,
            )
        )

    def add_from_result(self, result: Result, source: Optional[str] = None) -> None:
        """Add errors from a Result object."""
        if result.is_failure():
            for error in result.get_errors():
                # Override source if provided
                if source:
                    new_error = ErrorDetail(
                        code=error.code,
                        message=error.message,
                        context=error.context,
                        severity=error.severity,
                        source=source,
                    )
                    if new_error.severity == ErrorSeverity.WARNING:
                        self._warnings.append(new_error)
                    else:
                        self._errors.append(new_error)
                elif error.severity == ErrorSeverity.WARNING:
                    self._warnings.append(error)
                else:
                    self._errors.append(error)

    def has_errors(self) -> bool:
        """Check if any errors have been collected."""
        return len(self._errors) > 0

    def has_warnings(self) -> bool:
        """Check if any warnings have been collected."""
        return len(self._warnings) > 0

    def get_errors(self) -> list[ErrorDetail]:
        """Get all collected errors (not warnings)."""
        return list(self._errors)

    def get_warnings(self) -> list[ErrorDetail]:
        """Get all collected warnings."""
        return list(self._warnings)

    def get_all(self) -> list[ErrorDetail]:
        """Get all collected errors and warnings."""
        return self._errors + self._warnings

    def to_result(self, value: T = None) -> Result[T]:
        """Convert collector state to a Result.

        If there are errors, returns Failure.
        Otherwise returns Success with the provided value.
        """
        if self.has_errors():
            return Failure(errors=self._errors)
        return Success(value=value)

    def log_all(self, prefix: str = "") -> None:
        """Log all collected errors and warnings."""
        for warning in self._warnings:
            logger.warning(f"{prefix}{warning}")
        for error in self._errors:
            logger.error(f"{prefix}{error}")


# Type aliases for common result types
OperationResult = Result[
    bool
]  # For operations that succeed/fail without returning data
ValidationResult = Result[bool]  # For validation operations
ProcessingResult = Result[dict[str, any]]  # For processing operations that return data


def combine_results(*results: Result[T]) -> Result[list[T]]:
    """Combine multiple results into a single result.

    If all results are successful, returns Success with list of values.
    If any result is a failure, returns Failure with all errors combined.
    """
    collector = ErrorCollector()
    values = []

    for result in results:
        if result.is_success():
            values.append(result.get_value())
        else:
            for error in result.get_errors():
                if error.severity == ErrorSeverity.WARNING:
                    collector.add_warning(
                        error.code, error.message, error.context, error.source
                    )
                else:
                    collector.add_error(
                        error.code, error.message, error.context, error.source
                    )

    if collector.has_errors():
        return Failure(errors=collector.get_all())
    return Success(value=values)

# --- END FILE: fin_statement_model/statements/utilities/result_types.py ---

# --- START FILE: fin_statement_model/statements/utilities/retry_handler.py ---
"""Retry handler for managing transient failures in statement operations.

This module provides a flexible retry mechanism that can be used throughout
the statements package to handle transient failures gracefully. It supports
configurable retry strategies, backoff algorithms, and error classification.
"""

import logging
import random
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Generic, Optional, TypeVar
from collections.abc import Callable

from fin_statement_model.statements.utilities.result_types import (
    Result,
    Failure,
    ErrorDetail,
    ErrorCollector,
)

logger = logging.getLogger(__name__)

__all__ = [
    "BackoffStrategy",
    "ConstantBackoff",
    "ExponentialBackoff",
    "LinearBackoff",
    "RetryConfig",
    "RetryHandler",
    "RetryResult",
    "RetryStrategy",
    "is_retryable_error",
    "retry_on_specific_errors",
    "retry_with_exponential_backoff",
]

T = TypeVar("T")


class RetryStrategy(Enum):
    """Strategy for determining when to retry."""

    IMMEDIATE = "immediate"  # Retry immediately on failure
    BACKOFF = "backoff"  # Use backoff strategy between retries
    CONDITIONAL = "conditional"  # Retry only for specific error types


@dataclass
class RetryConfig:
    """Configuration for retry behavior.

    Attributes:
        max_attempts: Maximum number of attempts (including initial)
        strategy: Retry strategy to use
        backoff: Optional backoff strategy for delays
        retryable_errors: Set of error codes that are retryable
        log_retries: Whether to log retry attempts
        collect_all_errors: Whether to collect errors from all attempts
    """

    max_attempts: int = 3
    strategy: RetryStrategy = RetryStrategy.BACKOFF
    backoff: Optional["BackoffStrategy"] = None
    retryable_errors: Optional[set[str]] = None
    log_retries: bool = True
    collect_all_errors: bool = False

    def __post_init__(self):
        """Validate configuration and set defaults."""
        if self.max_attempts < 1:
            raise ValueError("max_attempts must be at least 1")

        if self.strategy == RetryStrategy.BACKOFF and not self.backoff:
            # Default to exponential backoff
            self.backoff = ExponentialBackoff()

        if self.retryable_errors is None:
            # Default retryable errors
            self.retryable_errors = {
                "timeout",
                "connection_error",
                "rate_limit",
                "temporary_failure",
                "calculation_error",  # For graph calculations
                "node_not_ready",  # For dependency issues
            }


class BackoffStrategy(ABC):
    """Abstract base class for backoff strategies."""

    @abstractmethod
    def get_delay(self, attempt: int) -> float:
        """Get delay in seconds for the given attempt number.

        Args:
            attempt: The attempt number (1-based)

        Returns:
            Delay in seconds before the next retry
        """


class ExponentialBackoff(BackoffStrategy):
    """Exponential backoff strategy with optional jitter."""

    def __init__(
        self,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        multiplier: float = 2.0,
        jitter: bool = True,
    ):
        """Initialize exponential backoff.

        Args:
            base_delay: Initial delay in seconds
            max_delay: Maximum delay in seconds
            multiplier: Multiplier for each retry
            jitter: Whether to add random jitter
        """
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.multiplier = multiplier
        self.jitter = jitter

    def get_delay(self, attempt: int) -> float:
        """Calculate exponential backoff delay."""
        delay = min(
            self.base_delay * (self.multiplier ** (attempt - 1)), self.max_delay
        )

        if self.jitter:
            # Add up to 20% jitter
            jitter_amount = delay * 0.2 * random.random()  # noqa: S311
            delay += jitter_amount

        return delay


class LinearBackoff(BackoffStrategy):
    """Linear backoff strategy."""

    def __init__(self, base_delay: float = 1.0, increment: float = 1.0):
        """Initialize linear backoff.

        Args:
            base_delay: Initial delay in seconds
            increment: Increment for each retry
        """
        self.base_delay = base_delay
        self.increment = increment

    def get_delay(self, attempt: int) -> float:
        """Calculate linear backoff delay."""
        return self.base_delay + (attempt - 1) * self.increment


class ConstantBackoff(BackoffStrategy):
    """Constant delay backoff strategy."""

    def __init__(self, delay: float = 1.0):
        """Initialize constant backoff.

        Args:
            delay: Constant delay in seconds
        """
        self.delay = delay

    def get_delay(self, attempt: int) -> float:
        """Return constant delay."""
        return self.delay


@dataclass
class RetryResult(Generic[T]):
    """Result of a retry operation.

    Attributes:
        result: The final result (success or failure)
        attempts: Number of attempts made
        total_delay: Total delay time in seconds
        all_errors: All errors collected if configured
    """

    result: Result[T]
    attempts: int
    total_delay: float
    all_errors: Optional[list[ErrorDetail]] = None

    @property
    def success(self) -> bool:
        """Check if the operation eventually succeeded."""
        return self.result.is_success()

    def unwrap(self) -> T:
        """Get the value or raise an exception."""
        return self.result.unwrap()

    def unwrap_or(self, default: T) -> T:
        """Get the value or return default."""
        return self.result.unwrap_or(default)


def is_retryable_error(error: ErrorDetail, retryable_codes: set[str]) -> bool:
    """Check if an error is retryable based on its code.

    Args:
        error: The error to check
        retryable_codes: Set of error codes that are retryable

    Returns:
        True if the error should be retried
    """
    return error.code in retryable_codes


class RetryHandler:
    """Handles retry logic for operations that may fail transiently."""

    def __init__(self, config: Optional[RetryConfig] = None):
        """Initialize retry handler.

        Args:
            config: Retry configuration, uses defaults if not provided
        """
        self.config = config or RetryConfig()

    def retry(
        self,
        operation: Callable[[], Result[T]],
        operation_name: Optional[str] = None,
    ) -> RetryResult[T]:
        """Execute an operation with retry logic.

        Args:
            operation: Callable that returns a Result
            operation_name: Optional name for logging

        Returns:
            RetryResult containing the final result and metadata
        """
        error_collector = ErrorCollector() if self.config.collect_all_errors else None
        total_delay = 0.0
        op_name = operation_name or operation.__name__

        for attempt in range(1, self.config.max_attempts + 1):
            if attempt > 1 and self.config.log_retries:
                logger.info(
                    f"Retrying {op_name} (attempt {attempt}/{self.config.max_attempts})"
                )

            # Execute the operation
            try:
                result = operation()
            except Exception as e:
                # Convert exception to Result
                result = Failure.from_exception(e)

            # Check if successful
            if result.is_success():
                return RetryResult(
                    result=result,
                    attempts=attempt,
                    total_delay=total_delay,
                    all_errors=error_collector.get_all() if error_collector else None,
                )

            # Handle failure
            errors = result.get_errors()

            # Collect errors if configured
            if error_collector:
                for error in errors:
                    error_collector.add_error(
                        code=error.code,
                        message=f"Attempt {attempt}: {error.message}",
                        context=error.context,
                        source=error.source,
                    )

            # Check if we should retry
            if attempt >= self.config.max_attempts:
                # No more retries
                if self.config.log_retries:
                    logger.warning(
                        f"{op_name} failed after {attempt} attempts. "
                        f"Final error: {errors[0].message if errors else 'Unknown'}"
                    )
                break

            # Check if errors are retryable
            if self.config.strategy == RetryStrategy.CONDITIONAL:
                retryable = any(
                    is_retryable_error(error, self.config.retryable_errors)
                    for error in errors
                )
                if not retryable:
                    if self.config.log_retries:
                        logger.debug(
                            f"{op_name} failed with non-retryable error: "
                            f"{errors[0].code if errors else 'Unknown'}"
                        )
                    break

            # Calculate delay
            if self.config.strategy == RetryStrategy.BACKOFF and self.config.backoff:
                delay = self.config.backoff.get_delay(attempt)
                if self.config.log_retries:
                    logger.debug(f"Waiting {delay:.2f}s before retry")
                time.sleep(delay)
                total_delay += delay

        # Return final result
        return RetryResult(
            result=result,
            attempts=attempt,
            total_delay=total_delay,
            all_errors=error_collector.get_all() if error_collector else None,
        )

    def retry_async(
        self,
        operation: Callable[[], Result[T]],
        operation_name: Optional[str] = None,
    ) -> RetryResult[T]:
        """Execute an async operation with retry logic.

        Note: This is a placeholder for future async support.
        Currently just delegates to sync retry.

        Args:
            operation: Callable that returns a Result
            operation_name: Optional name for logging

        Returns:
            RetryResult containing the final result and metadata
        """
        # TODO: Implement proper async support when needed
        return self.retry(operation, operation_name)


# Convenience functions for common retry patterns


def retry_with_exponential_backoff(
    operation: Callable[[], Result[T]],
    max_attempts: int = 3,
    base_delay: float = 1.0,
    operation_name: Optional[str] = None,
) -> RetryResult[T]:
    """Retry an operation with exponential backoff.

    Args:
        operation: The operation to retry
        max_attempts: Maximum number of attempts
        base_delay: Initial delay in seconds
        operation_name: Optional name for logging

    Returns:
        RetryResult with the final outcome
    """
    config = RetryConfig(
        max_attempts=max_attempts,
        strategy=RetryStrategy.BACKOFF,
        backoff=ExponentialBackoff(base_delay=base_delay),
    )
    handler = RetryHandler(config)
    return handler.retry(operation, operation_name)


def retry_on_specific_errors(
    operation: Callable[[], Result[T]],
    retryable_errors: set[str],
    max_attempts: int = 3,
    operation_name: Optional[str] = None,
) -> RetryResult[T]:
    """Retry an operation only for specific error codes.

    Args:
        operation: The operation to retry
        retryable_errors: Set of error codes that trigger retry
        max_attempts: Maximum number of attempts
        operation_name: Optional name for logging

    Returns:
        RetryResult with the final outcome
    """
    config = RetryConfig(
        max_attempts=max_attempts,
        strategy=RetryStrategy.CONDITIONAL,
        retryable_errors=retryable_errors,
        backoff=ExponentialBackoff(),
    )
    handler = RetryHandler(config)
    return handler.retry(operation, operation_name)

# --- END FILE: fin_statement_model/statements/utilities/retry_handler.py ---


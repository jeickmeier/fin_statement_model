# --- START FILE: fin_statement_model/__init__.py ---
"""finlib - A Python library for financial statement analysis and forecasting."""

__all__ = [
    "CalculationNode",
    "CurveGrowthForecastNode",
    "CustomGrowthForecastNode",
    "FinancialStatementGraph",
    "FinancialStatementItemNode",
    "FixedGrowthForecastNode",
    "ForecastNode",
    "Graph",
    "LLMClient",
    "LLMConfig",
    "MultiPeriodStatNode",
    "Node",
    "StatisticalGrowthForecastNode",
    "YoYGrowthNode",
]

from .extensions.llm.llm_client import LLMClient, LLMConfig
from .core.graph import Graph
from .core.nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    YoYGrowthNode,
    MultiPeriodStatNode,
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    CustomGrowthForecastNode,
)

# ensure our library-wide logging policy is applied immediately
from . import logging_config  # noqa: F401

# --------------------------------------------------------------------------
# Optional Extensions (via entry points / importlib.metadata)
# --------------------------------------------------------------------------
# Extensions are optional modules that add functionality without modifying
# the core library. They might depend on heavy libraries (e.g., LLMs,
# ML frameworks) and should be lazy-loaded.
# Example entry point group: 'fin_statement_model.extensions.reporting'
# Expected interface: TBD (e.g., a class with specific methods)
# Note: Avoid hard imports from extensions into core/statements/io.
# Goal: Keep core library lean, allow users to install extras like:
# pip install fin-statement-model[openai]
# pip install fin-statement-model[reporting-tools]
# --------------------------------------------------------------------------


# Core API Exports (ensure essential classes/functions are accessible)
# Example:
# from .core.graph import Graph
# from .core.nodes import Node, FinancialStatementItemNode
# from .core.calculation_engine import CalculationEngine
# from .statements.manager import StatementManager

# Placeholder: Explicitly list key public API components later.
# For now, just rely on sub-package __init__ files if they exist.

__version__ = "0.1.0"  # Central version definition

# --- END FILE: fin_statement_model/__init__.py ---

# --- START FILE: fin_statement_model/core/__init__.py ---
"""Core components for the Financial Statement Model.

This package forms the foundation of the library, providing the core infrastructure
for building, calculating, and managing financial models. It includes:

- Graph engine (`core.graph`): For representing financial relationships.
- Base node hierarchy (`core.nodes`): Abstract and concrete node types.
- Calculation engine (`calculation_engine.py`): For evaluating the graph.
- Metric registry and definitions (`core.metrics`): For managing financial metrics.
- Data management (`data_manager.py`): For handling financial data.
- Calculation strategies (`core.strategies`): Reusable calculation logic.
- Core utilities and exceptions (`errors.py`, `node_factory.py`).

This `core` package is designed to be self-contained and does not depend on
other higher-level packages like `statements`, `io`, or `forecasting`.
"""

from .node_factory import NodeFactory
from .graph import Graph
from .nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    YoYGrowthNode,
    MultiPeriodStatNode,
    FormulaCalculationNode,
    CustomCalculationNode,
    TwoPeriodAverageNode,
)
from .calculations import (
    AdditionCalculation,
    SubtractionCalculation,
    MultiplicationCalculation,
    DivisionCalculation,
)
from .errors import (
    FinancialModelError,
    ConfigurationError,
    CalculationError,
    NodeError,
    GraphError,
    DataValidationError,
    CircularDependencyError,
    PeriodError,
    StatementError,
    StrategyError,
    TransformationError,
)

__all__ = [
    "AdditionCalculation",
    "CalculationError",
    "CalculationNode",
    "CircularDependencyError",
    "ConfigurationError",
    "CustomCalculationNode",
    "DataValidationError",
    "DivisionCalculation",
    "FinancialModelError",
    "FinancialStatementItemNode",
    "FormulaCalculationNode",
    "Graph",
    "GraphError",
    "MultiPeriodStatNode",
    "MultiplicationCalculation",
    "Node",
    "NodeError",
    "NodeFactory",
    "PeriodError",
    "StatementError",
    "StrategyError",
    "SubtractionCalculation",
    "TransformationError",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]

# --- END FILE: fin_statement_model/core/__init__.py ---

# --- START FILE: fin_statement_model/core/calculations/__init__.py ---
"""Calculations module for the Financial Statement Model.

This module provides classes for implementing the Calculation Pattern for calculations
in the Financial Statement Model. It allows different calculation algorithms to be
defined, registered, and applied to financial data.
"""

from .calculation import (
    Calculation,
    AdditionCalculation,
    SubtractionCalculation,
    MultiplicationCalculation,
    DivisionCalculation,
    WeightedAverageCalculation,
    CustomFormulaCalculation,
)
from .registry import Registry

# Register calculations
Registry.register(AdditionCalculation)
Registry.register(SubtractionCalculation)
Registry.register(MultiplicationCalculation)
Registry.register(DivisionCalculation)
Registry.register(WeightedAverageCalculation)
Registry.register(CustomFormulaCalculation)

__all__ = [
    "AdditionCalculation",
    "Calculation",
    "CustomFormulaCalculation",
    "DivisionCalculation",
    "MultiplicationCalculation",
    "Registry",
    "SubtractionCalculation",
    "WeightedAverageCalculation",
]

# --- END FILE: fin_statement_model/core/calculations/__init__.py ---

# --- START FILE: fin_statement_model/core/calculations/calculation.py ---
"""Calculation for the Financial Statement Model.

This module provides the Calculation Pattern implementation for calculations,
allowing different calculation types to be encapsulated in calculation classes.
"""

from abc import ABC, abstractmethod
import logging
from typing import Callable, Optional

from fin_statement_model.core.nodes.base import Node  # Absolute

# Configure logging
logger = logging.getLogger(__name__)


class Calculation(ABC):
    """Abstract base class for all calculations.

    This class defines the interface that all concrete calculation classes must
    implement. It employs a calculation pattern, allowing the algorithm
    used by a CalculationNode to be selected at runtime.

    Each concrete calculation encapsulates a specific method for computing a
    financial value based on a list of input nodes and a given time period.
    """

    @abstractmethod
    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculate a value based on input nodes for a specific period.

        This abstract method must be implemented by all concrete calculation classes.
        It defines the core logic for the calculation.

        Args:
            inputs: A list of input Node objects whose values will be used in
                the calculation.
            period: The time period string (e.g., "2023Q1") for which the
                calculation should be performed.

        Returns:
            The calculated numerical value as a float.

        Raises:
            NotImplementedError: If the method is not implemented by a subclass.
            ValueError: If the inputs are invalid for the specific calculation
                (e.g., wrong number of inputs, incompatible types).
            ZeroDivisionError: If the calculation involves division and a divisor
                is zero.
            Exception: Other exceptions depending on the calculation logic.
        """
        # pragma: no cover

    @property
    def description(self) -> str:
        """Provides a human-readable description of the calculation.

        This is useful for documentation, debugging, and for user interfaces
        that need to explain how a value is derived.

        Returns:
            A string describing the calculation.
        """
        # Default implementation returns the class name. Subclasses should override
        # for more specific descriptions.
        class_name = self.__class__.__name__  # pragma: no cover
        return class_name


class AdditionCalculation(Calculation):
    """Implements an addition calculation, summing values from multiple input nodes.

    This calculation sums the values obtained from calling
    the `calculate` method on each of the provided input nodes for a given period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Sums the calculated values from all input nodes for the specified period.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023Q4") for the calculation.

        Returns:
            The total sum of the values calculated from the input nodes. Returns
            0.0 if the input list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = AdditionCalculation()
            >>> nodes = [MockNode(10), MockNode(20), MockNode(5)]
            >>> strategy.calculate(nodes, "2023")
            35.0
            >>> strategy.calculate([], "2023")
            0.0
        """
        logger.debug(f"Applying addition calculation for period {period}")
        # Using a generator expression for potentially better memory efficiency
        return sum(input_node.calculate(period) for input_node in inputs)

    @property
    def description(self) -> str:
        """Returns a description of the addition calculation."""
        return "Addition (sum of all inputs)"


class SubtractionCalculation(Calculation):
    """Implements a subtraction calculation: first input minus the sum of the rest.

    This calculation takes the calculated value of the first node in the input list
    and subtracts the sum of the calculated values of all subsequent nodes for
    a specific period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the difference: value of the first input minus the sum of others.

        Args:
            inputs: A list of Node objects. Must contain at least one node.
            period: The time period string (e.g., "2024Q1") for the calculation.

        Returns:
            The result of the subtraction. If only one input node is provided,
            its value is returned.

        Raises:
            ValueError: If the `inputs` list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = SubtractionCalculation()
            >>> nodes = [MockNode(100), MockNode(20), MockNode(30)]
            >>> strategy.calculate(nodes, "2023")
            50.0
            >>> nodes_single = [MockNode(100)]
            >>> strategy.calculate(nodes_single, "2023")
            100.0
        """
        if not inputs:
            raise ValueError("Subtraction calculation requires at least one input node")

        logger.debug(f"Applying subtraction calculation for period {period}")
        # Calculate values first to avoid multiple calls if nodes are complex
        values = [node.calculate(period) for node in inputs]
        return values[0] - sum(values[1:])

    @property
    def description(self) -> str:
        """Returns a description of the subtraction calculation."""
        return "Subtraction (first input minus sum of subsequent inputs)"


class MultiplicationCalculation(Calculation):
    """Implements a multiplication calculation, calculating the product of input values.

    This calculation multiplies the calculated values of all provided input nodes
    for a given period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the product of the values from all input nodes.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023FY") for the calculation.

        Returns:
            The product of all input values. Returns 1.0 (multiplicative identity)
            if the input list is empty.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = MultiplicationCalculation()
            >>> nodes = [MockNode(2), MockNode(3), MockNode(4)]
            >>> strategy.calculate(nodes, "2023")
            24.0
            >>> strategy.calculate([], "2023")
            1.0
        """
        # Multiplication calculation should ideally return 1.0 for empty inputs.
        # Raising error if empty seems less conventional for multiplication.
        if not inputs:
            logger.warning("Multiplication calculation called with empty inputs, returning 1.0")
            return 1.0

        logger.debug(f"Applying multiplication calculation for period {period}")
        result = 1.0
        for input_node in inputs:
            result *= input_node.calculate(period)
        return result

    @property
    def description(self) -> str:
        """Returns a description of the multiplication calculation."""
        return "Multiplication (product of all inputs)"


class DivisionCalculation(Calculation):
    """Implements a division calculation: first input divided by the product of the rest.

    This calculation takes the calculated value of the first node (numerator) and
    divides it by the product of the calculated values of all subsequent nodes
    (denominator) for a specific period.
    """

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Calculates the division: first input / (product of subsequent inputs).

        Args:
            inputs: A list of Node objects. Must contain at least two nodes.
            period: The time period string (e.g., "2024Q2") for the calculation.

        Returns:
            The result of the division.

        Raises:
            ValueError: If `inputs` list contains fewer than two nodes.
            ZeroDivisionError: If the calculated product of the subsequent nodes
                (denominator) is zero.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> strategy = DivisionCalculation()
            >>> nodes = [MockNode(100), MockNode(5), MockNode(2)]
            >>> strategy.calculate(nodes, "2023")
            10.0
            >>> nodes_zero_denom = [MockNode(100), MockNode(5), MockNode(0)]
            >>> try:
            ...     strategy.calculate(nodes_zero_denom, "2023")
            ... except ZeroDivisionError as e:
            ...     # Example: logging the error instead of printing
            ...     logger.error(e)
            Division by zero: Denominator product is zero
        """
        if len(inputs) < 2:
            raise ValueError("Division calculation requires at least two input nodes")

        logger.debug(f"Applying division calculation for period {period}")

        values = [node.calculate(period) for node in inputs]
        numerator = values[0]

        denominator = 1.0
        for val in values[1:]:
            denominator *= val

        if denominator == 0.0:
            raise ZeroDivisionError("Division by zero: Denominator product is zero")

        return numerator / denominator

    @property
    def description(self) -> str:
        """Returns a description of the division calculation."""
        return "Division (first input / product of subsequent inputs)"


class WeightedAverageCalculation(Calculation):
    """Calculates the weighted average of input node values.

    This calculation computes the average of the values from input nodes, where each
    node's contribution is weighted. If no weights are provided during
    initialization, it defaults to an equal weighting (simple average).
    """

    def __init__(self, weights: Optional[list[float]] = None):
        """Initializes the WeightedAverageCalculation.

        Args:
            weights: An optional list of floats representing the weight for each
                corresponding input node. The length of this list must match the
                number of input nodes provided to the `calculate` method. If None,
                equal weights are assumed.
        """
        # Validate weights if provided immediately? No, validation happens in calculate
        # as the number of inputs isn't known here.
        self.weights = weights
        logger.info(f"Initialized WeightedAverageCalculation with weights: {weights}")

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Computes the weighted average of the input node values for the period.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2023H1") for the calculation.

        Returns:
            The calculated weighted average as a float.

        Raises:
            ValueError: If the `inputs` list is empty.
            ValueError: If `weights` were provided during initialization and their
                count does not match the number of `inputs`.
            ValueError: If the sum of weights is zero (to prevent division by zero
                if normalization were implemented differently).

        Examples:
            >>> class MockNode:
            ...     def __init__(self, value): self._value = value
            ...     def calculate(self, period): return self._value
            >>> # Equal weights (simple average)
            >>> strategy_equal = WeightedAverageCalculation()
            >>> nodes = [MockNode(10), MockNode(20), MockNode(30)]
            >>> strategy_equal.calculate(nodes, "2023")
            20.0
            >>> # Custom weights
            >>> strategy_custom = WeightedAverageCalculation(weights=[0.5, 0.3, 0.2])
            >>> strategy_custom.calculate(nodes, "2023")
            17.0
            >>> # Mismatched weights
            >>> strategy_mismatch = WeightedAverageCalculation(weights=[0.5, 0.5])
            >>> try:
            ...     strategy_mismatch.calculate(nodes, "2023")
            ... except ValueError as e:
            ...     # Example: logging the error instead of printing
            ...     logger.error(e)
            Number of weights (2) must match number of inputs (3)
        """
        if not inputs:
            raise ValueError("Weighted average calculation requires at least one input node")

        num_inputs = len(inputs)
        effective_weights: list[float]

        if self.weights is None:
            # Use equal weights if none provided
            if num_inputs == 0:  # Should be caught by the check above, but defensive
                return 0.0
            equal_weight = 1.0 / num_inputs
            effective_weights = [equal_weight] * num_inputs
            logger.debug("Using equal weights for weighted average.")
        elif len(self.weights) == num_inputs:
            effective_weights = self.weights
            logger.debug(f"Using provided weights: {effective_weights}")
        else:
            raise ValueError(
                f"Number of weights ({len(self.weights)}) must match "
                f"number of inputs ({num_inputs})"
            )

        logger.debug(f"Applying weighted average calculation for period {period}")
        weighted_sum = 0.0
        total_weight = sum(effective_weights)
        input_values = [node.calculate(period) for node in inputs]

        if total_weight == 0.0:
            # Avoid division by zero. If weights are all zero, the concept is ill-defined.
            # Returning 0 might be a reasonable default, or raising an error.
            # Let's raise ValueError for clarity.
            raise ValueError("Total weight for weighted average cannot be zero.")

        for value, weight in zip(input_values, effective_weights):
            weighted_sum += value * weight

        # If weights don't sum to 1, this isn't a standard weighted average.
        # Decide whether to normalize or return the weighted sum directly.
        # Normalize by total weight for a true weighted average.
        return weighted_sum / total_weight

    @property
    def description(self) -> str:
        """Returns a description of the weighted average calculation."""
        if self.weights:
            return f"Weighted Average (using provided weights: {self.weights})"
        else:
            return "Weighted Average (using equal weights)"


# Type alias for the custom formula function
FormulaFunc = Callable[[dict[str, float]], float]


class CustomFormulaCalculation(Calculation):
    """Executes a user-defined Python function to calculate a value.

    This calculation provides maximum flexibility by allowing any custom Python
    function to be used for calculation. The function receives a dictionary
    mapping input node names (or fallback names) to their calculated values
    for the period and should return a single float result.
    """

    def __init__(self, formula_function: FormulaFunc):
        """Initializes the CustomFormulaCalculation with a calculation function.

        Args:
            formula_function: A callable (function, lambda, etc.) that accepts
                a single argument: a dictionary mapping string keys (input node
                names or `input_<i>`) to their float values for the period.
                It must return a float.

        Raises:
            TypeError: If `formula_function` is not callable.
        """
        if not callable(formula_function):
            raise TypeError("formula_function must be callable")
        self.formula_function = formula_function
        logger.info(f"Initialized CustomFormulaCalculation with function: {formula_function.__name__}")

    def calculate(self, inputs: list[Node], period: str) -> float:
        """Applies the custom formula function to the calculated input values.

        Args:
            inputs: A list of Node objects.
            period: The time period string (e.g., "2025M1") for the calculation.

        Returns:
            The float result returned by the `formula_function`.

        Raises:
            ValueError: If the `formula_function` encounters an error during execution
                (e.g., incorrect input keys, calculation errors). Wraps the original
                exception.

        Examples:
            >>> class MockNode:
            ...     def __init__(self, name, value): self.name = name; self._value = value
            ...     def calculate(self, period): return self._value
            >>> def my_formula(data):
            ...     # Example: Gross Profit Margin
            ...     return (data['revenue'] - data['cogs']) / data['revenue'] * 100
            >>> strategy = CustomFormulaCalculation(my_formula)
            >>> nodes = [MockNode('revenue', 1000), MockNode('cogs', 600)]
            >>> strategy.calculate(nodes, "2023")
            40.0
            >>> # Example with unnamed nodes
            >>> def simple_sum(data):
            ...     return data['input_0'] + data['input_1']
            >>> strategy_unnamed = CustomFormulaCalculation(simple_sum)
            >>> nodes_unnamed = [MockNode(None, 10), MockNode(None, 20)] # No names
            >>> strategy_unnamed.calculate(nodes_unnamed, "2023")
            30.0
        """
        # Prepare input values dictionary, using names if available
        input_values: dict[str, float] = {}
        for i, node in enumerate(inputs):
            # Prefer node.name if it exists and is a non-empty string
            key = getattr(node, "name", None)
            if not isinstance(key, str) or not key:
                key = f"input_{i}"
            input_values[key] = node.calculate(period)

        logger.debug(
            f"Applying custom formula calculation for period {period} with inputs: {input_values}"
        )
        try:
            # Execute the user-provided function
            result = self.formula_function(input_values)
            if not isinstance(result, (int, float)):
                logger.warning(
                    f"Custom formula function {self.formula_function.__name__} "
                    f"returned non-numeric type: {type(result)}. Attempting cast."
                )
                # Attempt conversion, but be aware this might fail or be lossy
                try:
                    return float(result)
                except (ValueError, TypeError) as cast_err:
                    raise ValueError(
                        f"Custom formula {self.formula_function.__name__} result "
                        f"({result!r}) could not be cast to float."
                    ) from cast_err
            return float(result)  # Ensure result is float
        except Exception as e:
            # Catch any exception from the custom function and wrap it
            logger.error(
                f"Error executing custom formula '{self.formula_function.__name__}': {e}",
                exc_info=True,
            )
            raise ValueError(
                f"Error in custom formula '{self.formula_function.__name__}': {e}"
            ) from e

    @property
    def description(self) -> str:
        """Returns a description of the custom formula calculation."""
        func_name = getattr(self.formula_function, "__name__", "[anonymous function]")
        return f"Custom Formula (using function: {func_name})"

# --- END FILE: fin_statement_model/core/calculations/calculation.py ---

# --- START FILE: fin_statement_model/core/calculations/registry.py ---
"""Registry for calculation classes in the Financial Statement Model.

This module provides a central registry for discovering and accessing different
calculation classes. Calculations can be registered using their class
object and later retrieved by their class name.
"""

# Use lowercase built-in types
from typing import ClassVar  # Keep Type for now
import logging

from .calculation import Calculation

# Configure logging
logger = logging.getLogger(__name__)


class Registry:
    """A central registry for managing and accessing calculation classes.

    This class uses class methods to provide a global registry. Calculations
    are stored in a dictionary mapping their class name (string) to the
    calculation class itself.

    Attributes:
        _strategies: A dictionary holding the registered calculation classes.
                     Keys are calculation class names (str), values are calculation
                     types (Type[Calculation]).
    """

    _strategies: ClassVar[dict[str, type[Calculation]]] = {}  # Use dict, type

    @classmethod
    def register(cls, calculation: type[Calculation]) -> None:
        """Register a calculation class with the registry.

        If a calculation with the same name is already registered, it will be
        overwritten.

        Args:
            calculation: The calculation class (Type[Calculation]) to register.
                         The class's __name__ attribute will be used as the key.
        """
        if not issubclass(calculation, Calculation):
            raise TypeError(f"Can only register subclasses of Calculation, not {calculation}")
        cls._strategies[calculation.__name__] = calculation
        logger.debug(f"Registered calculation: {calculation.__name__}")

    @classmethod
    def get(cls, name: str) -> type[Calculation]:
        """Retrieve a calculation class from the registry by its name.

        Args:
            name: The string name of the calculation class to retrieve.

        Returns:
            The calculation class (Type[Calculation]) associated with the given name.

        Raises:
            KeyError: If no calculation with the specified name is found in the
                      registry.
        """
        # Debug print including id of the dictionary
        if name not in cls._strategies:
            logger.error(f"Attempted to access unregistered calculation: {name}")
            raise KeyError(f"Calculation '{name}' not found in registry.")
        return cls._strategies[name]

    @classmethod
    def list(cls) -> dict[str, type[Calculation]]:  # Use dict, type
        """List all registered calculation classes.

        Returns:
            A dictionary containing all registered calculation names (str) and their
            corresponding calculation classes (Type[Calculation]). Returns a copy
            to prevent modification of the internal registry.
        """
        return cls._strategies.copy()

# --- END FILE: fin_statement_model/core/calculations/registry.py ---

# --- START FILE: fin_statement_model/core/errors.py ---
"""Define custom exceptions for the Financial Statement Model.

This module defines exception classes for specific error cases in the
Financial Statement Model, allowing for more precise error handling
and better error messages.
"""

from typing import Optional, Any


class FinancialModelError(Exception):
    """Define the base exception class for all Financial Statement Model errors.

    All custom exceptions raised within the library should inherit from this class.

    Args:
        message: A human-readable description of the error.
    """

    def __init__(self, message: str):
        """Initializes the FinancialModelError."""
        self.message = message
        super().__init__(self.message)


class ConfigurationError(FinancialModelError):
    """Raise an error for invalid configuration files or objects.

    This typically occurs when parsing or validating configuration data,
    such as YAML files defining metrics or statement structures.

    Args:
        message: The base error message.
        config_path: Optional path to the configuration file where the error occurred.
        errors: Optional list of specific validation errors found.

    Examples:
        >>> raise ConfigurationError("Invalid syntax", config_path="config.yaml")
        >>> raise ConfigurationError(
        ...     "Missing required fields",
        ...     config_path="metrics.yaml",
        ...     errors=["Missing 'formula' for 'revenue'"]
        ... )
    """

    def __init__(
        self,
        message: str,
        config_path: Optional[str] = None,
        errors: Optional[list[str]] = None,
    ):
        """Initializes the ConfigurationError."""
        self.config_path = config_path
        self.errors = errors or []

        if config_path and errors:
            full_message = f"{message} in {config_path}: {'; '.join(errors)}"
        elif config_path:
            full_message = f"{message} in {config_path}"
        elif errors:
            full_message = f"{message}: {'; '.join(errors)}"
        else:
            full_message = message

        super().__init__(full_message)


class CalculationError(FinancialModelError):
    """Raise an error during calculation operations.

    This indicates a problem while computing the value of a node, often due
    to issues with the calculation logic, input data, or strategy used.

    Args:
        message: The base error message.
        node_id: Optional ID of the node where the calculation failed.
        period: Optional period for which the calculation failed.
        details: Optional dictionary containing additional context about the error.

    Examples:
        >>> raise CalculationError("Division by zero", node_id="profit_margin", period="2023-Q1")
        >>> raise CalculationError(
        ...     "Incompatible input types",
        ...     node_id="total_assets",
        ...     details={"input_a_type": "str", "input_b_type": "int"}
        ... )
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str] = None,
        period: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
    ):
        """Initializes the CalculationError."""
        self.node_id = node_id
        self.period = period
        self.details = details or {}

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if period:
            context.append(f"period '{period}'")

        full_message = f"{message} for {' and '.join(context)}" if context else message

        # Append details to the message for better context
        if self.details:
            details_str = ", ".join(f'{k}="{v}"' for k, v in self.details.items())
            # Prioritize showing the original underlying error if captured
            original_error_str = self.details.get("original_error")
            if original_error_str:
                full_message = f"{full_message}: {original_error_str}"
            else:
                full_message = f"{full_message} (Details: {details_str})"

        super().__init__(full_message)


class NodeError(FinancialModelError):
    """Raise an error for issues related to graph nodes.

    This covers issues like trying to access a non-existent node,
    invalid node configurations, or type mismatches related to nodes.

    Args:
        message: The base error message.
        node_id: Optional ID of the node related to the error.

    Examples:
        >>> raise NodeError("Node not found", node_id="non_existent_node")
        >>> raise NodeError("Invalid node type for operation", node_id="revenue")
    """

    def __init__(self, message: str, node_id: Optional[str] = None):
        """Initializes the NodeError."""
        self.node_id = node_id

        full_message = f"{message} for node '{node_id}'" if node_id else message

        super().__init__(full_message)


class MissingInputError(FinancialModelError):
    """Raise an error when a required calculation input is missing.

    This occurs when a calculation node needs data from another node for a
    specific period, but that data is unavailable.

    Args:
        message: The base error message.
        node_id: Optional ID of the node requiring the input.
        input_name: Optional name or ID of the missing input node.
        period: Optional period for which the input was missing.

    Examples:
        >>> raise MissingInputError(
        ...     "Required input data unavailable",
        ...     node_id="cogs",
        ...     input_name="inventory",
        ...     period="2023-12-31"
        ... )
    """

    def __init__(
        self,
        message: str,
        node_id: Optional[str] = None,
        input_name: Optional[str] = None,
        period: Optional[str] = None,
    ):
        """Initializes the MissingInputError."""
        self.node_id = node_id
        self.input_name = input_name
        self.period = period

        context = []
        if node_id:
            context.append(f"node '{node_id}'")
        if input_name:
            context.append(f"input '{input_name}'")
        if period:
            context.append(f"period '{period}'")

        full_message = f"{message} for {' in '.join(context)}" if context else message

        super().__init__(full_message)


class GraphError(FinancialModelError):
    """Raise an error for invalid graph structure or operations.

    This covers issues like inconsistencies in the graph (e.g., orphaned nodes),
    problems during graph traversal, or invalid modifications to the graph.

    Args:
        message: The base error message.
        nodes: Optional list of node IDs involved in the graph error.

    Examples:
        >>> raise GraphError("Orphaned node detected", nodes=["unconnected_node"])
        >>> raise GraphError("Failed to add edge due to type mismatch")
    """

    def __init__(self, message: str, nodes: Optional[list[str]] = None):
        """Initializes the GraphError."""
        self.nodes = nodes or []

        full_message = f"{message} involving nodes: {', '.join(nodes)}" if nodes else message

        super().__init__(full_message)


class DataValidationError(FinancialModelError):
    """Raise an error for data validation failures.

    This typically occurs during data import or preprocessing when data
    does not conform to expected formats, types, or constraints.

    Args:
        message: The base error message.
        validation_errors: Optional list of specific validation failures.

    Examples:
        >>> raise DataValidationError(
        ...     "Input data failed validation",
        ...     validation_errors=["Column 'Date' has invalid format", "Value '-100' is not allowed for 'Revenue'"]
        ... )
    """

    def __init__(self, message: str, validation_errors: Optional[list[str]] = None):
        """Initializes the DataValidationError."""
        self.validation_errors = validation_errors or []

        if validation_errors:
            full_message = f"{message}: {'; '.join(validation_errors)}"
        else:
            full_message = message

        super().__init__(full_message)


class CircularDependencyError(FinancialModelError):
    """Raise an error when a circular dependency is detected in calculations.

    This occurs if the calculation graph contains cycles, meaning a node
    directly or indirectly depends on itself.

    Args:
        message: The base error message. Defaults to "Circular dependency detected".
        cycle: Optional list of node IDs forming the detected cycle.

    Examples:
        >>> raise CircularDependencyError(cycle=["node_a", "node_b", "node_c", "node_a"])
    """

    def __init__(
        self,
        message: str = "Circular dependency detected",
        cycle: Optional[list[str]] = None,
    ):
        """Initializes the CircularDependencyError."""
        self.cycle = cycle or []

        if cycle:
            cycle_str = " -> ".join(cycle)
            full_message = f"{message}: {cycle_str}"
        else:
            full_message = message

        super().__init__(full_message)


class PeriodError(FinancialModelError):
    """Raise an error for invalid or missing periods.

    This covers issues like requesting data for a non-existent period or
    using invalid period formats.

    Args:
        message: The base error message.
        period: Optional specific period involved in the error.
        available_periods: Optional list of valid periods.

    Examples:
        >>> raise PeriodError("Invalid period format", period="2023Q5")
        >>> raise PeriodError("Period not found", period="2024-01-01", available_periods=["2023-12-31"])
    """

    def __init__(
        self,
        message: str,
        period: Optional[str] = None,
        available_periods: Optional[list[str]] = None,
    ):
        """Initializes the PeriodError."""
        self.period = period
        self.available_periods = available_periods or []

        if period and available_periods:
            full_message = f"{message} for period '{period}'. Available periods: {', '.join(available_periods)}"
        elif period:
            full_message = f"{message} for period '{period}'"
        else:
            full_message = message

        super().__init__(full_message)


class StatementError(FinancialModelError):
    """Raise an error for issues related to financial statements.

    This is used for errors specific to the structure, definition, or
    processing of financial statements (e.g., Balance Sheet, P&L).

    Args:
        message: The base error message.
        statement_id: Optional ID or name of the statement involved.

    Examples:
        >>> raise StatementError("Balance sheet does not balance", statement_id="BS_2023")
        >>> raise StatementError("Required account missing from P&L", statement_id="PnL_Q1")
    """

    def __init__(self, message: str, statement_id: Optional[str] = None):
        """Initializes the StatementError."""
        self.statement_id = statement_id

        full_message = f"{message} for statement '{statement_id}'" if statement_id else message

        super().__init__(full_message)


class StrategyError(FinancialModelError):
    """Raise an error for issues related to calculation strategies.

    This indicates a problem with the configuration or execution of a
    specific calculation strategy (e.g., Summation, GrowthRate).

    Args:
        message: The base error message.
        strategy_type: Optional name or type of the strategy involved.
        node_id: Optional ID of the node using the strategy.

    Examples:
        >>> raise StrategyError("Invalid parameter for GrowthRate strategy", strategy_type="GrowthRate", node_id="revenue_forecast")
        >>> raise StrategyError("Strategy not applicable to node type", strategy_type="Summation", node_id="text_description")
    """

    def __init__(
        self,
        message: str,
        strategy_type: Optional[str] = None,
        node_id: Optional[str] = None,
    ):
        """Initializes the StrategyError."""
        self.strategy_type = strategy_type
        self.node_id = node_id

        context = []
        if strategy_type:
            context.append(f"strategy type '{strategy_type}'")
        if node_id:
            context.append(f"node '{node_id}'")

        full_message = f"{message} for {' in '.join(context)}" if context else message

        super().__init__(full_message)


class TransformationError(FinancialModelError):
    """Raise an error during data transformation.

    This occurs during preprocessing steps when a specific transformation
    (e.g., normalization, scaling) fails.

    Args:
        message: The base error message.
        transformer_type: Optional name or type of the transformer involved.
        parameters: Optional dictionary of parameters used by the transformer.

    Examples:
        >>> raise TransformationError("Log transform requires positive values", transformer_type="LogTransformer")
        >>> raise TransformationError(
        ...     "Incompatible data type for scaling",
        ...     transformer_type="MinMaxScaler",
        ...     parameters={"feature_range": (0, 1)}
        ... )
    """

    def __init__(
        self,
        message: str,
        transformer_type: Optional[str] = None,
        parameters: Optional[dict[str, Any]] = None,
    ):
        """Initializes the TransformationError."""
        self.transformer_type = transformer_type
        self.parameters = parameters or {}

        if transformer_type:
            full_message = f"{message} in transformer '{transformer_type}'"
            if parameters:
                params_str = ", ".join(f"{k}={v}" for k, v in parameters.items())
                full_message = f"{full_message} with parameters: {params_str}"
        else:
            full_message = message

        super().__init__(full_message)


class MetricError(FinancialModelError):
    """Raise an error for issues related to metric definitions or registry.

    This covers issues with loading, validating, or accessing financial metrics,
    whether defined in YAML or Python code.

    Args:
        message: The base error message.
        metric_name: Optional name of the metric involved in the error.
        details: Optional dictionary containing additional context about the error.

    Examples:
        >>> raise MetricError("Metric definition not found", metric_name="unknown_ratio")
        >>> raise MetricError(
        ...     "Invalid formula syntax in metric definition",
        ...     metric_name="profitability_index",
        ...     details={"formula": "NPV / Initial Investment)"} # Missing parenthesis
        ... )
    """

    def __init__(
        self,
        message: str,
        metric_name: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
    ):
        """Initializes the MetricError."""
        self.metric_name = metric_name
        self.details = details or {}

        full_message = f"{message} related to metric '{metric_name}'" if metric_name else message

        super().__init__(full_message)

# --- END FILE: fin_statement_model/core/errors.py ---

# --- START FILE: fin_statement_model/core/graph/__init__.py ---
"""Graph module for the financial statement model.

This module provides the core graph functionality for building and evaluating
financial statement models.
"""

from fin_statement_model.core.graph.graph import Graph

__all__ = ["Graph"]

# --- END FILE: fin_statement_model/core/graph/__init__.py ---

# --- START FILE: fin_statement_model/core/graph/graph.py ---
"""Provide graph operations for the financial statement model.

This module provides the `Graph` class that combines manipulation, traversal,
forecasting, and calculation capabilities for building and evaluating
financial statement models.
"""

import logging
from typing import Any, Callable, Optional

from fin_statement_model.core.node_factory import NodeFactory
from fin_statement_model.core.nodes import Node, FinancialStatementItemNode, FormulaCalculationNode, CalculationNode
from fin_statement_model.core.errors import NodeError, ConfigurationError, CalculationError, CircularDependencyError
from fin_statement_model.core.metrics import metric_registry
from fin_statement_model.core.calculations import Registry
from fin_statement_model.core.graph.manipulator import GraphManipulator
from fin_statement_model.core.graph.traverser import GraphTraverser


# Configure logging
logger = logging.getLogger(__name__)

__all__ = ["Graph"]

class Graph:
    """Represent the financial statement model as a directed graph.

    This class integrates graph manipulation, traversal, forecasting, and
    calculation capabilities. It serves as the central orchestrator for nodes,
    periods, caching, and calculation workflows.

    Attributes:
        _nodes: A dict mapping node names (str) to Node objects.
        _periods: A list of time period identifiers (str) managed by the graph.
        _cache: A nested dict caching calculated values per node per period.
        _metric_names: A set of node names (str) corresponding to metric nodes.
        _node_factory: An instance of NodeFactory for creating new nodes.
    """

    def __init__(self, periods: Optional[list[str]] = None):
        """Initialize the Graph instance.

        Set up core components: node registry, `DataManager`, and `CalculationEngine`.
        Optionally initialize the graph with a list of time periods.

        Args:
            periods: An optional list of strings representing the initial time
                     periods for the financial model (e.g., ["2023", "2024"]).
                     The `DataManager` will handle sorting and ensuring uniqueness.

        Raises:
            TypeError: If `periods` is provided but is not a list.

        Examples:
            >>> graph_no_periods = Graph()
            >>> logger.info(graph_no_periods.periods) # Output: []
            >>> graph_with_periods = Graph(periods=["2023", "2022"])
            >>> logger.info(graph_with_periods.periods) # Periods are sorted
            >>> try:
            ...     Graph(periods="2023") # Invalid type
            ... except TypeError as e:
            ...     logger.error(e)
        """
        # No super().__init__() needed as mixins don't have __init__
        # and GraphCore is removed.

        self._nodes: dict[str, Node] = {}

        # Initialize core attributes for periods, cache, metrics, and node factory
        self._periods: list[str] = []
        self._cache: dict[str, dict[str, float]] = {}
        self._metric_names: set[str] = set()
        self._node_factory: NodeFactory = NodeFactory()

        # Handle initial periods directly
        if periods:
            if not isinstance(periods, list):
                raise TypeError("Initial periods must be a list")
            self.add_periods(periods)

        self.manipulator = GraphManipulator(self)
        self.traverser = GraphTraverser(self)

    @property
    def nodes(self) -> dict[str, Node]:
        """Provide access to the dictionary of all nodes in the graph.

        Returns:
            A dictionary where keys are node names (str) and values are
            `Node` objects. This dictionary represents the shared node registry.

        Examples:
            >>> graph = Graph()
            >>> item_node = graph.add_financial_statement_item("Revenue", {"2023": 100})
            >>> logger.info(list(graph.nodes.keys()))
            >>> logger.info(graph.nodes["Revenue"] == item_node)
        """
        return self._nodes

    @property
    def periods(self) -> list[str]:
        """Retrieve the list of time periods currently managed by the graph.

        Returns:
            A sorted list of unique time period strings managed by the graph.

        Examples:
            >>> graph = Graph(periods=["2024", "2023"])
            >>> logger.info(graph.periods)
            >>> graph.add_periods(["2025"])
            >>> logger.info(graph.periods)
        """
        return self._periods

    def add_periods(self, periods: list[str]) -> None:
        """Add new time periods to the graph.

        Update the internal period list, ensuring uniqueness and sorting.

        Args:
            periods: A list of strings representing the time periods to add.

        Raises:
            TypeError: If `periods` is not a list.
        """
        if not isinstance(periods, list):
            raise TypeError("Periods must be provided as a list.")
        # Ensure unique and sorted periods
        combined = set(self._periods).union(periods)
        self._periods = sorted(combined)
        logger.debug(f"Added periods {periods}; current periods: {self._periods}")

    def add_calculation(
        self,
        name: str,
        input_names: list[str],
        operation_type: str,
        formula_variable_names: Optional[list[str]] = None,
        **calculation_kwargs: Any,
    ) -> Node:
        """Add a new calculation node to the graph using the node factory.

        Resolve input node names to Node objects, create a CalculationNode,
        register it in the graph, and return it.

        Args:
            name: Unique name for the calculation node.
            input_names: List of node names to use as inputs.
            operation_type: Calculation type key (e.g., 'addition').
            formula_variable_names: Optional list of variable names used in the formula
                string, required if creating a FormulaCalculationNode via this method.
            **calculation_kwargs: Additional parameters for the calculation constructor.

        Returns:
            The created calculation node.

        Raises:
            NodeError: If any input node name does not exist.
            ValueError: If the name is invalid or creation fails.
            TypeError: If inputs are invalid.
        """
        # Validate name
        if not name or not isinstance(name, str):
            raise ValueError("Calculation node name must be a non-empty string.")
        # Resolve input node names
        if not isinstance(input_names, list):
            raise TypeError("input_names must be a list of node names.")
        resolved_inputs: list[Node] = []
        missing = []
        for nm in input_names:
            nd = self._nodes.get(nm)
            if nd is None:
                missing.append(nm)
            else:
                resolved_inputs.append(nd)
        if missing:
            raise NodeError(
                f"Cannot create calculation node '{name}': missing input nodes {missing}",
                node_id=name,
            )
        # Create the node via factory
        try:
            node = self._node_factory.create_calculation_node(
                name=name,
                inputs=resolved_inputs,
                calculation_type=operation_type,
                formula_variable_names=formula_variable_names,
                **calculation_kwargs,
            )
        except (ValueError, TypeError):
            logger.exception(
                f"Failed to create calculation node '{name}' with type '{operation_type}'"
            )
            raise

        # --- Cycle Detection ---
        for input_node in resolved_inputs:
            try:
                bfs_levels = self.traverser.breadth_first_search(start_node=input_node.name, direction="successors")
                reachable_nodes = {n for level in bfs_levels for n in level}
                if name in reachable_nodes:
                    cycle_path_guess = [input_node.name, "...", name] # Simplified guess
                    raise CircularDependencyError(
                        message=f"Adding calculation node '{name}' would create a cycle from input '{input_node.name}'",
                        node_id=name,
                        cycle=cycle_path_guess
                    )
            except NodeError:
                # Should not happen if inputs were resolved, but handle defensively
                logger.warning(f"NodeError during cycle check pre-computation for {name} from {input_node.name}")
                # Or potentially re-raise depending on desired strictness
            except Exception as e:
                # Catch broader errors during traversal if needed
                logger.error(f"Unexpected error during cycle detection for node {name}: {e}", exc_info=True)
                # Optionally re-raise as a different error type or handle
                raise CalculationError(
                    message=f"Error during cycle detection for {name}",
                    node_id=name,
                    details={"original_error": str(e)}
                ) from e
        # --- End Cycle Detection ---

        # Register node in graph
        if name in self._nodes:
            logger.warning(f"Overwriting existing node '{name}' in graph.")
        self._nodes[name] = node
        logger.info(
            f"Added calculation node '{name}' of type '{operation_type}' with inputs {input_names}"
        )
        return node

    def add_metric(
        self, metric_name: str, node_name: Optional[str] = None
    ) -> Node:
        """Add a metric calculation node based on a metric definition.

        If `node_name` is None, uses `metric_name` as the node name.

        Uses the metric registry to load inputs and formula, creates a
        FormulaCalculationNode directly, registers it, and stores metric
        metadata on the node itself.

        Args:
            metric_name: Key of the metric definition to add.
            node_name: Optional name for the metric node; defaults to metric_name.

        Returns:
            The created FormulaCalculationNode.

        Raises:
            TypeError: If node_name is invalid.
            ValueError: If node_name already exists.
            ConfigurationError: If metric definition is missing or invalid.
            NodeError: If required input nodes are missing.
        """
        # Default node_name to metric_name if not provided
        if node_name is None:
            node_name = metric_name
        if not node_name or not isinstance(node_name, str):
            raise TypeError("Metric node name must be a non-empty string.")
        # Check for name conflict
        if node_name in self._nodes:
            raise ValueError(f"A node with name '{node_name}' already exists in the graph.")

        # Load metric definition (Pydantic model)
        try:
            metric_def = metric_registry.get(metric_name)
        except KeyError as e:
            raise ConfigurationError(f"Unknown metric definition: '{metric_name}'") from e

        # Extract required fields from definition
        required_inputs = metric_def.inputs
        formula = metric_def.formula
        description = metric_def.description

        resolved_inputs: dict[str, Node] = {}
        missing = []
        for req in required_inputs:
            nd = self._nodes.get(req)
            if nd is None:
                missing.append(req)
            else:
                # Use the required input name as the key for the Formula node
                resolved_inputs[req] = nd

        if missing:
            raise NodeError(
                f"Cannot create metric '{metric_name}': missing required nodes {missing}",
                node_id=node_name,
            )

        # Create FormulaCalculationNode directly
        try:
            new_node = FormulaCalculationNode(
                name=node_name,
                inputs=resolved_inputs,
                formula=formula,
                metric_name=metric_name,  # Store original metric key
                metric_description=description # Store description
            )
        except (ValueError, TypeError) as e: # Catch potential FormulaCalculationNode init errors
            logger.exception(
                f"Failed to instantiate FormulaCalculationNode for metric '{metric_name}' as node '{node_name}'"
            )
            # Re-raise as ConfigurationError or keep original, depending on desired error reporting
            raise ConfigurationError(
                f"Error creating node for metric '{metric_name}': {e}"
            ) from e

        # Register the new node using the graph's add_node method
        # (Assuming add_node handles potential overwrites and adds to self._nodes)
        self.manipulator.add_node(new_node)
        # self._metric_names.add(node_name) # Removed - no longer tracking separately

        logger.info(
            f"Added metric '{metric_name}' as FormulaCalculationNode '{node_name}' with inputs {list(resolved_inputs)}"
        )
        return new_node # Return the created FormulaCalculationNode

    def add_custom_calculation(
        self,
        name: str,
        calculation_func: Callable[..., float],
        inputs: Optional[list[str]] = None,
        description: str = "",
    ) -> Node:
        """Add a custom calculation node using a Python callable.

        Args:
            name: Unique name for the custom calculation node.
            calculation_func: A callable that accepts (period, **inputs) and returns float.
            inputs: Optional list of node names to use as inputs.
            description: Optional description of the calculation.

        Returns:
            The created custom calculation node.

        Raises:
            NodeError: If any specified input nodes are missing.
            TypeError: If calculation_func is not callable.
        """
        # Validate inputs list
        resolved_inputs: list[Node] = []
        if inputs is not None:
            if not isinstance(inputs, list):
                raise TypeError("inputs must be a list of node names.")
            missing = []
            for nm in inputs:
                nd = self._nodes.get(nm)
                if nd is None:
                    missing.append(nm)
                else:
                    resolved_inputs.append(nd)
            if missing:
                raise NodeError(
                    f"Cannot create custom calculation '{name}': missing input nodes {missing}",
                    node_id=name,
                )
        # Validate callable
        if not callable(calculation_func):
            raise TypeError("calculation_func must be callable.")
        # Create custom node via factory
        try:
            custom_node = self._node_factory._create_custom_node_from_callable(
                name=name,
                inputs=resolved_inputs,
                formula=calculation_func,
                description=description,
            )
        except (ValueError, TypeError):
            logger.exception(f"Failed to create custom calculation node '{name}'")
            raise
        # Register custom node
        if name in self._nodes:
            logger.warning(f"Overwriting existing node '{name}' in graph.")
        self._nodes[name] = custom_node
        logger.info(f"Added custom calculation node '{name}' with inputs {inputs}")
        return custom_node

    def change_calculation_method(
        self,
        node_name: str,
        new_method_key: str,
        **kwargs: dict[str, Any],
    ) -> None:
        """Change the calculation method for an existing calculation-based node.

        Args:
            node_name: Name of the existing calculation node.
            new_method_key: Key of the new calculation method to apply.
            **kwargs: Additional parameters required by the new calculation.

        Returns:
            None

        Raises:
            NodeError: If the target node does not exist or is not a CalculationNode.
            ValueError: If `new_method_key` is not a recognized calculation key.
            TypeError: If the new calculation cannot be instantiated with the provided arguments.

        Examples:
            >>> graph.change_calculation_method("GrossProfit", "addition")
        """
        node = self.manipulator.get_node(node_name)
        if node is None:
            raise NodeError("Node not found for calculation change", node_id=node_name)
        if not isinstance(node, CalculationNode):
            raise NodeError(
                f"Node '{node_name}' is not a CalculationNode", node_id=node_name
            )
        # Map method key to registry name
        if new_method_key not in self._node_factory._calculation_methods:
            raise ValueError(
                f"Calculation '{new_method_key}' is not recognized."
            )
        calculation_class_name = self._node_factory._calculation_methods[new_method_key]
        try:
            calculation_cls = Registry.get(calculation_class_name)
        except KeyError as e:
            raise ValueError(
                f"Calculation class '{calculation_class_name}' not found in registry."
            ) from e
        try:
            calculation_instance = calculation_cls(**kwargs)
        except TypeError as e:
            raise TypeError(
                f"Failed to instantiate calculation '{new_method_key}': {e}"
            )
        # Apply new calculation
        node.set_calculation(calculation_instance)
        # Clear cached calculations for this node
        if node_name in self._cache:
            del self._cache[node_name]
        logger.info(
            f"Changed calculation for node '{node_name}' to '{new_method_key}'"
        )

    def get_metric(self, metric_id: str) -> Optional[Node]:
        """Return the metric node for a given metric ID, if present.

        Searches for a node with the given ID that was created as a metric
        (identified by having a `metric_name` attribute).

        Args:
            metric_id: Identifier of the metric node to retrieve.

        Returns:
            The Node corresponding to `metric_id` if it's a metric node, or None.

        Examples:
            >>> m = graph.get_metric("current_ratio")
            >>> if m:
            ...     logger.info(m.name)
        """
        node = self._nodes.get(metric_id)
        # Check if the node exists and has the metric_name attribute populated
        if node and getattr(node, "metric_name", None) == metric_id:
            return node
        return None

    def get_available_metrics(self) -> list[str]:
        """Return a sorted list of all metric node IDs currently in the graph.

        Identifies metric nodes by checking for the presence and non-None value
        of the `metric_name` attribute.

        Returns:
            A sorted list of metric node names.

        Examples:
            >>> graph.get_available_metrics()
            ['current_ratio', 'debt_equity_ratio']
        """
        # Iterate through all nodes and collect names of those that are metrics
        metric_node_names = [
            node.name
            for node in self._nodes.values()
            if getattr(node, "metric_name", None) is not None
        ]
        return sorted(metric_node_names)

    def get_metric_info(self, metric_id: str) -> dict:
        """Return detailed information for a specific metric node.

        Args:
            metric_id: Identifier of the metric node to inspect.

        Returns:
            A dict containing 'id', 'name', 'description', and 'inputs' for the metric.

        Raises:
            ValueError: If `metric_id` does not correspond to a metric node.

        Examples:
            >>> info = graph.get_metric_info("current_ratio")
            >>> logger.info(info['inputs'])
        """
        metric_node = self.get_metric(metric_id)
        if metric_node is None:
            if metric_id in self._nodes:
                raise ValueError(f"Node '{metric_id}' exists but is not a metric (missing metric_name attribute).")
            raise ValueError(f"Metric node '{metric_id}' not found in graph.")

        # Extract info directly from the FormulaCalculationNode
        try:
            # Use getattr for safety, retrieving stored metric metadata
            description = getattr(metric_node, "metric_description", "N/A")
            # metric_name stored on the node is the key from the registry
            registry_key = getattr(metric_node, "metric_name", metric_id)

            # We might want the display name from the original definition.
            # Fetch the definition again if needed for the display name.
            try:
                metric_def = metric_registry.get(registry_key)
                display_name = metric_def.name
            except Exception:
                logger.warning(f"Could not reload metric definition for '{registry_key}' to get display name. Using node name '{metric_id}' instead.")
                display_name = metric_id # Fallback to node name

            inputs = metric_node.get_dependencies()
        except Exception as e:
            # Catch potential attribute errors or other issues
            logger.error(f"Error retrieving info for metric node '{metric_id}': {e}", exc_info=True)
            raise ValueError(
                f"Failed to retrieve metric info for '{metric_id}': {e}"
            ) from e

        return {"id": metric_id, "name": display_name, "description": description, "inputs": inputs}

    def calculate(self, node_name: str, period: str) -> float:
        """Calculate and return the value of a specific node for a given period.

        This method uses internal caching to speed repeated calls, and wraps
        underlying errors in CalculationError for clarity.

        Args:
            node_name: Name of the node to calculate.
            period: Time period identifier for the calculation.

        Returns:
            The calculated float value for the node and period.

        Raises:
            NodeError: If the specified node does not exist.
            TypeError: If the node has no callable `calculate` method.
            CalculationError: If an error occurs during the node's calculation.

        Examples:
            >>> value = graph.calculate("Revenue", "2023")
        """
        # Return cached value if present
        if node_name in self._cache and period in self._cache[node_name]:
            logger.debug(f"Cache hit for node '{node_name}', period '{period}'")
            return self._cache[node_name][period]
        # Resolve node
        node = self.manipulator.get_node(node_name)
        if node is None:
            raise NodeError(f"Node '{node_name}' not found", node_id=node_name)
        # Validate calculate method
        if not hasattr(node, "calculate") or not callable(node.calculate):
            raise TypeError(f"Node '{node_name}' has no callable calculate method.")
        # Perform calculation with error handling
        try:
            value = node.calculate(period)
        except (NodeError, ConfigurationError, CalculationError, ValueError, KeyError, ZeroDivisionError) as e:
            logger.error(f"Error calculating node '{node_name}' for period '{period}': {e}", exc_info=True)
            raise CalculationError(
                message=f"Failed to calculate node '{node_name}'",
                node_id=node_name,
                period=period,
                details={"original_error": str(e)},
            ) from e
        # Cache and return
        self._cache.setdefault(node_name, {})[period] = value
        logger.debug(f"Cached value for node '{node_name}', period '{period}': {value}")
        return value

    def recalculate_all(self, periods: Optional[list[str]] = None) -> None:
        """Recalculate all nodes for given periods, clearing all caches first.

        Args:
            periods: List of period strings, a single string, or None to use all periods.

        Returns:
            None

        Raises:
            TypeError: If `periods` is not a list, string, or None.

        Examples:
            >>> graph.recalculate_all(["2023", "2024"])
        """
        # Normalize periods input
        if periods is None:
            periods_to_use = self.periods
        elif isinstance(periods, str):
            periods_to_use = [periods]
        elif isinstance(periods, list):
            periods_to_use = periods
        else:
            raise TypeError("Periods must be a list of strings, a single string, or None.")
        # Clear all caches (node-level and central) to force full recalculation
        self.clear_all_caches()
        if not periods_to_use:
            return
        # Recalculate each node for each period
        for node_name in list(self._nodes.keys()):
            for period in periods_to_use:
                try:
                    self.calculate(node_name, period)
                except Exception as e:
                    logger.warning(f"Error recalculating node '{node_name}' for period '{period}': {e}")

    def clear_all_caches(self) -> None:
        """Clear all node-level and central calculation caches.

        Returns:
            None

        Examples:
            >>> graph.clear_all_caches()
        """
        logger.debug(f"Clearing node-level caches for {len(self.nodes)} nodes.")
        for node in self.nodes.values():
            if hasattr(node, "clear_cache"):
                try:
                    node.clear_cache()
                except Exception as e:
                    logger.warning(f"Failed to clear cache for node '{node.name}': {e}")
        # Clear central calculation cache
        self.clear_calculation_cache()
        logger.debug("Cleared central calculation cache.")

    def clear_calculation_cache(self) -> None:
        """Clear the graph's internal calculation cache.

        Returns:
            None

        Examples:
            >>> graph.clear_calculation_cache()
        """
        self._cache.clear()
        logger.debug("Cleared graph calculation cache.")

    def clear(self) -> None:
        """Reset the graph by clearing nodes, periods, metrics, and caches.

        Returns:
            None

        Examples:
            >>> graph.clear()
        """
        self._nodes = {}
        self._periods = []
        self._cache = {}
        self._metric_names = set()
        logger.info("Graph cleared: nodes, periods, metrics, and caches reset.")

    def add_financial_statement_item(self, name: str, values: dict[str, float]) -> FinancialStatementItemNode:
        """Add a basic financial statement item (data node) to the graph.

        Args:
            name: Unique name for the financial statement item node.
            values: Mapping of period strings to float values for this item.

        Returns:
            The newly created `FinancialStatementItemNode`.

        Raises:
            NodeError: If a node with the same name already exists.
            TypeError: If `values` is not a dict or contains invalid types.

        Examples:
            >>> item_node = graph.add_financial_statement_item("SG&A", {"2023": 50.0})
            >>> item_node.get_value("2023")
            50.0
        """
        if name in self._nodes:
            raise NodeError("Node with name already exists", node_id=name)
        if not isinstance(values, dict):
            raise TypeError("Values must be provided as a dict[str, float]")
        # Create a new financial statement item node
        new_node = self._node_factory.create_financial_statement_item(
            name=name, values=values.copy()
        )
        self._nodes[name] = new_node
        # Update periods from node values
        self.add_periods(list(values.keys()))
        logger.info(f"Added FinancialStatementItemNode '{name}' with periods {list(values.keys())}")
        return new_node

    def update_financial_statement_item(
        self, name: str, values: dict[str, float], replace_existing: bool = False
    ) -> FinancialStatementItemNode:
        """Update values for an existing financial statement item node.

        Args:
            name: Name of the existing financial statement item node.
            values: Mapping of new period strings to float values.
            replace_existing: If True, replace existing values entirely; otherwise merge.

        Returns:
            The updated `FinancialStatementItemNode`.

        Raises:
            NodeError: If the node does not exist.
            TypeError: If the node is not a `FinancialStatementItemNode` or `values` is not a dict.

        Examples:
            >>> graph.update_financial_statement_item("SG&A", {"2024": 60.0})
        """
        node = self.manipulator.get_node(name)
        if node is None:
            raise NodeError("Node not found", node_id=name)
        if not isinstance(node, FinancialStatementItemNode):
            raise TypeError(f"Node '{name}' is not a FinancialStatementItemNode")
        if not isinstance(values, dict):
            raise TypeError("Values must be provided as a dict[str, float]")
        if replace_existing:
            node.values = values.copy()
        else:
            node.values.update(values)
        self.add_periods(list(values.keys()))
        logger.info(f"Updated FinancialStatementItemNode '{name}' with periods {list(values.keys())}; replace_existing={replace_existing}")
        return node

    def get_financial_statement_items(self) -> list[Node]:
        """Retrieve all financial statement item nodes from the graph.

        Returns:
            A list of `FinancialStatementItemNode` objects currently in the graph.

        Examples:
            >>> items = graph.get_financial_statement_items()
        """
        from fin_statement_model.core.nodes import (
            FinancialStatementItemNode,
        )  # Keep import local as it's specific

        return [
            node
            for node in self.nodes.values()
            if isinstance(node, FinancialStatementItemNode)
        ]

    def __repr__(self) -> str:
        """Provide a concise, developer-friendly string representation of the graph.

        Summarize total nodes, FS items, calculations, dependencies, and periods.

        Returns:
            A string summarizing the graph's structure and contents.

        Examples:
            >>> logger.info(repr(graph))
        """
        from fin_statement_model.core.nodes import (
            FinancialStatementItemNode,
        )  # Keep import local

        num_nodes = len(self.nodes)
        periods_str = ", ".join(map(repr, self.periods)) if self.periods else "None"

        fs_item_count = 0
        calc_node_count = 0
        other_node_count = 0
        dependencies_count = 0

        for node in self.nodes.values():
            if isinstance(node, FinancialStatementItemNode):
                fs_item_count += 1
            elif node.has_calculation():
                calc_node_count += 1
                # Prioritize get_dependencies if available, otherwise check inputs
                if hasattr(node, "get_dependencies"):
                    try:
                        dependencies_count += len(node.get_dependencies())
                    except Exception as e:
                        logger.warning(
                            f"Error calling get_dependencies for node '{node.name}': {e}"
                        )
                elif hasattr(node, "inputs"):
                    try:
                        if isinstance(node.inputs, list):
                            # Ensure inputs are nodes with names
                            dep_names = [inp.name for inp in node.inputs if hasattr(inp, "name")]
                            dependencies_count += len(dep_names)
                        elif isinstance(node.inputs, dict):
                            # Assume keys are dependency names for dict inputs
                            dependencies_count += len(node.inputs)
                    except Exception as e:
                        logger.warning(f"Error processing inputs for node '{node.name}': {e}")
            else:
                other_node_count += 1

        repr_parts = [
            f"Total Nodes: {num_nodes}",
            f"FS Items: {fs_item_count}",
            f"Calculations: {calc_node_count}",
        ]
        if other_node_count > 0:
            repr_parts.append(f"Other: {other_node_count}")
        repr_parts.append(f"Dependencies: {dependencies_count}")
        repr_parts.append(f"Periods: [{periods_str}]")

        return f"<{type(self).__name__}({', '.join(repr_parts)})>"

    def has_cycle(self, source_node: Node, target_node: Node) -> bool:
        """Check if a cycle exists from a source node to a target node.

        This method delegates to GraphTraverser.breadth_first_search to determine
        if `target_node` is reachable from `source_node` via dependencies, indicating
        that adding an edge from `target_node` to `source_node` would create a cycle.

        Args:
            source_node: The starting node for cycle detection.
            target_node: The node to detect return path to.

        Returns:
            True if a cycle exists, False otherwise.
        """
        if source_node.name not in self._nodes or target_node.name not in self._nodes:
            return False

        # Use BFS to check reachability via predecessors (dependencies)
        bfs_levels = self.traverser.breadth_first_search(source_node.name, direction="predecessors")
        reachable_nodes = {n for level in bfs_levels for n in level}
        return target_node.name in reachable_nodes

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its name.

        Args:
            name: The unique name of the node to retrieve.

        Returns:
            The `Node` instance if found, else None.

        Examples:
            >>> node = graph.get_node("Revenue")
        """
        return self.manipulator.get_node(name)

    def add_node(self, node: Node) -> None:
        """Add a node to the graph, replacing any existing node with the same name.

        Args:
            node: The `Node` instance to add.

        Returns:
            None

        Examples:
            >>> graph.add_node(custom_node)
        """
        return self.manipulator.add_node(node)

    def remove_node(self, node_name: str) -> None:
        """Remove a node from the graph by name, updating dependencies.

        Args:
            node_name: The name of the node to remove.

        Returns:
            None

        Examples:
            >>> graph.remove_node("OldItem")
        """
        return self.manipulator.remove_node(node_name)

    def replace_node(self, node_name: str, new_node: Node) -> None:
        """Replace an existing node with a new node instance.

        Args:
            node_name: Name of the node to replace.
            new_node: The new `Node` instance to substitute.

        Returns:
            None

        Examples:
            >>> graph.replace_node("Item", updated_node)
        """
        return self.manipulator.replace_node(node_name, new_node)

    def has_node(self, node_id: str) -> bool:
        """Check if a node with the given ID exists in the graph.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> graph.has_node("Revenue")
        """
        return self.manipulator.has_node(node_id)

    def set_value(self, node_id: str, period: str, value: float) -> None:
        """Set or update the value for a node in a specific period.

        Args:
            node_id: The name of the node.
            period: The period identifier to set the value for.
            value: The float value to assign.

        Returns:
            None

        Raises:
            ValueError: If the period is not recognized by the graph.
            NodeError: If the node does not exist.
            TypeError: If the node does not support setting a value.

        Examples:
            >>> graph.set_value("SG&A", "2024", 55.0)
        """
        return self.manipulator.set_value(node_id, period, value)

    def topological_sort(self) -> list[str]:
        """Perform a topological sort of all graph nodes.

        Returns:
            A list of node IDs in topological order.

        Raises:
            ValueError: If a cycle is detected in the graph.

        Examples:
            >>> order = graph.topological_sort()
        """
        return self.traverser.topological_sort()

    def get_calculation_nodes(self) -> list[str]:
        """Get all calculation node IDs in the graph.

        Returns:
            A list of node names that have associated calculations.

        Examples:
            >>> graph.get_calculation_nodes()
        """
        return self.traverser.get_calculation_nodes()

    def get_dependencies(self, node_id: str) -> list[str]:
        """Get the direct predecessor node IDs (dependencies) for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node depends on.

        Examples:
            >>> graph.get_dependencies("GrossProfit")
        """
        return self.traverser.get_dependencies(node_id)

    def get_dependency_graph(self) -> dict[str, list[str]]:
        """Get the full dependency graph mapping of node IDs to their inputs.

        Returns:
            A dict mapping each node ID to a list of its dependency node IDs.

        Examples:
            >>> graph.get_dependency_graph()
        """
        return self.traverser.get_dependency_graph()

    def detect_cycles(self) -> list[list[str]]:
        """Detect all cycles in the graph's dependency structure.

        Returns:
            A list of cycles, each represented as a list of node IDs.

        Examples:
            >>> graph.detect_cycles()
        """
        return self.traverser.detect_cycles()

    def validate(self) -> list[str]:
        """Validate the graph structure for errors such as cycles or missing nodes.

        Returns:
            A list of validation error messages, empty if valid.

        Examples:
            >>> graph.validate()
        """
        return self.traverser.validate()

    def breadth_first_search(self, start_node: str, direction: str = "successors") -> list[str]:
        """Perform a breadth-first search (BFS) traversal of the graph.

        Args:
            start_node: The starting node ID for BFS.
            direction: Either 'successors' or 'predecessors' to traverse.

        Returns:
            A nested list of node IDs per BFS level.

        Raises:
            ValueError: If `direction` is not 'successors' or 'predecessors'.

        Examples:
            >>> graph.breadth_first_search("Revenue", "successors")
        """
        return self.traverser.breadth_first_search(start_node, direction)

    def get_direct_successors(self, node_id: str) -> list[str]:
        """Get immediate successor node IDs for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that directly follow the given node.

        Examples:
            >>> graph.get_direct_successors("Revenue")
        """
        return self.traverser.get_direct_successors(node_id)

    def get_direct_predecessors(self, node_id: str) -> list[str]:
        """Get immediate predecessor node IDs (inputs) for a given node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node directly depends on.

        Examples:
            >>> graph.get_direct_predecessors("GrossProfit")
        """
        return self.traverser.get_direct_predecessors(node_id)

    def merge_from(self, other_graph: "Graph") -> None:
        """Merge nodes and periods from another Graph into this one.

        Adds periods from the other graph if they don't exist in this graph.
        Adds nodes from the other graph if they don't exist.
        If a node exists in both graphs, attempts to merge the 'values' dictionary
        from the other graph's node into this graph's node.

        Args:
            other_graph: The Graph instance to merge data from.

        Raises:
            TypeError: If other_graph is not a Graph instance.
        """
        if not isinstance(other_graph, Graph):
            raise TypeError("Can only merge from another Graph instance.")

        logger.info(f"Starting merge from graph {other_graph!r} into {self!r}")

        # 1. Update periods
        new_periods = [p for p in other_graph.periods if p not in self.periods]
        if new_periods:
            self.add_periods(new_periods)
            logger.debug(f"Merged periods: {new_periods}")

        # 2. Merge nodes
        nodes_added = 0
        nodes_updated = 0
        for node_name, other_node in other_graph.nodes.items():
            existing_node = self.get_node(node_name)
            if existing_node is not None:
                # Node exists, merge values if applicable
                if hasattr(existing_node, "values") and hasattr(other_node, "values") and \
                   isinstance(getattr(existing_node, "values", None), dict) and \
                   isinstance(getattr(other_node, "values", None), dict):
                    try:
                        # Perform the update
                        existing_node.values.update(other_node.values) # type: ignore
                        nodes_updated += 1
                        logger.debug(f"Merged values into existing node '{node_name}'")
                        # No need to call self.add_node(existing_node) as it's already there
                    except AttributeError:
                         # Should not happen due to hasattr checks, but defensive
                         logger.warning(f"Could not merge values for node '{node_name}' due to missing 'values' attribute despite hasattr check.")
                    except Exception as e:
                         logger.warning(f"Could not merge values for node '{node_name}': {e}")
                else:
                    # Nodes exist but cannot merge values (e.g., calculation nodes without stored values)
                    logger.debug(f"Node '{node_name}' exists in both graphs, but values not merged (missing/incompatible 'values' attribute). Keeping target graph's node.")
            else:
                # Node doesn't exist in target graph, add it
                try:
                    # Ensure we add a copy if nodes might be shared or mutable in complex ways,
                    # but for now, assume adding the instance is okay.
                    self.add_node(other_node)
                    nodes_added += 1
                except Exception:
                     logger.exception(f"Failed to add new node '{node_name}' during merge:")

        logger.info(f"Merge complete. Nodes added: {nodes_added}, Nodes updated (values merged): {nodes_updated}")

# --- END FILE: fin_statement_model/core/graph/graph.py ---

# --- START FILE: fin_statement_model/core/graph/manipulator.py ---
"""Provide graph manipulation utilities.

This module defines the GraphManipulator class, encapsulating node-level mutation helpers for Graph.
"""

import logging
from typing import Optional, Any
from fin_statement_model.core.errors import NodeError
from fin_statement_model.core.nodes import Node

logger = logging.getLogger(__name__)

class GraphManipulator:
    """Encapsulate node-level mutation helpers for Graph.

    Attributes:
        graph: The Graph instance this manipulator operates on.
    """

    def __init__(self, graph: Any) -> None:
        """Initialize the GraphManipulator with a Graph reference.

        Args:
            graph: The Graph instance to manipulate.
        """
        self.graph = graph

    def add_node(self, node: Node) -> None:
        """Add a node to the graph, replacing any existing node with the same name.

        Args:
            node: The Node instance to add.

        Returns:
            None

        Raises:
            TypeError: If the provided object is not a Node instance.

        Examples:
            >>> manipulator.add_node(node)
        """
        if not isinstance(node, Node):
            raise TypeError(f"Object {node} is not a valid Node instance.")
        if self.has_node(node.name):
            self.remove_node(node.name)
        self.graph._nodes[node.name] = node

    def _update_calculation_nodes(self) -> None:
        """Refresh input references for all calculation nodes after structure changes.

        This method re-resolves `input_names` to current Node objects and clears
        individual node caches.

        Returns:
            None
        """
        for nd in self.graph._nodes.values():
            if nd.has_calculation() and hasattr(nd, "input_names") and nd.input_names:
                try:
                    resolved_inputs: list[Node] = []
                    for name in nd.input_names:
                        input_node = self.get_node(name)
                        if input_node is None:
                            raise NodeError(
                                f"Input node '{name}' not found for calculation node '{nd.name}'"
                            )
                        resolved_inputs.append(input_node)
                    nd.inputs = resolved_inputs
                    if hasattr(nd, "clear_cache"):
                        nd.clear_cache()
                except NodeError:
                    logger.exception(f"Error updating inputs for node '{nd.name}'")
                except AttributeError:
                    logger.warning(
                        f"Node '{nd.name}' has input_names but no 'inputs' attribute to update."
                    )

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its unique name.

        Args:
            name: The unique node name to retrieve.

        Returns:
            The Node instance if found, else None.

        Examples:
            >>> manipulator.get_node("Revenue")
        """
        return self.graph._nodes.get(name)

    def replace_node(self, node_name: str, new_node: Node) -> None:
        """Replace an existing node with a new one, ensuring consistency.

        Args:
            node_name: Name of the node to replace.
            new_node: The new Node instance; its name must match `node_name`.

        Returns:
            None

        Raises:
            NodeError: If `node_name` does not exist.
            ValueError: If `new_node.name` does not match `node_name`.

        Examples:
            >>> manipulator.replace_node("Revenue", updated_node)
        """
        if not self.has_node(node_name):
            raise NodeError(f"Node '{node_name}' not found, cannot replace.")
        if node_name != new_node.name:
            raise ValueError("New node name must match the name of the node being replaced.")
        self.remove_node(node_name)
        self.add_node(new_node)

    def has_node(self, node_id: str) -> bool:
        """Check if a node with the given ID exists.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> manipulator.has_node("Revenue")
        """
        return node_id in self.graph._nodes

    def remove_node(self, node_name: str) -> None:
        """Remove a node from the graph and update calculation nodes.

        Args:
            node_name: The name of the node to remove.

        Returns:
            None

        Examples:
            >>> manipulator.remove_node("OldItem")
        """
        if not self.has_node(node_name):
            return
        self.graph._nodes.pop(node_name, None)
        self._update_calculation_nodes()

    def set_value(self, node_id: str, period: str, value: float) -> None:
        """Set the value for a specific node and period, clearing all caches.

        Args:
            node_id: The name of the node.
            period: The time period identifier.
            value: The numeric value to assign.

        Returns:
            None

        Raises:
            ValueError: If `period` is not recognized by the graph.
            NodeError: If the node does not exist.
            TypeError: If the node does not support setting a value.

        Examples:
            >>> manipulator.set_value("Revenue", "2023", 1100.0)
        """
        if period not in self.graph._periods:
            raise ValueError(f"Period '{period}' not in graph periods")
        nd = self.get_node(node_id)
        if not nd:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if not hasattr(nd, "set_value"):
            raise TypeError(
                f"Node '{node_id}' of type {type(nd).__name__} does not support set_value."
            )
        nd.set_value(period, value)
        self.graph.clear_all_caches()

    def clear_all_caches(self) -> None:
        """Clear caches associated with individual nodes in the graph.

        Returns:
            None

        Examples:
            >>> manipulator.clear_all_caches()
        """
        for nd in self.graph._nodes.values():
            if hasattr(nd, "clear_cache"):
                nd.clear_cache()

# --- END FILE: fin_statement_model/core/graph/manipulator.py ---

# --- START FILE: fin_statement_model/core/graph/traverser.py ---
"""Provide graph traversal and validation utilities.

This module defines the GraphTraverser class, encapsulating read-only graph traversal helpers.
"""

import logging
from typing import Optional, Any
from collections import deque

from fin_statement_model.core.errors import NodeError
from fin_statement_model.core.nodes import Node

logger = logging.getLogger(__name__)

class GraphTraverser:
    """Encapsulate traversal and validation helpers for Graph.

    Attributes:
        graph: The Graph instance this traverser operates on.
    """

    def __init__(self, graph: Any) -> None:
        """Initialize the GraphTraverser with a Graph reference.

        Args:
            graph: The Graph instance to traverse.
        """
        self.graph = graph

    def get_node(self, name: str) -> Optional[Node]:
        """Retrieve a node from the graph by its name.

        Args:
            name: The unique name of the node to retrieve.

        Returns:
            The Node instance if found, else None.

        Examples:
            >>> traverser.get_node("Revenue")
        """
        return self.graph.manipulator.get_node(name)

    def has_node(self, node_id: str) -> bool:
        """Check if a node exists in the graph.

        Args:
            node_id: The name of the node to check.

        Returns:
            True if the node exists, False otherwise.

        Examples:
            >>> traverser.has_node("Revenue")
        """
        return self.graph.manipulator.has_node(node_id)

    @property
    def nodes(self) -> dict[str, Node]:
        """Access the full node registry dictionary.

        Returns:
            A dict mapping node names to Node instances.

        Examples:
            >>> list(traverser.nodes.keys())
        """
        return self.graph.nodes

    def get_direct_successors(self, node_id: str) -> list[str]:
        """Get immediate successor node IDs for a given node.

        Args:
            node_id: The name of the node whose successors to retrieve.

        Returns:
            A list of node IDs that directly follow the given node.

        Examples:
            >>> traverser.get_direct_successors("Revenue")
        """
        successors: list[str] = []
        for other_id, node in self.nodes.items():
            if hasattr(node, "inputs"):
                input_nodes: list[Node] = []
                if isinstance(node.inputs, list):
                    input_nodes = node.inputs
                elif isinstance(node.inputs, dict):
                    input_nodes = list(node.inputs.values())
                
                if any(inp.name == node_id for inp in input_nodes if hasattr(inp, 'name')):
                    successors.append(other_id)
        return successors

    def get_direct_predecessors(self, node_id: str) -> list[str]:
        """Get immediate predecessor node IDs (dependencies) for a given node.

        Args:
            node_id: The name of the node whose dependencies to retrieve.

        Returns:
            A list of node IDs that the given node depends on.

        Raises:
            NodeError: If the node does not exist.

        Examples:
            >>> traverser.get_direct_predecessors("GrossProfit")
        """
        node = self.get_node(node_id)
        if not node:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if hasattr(node, "inputs"):
            return [inp.name for inp in node.inputs]
        return []

    def topological_sort(self) -> list[str]:
        """Perform a topological sort of nodes based on dependencies.

        Returns:
            A list of node IDs in topological order.

        Raises:
            ValueError: If a cycle is detected in the graph.

        Examples:
            >>> traverser.topological_sort()
        """
        in_degree: dict[str, int] = {n: 0 for n in self.nodes}
        adjacency: dict[str, list[str]] = {n: [] for n in self.nodes}
        for name, node in self.nodes.items():
            if hasattr(node, "inputs"):
                for inp in node.inputs:
                    adjacency[inp.name].append(name)
                    in_degree[name] += 1
        queue: list[str] = [n for n, d in in_degree.items() if d == 0]
        topo_order: list[str] = []
        while queue:
            current = queue.pop()
            topo_order.append(current)
            for nbr in adjacency[current]:
                in_degree[nbr] -= 1
                if in_degree[nbr] == 0:
                    queue.append(nbr)
        if len(topo_order) != len(self.nodes):
            raise ValueError("Cycle detected in graph, can't do a valid topological sort.")
        return topo_order

    def get_calculation_nodes(self) -> list[str]:
        """Identify all nodes in the graph that represent calculations.

        Returns:
            A list of node IDs for nodes with calculations.

        Examples:
            >>> traverser.get_calculation_nodes()
        """
        return [node_id for node_id, node in self.nodes.items() if node.has_calculation()]

    def get_dependencies(self, node_id: str) -> list[str]:
        """Retrieve the direct dependencies (inputs) of a specific node.

        Args:
            node_id: The name of the node to inspect.

        Returns:
            A list of node IDs that the given node depends on.

        Raises:
            NodeError: If the node does not exist.

        Examples:
            >>> traverser.get_dependencies("GrossProfit")
        """
        node = self.get_node(node_id)
        if not node:
            raise NodeError(message=f"Node '{node_id}' does not exist", node_id=node_id)
        if hasattr(node, "inputs"):
            return [inp.name for inp in node.inputs]
        return []

    def get_dependency_graph(self) -> dict[str, list[str]]:
        """Construct a representation of the full dependency graph.

        Returns:
            A dict mapping each node ID to its list of dependency node IDs.

        Examples:
            >>> traverser.get_dependency_graph()
        """
        dependencies: dict[str, list[str]] = {}
        for node_id, node in self.nodes.items():
            try:
                if hasattr(node, "inputs"):
                    dependencies[node_id] = [inp.name for inp in node.inputs]
                else:
                    dependencies[node_id] = []
            except NodeError:
                dependencies[node_id] = []
        return dependencies

    def detect_cycles(self) -> list[list[str]]:
        """Detect all cycles present in the graph's dependency structure.

        Returns:
            A list of cycles, each cycle is a list of node IDs forming the cycle.

        Examples:
            >>> traverser.detect_cycles()
        """
        dependency_graph = self.get_dependency_graph()
        visited: set[str] = set()
        rec_stack: set[str] = set()
        cycles: list[list[str]] = []

        def dfs_detect_cycles(n_id: str, path: Optional[list[str]] = None) -> None:
            if path is None:
                path = []
            if n_id in rec_stack:
                cycle_start = path.index(n_id)
                cycle = path[cycle_start:] + [n_id]
                if cycle not in cycles:
                    cycles.append(cycle)
                return
            if n_id in visited:
                return
            visited.add(n_id)
            rec_stack.add(n_id)
            path.append(n_id)
            for dep in dependency_graph.get(n_id, []):
                dfs_detect_cycles(dep, path[:])
            rec_stack.remove(n_id)

        for node_id in self.nodes:
            if node_id not in visited:
                dfs_detect_cycles(node_id)
        return cycles

    def validate(self) -> list[str]:
        """Perform validation checks on the graph structure.

        Returns:
            A list of validation error messages; empty list if graph is valid.

        Examples:
            >>> traverser.validate()
        """
        errors: list[str] = [
            f"Circular dependency detected: {' -> '.join(cycle)}"
            for cycle in self.detect_cycles()
        ]
        errors.extend(
            f"Node '{node_id}' depends on non-existent node '{inp.name}'"
            for node_id, node in self.nodes.items()
            if hasattr(node, "inputs")
            for inp in node.inputs
            if not self.has_node(inp.name)
        )
        return errors

    def breadth_first_search(self, start_node: str, direction: str = "successors") -> list[str]:
        """Perform a breadth-first search (BFS) traversal of the graph.

        Args:
            start_node: The starting node ID for the traversal.
            direction: The traversal direction, either 'successors' or 'predecessors'.

        Returns:
            A list of levels, each level is a list of node IDs visited at that depth.

        Raises:
            ValueError: If `direction` is not 'successors' or 'predecessors'.

        Examples:
            >>> traverser.breadth_first_search("Revenue", "successors")
        """
        if direction not in ["successors", "predecessors"]:
            raise ValueError("Invalid direction. Use 'successors' or 'predecessors'.")

        visited = set()
        queue = deque([start_node])
        visited.add(start_node)
        traversal_order = []

        while queue:
            level_size = len(queue)
            current_level = []

            for _ in range(level_size):
                n_id = queue.popleft()
                current_level.append(n_id)

                if direction == "successors":
                    for successor in self.get_direct_successors(n_id):
                        if successor not in visited:
                            visited.add(successor)
                            queue.append(successor)
                elif direction == "predecessors":
                    for predecessor in self.get_direct_predecessors(n_id):
                        if predecessor not in visited:
                            visited.add(predecessor)
                            queue.append(predecessor)

            traversal_order.append(current_level)

        return traversal_order

# --- END FILE: fin_statement_model/core/graph/traverser.py ---

# --- START FILE: fin_statement_model/core/metrics/__init__.py ---
"""Metrics Subpackage.

Handles definition, loading, and access for financial metrics.
"""

import logging
from pathlib import Path

from .registry import MetricRegistry, HAS_YAML

logger = logging.getLogger(__name__)

# --- Singleton Registry Instance ---
# Create a single instance of the registry for the application lifetime.
metric_registry = MetricRegistry()

# --- Auto-load Built-in Metrics ---
# Determine the path to the built-in metrics directory relative to this file.
_current_dir = Path(__file__).parent
_builtin_dir = _current_dir / "builtin"

# Attempt to load metrics only if PyYAML is installed and the directory exists.
if HAS_YAML:
    if _builtin_dir.is_dir():
        try:
            loaded_count = metric_registry.load_metrics_from_directory(_builtin_dir)
            logger.info(f"Auto-loaded {loaded_count} built-in metrics from {_builtin_dir}")
        except Exception as e:
            # Log error but don't prevent library import if built-ins fail to load
            logger.error(
                f"Failed to auto-load built-in metrics from {_builtin_dir}: {e}",
                exc_info=True,
            )
    else:
        logger.warning(
            f"Built-in metric directory not found: {_builtin_dir}. No built-in metrics loaded."
        )
else:
    logger.warning("PyYAML not installed. Cannot load YAML metrics. Skipping auto-load.")


# --- Public API ---
__all__ = [
    "MetricRegistry",
    "metric_registry",  # Expose the singleton instance
]

# --- END FILE: fin_statement_model/core/metrics/__init__.py ---

# --- START FILE: fin_statement_model/core/metrics/models.py ---
"""Models for metric definitions."""

from typing import Optional, Any
from pydantic import BaseModel, Field, model_validator

class MetricDefinition(BaseModel):
    """Schema for one metric definition loaded from YAML."""

    name: str = Field(..., min_length=1, description="The name of the metric")
    description: str = Field(..., min_length=1, max_length=300, description="The description of the metric")
    inputs: list[str] = Field(..., min_items=1, description="The inputs of the metric")
    formula: str = Field(..., min_length=1, description="The formula of the metric")
    tags: list[str] = Field(default_factory=list, description="The tags of the metric")
    units: Optional[str] = Field(None, description="The units of the metric")

    class Config:
        """Configuration for the MetricDefinition model."""
        extra = "forbid"        # reject unknown keys


    @model_validator(mode="before")
    def _strip_whitespace(cls, values: dict[str, Any]) -> dict[str, Any]:
        # tiny quality-of-life clean-up
        for k, v in values.items():
            if isinstance(v, str):
                values[k] = v.strip()
        return values

# --- END FILE: fin_statement_model/core/metrics/models.py ---

# --- START FILE: fin_statement_model/core/metrics/registry.py ---
"""Manage loading and accessing metric definitions from YAML files.

This module provides a registry to discover, validate, and retrieve
metric definitions from YAML files and associate them with calculation classes.
"""


import logging
from pathlib import Path
from typing import ClassVar, Union

# Use a try-except block for the YAML import
try:
    import yaml

    HAS_YAML = True
except ImportError:
    HAS_YAML = False

from pydantic import ValidationError

from fin_statement_model.core.errors import ConfigurationError
from fin_statement_model.core.metrics.models import MetricDefinition

logger = logging.getLogger(__name__)

# Define the path to the built-in metrics directory relative to this file
BUILTIN_METRICS_DIR = Path(__file__).parent / "builtin"


class MetricRegistry:
    """Manage loading and accessing metric definitions from YAML files.

    This includes discovering YAML definitions, validating their structure,
    and providing retrieval methods by metric ID.
    """

    _REQUIRED_FIELDS: ClassVar[list[str]] = ["inputs", "formula", "description", "name"]

    def __init__(self):
        """Initialize the MetricRegistry with an empty metrics store.

        Examples:
            >>> registry = MetricRegistry()
            >>> len(registry)
            0
        """
        self._metrics: dict[str, MetricDefinition] = {}
        logger.info("MetricRegistry initialized.")
        # Automatically load built-in metrics on initialization
        if BUILTIN_METRICS_DIR.is_dir():
            try:
                logger.info(f"Attempting to load built-in metrics from: {BUILTIN_METRICS_DIR}")
                count = self.load_metrics_from_directory(BUILTIN_METRICS_DIR)
                logger.info(f"Successfully loaded {count} built-in metrics.")
            except (ImportError, FileNotFoundError, ConfigurationError) as e:
                logger.error(f"Failed to load built-in metrics: {e}", exc_info=True)
            except Exception as e: # Catch any other unexpected errors during load
                 logger.error(f"Unexpected error loading built-in metrics: {e}", exc_info=True)
        else:
            logger.warning(f"Built-in metrics directory not found: {BUILTIN_METRICS_DIR}")

    def load_metrics_from_directory(self, directory_path: Union[str, Path]) -> int:
        """Load all metric definitions from a directory.

        This method searches for '*.yaml' files, validates their content,
        and stores them in the registry.

        Args:
            directory_path: Path to the directory containing metric YAML files.

        Returns:
            The number of metrics successfully loaded.

        Raises:
            ImportError: If PyYAML is not installed.
            FileNotFoundError: If the directory_path does not exist.
            ConfigurationError: If a YAML file is invalid or missing required fields.

        Examples:
            >>> registry = MetricRegistry()
            >>> count = registry.load_metrics_from_directory("./metrics")
            >>> print(f"Loaded {count} metrics.")
        """
        if not HAS_YAML:
            logger.error("PyYAML is required to load metrics from YAML files. Please install it.")
            raise ImportError("PyYAML is required to load metrics from YAML files.")

        dir_path = Path(directory_path)
        if not dir_path.is_dir():
            logger.error(f"Metric directory not found: {dir_path}")
            raise FileNotFoundError(f"Metric directory not found: {dir_path}")

        logger.info(f"Loading metrics from directory: {dir_path}")
        loaded_count = 0
        for filepath in dir_path.glob("*.yaml"):
            metric_id = filepath.stem  # Use filename without extension as ID (e.g., "gross_profit")
            logger.debug(f"Attempting to load metric '{metric_id}' from {filepath}")
            try:
                with open(filepath, encoding="utf-8") as f:
                    data = yaml.safe_load(f)

                try:
                    model = MetricDefinition.parse_obj(data)
                except ValidationError as e:
                    raise ConfigurationError(
                        f"Invalid metric '{filepath.name}': {e}",
                        config_path=str(filepath),
                    ) from e

                if metric_id in self._metrics:
                    logger.warning(
                        f"Overwriting existing metric definition for '{metric_id}' from {filepath}"
                    )
                self._metrics[metric_id] = model
                logger.debug(f"Successfully loaded and validated metric '{metric_id}'")
                loaded_count += 1

            except yaml.YAMLError as e:
                logger.exception(f"Error parsing YAML file {filepath}")
                raise ConfigurationError(
                    f"Invalid YAML syntax in {filepath}", config_path=str(filepath)
                ) from e
            except Exception as e:
                logger.error(
                    f"Unexpected error loading metric from {filepath}",
                    exc_info=True,
                )
                raise ConfigurationError(
                    f"Failed to load metric from {filepath} due to: {e}",
                    config_path=str(filepath),
                ) from e

        logger.info(f"Successfully loaded {loaded_count} metrics from {dir_path}.")
        return loaded_count

    def get(self, metric_id: str) -> MetricDefinition:
        """Retrieve a loaded metric definition by its ID.

        Args:
            metric_id: Identifier of the metric (filename stem).

        Returns:
            A MetricDefinition object containing the metric definition.

        Raises:
            KeyError: If the metric_id is not found in the registry.

        Examples:
            >>> definition = registry.get("gross_profit")
            >>> print(definition["formula"])
        """
        try:
            return self._metrics[metric_id]
        except KeyError:
            logger.warning(f"Metric ID '{metric_id}' not found in registry.")
            raise KeyError(f"Metric ID '{metric_id}' not found. Available: {self.list_metrics()}")

    def list_metrics(self) -> list[str]:
        """Get a sorted list of all loaded metric IDs.

        Returns:
            A sorted list of available metric IDs.

        Examples:
            >>> registry.list_metrics()
            ['current_ratio', 'debt_equity_ratio']
        """
        return sorted(self._metrics.keys())

    def __len__(self) -> int:
        """Return the number of loaded metrics.

        Returns:
            The count of metrics loaded into the registry.

        Examples:
            >>> len(registry)
            5
        """
        return len(self._metrics)

    def __contains__(self, metric_id: str) -> bool:
        """Check if a metric ID exists in the registry.

        Args:
            metric_id: The metric identifier to check.

        Returns:
            True if the metric is present, False otherwise.

        Examples:
            >>> 'current_ratio' in registry
        """
        return metric_id in self._metrics

# --- END FILE: fin_statement_model/core/metrics/registry.py ---

# --- START FILE: fin_statement_model/core/node_factory.py ---
"""Provide a factory for creating nodes in the financial statement model.

This module centralizes node creation logic and ensures consistent initialization
for different types of nodes used in the financial statement model.
"""

import logging
from typing import Callable, Any, Union, Optional, ClassVar

# Force import of strategies package to ensure registration happens

from .nodes import (
    Node,
    FinancialStatementItemNode,
    CalculationNode,
    CustomCalculationNode,
)
from .nodes.forecast_nodes import (
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
)

# Force import of calculations package to ensure registration happens
from fin_statement_model.core.calculations import Registry, Calculation

# Configure logging
logger = logging.getLogger(__name__)


class NodeFactory:
    """Provide a factory for creating nodes in the financial statement model.

    This class centralizes node creation for financial statement items,
    calculations, metrics, forecasts, and custom logic.

    Attributes:
        _calculation_methods: Maps simple string keys (e.g., 'addition') to
            the class names of Calculation implementations registered in the
            `Registry`. This allows creating CalculationNodes without
            directly importing Calculation classes.
    """

    # Mapping of calculation type strings to Calculation class names (keys in the Registry)
    _calculation_methods: ClassVar[dict[str, str]] = {
        "addition": "AdditionCalculation",
        "subtraction": "SubtractionCalculation",
        "multiplication": "MultiplicationCalculation",
        "division": "DivisionCalculation",
        "weighted_average": "WeightedAverageCalculation",
        "custom_formula": "CustomFormulaCalculation",
    }

    # Mapping from node type names to Node classes
    _node_types: ClassVar[dict[str, type[Node]]] = {
        "financial_statement_item": FinancialStatementItemNode,
    }

    @classmethod
    def create_financial_statement_item(
        cls, name: str, values: dict[str, float]
    ) -> FinancialStatementItemNode:
        """Create a FinancialStatementItemNode representing a base financial item.

        This node holds historical or projected values for a specific
        line item (e.g., Revenue, COGS) over different periods.

        Args:
            name: Identifier for the node (e.g., "Revenue").
            values: Mapping of period identifiers to numerical values.

        Returns:
            A FinancialStatementItemNode initialized with the provided values.

        Raises:
            ValueError: If the provided name is empty or not a string.

        Examples:
            >>> revenue_node = NodeFactory.create_financial_statement_item(
            ...     name="Revenue",
            ...     values={"2023": 1000.0, "2024": 1100.0}
            ... )
            >>> revenue_node.get_value("2023")
            1000.0
        """
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        logger.debug(f"Creating financial statement item node: {name}")
        return FinancialStatementItemNode(name, values)

    @classmethod
    def create_calculation_node(
        cls,
        name: str,
        inputs: list[Node],
        calculation_type: str,
        formula_variable_names: Optional[list[str]] = None,
        **calculation_kwargs: Any,
    ) -> CalculationNode:
        """Create a CalculationNode using a pre-defined calculation.

        This method resolves a calculation class from a calculation_type key,
        instantiates it with optional parameters, and wraps it in
        a CalculationNode.

        Args:
            name: Identifier for the calculation node instance.
            inputs: List of Node instances serving as inputs to the calculation.
            calculation_type: Key for the desired calculation in the registry.
            formula_variable_names: Optional list of variable names used in the formula
                string. Required & used only if creating a FormulaCalculationNode
                via the 'custom_formula' type with a 'formula' kwarg.
            **calculation_kwargs: Additional parameters for the calculation constructor.

        Returns:
            A CalculationNode configured with the selected calculation.

        Raises:
            ValueError: If name is invalid, inputs list is empty, or the
                calculation_type is unrecognized.
            TypeError: If the calculation cannot be instantiated with given kwargs.

        Examples:
            >>> gross_profit = NodeFactory.create_calculation_node(
            ...     name="GrossProfit",
            ...     inputs=[revenue, cogs],
            ...     calculation_type="subtraction"
            ... )
        """
        # Validate inputs
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        if not inputs:
            raise ValueError("Calculation node must have at least one input")

        # Check if the calculation type maps to a known calculation name
        if calculation_type not in cls._calculation_methods:
            valid_types = list(cls._calculation_methods.keys())
            raise ValueError(
                f"Invalid calculation type: '{calculation_type}'. Valid types are: {valid_types}"
            )

        # Get the calculation name and resolve the calculation class from the registry
        calculation_name = cls._calculation_methods[calculation_type]
        try:
            calculation_cls: type[Calculation] = Registry.get(calculation_name)
        except KeyError:
            raise ValueError(
                f"Calculation '{calculation_name}' not found in Registry for type '{calculation_type}'"
            )

        # --- SPECIAL HANDLING for formula strings from metrics --- #
        # Check if we intended to use a formula string (likely from a metric YAML)
        # and if the resolved class is CustomFormulaCalculation (which expects a callable)
        if calculation_name == "CustomFormulaCalculation" and "formula" in calculation_kwargs:
            try:
                # We actually want a FormulaCalculationNode in this case.
                # Note: FormulaCalculationNode expects inputs as a dict {var_name: Node}
                #       but we only have a list[Node]. We need to adapt.
                #       For simplicity, we'll map input_0, input_1 etc. to nodes.
                #       A more robust solution might involve passing var names from metric.
                logger.debug(f"Intercepted 'custom_formula' with string. Creating FormulaCalculationNode for '{name}'")
                # Use the provided variable names to map to the input nodes
                input_map = {var_name: node for var_name, node in zip(formula_variable_names, inputs)}

                # Import FormulaCalculationNode locally to avoid circular dependency at module level
                from .nodes.calculation_nodes import FormulaCalculationNode

                formula_str = calculation_kwargs["formula"]
                # Optional: Extract metric details if passed in kwargs
                metric_name = calculation_kwargs.get("metric_name")
                metric_desc = calculation_kwargs.get("metric_description")

                return FormulaCalculationNode(
                    name=name,
                    inputs=input_map,
                    formula=formula_str,
                    metric_name=metric_name,
                    metric_description=metric_desc
                )
            except Exception as e:
                # Catch potential errors during FormulaCalculationNode creation
                logger.exception(f"Error creating FormulaCalculationNode for metric-like node '{name}'")
                raise ConfigurationError(
                     f"Failed to create FormulaCalculationNode for '{name}' from metric definition: {e}"
                 ) from e
        # --- END SPECIAL HANDLING --- #

        # Instantiate the calculation, passing any extra kwargs
        try:
            calculation_instance = calculation_cls(**calculation_kwargs)
        except TypeError as e:
            logger.exception(
                f"Failed to instantiate calculation '{calculation_name}' with kwargs {calculation_kwargs}"
            )
            raise TypeError(
                f"Could not instantiate calculation '{calculation_name}' for node '{name}'. "
                f"Check required arguments for {calculation_cls.__name__}. Provided kwargs: {calculation_kwargs}"
            ) from e

        # Create and return a CalculationNode with the instantiated calculation
        logger.debug(
            f"Creating calculation node '{name}' with '{calculation_name}' calculation."
        )
        return CalculationNode(name, inputs, calculation_instance)

    @classmethod
    def create_forecast_node(
        cls,
        name: str,
        base_node: Node,
        base_period: str,
        forecast_periods: list[str],
        forecast_type: str,
        growth_params: Union[float, list[float], Callable[[], float]],
    ) -> Node:
        """Create a forecast node of the specified type using core forecast classes.

        Args:
            name: Custom name for the forecast node.
            base_node: The Node instance to base projections on.
            base_period: Period identifier providing the base value.
            forecast_periods: List of periods for which to forecast.
            forecast_type: Forecast method ('fixed', 'curve', 'statistical',
                'average', 'historical_growth').
            growth_params: Parameters controlling forecast behavior (float,
                list of floats, or callable). Ignored for 'average' and 'historical_growth'.

        Returns:
            A Node instance implementing the chosen forecast.

        Raises:
            ValueError: If an unsupported forecast_type is provided.

        Examples:
            >>> forecast = NodeFactory.create_forecast_node(
            ...     name="RevForecast",
            ...     base_node=revenue,
            ...     base_period="2023",
            ...     forecast_periods=["2024", "2025"],
            ...     forecast_type="fixed",
            ...     growth_params=0.05
            ... )
        """
        # Instantiate the appropriate forecast node
        if forecast_type == "fixed":
            node = FixedGrowthForecastNode(base_node, base_period, forecast_periods, growth_params)
        elif forecast_type == "curve":
            node = CurveGrowthForecastNode(base_node, base_period, forecast_periods, growth_params)
        elif forecast_type == "statistical":
            node = StatisticalGrowthForecastNode(base_node, base_period, forecast_periods, growth_params)
        elif forecast_type == "average":
            node = AverageValueForecastNode(base_node, base_period, forecast_periods)
        elif forecast_type == "historical_growth":
            node = AverageHistoricalGrowthForecastNode(base_node, base_period, forecast_periods)
        else:
            raise ValueError(f"Invalid forecast type: {forecast_type}")

        # Override forecast node's name to match factory 'name' argument
        node.name = name
        logger.debug(f"Forecast node created with custom name: {name} (original: {base_node.name})")
        return node

    @classmethod
    def _create_custom_node_from_callable(
        cls,
        name: str,
        inputs: list[Node],
        formula: Callable,
        description: Optional[str] = None,
    ) -> CustomCalculationNode:
        """Create a CustomCalculationNode using a Python callable for the calculation logic.

        This supports ad-hoc or complex calculations not covered by standard
        strategies or metrics. The `formula` callable will be invoked with
        input node values at calculation time.

        Note:
            Renamed from `create_metric_node` to avoid confusion with metric-based nodes.

        Args:
            name: Identifier for the custom calculation node.
            inputs: List of Node instances providing values to the formula.
            formula: Callable that computes a value from input node values.
            description: Optional description of the calculation logic.

        Returns:
            A CustomCalculationNode configured with the provided formula.

        Raises:
            ValueError: If name is empty or not a string.
            TypeError: If formula is not callable or inputs contain non-Node items.

        Examples:
            >>> def complex_tax_logic(revenue, expenses, tax_rate_node):
            ...     profit = revenue - expenses
            ...     if profit <= 0:
            ...         return 0.0
            ...     tax_rate = tax_rate_node
            ...     return profit * tax_rate
            ...
            >>> tax_node = NodeFactory._create_custom_node_from_callable(
            ...     name="CalculatedTaxes",
            ...     inputs=[revenue_node, expenses_node, tax_rate_schedule_node],
            ...     formula=complex_tax_logic,
            ...     description="Calculates income tax based on profit and a variable rate."
            ... )

            Using a lambda for a simple ratio:
            >>> quick_ratio_node = NodeFactory._create_custom_node_from_callable(
            ...    name="QuickRatioCustom",
            ...    inputs=[cash_node, receivables_node, current_liabilities_node],
            ...    formula=lambda cash, rec, liab: (cash + rec) / liab if liab else 0
            ... )
        """
        # Validate inputs
        if not name or not isinstance(name, str):
            raise ValueError("Node name must be a non-empty string")

        if not inputs:
            # Allowing no inputs might be valid for some custom functions (e.g., constants)
            # Reconsider if this check is always needed here.
            logger.warning(f"Creating CustomCalculationNode '{name}' with no inputs.")
            # raise ValueError("Custom node must have at least one input")

        if not callable(formula):
            raise TypeError("Formula must be a callable function")
        if not all(isinstance(i, Node) for i in inputs):
            raise TypeError("All items in inputs must be Node instances.")

        # Use the imported CustomCalculationNode
        logger.debug(f"Creating CustomCalculationNode: {name} using provided callable.")
        return CustomCalculationNode(name, inputs, formula_func=formula, description=description)

    # Consider adding a method for creating FormulaCalculationNode if needed directly
    # @classmethod
    # def create_formula_node(cls, name: str, inputs: Dict[str, Node], formula: str) -> FormulaCalculationNode:
    #     ...

# --- END FILE: fin_statement_model/core/node_factory.py ---

# --- START FILE: fin_statement_model/core/nodes/__init__.py ---
"""Core Node Implementations for the Financial Statement Model.

This package exports the base `Node` class and various concrete node types
used to build the financial model graph. These include:

- Data Nodes:
    - `FinancialStatementItemNode`: Stores raw numerical data for specific periods.

- Calculation Nodes:
    - `FormulaCalculationNode`: Calculates based on mathematical string formulas.
    - `CalculationNode`: Uses a calculation object for calculation logic.
    - `CustomCalculationNode`: Uses arbitrary Python functions for calculation.

- Statistical Nodes:
    - `YoYGrowthNode`: Calculates year-over-year percentage growth.
    - `MultiPeriodStatNode`: Computes statistics (mean, stddev) over multiple periods.
    - `TwoPeriodAverageNode`: Calculates the average over two specific periods.

The `__all__` list defines the public API of this package.
"""

# Core node package public interface: re-export submodules
from .base import Node
from .item_node import FinancialStatementItemNode

# Imports from consolidated files
from .calculation_nodes import (
    FormulaCalculationNode,
    CalculationNode,
    CustomCalculationNode,
)
from .stats_nodes import (
    YoYGrowthNode,
    MultiPeriodStatNode,
    TwoPeriodAverageNode,
)
from .forecast_nodes import (
    ForecastNode,
    FixedGrowthForecastNode,
    CurveGrowthForecastNode,
    StatisticalGrowthForecastNode,
    CustomGrowthForecastNode,
    AverageValueForecastNode,
    AverageHistoricalGrowthForecastNode,
)

__all__ = [
    "AverageHistoricalGrowthForecastNode",
    "AverageValueForecastNode",
    "CalculationNode",
    "CurveGrowthForecastNode",
    "CustomCalculationNode",
    "CustomGrowthForecastNode",
    "FinancialStatementItemNode",
    "FixedGrowthForecastNode",
    "ForecastNode",
    "FormulaCalculationNode",
    "MultiPeriodStatNode",
    "Node",
    "StatisticalGrowthForecastNode",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]

# --- END FILE: fin_statement_model/core/nodes/__init__.py ---

# --- START FILE: fin_statement_model/core/nodes/base.py ---
"""Define the abstract base class for all nodes in the graph.

This module provides the Node base class with interfaces for calculation,
attribute access, and optional caching behavior.
"""

from abc import ABC, abstractmethod


class Node(ABC):
    """Define the abstract base class for graph nodes.

    Provide the essential interface for all nodes in the financial statement
    model graph, including calculation, caching, and attribute access.

    Attributes:
    name (str): Unique identifier for the node instance.
    """

    name: str

    def __init__(self, name: str):
        """Initialize the Node instance with a unique name.

        Args:
            name: Unique identifier for the node. Must be a non-empty string.

        Raises:
            ValueError: If `name` is empty or not a string.

        Examples:
            >>> class Dummy(Node):
            ...     def calculate(self, period): return 0.0
            >>> dn = Dummy("Revenue")
            >>> dn.name
            'Revenue'
        """
        if not isinstance(name, str) or not name:
            raise ValueError("Node name must be a non-empty string.")
        self.name = name

    @abstractmethod
    def calculate(self, period: str) -> float:
        """Calculate the node's value for a specific period.

        This abstract method must be implemented by subclasses to define how to
        determine the node's value for a given time period.

        Args:
            period: The time period identifier for the calculation.

        Returns:
            The calculated float value for the specified period.

        Raises:
            NotImplementedError: If the subclass does not implement this method.

        Examples:
            >>> class Dummy(Node):
            ...     def calculate(self, period): return 100.0
            >>> d = Dummy("Test")
            >>> d.calculate("2023")
            100.0
        """

    def clear_cache(self):
        """Clear cached calculation results for this node.

        Subclasses with caching should override this method to clear their internal cache.

        Returns:
            None

        Examples:
            >>> node.clear_cache()
        """
        # Default: no cache to clear

    def has_attribute(self, attr_name: str) -> bool:
        """Check if the node has a specific attribute.

        Args:
            attr_name: The name of the attribute to check.

        Returns:
            True if the attribute exists, otherwise False.

        Examples:
            >>> node.has_attribute("name")
            True
        """
        return hasattr(self, attr_name)

    def get_attribute(self, attribute_name: str) -> object:
        """Get a named attribute from the node.

        Args:
            attribute_name: The name of the attribute to retrieve.

        Returns:
            The value of the specified attribute.

        Raises:
            AttributeError: If the attribute does not exist.

        Examples:
            >>> node.get_attribute("name")
            'Revenue'
        """
        try:
            return getattr(self, attribute_name)
        except AttributeError:
            raise AttributeError(f"Node '{self.name}' has no attribute '{attribute_name}'")

    def has_value(self, period: str) -> bool:
        """Indicate whether the node stores a direct value for a period.

        Primarily for data-bearing nodes; calculation nodes override has_calculation.

        Args:
            period: The time period to check for a stored value.

        Returns:
            True if a direct value is stored, otherwise False.

        Examples:
            >>> node.has_value("2023")
            False
        """
        return False

    def get_value(self, period: str) -> float:
        """Retrieve the node's directly stored value for a period.

        This method must be overridden by data-bearing nodes to return stored values.

        Args:
            period: The time period string for which to retrieve the value.

        Returns:
            The float value stored for the given period.

        Raises:
            NotImplementedError: If the node does not store direct values.

        Examples:
            >>> node.get_value("2023")
        """
        raise NotImplementedError(f"Node {self.name} does not implement get_value")

    def has_calculation(self) -> bool:
        """Indicate whether this node performs calculation.

        Distinguish calculation nodes from data-holding nodes.

        Returns:
            True if the node performs calculations, otherwise False.

        Examples:
            >>> node.has_calculation()
            False
        """
        return False

# --- END FILE: fin_statement_model/core/nodes/base.py ---

# --- START FILE: fin_statement_model/core/nodes/calculation_nodes.py ---
"""Provide node implementations for performing calculations in the financial statement model.

This module defines the different types of calculation nodes available in the system:
- FormulaCalculationNode: Evaluates a formula expression string (e.g., "a + b / 2")
- CalculationNode: Uses a calculation object for calculation logic
- CustomCalculationNode: Calculates using a Python callable/function
"""

import ast
import operator
from typing import Callable, Optional, ClassVar

from fin_statement_model.core.calculations.calculation import Calculation
from fin_statement_model.core.errors import (
    CalculationError,
    ConfigurationError,
    MetricError,
)
from fin_statement_model.core.nodes.base import Node

# === FormulaCalculationNode ===


class FormulaCalculationNode(Node):
    """Calculate a value based on a mathematical formula string.

    Parses and evaluates simple mathematical expressions involving input nodes.
    Supports basic arithmetic operators (+, -, *, /) and unary negation.

    Attributes:
        name (str): Identifier for this node.
        inputs (Dict[str, Node]): Mapping of variable names used in the formula
            to their corresponding input Node instances.
        formula (str): The mathematical expression string to evaluate (e.g., "a + b").
        metric_name (Optional[str]): The original metric identifier from the registry, if applicable.
        metric_description (Optional[str]): The description from the metric definition, if applicable.
        _ast (ast.Expression): The parsed Abstract Syntax Tree of the formula.

    Examples:
        >>> # Assume revenue and cogs are Node instances
        >>> revenue = FinancialStatementItemNode("revenue", {"2023": 100})
        >>> cogs = FinancialStatementItemNode("cogs", {"2023": 60})
        >>> gross_profit = FormulaCalculationNode(
        ...     "gross_profit",
        ...     inputs={"rev": revenue, "cost": cogs},
        ...     formula="rev - cost"
        ... )
        >>> print(gross_profit.calculate("2023"))
        40.0
    """

    # Supported AST operators mapping to Python operator functions
    OPERATORS: ClassVar[dict[type, Callable]] = {
        ast.Add: operator.add,
        ast.Sub: operator.sub,
        ast.Mult: operator.mul,
        ast.Div: operator.truediv,
        ast.USub: operator.neg,
    }

    def __init__(
        self,
        name: str,
        inputs: dict[str, Node],
        formula: str,
        metric_name: Optional[str] = None,
        metric_description: Optional[str] = None,
    ):
        """Initialize the FormulaCalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (Dict[str, Node]): Dictionary mapping variable names in the
                formula to the corresponding input nodes.
            formula (str): The mathematical formula string.
            metric_name (Optional[str]): The original metric identifier from the
                registry, if this node represents a defined metric. Defaults to None.
            metric_description (Optional[str]): The description from the metric
                definition, if applicable. Defaults to None.

        Raises:
            ValueError: If the formula string has invalid syntax.
            TypeError: If any value in `inputs` is not a Node instance.
        """
        super().__init__(name)
        if not isinstance(inputs, dict) or not all(isinstance(n, Node) for n in inputs.values()):
            raise TypeError("FormulaCalculationNode inputs must be a dict of Node instances.")
        self.inputs = inputs
        self.formula = formula
        self.metric_name = metric_name
        self.metric_description = metric_description
        try:
            # Parse the formula string into an AST expression
            self._ast = ast.parse(formula, mode="eval").body
        except SyntaxError as e:
            raise ValueError(f"Invalid formula syntax for node '{name}': {formula}") from e

    def calculate(self, period: str) -> float:
        """Calculate the node's value for a period by evaluating the formula.

        Args:
            period (str): The time period for which to perform the calculation.

        Returns:
            float: The result of the formula evaluation.

        Raises:
            CalculationError: If an error occurs during evaluation, such as
                an unknown variable, unsupported operator, or if an input node
                fails to provide a numeric value for the period.
        """
        try:
            return self._evaluate(self._ast, period)
        except (ValueError, TypeError, KeyError, ZeroDivisionError) as e:
            raise CalculationError(
                message=f"Error evaluating formula for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={"formula": self.formula, "error": str(e)},
            ) from e

    def _evaluate(self, node: ast.AST, period: str) -> float:
        """Recursively evaluate the parsed AST node for the formula.

        Args:
            node (ast.AST): The current AST node to evaluate.
            period (str): The time period context for the evaluation.

        Returns:
            float: The result of evaluating the AST node.

        Raises:
            TypeError: If a non-numeric constant or input node value is encountered.
            ValueError: If an unknown variable or unsupported operator/syntax is found.
            ZeroDivisionError: If division by zero occurs.
        """
        # Numeric literal (Constant in Python 3.8+, Num in earlier versions)
        if isinstance(node, ast.Constant):
            if isinstance(node.value, (int, float)):
                return float(node.value)
            else:
                raise TypeError(
                    f"Unsupported constant type '{type(node.value).__name__}' in formula for node '{self.name}'"
                )

        # Variable reference
        elif isinstance(node, ast.Name):
            var_name = node.id
            if var_name not in self.inputs:
                raise ValueError(
                    f"Unknown variable '{var_name}' in formula for node '{self.name}'. Available: {list(self.inputs.keys())}"
                )
            input_node = self.inputs[var_name]
            # Recursively calculate the value of the input node
            value = input_node.calculate(period)
            if not isinstance(value, (int, float)):
                raise TypeError(
                    f"Input node '{input_node.name}' (variable '{var_name}') did not return a numeric value for period '{period}'"
                )
            return float(value)

        # Binary operation (e.g., a + b)
        elif isinstance(node, ast.BinOp):
            left_val = self._evaluate(node.left, period)
            right_val = self._evaluate(node.right, period)
            op_type = type(node.op)
            if op_type not in self.OPERATORS:
                raise ValueError(
                    f"Unsupported binary operator '{op_type.__name__}' in formula for node '{self.name}'"
                )
            # Perform the operation
            return float(self.OPERATORS[op_type](left_val, right_val))

        # Unary operation (e.g., -a)
        elif isinstance(node, ast.UnaryOp):
            operand_val = self._evaluate(node.operand, period)
            op_type = type(node.op)
            if op_type not in self.OPERATORS:
                raise ValueError(
                    f"Unsupported unary operator '{op_type.__name__}' in formula for node '{self.name}'"
                )
            # Perform the operation
            return float(self.OPERATORS[op_type](operand_val))

        # If the node type is unsupported
        else:
            raise TypeError(
                f"Unsupported syntax node type '{type(node).__name__}' in formula for node '{self.name}': {ast.dump(node)}"
            )

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used in the formula.

        Returns:
            A list of variable names corresponding to the formula inputs.
        """
        return [node.name for node in self.inputs.values()]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as FormulaCalculationNode performs calculations.
        """
        return True


# === CalculationNode ===


class CalculationNode(Node):
    """Delegate calculation logic to a separate calculation object.

    Uses a calculation object. The actual calculation algorithm is
    encapsulated in a `calculation` object provided during initialization.
    This allows for flexible and interchangeable calculation logic.

    Attributes:
        name (str): Identifier for this node.
        inputs (List[Node]): A list of input nodes required by the calculation.
        calculation (Any): An object possessing a `calculate(inputs: List[Node], period: str) -> float` method.
        _values (Dict[str, float]): Internal cache for calculated results.

    Examples:
        >>> class SumCalculation:
        ...     def calculate(self, inputs: List[Node], period: str) -> float:
        ...         return sum(node.calculate(period) for node in inputs)
        >>> node_a = FinancialStatementItemNode("a", {"2023": 10})
        >>> node_b = FinancialStatementItemNode("b", {"2023": 20})
        >>> sum_node = CalculationNode(
        ...     "sum_ab",
        ...     inputs=[node_a, node_b],
        ...     calculation=SumCalculation()
        ... )
        >>> print(sum_node.calculate("2023"))
        30.0
    """

    def __init__(self, name: str, inputs: list[Node], calculation: Calculation):
        """Initialize the CalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (List[Node]): List of input nodes needed by the calculation.
            calculation (Any): The calculation object implementing the calculation.
                Must have a `calculate` method.

        Raises:
            TypeError: If `inputs` is not a list of Nodes, or if `calculation`
                does not have a callable `calculate` method.
        """
        super().__init__(name)
        if not isinstance(inputs, list) or not all(isinstance(n, Node) for n in inputs):
            raise TypeError("CalculationNode inputs must be a list of Node instances.")
        if not hasattr(calculation, "calculate") or not callable(getattr(calculation, "calculate")):
            raise TypeError("Calculation object must have a callable 'calculate' method.")

        self.inputs = inputs
        self.calculation = calculation
        self._values: dict[str, float] = {}  # Cache for calculated values

    def calculate(self, period: str) -> float:
        """Calculate the value for a period using the assigned calculation.

        Checks the cache first. If not found, delegates to the calculation's
        `calculate` method and stores the result.

        Args:
            period (str): The time period for the calculation.

        Returns:
            float: The calculated value from the calculation.

        Raises:
            CalculationError: If the calculation fails or returns
                a non-numeric value.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Delegate to the calculation object's calculate method
            result = self.calculation.calculate(self.inputs, period)
            if not isinstance(result, (int, float)):
                raise TypeError(
                    f"Calculation for node '{self.name}' did not return a numeric value (got {type(self.calculation).__name__})."
                )
            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the calculation
            raise CalculationError(
                message=f"Error during calculation for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={"calculation": type(self.calculation).__name__, "error": str(e)},
            ) from e

    def set_calculation(self, calculation: Calculation) -> None:
        """Change the calculation object for the node.

        Args:
            calculation (Any): The new calculation object. Must have a callable
                `calculate` method.

        Raises:
            TypeError: If the new calculation is invalid.
        """
        if not hasattr(calculation, "calculate") or not callable(getattr(calculation, "calculate")):
            raise TypeError("New calculation object must have a callable 'calculate' method.")
        self.calculation = calculation
        self.clear_cache()  # Clear cache as logic has changed

    def clear_cache(self) -> None:
        """Clear the internal cache of calculated values.

        Returns:
            None
        """
        self._values.clear()

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used by the calculation.

        Returns:
            A list of input node names.
        """
        return [node.name for node in self.inputs]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as CalculationNode performs calculations.
        """
        return True


# === CustomCalculationNode ===


class CustomCalculationNode(Node):
    """Calculate a value using a Python callable/function.

    Uses a Python callable/function to calculate the value for a node.
    The function is provided during initialization.

    Attributes:
        name (str): Identifier for this node.
        inputs (List[Node]): List of input nodes needed for calculation.
        formula_func (Callable): The Python callable function to use for calculation.
        description (str, optional): Description of what this calculation does.
        _values (Dict[str, float]): Internal cache for calculated results.

    Examples:
        >>> def custom_calculation(a, b):
        ...     return a + b
        >>> node_a = FinancialStatementItemNode("NodeA", values={"2023": 10.0})
        >>> node_b = FinancialStatementItemNode("NodeB", values={"2023": 5.0})
        >>> node = CustomCalculationNode(
        ...     "custom_calc",
        ...     inputs=[node_a, node_b],
        ...     formula_func=custom_calculation
        ... )
        >>> print(node.calculate("2023"))
        15.0
    """

    def __init__(
        self,
        name: str,
        inputs: list[Node],
        formula_func: Callable,
        description: Optional[str] = None,
    ):
        """Initialize the CustomCalculationNode.

        Args:
            name (str): The unique identifier for this node.
            inputs (List[Node]): The input nodes whose values will be passed to formula_func.
            formula_func (Callable): The Python callable function to use for calculation.
            description (str, optional): Description of what this calculation does.

        Raises:
            TypeError: If `inputs` is not a list of Nodes or `formula_func` is not a callable.
        """
        super().__init__(name)
        if not isinstance(inputs, list) or not all(isinstance(n, Node) for n in inputs):
            raise TypeError("CustomCalculationNode inputs must be a list of Node instances")
        if not callable(formula_func):
            raise TypeError("CustomCalculationNode formula_func must be a callable function")

        self.inputs = inputs
        self.formula_func = formula_func
        self.description = description
        self._values: dict[str, float] = {}  # Cache for calculated results

    def calculate(self, period: str) -> float:
        """Calculate the node's value for a period using the provided function.

        Args:
            period (str): The time period for which to perform the calculation.

        Returns:
            float: The calculated value from the function.

        Raises:
            CalculationError: If an error occurs during calculation, such as
                if an input node fails to provide a numeric value for the period.
        """
        if period in self._values:
            return self._values[period]

        try:
            # Get input values
            input_values = []
            for node in self.inputs:
                value = node.calculate(period)
                if not isinstance(value, (int, float)):
                    raise TypeError(
                        f"Input node '{node.name}' did not return a numeric value for period '{period}'. Got {type(value).__name__}."
                    )
                input_values.append(value)

            # Calculate the value using the provided function
            result = self.formula_func(*input_values)
            if not isinstance(result, (int, float)):
                raise TypeError(
                    f"Formula did not return a numeric value. Got {type(result).__name__}."
                )

            # Cache and return the result
            self._values[period] = float(result)
            return self._values[period]
        except Exception as e:
            # Wrap potential errors from the function
            raise CalculationError(
                message=f"Error during custom calculation for node '{self.name}'",
                node_id=self.name,
                period=period,
                details={"function": self.formula_func.__name__, "error": str(e)},
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of input nodes used in the function.

        Returns:
            A list of input node names.
        """
        return [node.name for node in self.inputs]

    def has_calculation(self) -> bool:
        """Indicate that this node performs calculation.

        Returns:
            True, as CustomCalculationNode performs calculations.
        """
        return True

# --- END FILE: fin_statement_model/core/nodes/calculation_nodes.py ---

# --- START FILE: fin_statement_model/core/nodes/forecast_nodes.py ---
"""Provide forecast nodes to project future values from historical data.

This module defines the base `ForecastNode` class and its subclasses,
implementing various forecasting strategies (fixed, curve, statistical,
custom, average, and historical growth).
"""

import logging
from typing import Callable

# Use absolute imports
from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class ForecastNode(Node):
    """Define base class for forecast nodes to project future values from historical data.

    A forecast node takes an input node (typically a financial statement item) and projects its
    future values using various growth methods. The node caches calculated values to avoid
    redundant computations.

    Attributes:
        name (str): Identifier for the forecast node (derived from input_node.name)
        input_node (Node): Source node containing historical values to forecast from
        base_period (str): Last historical period to use as basis for forecasting
        forecast_periods (List[str]): List of future periods to generate forecasts for
        _cache (dict): Internal cache of calculated values
        values (dict): Dictionary mapping periods to values (including historical)

    Methods:
        calculate(period): Get value for a specific period (historical or forecast)
        _calculate_value(period): Core calculation logic for a period
        _get_previous_period(period): Helper to get chronologically previous period
        _get_growth_factor_for_period(): Abstract method for growth rate calculation

    Examples:
        # Create 5% fixed growth forecast for revenue
        base = "FY2022"
        forecasts = ["FY2023", "FY2024", "FY2025"]
        node = FixedGrowthForecastNode(revenue_node, base, forecasts, 0.05)

        # Get forecasted value
        fy2024_revenue = node.calculate("FY2024")
    """

    def __init__(self, input_node: Node, base_period: str, forecast_periods: list[str]):
        """Initialize ForecastNode with input node and forecast periods.

        Args:
            input_node: Source node containing historical values.
            base_period: The last historical period serving as the forecast base.
            forecast_periods: List of future periods for which forecasts will be generated.
        """
        self.name = input_node.name
        self.input_node = input_node
        self.base_period = base_period
        self.forecast_periods = forecast_periods
        self._cache = {}

        # Copy historical values from input node
        if hasattr(input_node, "values"):
            self.values = input_node.values.copy()
        else:
            self.values = {}

    def calculate(self, period: str) -> float:
        """Calculate the value for a specific period, using cached results if available.

        This method returns historical values for periods up to the base period, and
        calculates forecasted values for future periods. Results are cached to avoid
        redundant calculations.

        Args:
            period (str): The period to calculate the value for (e.g. "FY2023")

        Returns:
            float: The calculated value for the specified period

        Raises:
            ValueError: If the requested period is not in base_period or forecast_periods

        Examples:
            # Get historical value
            base_value = node.calculate("FY2022")  # Returns actual historical value

            # Get forecasted value
            forecast_value = node.calculate("FY2024")  # Returns projected value
        """
        if period not in self._cache:
            self._cache[period] = self._calculate_value(period)
        return self._cache[period]

    def clear_cache(self):
        """Clear the calculation cache.

        This method clears any cached calculation results, forcing future calls to
        calculate() to recompute values rather than using cached results.

        Examples:
            # Clear cached calculations
            node.clear_cache()  # Future calculate() calls will recompute values
        """
        self._cache.clear()

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True

    def _calculate_value(self, period: str) -> float:
        """Calculate the value for a specific period.

        For historical periods (up to base_period), returns the actual value.
        For forecast periods, calculates the value using the growth rate.

        Args:
            period: The period to calculate the value for

        Returns:
            float: The calculated value for the period

        Raises:
            ValueError: If the period is not in base_period or forecast_periods
        """
        # For historical periods, return the actual value
        if period <= self.base_period:
            return self.values.get(period, 0.0)

        # For forecast periods, calculate using growth rate
        if period not in self.forecast_periods:
            raise ValueError(f"Period '{period}' not in forecast periods for {self.name}")

        # Get the previous period's value
        prev_period = self._get_previous_period(period)
        prev_value = self.calculate(prev_period)

        # Get the growth rate for this period
        growth_factor = self._get_growth_factor_for_period(period, prev_period, prev_value)

        # Calculate the new value
        return prev_value * (1 + growth_factor)

    def _get_previous_period(self, current_period: str) -> str:
        all_periods = sorted([self.base_period, *self.forecast_periods])
        idx = all_periods.index(current_period)
        return all_periods[idx - 1]

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        raise NotImplementedError("Implement in subclass.")


class FixedGrowthForecastNode(ForecastNode):
    """A forecast node that applies a fixed growth rate to project future values.

    This node takes a constant growth rate and applies it to each forecast period,
    compounding from the base period value. It's useful for simple forecasting scenarios
    where steady growth is expected.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_rate (float): The fixed growth rate to apply (e.g. 0.05 for 5% growth)

    Examples:
        # Create node forecasting 5% annual revenue growth
        forecast = FixedGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            0.05
        )

        # Get forecasted value
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1.05)^2
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_rate: float,
    ):
        """Initialize FixedGrowthForecastNode with a constant growth rate.

        Args:
            input_node: Node containing historical values to base the forecast on.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_rate: Fixed growth rate (e.g., 0.05 for 5% growth).
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.growth_rate = float(growth_rate)  # Ensure it's a float
        logger.debug(f"Created FixedGrowthForecastNode with growth rate: {self.growth_rate}")

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        logger.debug(
            f"FixedGrowthForecastNode: Using growth rate {self.growth_rate} for period {period}"
        )
        return self.growth_rate


class CurveGrowthForecastNode(ForecastNode):
    """A forecast node that applies different growth rates for each forecast period.

    This node takes a list of growth rates corresponding to each forecast period,
    allowing for varying growth assumptions over time. This is useful when you expect
    growth patterns to change, such as high initial growth followed by moderation.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_rates (List[float]): List of growth rates for each period (e.g. [0.08, 0.06, 0.04])
                                   Must match length of forecast_periods.

    Raises:
        ValueError: If length of growth_rates doesn't match forecast_periods

    Examples:
        # Create node with declining growth rates
        forecast = CurveGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            [0.08, 0.06, 0.04]  # 8% then 6% then 4% growth
        )

        # Get forecasted value
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1.08) * (1.06)
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_rates: list[float],
    ):
        """Initialize CurveGrowthForecastNode with variable growth rates per period.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_rates: List of growth rates matching each forecast period.
        """
        super().__init__(input_node, base_period, forecast_periods)
        if len(growth_rates) != len(forecast_periods):
            raise ValueError("Number of growth rates must match forecast periods.")
        self.growth_rates = [float(rate) for rate in growth_rates]  # Ensure all are floats
        logger.debug(f"Created CurveGrowthForecastNode with growth rates: {self.growth_rates}")
        logger.debug(f"  Base period: {base_period}")
        logger.debug(f"  Forecast periods: {forecast_periods}")
        logger.debug(f"  Base value: {input_node.calculate(base_period)}")

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Get the growth factor for a specific period."""
        idx = self.forecast_periods.index(period)
        growth_rate = self.growth_rates[idx]
        logger.debug(
            f"CurveGrowthForecastNode: Using growth rate {growth_rate} for period {period}"
        )
        logger.debug(f"  Previous period: {prev_period}")
        logger.debug(f"  Previous value: {prev_value}")
        return growth_rate


class StatisticalGrowthForecastNode(ForecastNode):
    """A forecast node that generates growth rates from a statistical distribution.

    This node uses a provided statistical distribution function to randomly generate
    growth rates for each forecast period. This is useful for modeling uncertainty
    and running Monte Carlo simulations of different growth scenarios.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        distribution_callable (Callable[[], float]): Function that returns random growth rates
                                                   from a statistical distribution

    Examples:
        # Create node with normally distributed growth rates
        from numpy.random import normal
        forecast = StatisticalGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            lambda: normal(0.05, 0.02)  # Mean 5% growth, 2% std dev
        )

        # Get forecasted value (will vary due to randomness)
        fy2024_revenue = forecast.calculate("FY2024")
        # Returns: base_value * (1 + r1) * (1 + r2) where r1,r2 are random
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        distribution_callable: Callable[[], float],
    ):
        """Initialize StatisticalGrowthForecastNode with a distribution function.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            distribution_callable: Function that returns a random growth rate.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.distribution_callable = distribution_callable

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.distribution_callable()


class CustomGrowthForecastNode(ForecastNode):
    """A forecast node that uses a custom function to determine growth rates.

    This node allows complete flexibility in how growth rates are calculated by accepting
    a custom function that can incorporate any logic or external data to determine the
    growth rate for each period.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
        growth_function (Callable[[str, str, float], float]): Function that returns growth rate
            given current period, previous period, and previous value

    The growth_function should accept three parameters:
        - current_period (str): The period being forecasted
        - prev_period (str): The previous period
        - prev_value (float): The value from the previous period
    And return a float representing the growth rate for that period.

    Examples:
        def custom_growth(period, prev_period, prev_value):
            # Growth rate increases by 1% each year, starting at 5%
            year_diff = int(period[-4:]) - int(prev_period[-4:])
            return 0.05 + (0.01 * year_diff)

        forecast = CustomGrowthForecastNode(
            revenue_node,
            "FY2022",
            ["FY2023", "FY2024", "FY2025"],
            custom_growth
        )
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
        growth_function: Callable[[str, str, float], float],
    ):
        """Initialize CustomGrowthForecastNode with a custom growth function.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
            growth_function: Callable(period, prev_period, prev_value) -> growth rate.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.growth_function = growth_function

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        return self.growth_function(period, prev_period, prev_value)


class AverageValueForecastNode(ForecastNode):
    """A forecast node that uses the average of historical values for all forecast periods.

    This node calculates the average of historical values and returns that constant value
    for all forecast periods. It's useful when you want to project future values based
    on the historical average, without any growth.
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
    ):
        """Initialize AverageValueForecastNode by computing historical average.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.

        """
        super().__init__(input_node, base_period, forecast_periods)
        self.average_value = self._calculate_average_value()
        logger.debug(
            f"Created AverageValueForecastNode with average value: {self.average_value}"
        )

    def _calculate_average_value(self) -> float:
        """Calculate the average historical value up to the base period.

        Returns:
            float: The average of historical values or 0.0 if none.
        """
        values = [value for period, value in self.values.items() if period <= self.base_period]
        if not values:
            logger.warning(
                f"No historical values found for {self.name}, using 0.0 as average"
            )
            return 0.0
        return sum(values) / len(values)

    def _calculate_value(self, period: str) -> float:
        """Calculate the value for a specific period using the computed average value."""
        # For historical periods, return the actual value
        if period <= self.base_period:
            return self.values.get(period, 0.0)

        # For forecast periods, return the constant average value
        if period not in self.forecast_periods:
            raise ValueError(f"Period '{period}' not in forecast periods for {self.name}")

        return self.average_value

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Not used for average value forecasts."""
        return 0.0


class AverageHistoricalGrowthForecastNode(ForecastNode):
    """A forecast node that uses the average historical growth rate for forecasting.

    This node calculates the average growth rate from historical values and applies
    that same growth rate consistently to all forecast periods. It's useful when you
    want to project future values based on the historical growth pattern.

    Args:
        input_node (Node): The node containing historical/base values
        base_period (str): The last historical period (e.g. "FY2022")
        forecast_periods (List[str]): List of future periods to forecast
    """

    def __init__(
        self,
        input_node: Node,
        base_period: str,
        forecast_periods: list[str],
    ):
        """Initialize AverageHistoricalGrowthForecastNode by computing average growth.

        Args:
            input_node: Node containing historical data.
            base_period: The last historical period.
            forecast_periods: List of future periods to forecast.
        """
        super().__init__(input_node, base_period, forecast_periods)
        self.avg_growth_rate = self._calculate_average_growth_rate()
        logger.debug(
            f"Created AverageHistoricalGrowthForecastNode with growth rate: {self.avg_growth_rate}"
        )

    def _calculate_average_growth_rate(self) -> float:
        """Calculate the average historical growth rate from input node values.

        Returns:
            float: The average growth rate across historical periods
        """
        if not self.values:
            logger.warning(f"No historical values found for {self.name}, using 0% growth")
            return 0.0

        # Get sorted historical periods up to base period
        historical_periods = sorted([p for p in self.values if p <= self.base_period])
        if len(historical_periods) < 2:
            logger.warning(f"Insufficient historical data for {self.name}, using 0% growth")
            return 0.0

        # Calculate growth rates between consecutive periods
        growth_rates = []
        for i in range(1, len(historical_periods)):
            prev_period = historical_periods[i - 1]
            curr_period = historical_periods[i]
            prev_value = self.values[prev_period]
            curr_value = self.values[curr_period]

            if prev_value == 0:
                logger.warning(
                    f"Zero value found for {self.name} in period {prev_period}, skipping growth rate"
                )
                continue

            growth_rate = (curr_value - prev_value) / prev_value
            growth_rates.append(growth_rate)

        if not growth_rates:
            logger.warning(f"No valid growth rates calculated for {self.name}, using 0% growth")
            return 0.0

        # Calculate and return average growth rate
        avg_growth = sum(growth_rates) / len(growth_rates)
        logger.debug(f"Calculated average growth rate for {self.name}: {avg_growth}")
        return avg_growth

    def _get_growth_factor_for_period(
        self, period: str, prev_period: str, prev_value: float
    ) -> float:
        """Get the growth factor for a specific period.

        Args:
            period (str): The current period
            prev_period (str): The previous period
            prev_value (float): The value from the previous period

        Returns:
            float: The growth rate to apply
        """
        logger.debug(
            f"AverageHistoricalGrowthForecastNode: Using growth rate {self.avg_growth_rate} for period {period}"
        )
        return self.avg_growth_rate

# --- END FILE: fin_statement_model/core/nodes/forecast_nodes.py ---

# --- START FILE: fin_statement_model/core/nodes/item_node.py ---
"""Define a node representing a basic financial statement item."""

import logging

# Use absolute imports
from fin_statement_model.core.nodes.base import Node

logger = logging.getLogger(__name__)


class FinancialStatementItemNode(Node):
    """Define a leaf node containing raw financial statement data.

    This node type typically stores actual reported values (e.g., Revenue,
    COGS) for different time periods.

    Attributes:
        name (str): The unique identifier for the financial item (e.g., "Revenue").
        values (Dict[str, float]): A dictionary mapping time periods (str)
            to their corresponding numerical values (float).

    Examples:
        >>> revenue_data = {"2022": 1000.0, "2023": 1200.0}
        >>> revenue_node = FinancialStatementItemNode("Revenue", revenue_data)
        >>> print(revenue_node.name)
        Revenue
        >>> print(revenue_node.get_value("2023"))
        1200.0
        >>> print(revenue_node.calculate("2022")) # Calculate retrieves the value
        1000.0
        >>> revenue_node.set_value("2024", 1500.0)
        >>> print(revenue_node.get_value("2024"))
        1500.0
        >>> print(revenue_node.has_value("2021"))
        False
    """

    def __init__(self, name: str, values: dict[str, float]):
        """Initialize the financial statement item node.

        Args:
            name (str): The name of the financial statement item.
            values (Dict[str, float]): Dictionary of period-value pairs.
        """
        # Call base Node init if it requires name
        # super().__init__(name)  # Assuming base Node init takes name
        self.name = name
        self.values = values

    def calculate(self, period: str) -> float:
        """Retrieve the value for the specified period.

        For this node type, calculation simply means retrieving the stored value.

        Args:
            period (str): The time period for which to retrieve the value.

        Returns:
            float: The value for the given period, or 0.0 if the period is not found.
        """
        return self.get_value(period)

    def set_value(self, period: str, value: float) -> None:
        """Update or add a value for a specific period.

        Modifies the stored data for the given period.

        Args:
            period (str): The time period to set the value for.
            value (float): The numerical value to store for the period.
        """
        self.values[period] = value

    def has_value(self, period: str) -> bool:
        """Check if a value exists for the specified period.

        Args:
            period (str): The time period to check.

        Returns:
            bool: True if a value is stored for the period, False otherwise.
        """
        return period in self.values

    def get_value(self, period: str) -> float:
        """Retrieve the stored value for a specific period.

        Args:
            period (str): The time period for which to get the value.

        Returns:
            float: The stored value, defaulting to 0.0 if the period is not found.
        """
        return self.values.get(period, 0.0)

# --- END FILE: fin_statement_model/core/nodes/item_node.py ---

# --- START FILE: fin_statement_model/core/nodes/stats_nodes.py ---
"""Provide nodes for statistical calculations on financial data across periods.

This module provides nodes for common time-series statistical analyses:
- `YoYGrowthNode`: Calculates year-over-year percentage growth.
- `MultiPeriodStatNode`: Computes statistics (mean, stddev, etc.) over a range of periods.
- `TwoPeriodAverageNode`: Calculates the simple average over two specific periods.
"""

import logging
import math
import statistics

# Use lowercase built-in types for annotations
from typing import Optional, Callable, Union
from collections.abc import Sequence

# Use absolute imports
from fin_statement_model.core.nodes.base import Node
from fin_statement_model.core.errors import CalculationError

# Added logger instance
logger = logging.getLogger(__name__)

Numeric = Union[int, float]
StatFunc = Callable[[Sequence[Numeric]], Numeric]


class YoYGrowthNode(Node):
    """Calculate year-over-year (YoY) percentage growth.

    Compares the value of an input node between two specified periods
    (prior and current) and calculates the relative change.

    Growth = (Current Value - Prior Value) / Prior Value

    Attributes:
        name (str): The node's identifier.
        input_node (Node): The node providing the values for comparison.
        prior_period (str): Identifier for the earlier time period.
        current_period (str): Identifier for the later time period.

    Examples:
        >>> # Assume revenue_node holds {"2022": 100, "2023": 120}
        >>> revenue_node = FinancialStatementItemNode("revenue", {"2022": 100.0, "2023": 120.0})
        >>> yoy_growth = YoYGrowthNode(
        ...     "revenue_yoy",
        ...     input_node=revenue_node,
        ...     prior_period="2022",
        ...     current_period="2023"
        ... )
        >>> print(yoy_growth.calculate("any_period")) # Period arg is ignored
        0.2
    """

    def __init__(self, name: str, input_node: Node, prior_period: str, current_period: str):
        """Initialize the YoY Growth node.

        Args:
            name (str): The identifier for this growth node.
            input_node (Node): The node whose values will be compared.
            prior_period (str): The identifier for the earlier period.
            current_period (str): The identifier for the later period.

        Raises:
            TypeError: If `input_node` is not a Node instance or periods are not strings.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError("YoYGrowthNode input_node must be a Node instance.")
        if not isinstance(prior_period, str) or not isinstance(current_period, str):
            raise TypeError("YoYGrowthNode prior_period and current_period must be strings.")

        self.input_node = input_node
        self.prior_period = prior_period
        self.current_period = current_period

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the year-over-year growth rate.

        Retrieves values for the prior and current periods from the input node
        and computes the percentage growth. The `period` argument is ignored
        as the calculation periods are fixed during initialization.

        Args:
            period (Optional[str]): Ignored. The calculation uses the periods
                defined during initialization.

        Returns:
            float: The calculated growth rate (e.g., 0.2 for 20% growth).
                   Returns `float('nan')` if the prior period value is zero
                   or non-numeric.

        Raises:
            CalculationError: If the input node fails to provide numeric values
                for the required periods.
        """
        try:
            prior_value = self.input_node.calculate(self.prior_period)
            current_value = self.input_node.calculate(self.current_period)

            # Validate input types
            if not isinstance(prior_value, (int, float)):
                raise TypeError(f"Prior period ('{self.prior_period}') value is non-numeric.")
            if not isinstance(current_value, (int, float)):
                raise TypeError(f"Current period ('{self.current_period}') value is non-numeric.")

            # Handle division by zero or non-finite prior value
            if prior_value == 0 or not math.isfinite(prior_value):
                logger.warning(
                    f"YoYGrowthNode '{self.name}': Prior period '{self.prior_period}' value is zero or non-finite ({prior_value}). Returning NaN."
                )
                return float("nan")

            # Calculate growth
            growth = (float(current_value) - float(prior_value)) / float(prior_value)
            return growth

        except Exception as e:
            # Wrap any exception during calculation
            raise CalculationError(
                message=f"Failed to calculate YoY growth for node '{self.name}'",
                node_id=self.name,
                period=f"{self.prior_period}_to_{self.current_period}",  # Indicate period span
                details={
                    "input_node": self.input_node.name,
                    "prior_period": self.prior_period,
                    "current_period": self.current_period,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True


class MultiPeriodStatNode(Node):
    """Calculate a statistical measure across multiple periods.

    Applies a specified statistical function (e.g., mean, standard deviation)
    to the values of an input node over a list of periods.

    Attributes:
        name (str): The node's identifier.
        input_node (Node): The node providing the values for analysis.
        periods (List[str]): The list of period identifiers to include.
        stat_func (StatFunc): The statistical function to apply (e.g.,
            `statistics.mean`, `statistics.stdev`). Must accept a sequence
            of numbers and return a single number.

    Examples:
        >>> # Assume sales_node holds {"Q1": 10, "Q2": 12, "Q3": 11, "Q4": 13}
        >>> sales_node = FinancialStatementItemNode("sales", {"Q1": 10, "Q2": 12, "Q3": 11, "Q4": 13})
        >>> mean_sales = MultiPeriodStatNode(
        ...     "avg_quarterly_sales",
        ...     input_node=sales_node,
        ...     periods=["Q1", "Q2", "Q3", "Q4"],
        ...     stat_func=statistics.mean
        ... )
        >>> print(mean_sales.calculate()) # Period arg is ignored
        11.5
        >>> stddev_sales = MultiPeriodStatNode(
        ...     "sales_volatility",
        ...     input_node=sales_node,
        ...     periods=["Q1", "Q2", "Q3", "Q4"],
        ...     stat_func=statistics.stdev # Default
        ... )
        >>> print(round(stddev_sales.calculate(), 2))
        1.29
    """

    def __init__(
        self,
        name: str,
        input_node: Node,
        periods: list[str],
        stat_func: StatFunc = statistics.stdev,  # Default to standard deviation
    ):
        """Initialize the multi-period statistics node.

        Args:
            name (str): The identifier for this statistical node.
            input_node (Node): The node providing the source values.
            periods (List[str]): A list of period identifiers to analyze.
            stat_func (StatFunc): The statistical function to apply. Defaults to
                `statistics.stdev`. It must accept a sequence of numerics and
                return a numeric value.

        Raises:
            ValueError: If `periods` is not a list or is empty.
            TypeError: If `input_node` is not a Node, `periods` contains non-strings,
                or `stat_func` is not callable.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError("MultiPeriodStatNode input_node must be a Node instance.")
        if not isinstance(periods, list) or not periods:
            raise ValueError("MultiPeriodStatNode periods must be a non-empty list.")
        if not all(isinstance(p, str) for p in periods):
            raise TypeError("MultiPeriodStatNode periods must contain only strings.")
        if not callable(stat_func):
            raise TypeError("MultiPeriodStatNode stat_func must be a callable function.")

        self.input_node = input_node
        self.periods = periods
        self.stat_func = stat_func

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the statistical measure across the specified periods.

        Retrieves values from the input node for each period in the configured list,
        then applies the `stat_func`. The `period` argument is ignored.

        Args:
            period (Optional[str]): Ignored. Calculation uses the periods defined
                during initialization.

        Returns:
            float: The result of the statistical function. Returns `float('nan')`
                   if the statistical function requires more data points than
                   available (e.g., standard deviation with < 2 values) or if
                   no valid numeric data is found.

        Raises:
            CalculationError: If retrieving input node values fails or if the
                statistical function itself raises an unexpected error.
        """
        values: list[Numeric] = []
        retrieval_errors = []
        try:
            for p in self.periods:
                try:
                    value = self.input_node.calculate(p)
                    if isinstance(value, (int, float)) and math.isfinite(value):
                        values.append(float(value))
                    else:
                        # Log non-numeric/non-finite values but continue if possible
                        logger.warning(
                            f"MultiPeriodStatNode '{self.name}': Input '{self.input_node.name}' gave non-numeric/non-finite value ({value}) for period '{p}'. Skipping."
                        )
                except Exception as node_err:
                    # Log error fetching data for a specific period but continue
                    logger.error(
                        f"MultiPeriodStatNode '{self.name}': Error getting value for period '{p}' from '{self.input_node.name}': {node_err}",
                        exc_info=True,
                    )
                    retrieval_errors.append(p)

            # If no valid numeric values were collected
            if not values:
                logger.warning(
                    f"MultiPeriodStatNode '{self.name}': No valid numeric data points found across periods {self.periods}. Returning NaN."
                )
                return float("nan")

            # Attempt the statistical calculation
            try:
                result = self.stat_func(values)
                # Ensure result is float, handle potential NaN from stat_func
                return float(result) if math.isfinite(result) else float("nan")
            except (statistics.StatisticsError, ValueError, TypeError) as stat_err:
                # Handle errors specific to statistical functions (e.g., stdev needs >= 2 points)
                logger.warning(
                    f"MultiPeriodStatNode '{self.name}': Stat function '{self.stat_func.__name__}' failed ({stat_err}). Values: {values}. Returning NaN."
                )
                return float("nan")

        except Exception as e:
            # Catch any other unexpected errors during the process
            raise CalculationError(
                message=f"Failed to calculate multi-period stat for node '{self.name}'",
                node_id=self.name,
                period="multi-period",  # Indicate calculation context
                details={
                    "input_node": self.input_node.name,
                    "periods": self.periods,
                    "stat_func": self.stat_func.__name__,
                    "collected_values_count": len(values),
                    "retrieval_errors_periods": retrieval_errors,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True


class TwoPeriodAverageNode(Node):
    """Compute the simple average of an input node's value over two periods.

    Calculates (Value at Period 1 + Value at Period 2) / 2.

    Attributes:
        name (str): Identifier for this node.
        input_node (Node): Node providing the values to be averaged.
        period1 (str): Identifier for the first period.
        period2 (str): Identifier for the second period.

    Examples:
        >>> # Assume price_node holds {"Jan": 10.0, "Feb": 11.0}
        >>> price_node = FinancialStatementItemNode("price", {"Jan": 10.0, "Feb": 11.0})
        >>> avg_price = TwoPeriodAverageNode(
        ...     "jan_feb_avg_price",
        ...     input_node=price_node,
        ...     period1="Jan",
        ...     period2="Feb"
        ... )
        >>> print(avg_price.calculate()) # Period arg is ignored
        10.5
    """

    def __init__(self, name: str, input_node: Node, period1: str, period2: str):
        """Initialize the two-period average node.

        Args:
            name (str): The identifier for this node.
            input_node (Node): The node providing values.
            period1 (str): The identifier for the first period.
            period2 (str): The identifier for the second period.

        Raises:
            TypeError: If `input_node` is not a Node, or periods are not strings.
        """
        super().__init__(name)
        if not isinstance(input_node, Node):
            raise TypeError(
                f"TwoPeriodAverageNode input_node must be a Node instance, got {type(input_node).__name__}"
            )
        if not isinstance(period1, str) or not isinstance(period2, str):
            raise TypeError("TwoPeriodAverageNode period1 and period2 must be strings.")

        self.input_node = input_node
        self.period1 = period1
        self.period2 = period2

    def calculate(self, period: Optional[str] = None) -> float:
        """Calculate the average of the input node for the two fixed periods.

        Ignores the `period` argument, using `period1` and `period2` defined
        during initialization.

        Args:
            period (Optional[str]): Ignored.

        Returns:
            float: The average of the input node's values for `period1` and `period2`.
                   Returns `float('nan')` if either input value is non-numeric.

        Raises:
            CalculationError: If retrieving values from the input node fails.
        """
        try:
            val1 = self.input_node.calculate(self.period1)
            val2 = self.input_node.calculate(self.period2)

            # Ensure values are numeric and finite
            if not isinstance(val1, (int, float)) or not math.isfinite(val1):
                logger.warning(
                    f"TwoPeriodAverageNode '{self.name}': Value for period '{self.period1}' is non-numeric/non-finite ({val1}). Returning NaN."
                )
                return float("nan")
            if not isinstance(val2, (int, float)) or not math.isfinite(val2):
                logger.warning(
                    f"TwoPeriodAverageNode '{self.name}': Value for period '{self.period2}' is non-numeric/non-finite ({val2}). Returning NaN."
                )
                return float("nan")

            # Calculate the average
            return (float(val1) + float(val2)) / 2.0

        except Exception as e:
            # Wrap potential errors during input node calculation
            raise CalculationError(
                message=f"Failed to calculate two-period average for node '{self.name}'",
                node_id=self.name,
                period=f"{self.period1}_and_{self.period2}",  # Indicate context
                details={
                    "input_node": self.input_node.name,
                    "period1": self.period1,
                    "period2": self.period2,
                    "original_error": str(e),
                },
            ) from e

    def get_dependencies(self) -> list[str]:
        """Return the names of nodes this node depends on."""
        return [self.input_node.name]

    def has_calculation(self) -> bool:
        """Indicate that this node performs a calculation."""
        return True


__all__ = [
    "MultiPeriodStatNode",
    "TwoPeriodAverageNode",
    "YoYGrowthNode",
]

# --- END FILE: fin_statement_model/core/nodes/stats_nodes.py ---

# --- START FILE: fin_statement_model/extensions/__init__.py ---
"""Extensions package for fin_statement_model.

This package hosts optional in-repo extensions and third-party plugins discovered via entry-points under
`fin_statement_model.extensions`.
"""

# --- END FILE: fin_statement_model/extensions/__init__.py ---

# --- START FILE: fin_statement_model/extensions/llm/__init__.py ---
"""LLM extension subpackage for fin_statement_model.

Provides built-in OpenAI-based LLM client extension for generating and injecting content.
"""

# --- END FILE: fin_statement_model/extensions/llm/__init__.py ---

# --- START FILE: fin_statement_model/extensions/llm/llm_client.py ---
"""LLM client module for OpenAI and backoff integration.

This module provides `LLMConfig` for client configuration and `LLMClient` for
asynchronous interactions with OpenAI's ChatCompletion API, including retry logic.
"""

import logging
import openai
import backoff
from dataclasses import dataclass
from typing import Optional, Any
from types import TracebackType

logger = logging.getLogger(__name__)


@dataclass
class LLMConfig:
    """Configuration data for LLMClient.

    Attributes:
        api_key: API key for OpenAI authentication.
        model_name: Model to use (e.g., 'gpt-4o').
        temperature: Sampling temperature setting.
        max_tokens: Maximum tokens to generate.
        timeout: Request timeout in seconds.
        max_retries: Number of retries on failure.
    """

    api_key: str
    model_name: str = "gpt-4o"
    temperature: float = 0.7
    max_tokens: int = 1500
    timeout: int = 30
    max_retries: int = 3
    # base_url is no longer needed as the openai library handles the endpoint configuration.


class LLMClientError(Exception):
    """Base exception for LLM client errors."""


class LLMTimeoutError(LLMClientError):
    """Exception for timeout errors."""


class LLMClient:
    """Asynchronous client for OpenAI ChatCompletion API with retry logic.

    Utilizes `LLMConfig` and supports retries on rate limits and timeouts.

    Methods:
        _make_api_call: Internal method for performing the API call with retry logic.
        get_completion: High-level method to obtain chat completions.
    """

    def __init__(self, config: Optional[LLMConfig] = None):
        """Initialize the async LLM client with configuration."""
        self.config = config or LLMConfig(api_key="")
        openai.api_key = self.config.api_key

    @backoff.on_exception(
        backoff.expo,
        (Exception, LLMTimeoutError),
        max_tries=3,
        giveup=lambda e: isinstance(e, LLMTimeoutError),
    )
    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=8)
    async def _make_api_call(self, messages: list[dict[str, str]]) -> dict[str, Any]:
        """Make the async API call to OpenAI with retry logic.

        Args:
            messages: List of message dicts for the ChatCompletion API

        Returns:
            Dict containing the API response

        Raises:
            LLMClientError: For any client-related errors, including timeout if applicable
        """
        try:
            logger.debug(f"Sending async request to OpenAI API with model {self.config.model_name}")
            response = await openai.ChatCompletion.acreate(
                model=self.config.model_name,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                timeout=self.config.timeout,
            )

            if not response.get("choices"):
                logger.error("No suggestions received from OpenAI API")
                raise LLMClientError("No suggestions received from API")

            return response
        except Exception as e:
            if "timeout" in str(e).lower():
                logger.exception("Async request timed out")
                raise LLMTimeoutError("Request timed out") from e
            logger.exception("OpenAI async API request failed")
            raise LLMClientError(f"API request failed: {e!s}") from e

    async def get_completion(self, messages: list[dict[str, str]]) -> str:
        """Get a completion result from the LLM using async API.

        Args:
            messages: List of message dictionaries for the ChatCompletion API

        Returns:
            str: The suggested completion from the LLM

        Raises:
            LLMClientError: For any client-related errors
        """
        try:
            logger.info("Requesting completion from OpenAI async API")
            response = await self._make_api_call(messages)
            completion = response["choices"][0]["message"]["content"].strip()
            logger.info("Successfully received completion")
            return completion
        except Exception as e:
            logger.exception("Error getting completion")
            raise LLMClientError(f"Failed to get completion: {e!s}") from e

    async def __aenter__(self) -> "LLMClient":
        """Enter the asynchronous context manager, returning the client."""
        return self

    async def __aexit__(
        self,
        exc_type: Optional[type[BaseException]],
        exc: Optional[BaseException],
        tb: Optional[TracebackType],
    ) -> None:
        """Exit the asynchronous context manager, performing cleanup."""
        # Ensure method has a body

# --- END FILE: fin_statement_model/extensions/llm/llm_client.py ---

# --- START FILE: fin_statement_model/forecasting/forecaster.py ---
"""Forecasting operations dedicated to statement-level financial graphs."""

import logging
from typing import Any, Optional, Union, Callable
import numpy as np

# Core imports
from fin_statement_model.core.nodes import Node
from fin_statement_model.core.node_factory import NodeFactory

logger = logging.getLogger(__name__)

class StatementForecaster:
    """Handles forecasting operations specifically for a FinancialStatementGraph."""

    def __init__(self, fsg: Any) -> None:
        """Initialize the forecaster.

        Args:
            fsg: The FinancialStatementGraph instance this forecaster will operate on.
        """
        self.fsg = fsg

    def create_forecast(
        self,
        forecast_periods: list[str],
        node_configs: Optional[dict[str, dict[str, Any]]] = None,
        historical_periods: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> None:
        """Create forecasts for financial statement items on the graph.

        Args:
            forecast_periods: List of future periods to forecast.
            node_configs: Mapping of node names to their forecast configurations.
            historical_periods: Optional list of historical periods to use as base.
            **kwargs: Additional arguments passed to the forecasting logic.

        Returns:
            None
        """
        logger.info(f"StatementForecaster: Creating forecast for periods {forecast_periods}")
        try:
            if historical_periods is None:
                if not self.fsg.periods or not forecast_periods:
                    raise ValueError("Cannot infer historical periods: missing graph or forecast periods.")
                first_forecast = forecast_periods[0]
                try:
                    idx = self.fsg.periods.index(first_forecast)
                    historical_periods = self.fsg.periods[:idx]
                    logger.debug(f"Inferred historical periods: {historical_periods}")
                except ValueError:
                    historical_periods = list(self.fsg.periods)
                    logger.warning(f"First forecast period {first_forecast} not found; using all periods.")
            else:
                logger.debug(f"Using explicitly provided historical periods: {historical_periods}")

            if not historical_periods:
                raise ValueError("No historical periods found for forecasting")
            if not forecast_periods:
                raise ValueError("No forecast periods provided")

            # Ensure forecast periods exist in the graph
            existing_periods = set(self.fsg.periods)
            new_periods = []
            for period in forecast_periods:
                if period not in existing_periods:
                    new_periods.append(period)
                    # Correct the call: add_periods is on fsg (the graph), not manipulator
                    self.fsg.add_periods([period])
            if new_periods:
                logger.info(f"Added new periods to graph for forecasting: {new_periods}")

            if node_configs is None:
                node_configs = {}

            for node_name, config in node_configs.items():
                node = self.fsg.get_node(node_name)
                if node is None:
                    raise ValueError(f"Node {node_name} not found in graph")
                if not hasattr(node, "values") or not isinstance(node.values, dict):
                    logger.warning(f"Skipping forecast for node {node_name}: not a value-storing node")
                    continue

                method = config.get("method", "simple")
                growth_config = config.get("config")
                if method == "simple":
                    if isinstance(growth_config, list):
                        growth_config = growth_config[0]
                elif method == "curve":
                    if not isinstance(growth_config, list):
                        growth_config = [growth_config] * len(forecast_periods)
                elif method == "statistical":
                    if not isinstance(growth_config, dict) or "distribution" not in growth_config:
                        raise ValueError(f"Statistical method requires distribution parameters for {node_name}")
                elif method in {"average", "historical_growth"}:
                    growth_config = 0.0
                else:
                    raise ValueError(f"Invalid forecasting method: {method}")

                self._forecast_node(node, historical_periods, forecast_periods, growth_config, method)

            logger.info(f"Created forecast for {len(forecast_periods)} periods and {len(node_configs)} nodes")
        except Exception as e:
            logger.error(f"Error creating forecast: {e}", exc_info=True)
            raise

    def _forecast_node(
        self,
        node: Node,
        historical_periods: list[str],
        forecast_periods: list[str],
        growth_config: Union[float, list[float], Callable[[], float]],
        method: str,
        **kwargs: dict[str, Any],
    ) -> None:
        """Calculate forecast values and update the original node.

        Args:
            node: The graph node to forecast.
            historical_periods: List of historical periods for base values.
            forecast_periods: List of periods for which to calculate forecasts.
            growth_config: Growth parameter(s) (rate, list, or generator).
            method: Forecasting method name.
            **kwargs: Additional arguments passed to growth logic.

        Returns:
            None
        """
        logger.debug(f"StatementForecaster: Forecasting node {node.name} using method {method}")
        if not historical_periods:
            raise ValueError(f"No historical periods for node {node.name}")

        # Determine base period (last available historical period for the node)
        base_period = None
        if hasattr(node, "values") and isinstance(node.values, dict):
            available_historical = sorted([p for p in node.values if p in historical_periods], reverse=True)
            if available_historical:
                base_period = available_historical[0]
        
        # Fallback or if node has no values dict
        if base_period is None:
             base_period = historical_periods[-1] # Use last provided historical period
             logger.info(f"Using {base_period} as base period for {node.name} (node might lack values or specific history)")

        # --- Keep logic to determine forecast_type and growth_params --- 
        method_map = {
            "simple": "fixed",
            "curve": "curve",
            "statistical": "statistical",
            "average": "average",
            "historical_growth": "historical_growth",
        }
        forecast_type = method_map.get(method)
        if forecast_type is None:
            raise ValueError(f"Invalid forecasting method: {method}")

        growth_params: Any # Define type more precisely if possible
        if method == "simple":
            growth_params = float(growth_config)
        elif method == "curve":
            if not isinstance(growth_config, list) or len(growth_config) != len(forecast_periods):
                raise ValueError("Curve growth list length mismatch")
            growth_params = [float(x) for x in growth_config]
        elif method == "statistical":
            if not isinstance(growth_config, dict) or "distribution" not in growth_config:
                raise ValueError(f"Statistical method requires distribution parameters for {node.name}")
            distr = growth_config["distribution"]
            params = growth_config["params"]
            def gen() -> float:
                if distr == "normal":
                    return np.random.normal(params["mean"], params["std"])
                elif distr == "uniform":
                    return np.random.uniform(params["low"], params["high"])
                else:
                    raise ValueError(f"Unsupported distribution: {distr}")
            growth_params = gen
        elif method == "average":
            if not hasattr(node, "calculate") or not callable(node.calculate):
                 raise ValueError(f"Node {node.name} cannot be calculated for average method.")
            hist_vals = [node.calculate(p) for p in historical_periods if hasattr(node, 'values') and p in node.values]
            valid = [v for v in hist_vals if v is not None and not np.isnan(v)]
            if not valid:
                raise ValueError(f"No valid historical data to compute average for {node.name}")
            # For 'average' forecast node type, growth_params is not directly used by factory
            # The node itself calculates the average. Set growth_params=None.
            growth_params = None 
        elif method == "historical_growth":
             # For 'historical_growth' forecast node type, growth_params is not directly used by factory
             # The node itself calculates the average growth. Set growth_params=None.
            growth_params = None
        else:
            growth_params = growth_config # Pass through for potential custom types
        # --- End logic for forecast_type and growth_params ---

        # --- START RESTORED LOGIC --- 
        # Create a temporary node to perform calculations
        tmp_node = NodeFactory.create_forecast_node(
            name=f"{node.name}_forecast_temp",
            base_node=node,
            base_period=base_period,
            forecast_periods=forecast_periods,
            forecast_type=forecast_type,
            growth_params=growth_params,
        )
        
        # Ensure the original node has a values dictionary
        if not hasattr(node, "values") or not isinstance(node.values, dict):
             logger.error(f"Cannot store forecast for node {node.name}: node does not have a 'values' dictionary.")
             return # Cannot proceed
        
        # Calculate and update original node's values
        for period in forecast_periods:
            try:
                val = tmp_node.calculate(period)
                if np.isnan(val) or np.isinf(val):
                    logger.warning(f"Bad forecast {val} for {node.name}@{period}; defaulting to 0.0")
                    val = 0.0
                node.values[period] = float(val) # Update the original node
            except Exception as e:
                logger.error(f"Error forecasting {node.name}@{period}: {e}", exc_info=True)
                node.values[period] = 0.0 # Set default on error

        # --- END RESTORED LOGIC --- 

        # Clear cache of the original node as its values have changed
        if hasattr(node, "clear_cache") and callable(node.clear_cache):
            node.clear_cache()
            logger.debug(f"Cleared cache for node {node.name} after forecast update.")

    def forecast_value(
        self,
        node_name: str,
        forecast_periods: list[str],
        base_period: Optional[str] = None,
        forecast_config: Optional[dict[str, Any]] = None,
        **kwargs: Any,
    ) -> dict[str, float]:
        """Forecast and return values for a node without mutating the graph.

        Args:
            node_name: Name of the node to forecast.
            forecast_periods: List of future periods to forecast.
            base_period: Optional base period to use for forecasting. If omitted, inferred from graph.
            forecast_config: Forecast configuration dict mapping 'method' and 'config'.
            **kwargs: Additional arguments passed to the internal forecasting logic.

        Returns:
            A mapping from forecast period to its forecasted float value.
        """
        # Locate the node
        node = self.fsg.get_node(node_name)
        if node is None:
            raise ValueError(f"Node {node_name} not found in graph")

        # Determine historical periods list
        if base_period:
            historical_periods = [base_period]
        # Try custom getter, otherwise infer
        elif hasattr(self.fsg, "get_historical_periods") and callable(
            getattr(self.fsg, "get_historical_periods")
        ):
            historical_periods = self.fsg.get_historical_periods()
        else:
            if not self.fsg.periods or not forecast_periods:
                raise ValueError(
                    "Cannot infer historical periods: graph or forecast periods missing"
                )
            first = forecast_periods[0]
            try:
                idx = self.fsg.periods.index(first)
                historical_periods = self.fsg.periods[:idx]
            except ValueError:
                historical_periods = list(self.fsg.periods)

        if not historical_periods:
            raise ValueError("No historical periods found for forecasting")
        if not forecast_periods:
            raise ValueError("No forecast periods provided")
            
        # --- Note: Do NOT add forecast periods to the main graph here --- 
        # for period in forecast_periods:
        #     if period not in self.fsg.periods:
        #         self.fsg.add_periods([period]) # Don't modify graph state

        config = forecast_config or {}
        method = config.get("method", "simple")
        growth_cfg = config.get("config")
        # Normalize configuration parameters
        if method == "simple" and isinstance(growth_cfg, list):
            growth_cfg = growth_cfg[0]
        elif method == "curve" and not isinstance(growth_cfg, list):
            growth_cfg = [growth_cfg] * len(forecast_periods)
        elif method == "statistical":
            if not isinstance(growth_cfg, dict) or "distribution" not in growth_cfg:
                raise ValueError(f"Statistical forecast requires distribution for {node_name}")
        elif method in {"average", "historical_growth"}:
            growth_cfg = 0.0 # Will be handled by forecast node type
        else:
            # Leave other methods as-is
            pass
        
        # --- START Non-Mutating Calculation Logic --- 
        # Determine base period (ensure logic matches _forecast_node)
        calc_base_period = None
        if hasattr(node, "values") and isinstance(node.values, dict):
            available_historical = sorted([p for p in node.values if p in historical_periods], reverse=True)
            if available_historical:
                calc_base_period = available_historical[0]
        if calc_base_period is None:
             calc_base_period = historical_periods[-1] 

        # Prepare forecast_type and growth_params (reuse logic from _forecast_node)
        method_map = {
            "simple": "fixed", "curve": "curve", "statistical": "statistical",
            "average": "average", "historical_growth": "historical_growth",
        }
        forecast_type = method_map.get(method)
        if forecast_type is None: raise ValueError(f"Invalid method: {method}")
        
        growth_params: Any = None # Default
        if method == "simple": growth_params = float(growth_cfg)
        elif method == "curve":
            if not isinstance(growth_cfg, list) or len(growth_cfg) != len(forecast_periods):
                raise ValueError("Curve growth list length mismatch")
            growth_params = [float(x) for x in growth_cfg]
        elif method == "statistical":
            # Simplified - assume growth_cfg has needed keys
            if not isinstance(growth_cfg, dict): raise ValueError("Invalid config for statistical method")
            distr = growth_cfg.get("distribution")
            params = growth_cfg.get("params")
            if not distr or not params: raise ValueError("Missing distribution or params for statistical")
            def gen() -> float:
                if distr == "normal": return np.random.normal(params["mean"], params["std"])
                elif distr == "uniform": return np.random.uniform(params["low"], params["high"])
                else: raise ValueError(f"Unsupported distribution: {distr}")
            growth_params = gen
        elif method == "average": growth_params = None
        elif method == "historical_growth": growth_params = None
        else: growth_params = growth_cfg

        # Create a temporary forecast node (DO NOT add to graph)
        try:
            temp_forecast_node = NodeFactory.create_forecast_node(
                name=f"{node_name}_temp_forecast", # Temporary name
                base_node=node, # Use the actual node from graph as base
                base_period=calc_base_period,
                forecast_periods=forecast_periods,
                forecast_type=forecast_type,
                growth_params=growth_params,
            )
        except Exception as e:
             logger.error(f"Failed to create temporary forecast node for '{node_name}': {e}", exc_info=True)
             raise ValueError(f"Could not create temporary forecast node: {e}") from e

        # Calculate results using the temporary node
        results: dict[str, float] = {}
        for period in forecast_periods:
            try:
                value = temp_forecast_node.calculate(period)
                # Handle potential NaN/Inf results from calculation
                results[period] = 0.0 if not np.isfinite(value) else float(value)
            except Exception as e:
                logger.warning(f"Error calculating temporary forecast for {node_name}@{period}: {e}. Returning 0.0")
                results[period] = 0.0
                
        return results
        # --- END Non-Mutating Calculation Logic --- 

# --- END FILE: fin_statement_model/forecasting/forecaster.py ---

# --- START FILE: fin_statement_model/io/__init__.py ---
"""Input/Output components for the Financial Statement Model.

This package provides a unified interface for reading and writing financial model
data from/to various formats using a registry-based approach.
"""

import logging
from typing import Union, Any

from fin_statement_model.core.graph import Graph

from .base import DataReader, DataWriter
from .registry import get_reader, get_writer, list_readers, list_writers
from .exceptions import IOError, ReadError, WriteError, FormatNotSupportedError

# Configure logging for the io package
logger = logging.getLogger(__name__)

# --- Trigger Registration ---
# Import reader/writer modules to ensure their @register decorators run.
# This makes them available in the registry when the io package is imported.
try:
    from . import readers  # noqa: F401
    from . import writers  # noqa: F401
except ImportError:
    # This might happen during setup or if directories are missing
    logger.warning("Could not automatically import readers/writers")


# --- Facade Functions ---

# Define known keyword arguments for reader/writer initialization
_READER_INIT_KWARGS = {"api_key", "mapping_config"}
_WRITER_INIT_KWARGS = {"target"}  # Use 'target' as init kwarg for writer Pydantic config


def read_data(
    format_type: str, source: Any, **kwargs: dict[str, Union[str, int, float, bool]]
) -> Graph:
    """Reads data from a source using the specified format.

    This function acts as a facade for the underlying reader implementations.
    It uses the `format_type` to look up the appropriate reader class in the registry.
    The `source` and `**kwargs` are combined and validated against the specific
    reader's Pydantic configuration model (e.g., `CsvReaderConfig`).

    The validated configuration is used to initialize the reader instance.
    The `source` (which might be the original object for dict/dataframe formats, or
    the validated string path/ticker otherwise) and the original `**kwargs` are then
    passed to the reader instance's `.read()` method, which handles format-specific
    read-time options.

    Args:
        format_type (str): The format identifier (e.g., 'excel', 'csv', 'fmp', 'dict').
        source (Any): The data source. Its type depends on `format_type`:
            - `str`: file path (for 'excel', 'csv'), ticker symbol (for 'fmp').
            - `pd.DataFrame`: for 'dataframe'.
            - `dict`: for 'dict'.
        **kwargs: Additional keyword arguments used for reader configuration (e.g.,
            `api_key`, `delimiter`, `sheet_name`, `mapping_config`) and potentially
            passed to the reader's `.read()` method (e.g., `periods`). Consult the
            specific reader's Pydantic config model and `.read()` docstring.

    Returns:
        Graph: A new Graph object populated with the read data.

    Raises:
        ReadError: If reading fails.
        FormatNotSupportedError: If the format_type is not registered.
        Exception: Other errors during reader initialization or reading.
    """
    logger.info(
        f"Attempting to read data using format '{format_type}' from source type '{type(source).__name__}'"
    )

    # Prepare kwargs for registry validation (includes source and format_type)
    config_kwargs = {**kwargs, "source": source, "format_type": format_type}
    # Keep separate kwargs for the read method itself (e.g., 'periods')
    # This assumes Pydantic configs *don't* capture read-time args.


    try:
        # Pass the config kwargs directly to get_reader
        reader = get_reader(**config_kwargs)

        # Determine the actual source object for the read method
        actual_source = source if format_type in ("dict", "dataframe") else config_kwargs["source"]

        # Pass the determined source and the original kwargs (excluding config keys potentially)
        # to the read method. Specific readers handle relevant kwargs.
        return reader.read(actual_source, **kwargs)
    except (IOError, FormatNotSupportedError):
        logger.exception("IO Error reading data")
        raise  # Re-raise specific IO errors
    except Exception as e:
        logger.exception(f"Unexpected error reading data with format '{format_type}'")
        # Wrap unexpected errors in ReadError for consistency?
        raise ReadError(
            "Unexpected error during read",
            source=str(source),
            reader_type=format_type,
            original_error=e,
        ) from e


def write_data(
    format_type: str,
    graph: Graph,
    target: Any,
    **kwargs: dict[str, Union[str, int, float, bool]],
) -> object:
    """Writes graph data to a target using the specified format.

    Similar to `read_data`, this acts as a facade for writer implementations.
    It uses `format_type` to find the writer class in the registry.
    The `target` and `**kwargs` are combined and validated against the specific
    writer's Pydantic configuration model (e.g., `ExcelWriterConfig`).

    The validated configuration initializes the writer instance.
    The original `graph`, `target`, and `**kwargs` are then passed to the writer
    instance's `.write()` method for format-specific write-time options.

    Args:
        format_type (str): The format identifier (e.g., 'excel', 'dataframe', 'dict').
        graph (Graph): The graph object containing data to write.
        target (Any): The destination target. Its type depends on `format_type`:
            - `str`: file path (usually required for file-based writers like 'excel').
            - Ignored: for writers that return objects (like 'dataframe', 'dict').
        **kwargs: Additional keyword arguments used for writer configuration (e.g.,
            `sheet_name`, `recalculate`) and potentially passed to the writer's
            `.write()` method. Consult the specific writer's Pydantic config model
            and `.write()` docstring.

    Returns:
        object: The result of the write operation. For writers like DataFrameWriter
                or DictWriter, this is the created object. For file writers, it's None.

    Raises:
        WriteError: If writing fails.
        FormatNotSupportedError: If the format_type is not registered.
        Exception: Other errors during writer initialization or writing.
    """
    logger.info(
        f"Attempting to write graph data using format '{format_type}' to target type '{type(target).__name__}'"
    )

    # Prepare kwargs for registry validation (includes target and format_type)
    config_kwargs = {**kwargs, "target": target, "format_type": format_type}


    # Pass the config kwargs directly to get_writer
    writer = get_writer(**config_kwargs)
    # Now call write with all writer-specific kwargs
    try:
        # Pass original graph, target, and non-config kwargs to write()
        result = writer.write(graph, target, **kwargs)

        # If the writer returns a string and target is a path, write it to the file.
        if isinstance(result, str) and isinstance(target, str):
            try:
                logger.debug(f"Writing string result from writer '{type(writer).__name__}' to file: {target}")
                with open(target, "w", encoding="utf-8") as f:
                    f.write(result)
                return None # Consistent return for file writers
            except OSError as e:
                logger.exception(f"Failed to write writer output to target file: {target}")
                raise WriteError(
                    f"Failed to write writer output to file: {target}",
                    target=target,
                    writer_type=format_type,
                    original_error=e
                ) from e
        else:
            # Otherwise, return the original result (e.g., DataFrame, dict)
            return result
    except (IOError, FormatNotSupportedError):
        logger.exception("IO Error writing data")
        raise  # Re-raise specific IO errors
    except Exception as e:
        logger.exception(f"Unexpected error writing data with format '{format_type}'")
        # Wrap unexpected errors
        raise WriteError(
            "Unexpected error during write",
            target=str(target),
            writer_type=format_type,
            original_error=e,
        ) from e


# --- Public API ---

__all__ = [
    # "get_reader", # Probably don't expose getters directly
    # "get_writer",
    # Base classes (optional exposure)
    "DataReader",
    "DataWriter",
    "FormatNotSupportedError",
    # Exceptions
    "IOError",
    "ReadError",
    "WriteError",
    # Registry functions (optional exposure)
    "list_readers",
    "list_writers",
    # Facade functions
    "read_data",
    "write_data",
]

# --- END FILE: fin_statement_model/io/__init__.py ---

# --- START FILE: fin_statement_model/io/base.py ---
"""Base classes for data readers and writers."""

from abc import ABC, abstractmethod
from typing import Any

# Use absolute import based on project structure
from fin_statement_model.core.graph import Graph


class DataReader(ABC):
    """Abstract base class for all data readers.

    Defines the interface for classes that read data from various sources
    and typically populate or return a Graph object.
    """

    @abstractmethod
    def read(self, source: Any, **kwargs: dict[str, Any]) -> Graph:
        """Read data from the specified source and return a Graph.

        Args:
            source: The data source. Type depends on the reader implementation
                (e.g., file path `str`, ticker `str`, `pd.DataFrame`, `dict`).
            **kwargs: Additional format-specific options for reading.

        Returns:
            A Graph object populated with the data from the source.

        Raises:
            ReadError: If an error occurs during the reading process.
            NotImplementedError: If the method is not implemented by a subclass.
        """
        raise NotImplementedError


class DataWriter(ABC):
    """Abstract base class for all data writers.

    Defines the interface for classes that write graph data to various targets.
    """

    @abstractmethod
    def write(self, graph: Graph, target: Any, **kwargs: dict[str, Any]) -> object:
        """Write data from the Graph object to the specified target.

        Args:
            graph: The Graph object containing the data to write.
            target: The destination target. Type depends on the writer implementation
                (e.g., file path `str`, or ignored if the writer returns an object).
            **kwargs: Additional format-specific options for writing.

        Raises:
            WriteError: If an error occurs during the writing process.
            NotImplementedError: If the method is not implemented by a subclass.
        """
        raise NotImplementedError

# --- END FILE: fin_statement_model/io/base.py ---

# --- START FILE: fin_statement_model/io/config/models.py ---
"""Pydantic models for IO reader and writer configuration.

This module provides declarative schemas for validating configuration passed to IO readers.
"""

from __future__ import annotations

from typing import Optional, Literal, Any, Union
from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator

# Define MappingConfig locally to avoid circular import
MappingConfig = Union[
    dict[str, str],
    dict[Optional[str], dict[str, str]]
]


class BaseReaderConfig(BaseModel):
    """Base configuration for IO readers."""
    source: str = Field(
        ..., description="URI or path to data source (file path, ticker, etc.)"
    )
    format_type: Literal["csv", "excel", "dataframe", "dict", "fmp"] = Field(
        ..., description="Type of reader (csv, excel, dataframe, dict, fmp)."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class CsvReaderConfig(BaseReaderConfig):
    """CSV reader options."""
    delimiter: str = Field(
        ",", description="Field delimiter for CSV files."
    )
    header_row: int = Field(
        1, description="Row number containing column names (1-indexed)."
    )
    index_col: Optional[int] = Field(
        None, description="1-indexed column for row labels."
    )
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    @model_validator(mode="after")
    def check_header_row(cls, cfg: CsvReaderConfig) -> CsvReaderConfig:
        """Ensure header_row is at least 1."""
        if cfg.header_row < 1:
            raise ValueError("header_row must be >= 1")
        return cfg


class ExcelReaderConfig(BaseReaderConfig):
    """Excel reader options."""
    sheet_name: Optional[str] = Field(
        None, description="Worksheet name or index."
    )
    items_col: int = Field(
        1, description="1-indexed column where item names reside."
    )
    periods_row: int = Field(
        1, description="1-indexed row where periods reside."
    )
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    @model_validator(mode="after")
    def check_indices(cls, cfg: ExcelReaderConfig) -> ExcelReaderConfig:
        """Ensure items_col and periods_row are at least 1."""
        if cfg.items_col < 1 or cfg.periods_row < 1:
            raise ValueError("items_col and periods_row must be >= 1")
        return cfg


class FmpReaderConfig(BaseReaderConfig):
    """Financial Modeling Prep API reader options."""
    statement_type: Literal["income_statement", "balance_sheet", "cash_flow"] = Field(
        ..., description="Type of financial statement to fetch."
    )
    period_type: Literal["FY", "QTR"] = Field(
        "FY", description="Period type: 'FY' or 'QTR'."
    )
    limit: int = Field(
        5, description="Number of periods to fetch."
    )
    api_key: Optional[str] = Field(
        None, description="Financial Modeling Prep API key."
    )
    mapping_config: Optional[MappingConfig] = Field(
        None, description="Optional configuration for mapping source item names."
    )

    @field_validator("api_key", mode="before")
    def load_api_key_env(cls, value: Optional[str]) -> Optional[str]:
        """Load api_key from FMP_API_KEY env var if not provided."""
        if not value:
            import os
            return os.getenv("FMP_API_KEY")
        return value

    @model_validator(mode="after")
    def check_api_key(cls, cfg: FmpReaderConfig) -> FmpReaderConfig:
        """Ensure an API key is provided."""
        if not cfg.api_key:
            raise ValueError("api_key is required (env var FMP_API_KEY or param)")
        return cfg


# --- New Reader Configs for DataFrame and Dict readers ---


class DataFrameReaderConfig(BaseReaderConfig):
    """Configuration for DataFrameReader.

    No additional reader-specific options are required at the moment because
    the reader consumes an in-memory :class:`pandas.DataFrame` supplied to
    :py:meth:`DataFrameReader.read`.  The `source` field therefore serves only
    to preserve a consistent registry-initialisation contract.
    """

    source: Any = Field(..., description="In-memory pandas DataFrame source")
    format_type: Literal["dataframe"] = "dataframe"


class DictReaderConfig(BaseReaderConfig):
    """Configuration for DictReader.

    Mirrors :class:`DataFrameReaderConfig` - no custom options yet.  The
    placeholder keeps the IO registry symmetric and future-proof.
    """

    source: dict[str, dict[str, float]] = Field(..., description="In-memory dictionary source")
    format_type: Literal["dict"] = "dict"


# --- Writer-side Pydantic configuration models ---
class BaseWriterConfig(BaseModel):
    """Base configuration for IO writers."""
    target: Optional[str] = Field(
        None, description="URI or path to data target (file path, in-memory target, etc.)"
    )
    format_type: Literal["excel", "dataframe", "dict", "markdown"] = Field(
        ..., description="Type of writer (excel, dataframe, dict, markdown)."
    )

    model_config = ConfigDict(extra="allow", frozen=True)


class ExcelWriterConfig(BaseWriterConfig):
    """Excel writer options."""
    sheet_name: str = Field(
        "Sheet1", description="Name of the sheet to write to."
    )
    recalculate: bool = Field(
        True, description="Whether to recalculate graph before export."
    )
    include_nodes: Optional[list[str]] = Field(
        None, description="Optional list of node names to include in export."
    )
    excel_writer_kwargs: dict[str, Any] = Field(
        default_factory=dict, description="Additional kwargs for pandas.DataFrame.to_excel."
    )


class DataFrameWriterConfig(BaseWriterConfig):
    """DataFrame writer options."""
    target: Optional[str] = Field(
        None, description="Optional target path (ignored by DataFrameWriter)."
    )
    recalculate: bool = Field(
        True, description="Whether to recalculate graph before export."
    )
    include_nodes: Optional[list[str]] = Field(
        None, description="Optional list of node names to include in export."
    )


class DictWriterConfig(BaseWriterConfig):
    """Dict writer has no additional options."""
    target: Optional[str] = Field(
        None, description="Optional target (ignored by DictWriter)."
    )


class MarkdownWriterConfig(BaseWriterConfig):
    """Markdown writer options."""
    indent_spaces: int = Field(
        4, description="Number of spaces per indentation level."
    )
    target: Optional[str] = Field(
        None, description="Optional target path (ignored by MarkdownWriter)."
    )

# --- END FILE: fin_statement_model/io/config/models.py ---

# --- START FILE: fin_statement_model/io/exceptions.py ---
"""IO specific exceptions."""

from typing import Optional

# Use absolute import based on project structure
from fin_statement_model.core.errors import FinancialModelError


class IOError(FinancialModelError):
    """Base exception for all Input/Output errors in the IO package."""

    def __init__(
        self,
        message: str,
        source_or_target: Optional[str] = None,
        format_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the IOError.

        Args:
            message: The base error message.
            source_or_target: Optional identifier for the source (read) or target (write).
            format_type: Optional name of the format or handler involved.
            original_error: Optional underlying exception that caused the failure.
        """
        self.source_or_target = source_or_target
        self.format_type = format_type
        self.original_error = original_error

        context = []
        if source_or_target:
            context.append(f"source/target '{source_or_target}'")
        if format_type:
            context.append(f"format '{format_type}'")

        full_message = f"{message} involving {' and '.join(context)}" if context else message

        if original_error:
            full_message = f"{full_message}: {original_error!s}"

        super().__init__(full_message)


class ReadError(IOError):
    """Exception raised specifically for errors during data read/import operations."""

    def __init__(
        self,
        message: str,
        source: Optional[str] = None,
        reader_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the ReadError.

        Args:
            message: The base error message.
            source: Optional identifier for the data source (e.g., file path, URL).
            reader_type: Optional name of the reader class used for importing.
            original_error: Optional underlying exception that caused the import failure.
        """
        super().__init__(
            message=message,
            source_or_target=source,
            format_type=reader_type,
            original_error=original_error,
        )


class WriteError(IOError):
    """Exception raised specifically for errors during data write/export operations."""

    def __init__(
        self,
        message: str,
        target: Optional[str] = None,
        writer_type: Optional[str] = None,
        original_error: Optional[Exception] = None,
    ):
        """Initializes the WriteError.

        Args:
            message: The base error message.
            target: Optional identifier for the export destination (e.g., file path).
            writer_type: Optional name of the writer class being used.
            original_error: Optional underlying exception that caused the export failure.
        """
        super().__init__(
            message=message,
            source_or_target=target,
            format_type=writer_type,
            original_error=original_error,
        )


class FormatNotSupportedError(IOError):
    """Exception raised when a requested IO format is not registered or supported."""

    def __init__(self, format_type: str, operation: str = "read/write"):
        """Initializes the FormatNotSupportedError.

        Args:
            format_type: The requested format identifier (e.g., 'excel', 'json').
            operation: The operation being attempted ('read' or 'write').
        """
        message = f"Format '{format_type}' is not supported for {operation} operations."
        super().__init__(message=message, format_type=format_type)

# --- END FILE: fin_statement_model/io/exceptions.py ---

# --- START FILE: fin_statement_model/io/readers/__init__.py ---
"""Readers for various data formats into the Financial Statement Model.

Exposes functions for reading data from different sources like configuration files,
Excel, CSV (TBD), etc.
"""

# Explicitly import each reader module to ensure registration decorators run.
from . import base
from . import cell_reader
from . import csv
from . import dataframe
from . import dict
from . import excel
from . import fmp # Ensure FMP reader module is imported
from . import statement_config_reader

# --- Public API Exports --- #

# Expose functions directly if preferred
from .statement_config_reader import (
    read_statement_config_from_path,
    list_available_builtin_configs,
    read_builtin_statement_config,
)
from .cell_reader import import_from_cells

# Expose reader base class and specific classes if needed (usually not needed by end-users)
# from .base import DataReader
# from .csv import CsvReader
# from .excel import ExcelReader
# ... etc

__all__ = [
    "base",
    "cell_reader",
    "csv",
    "dataframe",
    "dict",
    "excel",
    "fmp",
    "import_from_cells",
    "list_available_builtin_configs",
    "read_builtin_statement_config",
    "read_statement_config_from_path",
    "statement_config_reader",
]

# --- END FILE: fin_statement_model/io/readers/__init__.py ---

# --- START FILE: fin_statement_model/io/readers/base.py ---
"""Shared utilities for IO readers."""

from typing import Optional, Union

MappingConfig = Union[
    dict[str, str],
    dict[Optional[str], dict[str, str]]
]

def normalize_mapping(
    mapping_config: MappingConfig = None,
    context_key: Optional[str] = None
) -> dict[str, str]:
    """Turn a scoped MappingConfig into a unified flat dict with a required default mapping under None.

    Args:
        mapping_config: MappingConfig object defining name mappings.
        context_key: Optional key (e.g., sheet name or statement type) to select
            a scoped mapping within a scoped config.

    Returns:
        A flat dict mapping original names to canonical names.

    Raises:
        TypeError: If the provided mapping_config is not of a supported structure.
    """
    if mapping_config is None:
        return {}
    if not isinstance(mapping_config, dict):
        raise TypeError(
            f"mapping_config must be a dict, got {type(mapping_config).__name__}"
        )
    if None not in mapping_config:
        raise TypeError("mapping_config must include a default mapping under None")
    default_map = mapping_config[None]

    if not isinstance(default_map, dict):
        raise TypeError(
            "Default mapping (None key) must be a dict[str, str] if present"
        )
    result: dict[str, str] = dict(default_map)
    if context_key and context_key in mapping_config:
        scoped = mapping_config[context_key]
        if not isinstance(scoped, dict):
            raise TypeError(
                f"Scoped mapping for key '{context_key}' must be a dict[str, str]"
            )
        result.update(scoped)
    return result

# --- END FILE: fin_statement_model/io/readers/base.py ---

# --- START FILE: fin_statement_model/io/readers/cell_reader.py ---
"""Importer module for reading cell-based financial statement data into a Graph."""

from typing import Any
# from fin_statement_model.statements.graph.financial_graph import FinancialStatementGraph # Removed
from fin_statement_model.core.graph import Graph  # Added

__all__ = ["import_from_cells"]


def import_from_cells(cells_info: list[dict[str, Any]]) -> Graph:  # Changed return type
    """Import a list of cell dictionaries into a core Graph.

    Each cell dict should include at minimum:
    - 'row_name': identifier for the line item (becomes node ID)
    - 'column_name': the period label
    - 'value': the numeric value

    Args:
        cells_info: List of cell metadata dictionaries.

    Returns:
        A core Graph populated with detected periods and data nodes.
    """
    # Group cells by row_name to aggregate values per financial statement item
    items: dict[str, dict[str, Any]] = {}
    unique_periods: set = set()

    for cell in cells_info:
        # Clean the item name and period
        item_name = cell.get("row_name", "").strip()
        period = cell.get("column_name", "").strip()
        value = cell.get("value")

        if not item_name or not period:
            continue

        unique_periods.add(period)
        if item_name not in items:
            items[item_name] = {}
        items[item_name][period] = value

    # Sort periods and create the graph
    sorted_periods = sorted(list(unique_periods))  # Ensure list for Graph constructor
    graph = Graph(periods=sorted_periods)  # Changed to Graph

    # Add each financial statement item as a data node to the graph
    for name, values in items.items():
        # Assuming Graph has an add_data_node method or similar
        # This might need adjustment based on the actual core.Graph API
        graph.add_data_node(name, values) # Changed from fsg.add_financial_statement_item

    return graph # Changed from fsg

# --- END FILE: fin_statement_model/io/readers/cell_reader.py ---

# --- START FILE: fin_statement_model/io/readers/config/__init__.py ---
"""Package for IO reader default mappings.

This package supports resource loading via importlib.resources.
"""

# --- END FILE: fin_statement_model/io/readers/config/__init__.py ---

# --- START FILE: fin_statement_model/io/readers/csv.py ---
"""Data reader for CSV files."""

import logging
import os
from typing import Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataReader
from fin_statement_model.io.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import CsvReaderConfig

logger = logging.getLogger(__name__)


@register_reader("csv")
class CsvReader(DataReader):
    """Reads financial statement data from a CSV file into a Graph.

    Assumes a 'long' format where each row represents a single data point
    (item, period, value).
    Requires specifying the columns containing item names, period identifiers,
    and values.

    Supports a `mapping_config` constructor parameter for name mapping,
    accepting either a flat mapping or a statement-type scoped mapping.

    Configuration (delimiter, header_row, index_col, mapping_config) is passed
    via a `CsvReaderConfig` object during initialization (typically by the `read_data` facade).
    Method-specific options (`item_col`, `period_col`, `value_col`, `pandas_read_csv_kwargs`)
    are passed as keyword arguments to the `read()` method.
    """

    def __init__(self, cfg: CsvReaderConfig) -> None:
        """Initialize the CsvReader with validated configuration.

        Args:
            cfg: A validated `CsvReaderConfig` instance containing parameters like
                 `source`, `delimiter`, `header_row`, `index_col`, and `mapping_config`.
        """
        self.cfg = cfg

    def read(self, source: str, **kwargs: dict[str, Any]) -> Graph:
        """Read data from a CSV file into a new Graph.

        Args:
            source (str): Path to the CSV file.
            **kwargs: Read-time keyword arguments:
                item_col (str): Name of the column containing item identifiers.
                period_col (str): Name of the column containing period identifiers.
                value_col (str): Name of the column containing numeric values.
                pandas_read_csv_kwargs (dict): Additional arguments passed
                    directly to `pandas.read_csv()`. These can override settings
                    from the `CsvReaderConfig` (e.g., `delimiter`).

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the file cannot be read or required columns are missing.
        """
        file_path = source
        logger.info(f"Starting import from CSV file: {file_path}")

        # --- Validate Inputs ---
        if not os.path.exists(file_path):
            raise ReadError(
                f"File not found: {file_path}",
                source=file_path,
                reader_type="CsvReader",
            )

        item_col = kwargs.get("item_col")
        period_col = kwargs.get("period_col")
        value_col = kwargs.get("value_col")
        read_csv_options = kwargs.get("pandas_read_csv_kwargs", {})

        if not item_col or not period_col or not value_col:
            raise ReadError(
                "Missing required arguments: 'item_col', 'period_col', 'value_col' must be provided.",
                source=file_path,
                reader_type="CsvReader",
            )

        # --- Read CSV Data ---
        try:
            # Use configuration from self.cfg, allow overrides via read_csv_options
            read_options = {
                "delimiter": self.cfg.delimiter,
                "header": self.cfg.header_row - 1 if self.cfg.header_row else 0, # pandas is 0-indexed header
                "index_col": self.cfg.index_col - 1 if self.cfg.index_col else None, # pandas is 0-indexed index
            }
            # Merge user-provided kwargs, allowing them to override config
            read_options.update(read_csv_options)

            df = pd.read_csv(file_path, **read_options)

            # Validate required columns exist
            required_cols = {item_col, period_col, value_col}
            missing_cols = required_cols - set(df.columns)
            if missing_cols:
                raise ReadError(
                    f"Missing required columns in CSV: {missing_cols}",
                    source=file_path,
                    reader_type="CsvReader",
                )

            # Convert period column to string
            df[period_col] = df[period_col].astype(str)
            all_periods = sorted(df[period_col].unique().tolist())
            if not all_periods:
                raise ReadError(
                    "No periods found in the specified period column.",
                    source=file_path,
                    reader_type="CsvReader",
                )

            logger.info(f"Identified periods: {all_periods}")
            graph = Graph(periods=all_periods)

            # --- Populate Graph ---
            # Group data by item name
            grouped = df.groupby(item_col)
            validation_errors = []
            nodes_added = 0

            for item_name_csv, group in grouped:
                if pd.isna(item_name_csv) or not item_name_csv:
                    logger.debug("Skipping group with empty item name.")
                    continue

                item_name_csv_str = str(item_name_csv).strip()
                node_name = self.cfg.mapping_config.get(item_name_csv_str, item_name_csv_str)

                period_values: dict[str, float] = {}
                for _, row in group.iterrows():
                    period = row[period_col]
                    value = row[value_col]

                    if pd.isna(value):
                        continue  # Skip missing values

                    if not isinstance(value, (int, float)):
                        try:
                            value = float(value)
                            logger.warning(
                                f"Converted non-numeric value '{row[value_col]}' to float for node '{node_name}' period '{period}'"
                            )
                        except (ValueError, TypeError):
                            validation_errors.append(
                                f"Item '{item_name_csv_str}': Non-numeric value '{value}' for period '{period}'"
                            )
                            continue  # Skip this invalid value

                    if period in period_values:
                        logger.warning(
                            f"Duplicate value found for node '{node_name}' (from CSV item '{item_name_csv_str}') period '{period}'. Using the last one found."
                        )

                    period_values[period] = float(value)

                if period_values:
                    if graph.has_node(node_name):
                        logger.warning(
                            f"Node '{node_name}' (from CSV item '{item_name_csv_str}') already exists. Overwriting data is not standard for readers."
                        )
                        # Potentially update existing node? For now, log.
                    else:
                        new_node = FinancialStatementItemNode(name=node_name, values=period_values)
                        graph.add_node(new_node)
                        nodes_added += 1

            if validation_errors:
                raise ReadError(
                    f"Validation errors occurred while reading {file_path}: {'; '.join(validation_errors)}",
                    source=file_path,
                    reader_type="CsvReader",
                )

            logger.info(f"Successfully created graph with {nodes_added} nodes from {file_path}.")
            return graph

        except FileNotFoundError:
            raise ReadError(
                f"File not found: {file_path}",
                source=file_path,
                reader_type="CsvReader",
            )
        except ValueError as ve:
            raise ReadError(
                f"Error reading CSV file: {ve}",
                source=file_path,
                reader_type="CsvReader",
                original_error=ve,
            )
        except KeyError as ke:
            raise ReadError(
                f"Column not found error (check item/period/value_col names): {ke}",
                source=file_path,
                reader_type="CsvReader",
                original_error=ke,
            )
        except Exception as e:
            logger.error(f"Failed to read CSV file {file_path}: {e}", exc_info=True)
            raise ReadError(
                message=f"Failed to process CSV file: {e}",
                source=file_path,
                reader_type="CsvReader",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/io/readers/csv.py ---

# --- START FILE: fin_statement_model/io/readers/dataframe.py ---
"""Data reader for pandas DataFrames."""

import logging
import pandas as pd
import numpy as np
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataReader
from fin_statement_model.io.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import DataFrameReaderConfig

logger = logging.getLogger(__name__)


@register_reader("dataframe")
class DataFrameReader(DataReader):
    """Reads data from a pandas DataFrame into a Graph.

    Assumes the DataFrame index contains node names and columns contain periods.
    Values should be numeric.
    """

    def __init__(self, cfg: Optional[DataFrameReaderConfig] = None) -> None:
        """Initialize the DataFrameReader.

        Args:
            cfg: Optional validated `DataFrameReaderConfig` instance.
                 Currently unused but kept for registry symmetry and future options.
        """
        self.cfg = cfg  # For future use; currently no configuration options.

    def read(self, source: pd.DataFrame, **kwargs: Any) -> Graph:
        """Read data from a pandas DataFrame into a new Graph.

        Assumes DataFrame index = node names, columns = periods.

        Args:
            source (pd.DataFrame): The DataFrame to read data from.
            **kwargs: Read-time keyword arguments:
                periods (list[str], optional): Explicit list of periods (columns) to include.
                    If None, all columns are assumed to be periods.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the source is not a DataFrame or has invalid structure.
        """
        df = source
        logger.info("Starting import from DataFrame.")

        # --- Validate Inputs ---
        if not isinstance(df, pd.DataFrame):
            raise ReadError(
                "Source is not a pandas DataFrame.",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        if df.index.name is None and df.index.empty:
            logger.warning(
                "DataFrame index is unnamed and empty, assuming columns are nodes if periods kwarg is provided."
            )
            # Handle case where DF might be oriented differently if periods kwarg is present?
            # For now, stick to index=nodes assumption.

        # Determine periods: use explicit list or infer from columns
        graph_periods_arg = kwargs.get("periods")
        if graph_periods_arg:
            if not isinstance(graph_periods_arg, list):
                raise ReadError("'periods' argument must be a list of column names.")
            missing_cols = [p for p in graph_periods_arg if p not in df.columns]
            if missing_cols:
                raise ReadError(
                    f"Specified periods (columns) not found in DataFrame: {missing_cols}"
                )
            graph_periods = sorted(graph_periods_arg)
            df_subset = df[graph_periods]  # Select only specified period columns
        else:
            # Assume all columns are periods
            graph_periods = sorted(df.columns.astype(str).tolist())
            df_subset = df

        if not graph_periods:
            raise ReadError(
                "No periods identified in DataFrame columns.",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        logger.info(f"Using periods (columns): {graph_periods}")
        graph = Graph(periods=graph_periods)

        # --- Populate Graph ---
        validation_errors = []
        nodes_added = 0
        for node_name_df, row in df_subset.iterrows():
            if pd.isna(node_name_df) or not node_name_df:
                logger.debug("Skipping row with empty index name.")
                continue

            node_name = str(node_name_df).strip()
            period_values: dict[str, float] = {}
            for period in graph_periods:
                value = row[period]
                if pd.isna(value):
                    continue  # Skip NaN values

                if not isinstance(value, (int, float, np.number)):
                    try:
                        value = float(value)
                        logger.warning(
                            f"Converted non-numeric value '{row[period]}' to float for node '{node_name}' period '{period}'"
                        )
                    except (ValueError, TypeError):
                        validation_errors.append(
                            f"Node '{node_name}': Non-numeric value '{value}' for period '{period}'"
                        )
                        continue  # Skip invalid value

                period_values[period] = float(value)

            if period_values:
                if graph.has_node(node_name):
                    logger.warning(
                        f"Node '{node_name}' already exists. Overwriting data is not standard for readers."
                    )
                    # Update existing? Log for now.
                else:
                    new_node = FinancialStatementItemNode(name=node_name, values=period_values)
                    graph.add_node(new_node)
                    nodes_added += 1

        if validation_errors:
            raise ReadError(
                f"Validation errors occurred while reading DataFrame: {'; '.join(validation_errors)}",
                source="DataFrame",
                reader_type="DataFrameReader",
            )

        logger.info(f"Successfully created graph with {nodes_added} nodes from DataFrame.")
        return graph

        # No specific file operations, so less need for broad Exception catch
        # Specific errors handled above (TypeError, ValueError from float conversion)

# --- END FILE: fin_statement_model/io/readers/dataframe.py ---

# --- START FILE: fin_statement_model/io/readers/dict.py ---
"""Data reader for Python dictionaries."""

import logging
from typing import Optional, Any

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataReader
from fin_statement_model.io.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.config.models import DictReaderConfig

logger = logging.getLogger(__name__)


@register_reader("dict")
class DictReader(DataReader):
    """Reads data from a Python dictionary to create a new Graph.

    Expects a dictionary format: {node_name: {period: value, ...}, ...}
    Creates FinancialStatementItemNode instances for each entry.

    Note:
        Configuration is handled via `DictReaderConfig` during initialization.
        The `read()` method takes the source dictionary directly and an optional
        `periods` keyword argument.
    """

    def __init__(self, cfg: Optional[DictReaderConfig] = None) -> None:
        """Initialize the DictReader.

        Args:
            cfg: Optional validated `DictReaderConfig` instance.
                 Currently unused but kept for registry symmetry and future options.
        """
        self.cfg = cfg

    def read(self, source: dict[str, dict[str, float]], **kwargs: Any) -> Graph:
        """Create a new Graph from a dictionary.

        Args:
            source: Dictionary mapping node names to period-value dictionaries.
                    Format: {node_name: {period: value, ...}, ...}
            **kwargs: Read-time keyword arguments:
                periods (list[str], optional): Explicit list of periods for the new graph.
                    If None, inferred from data keys.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the source data format is invalid or processing fails.
            # DataValidationError: If data values are not numeric.
        """
        logger.info("Starting import from dictionary to create a new graph.")

        if not isinstance(source, dict):
            raise ReadError(
                message="Invalid source type for DictReader. Expected dict.",
                source="dict_input",
                reader_type="DictReader",
            )

        # Validate data structure and collect all periods
        all_periods = set()
        validation_errors = []
        try:
            for node_name, period_values in source.items():
                if not isinstance(period_values, dict):
                    validation_errors.append(
                        f"Node '{node_name}': Invalid format - expected dict, got {type(period_values).__name__}"
                    )
                    continue  # Skip further checks for this node
                for period, value in period_values.items():
                    # Basic type checks - can be expanded
                    if not isinstance(period, str):
                        validation_errors.append(
                            f"Node '{node_name}': Invalid period format '{period}' - expected string."
                        )
                    if not isinstance(value, (int, float)):
                        validation_errors.append(
                            f"Node '{node_name}' period '{period}': Invalid value type {type(value).__name__} - expected number."
                        )
                    all_periods.add(str(period))

            if validation_errors:
                # Use core DataValidationError if it exists and is suitable
                # Otherwise, stick to ReadError or a specific IOValidationError
                # raise DataValidationError(
                #     message="Input dictionary failed validation",
                #     validation_errors=validation_errors
                # )
                raise ReadError(
                    f"Input dictionary failed validation: {'; '.join(validation_errors)}",
                    source="dict_input",
                    reader_type="DictReader",
                )

        except Exception as e:
            # Catch unexpected validation errors
            raise ReadError(
                message=f"Error validating input dictionary: {e}",
                source="dict_input",
                reader_type="DictReader",
                original_error=e,
            ) from e

        # Determine graph periods
        graph_periods = kwargs.get("periods")
        if graph_periods is None:
            graph_periods = sorted(list(all_periods))
            logger.debug(f"Inferred graph periods from data: {graph_periods}")
        # Optional: Validate if all data periods are within the provided list
        elif not all_periods.issubset(set(graph_periods)):
            missing = all_periods - set(graph_periods)
            logger.warning(f"Data contains periods not in specified graph periods: {missing}")
            # Decide whether to error or just ignore extra data

        # Create graph and add nodes
        try:
            graph = Graph(periods=graph_periods)
            for node_name, period_values in source.items():
                # Filter values to only include those matching graph_periods
                filtered_values = {p: v for p, v in period_values.items() if p in graph_periods}
                if filtered_values:
                    # Create FinancialStatementItemNode directly
                    # Assumes FinancialStatementItemNode takes name and values dict
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=filtered_values.copy()
                    )
                    graph.add_node(new_node)
                else:
                    logger.debug(
                        f"Node '{node_name}' has no data for specified graph periods. Skipping."
                    )

            logger.info(
                f"Successfully created graph with {len(graph.nodes)} nodes from dictionary."
            )
            return graph

        except Exception as e:
            # Catch errors during graph/node creation
            logger.error(f"Failed to create graph from dictionary: {e}", exc_info=True)
            raise ReadError(
                message="Failed to build graph from dictionary data",
                source="dict_input",
                reader_type="DictReader",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/io/readers/dict.py ---

# --- START FILE: fin_statement_model/io/readers/excel.py ---
"""Data reader for Excel files."""

import logging
import os
from typing import Optional, Any

import pandas as pd

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataReader
from fin_statement_model.io.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.readers.base import normalize_mapping
from fin_statement_model.io.config.models import ExcelReaderConfig

logger = logging.getLogger(__name__)


@register_reader("excel")
class ExcelReader(DataReader):
    """Reads financial statement data from an Excel file into a Graph.

    Expects data in a tabular format where rows typically represent items
    and columns represent periods, or vice-versa.
    Requires specifying sheet name, period identification, and item identification.

    Configuration (sheet_name, items_col, periods_row, mapping_config) is passed
    via an `ExcelReaderConfig` object during initialization (typically by the `read_data` facade).
    Method-specific options (`statement_type`, `header_row`, `nrows`, `skiprows`)
    are passed as keyword arguments to the `read()` method.
    """

    def __init__(self, cfg: ExcelReaderConfig) -> None:
        """Initialize the ExcelReader with validated configuration.

        Args:
            cfg: A validated `ExcelReaderConfig` instance containing parameters like
                 `source`, `sheet_name`, `items_col`, `periods_row`, and `mapping_config`.
        """
        self.cfg = cfg

    def _get_mapping(
        self,
        statement_type: Optional[str],
    ) -> dict[str, str]:
        """Get the appropriate mapping based on statement type and the stored config."""
        # Use the mapping config stored in the validated Pydantic config object
        config = self.cfg.mapping_config
        # Normalize and overlay user-provided mappings
        mapping = normalize_mapping(config, context_key=statement_type)
        return mapping

    def read(self, source: str, **kwargs: dict[str, Any]) -> Graph:
        """Read data from an Excel file sheet into a new Graph based on instance config.

        Args:
            source (str): Path to the Excel file.
            **kwargs: Optional runtime keyword arguments:
                statement_type (str, optional): Type of statement ('income_statement', 'balance_sheet', 'cash_flow').
                    Used to select a scope within the `mapping_config` provided during initialization.
                header_row (int, optional): 1-based index for pandas header reading.
                    Defaults to `self.cfg.periods_row` if not provided.
                nrows (int, optional): Number of rows to read from the sheet.
                skiprows (int, optional): Number of rows to skip at the beginning.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If the file cannot be read or the configuration is invalid.
            AssertionError: If the reader was not initialized with a configuration.
        """
        assert self.cfg is not None, "ExcelReader must be initialized with a valid configuration."
        file_path = source
        logger.info(f"Starting import from Excel file: {file_path}")

        # --- Validate Inputs ---
        if not os.path.exists(file_path):
            raise ReadError(
                f"File not found: {file_path}",
                source=file_path,
                reader_type="ExcelReader",
            )
        if not file_path.endswith((".xls", ".xlsx", ".xlsm")):
            raise ReadError(
                f"Not a valid Excel file extension: {file_path}",
                source=file_path,
                reader_type="ExcelReader",
            )

        # Use configuration directly from self.cfg
        sheet_name = self.cfg.sheet_name
        periods_row = self.cfg.periods_row
        items_col = self.cfg.items_col

        # Convert to 0-based indices for pandas
        periods_row_0idx = periods_row - 1 if periods_row else None
        items_col_0idx = items_col - 1 if items_col else None
        # Get header_row from kwargs, default to configured periods_row
        header_row = kwargs.get("header_row", self.cfg.periods_row)
        header_row_0idx = header_row - 1 if header_row else periods_row_0idx

        if sheet_name is None or periods_row_0idx is None or items_col_0idx is None:
            raise ReadError(
                "Configuration is missing required arguments: 'sheet_name', 'periods_row', 'items_col'.",
                source=file_path,
                reader_type="ExcelReader",
            )

        # Runtime options from kwargs
        statement_type = kwargs.get("statement_type")
        nrows = kwargs.get("nrows")
        skiprows = kwargs.get("skiprows")

        # Determine mapping based on config and runtime statement_type
        try:
            mapping = self._get_mapping(statement_type)
        except TypeError as te:
            raise ReadError(
                "Invalid mapping_config provided.",
                source=file_path,
                reader_type="ExcelReader",
                original_error=te,
            )
        logger.debug(f"Using mapping for statement type '{statement_type}': {mapping}")

        # --- Read Excel Data ---
        try:
            # Read the sheet, potentially skipping rows and limiting rows read
            # Use header_row_0idx to correctly identify column headers
            df = pd.read_excel(
                file_path,
                sheet_name=sheet_name,
                header=header_row_0idx,
                skiprows=skiprows,  # skiprows is applied *before* header selection
                nrows=nrows,
            )

            # Identify the actual period columns based on the periods_row
            # Read the periods row separately if header is different
            if header_row_0idx != periods_row_0idx:
                periods_df = pd.read_excel(
                    file_path,
                    sheet_name=sheet_name,
                    header=None,
                    skiprows=periods_row_0idx,
                    nrows=1,
                )
                period_headers = periods_df.iloc[0].astype(str).tolist()
            else:
                # Periods are in the main header row read by pandas
                period_headers = df.columns.astype(str).tolist()

            # Find the item column name from the initial read
            if items_col_0idx >= len(df.columns):
                raise ReadError(
                    f"items_col index ({items_col_0idx + 1}) is out of bounds for sheet '{sheet_name}'. Found columns: {df.columns.tolist()}",
                    source=file_path,
                    reader_type="ExcelReader",
                )
            # item_column_name = df.columns[items_col_0idx] # This variable is assigned but never used.

            # Filter period headers: exclude the item column header itself
            # Assuming periods start *after* the item column typically
            graph_periods = [p for i, p in enumerate(period_headers) if i > items_col_0idx and p]
            if not graph_periods:
                raise ReadError(
                    f"Could not identify period columns in row {periods_row_0idx + 1} after column {items_col_0idx + 1} in sheet '{sheet_name}'. Headers found: {period_headers}",
                    source=file_path,
                    reader_type="ExcelReader",
                )
            logger.info(f"Identified periods: {graph_periods}")

            # Create graph
            graph = Graph(periods=graph_periods)

            # --- Populate Graph ---
            validation_errors = []
            nodes_added = 0
            for index, row in df.iterrows():
                item_name_excel = row.iloc[
                    items_col_0idx
                ]  # Get item name using the identified column
                if pd.isna(item_name_excel) or not item_name_excel:
                    # logger.debug(f"Skipping row {index + (skiprows or 0) + (header_row_0idx or 0) + 1}: Empty item name.")
                    continue

                item_name_excel = str(item_name_excel).strip()
                node_name = mapping.get(
                    item_name_excel, item_name_excel
                )  # Use mapping or fallback to original name

                # Get values for the identified periods
                period_values: dict[str, float] = {}
                for period in graph_periods:
                    try:
                        # Find the corresponding column in the DataFrame using the period header
                        # This assumes the period headers read initially match the df columns
                        if period in df.columns:
                            value = row[period]
                            if pd.isna(value):
                                # Keep NaN or skip? For now, skip. Could represent as None or NaN later.
                                # logger.debug(f"NaN value for {node_name} period {period}")
                                continue
                            elif isinstance(value, (int, float)):
                                period_values[period] = float(value)
                            else:
                                # Attempt conversion, log warning if fails
                                try:
                                    period_values[period] = float(value)
                                    logger.warning(
                                        f"Converted non-numeric value '{value}' to float for node '{node_name}' period '{period}'"
                                    )
                                except (ValueError, TypeError):
                                    validation_errors.append(
                                        f"Row {index}: Non-numeric value '{value}' for node '{node_name}' (from '{item_name_excel}') period '{period}'"
                                    )
                        else:
                            # This shouldn't happen if period_headers came from df.columns
                            logger.warning(
                                f"Period header '{period}' not found in DataFrame columns for row {index}."
                            )
                    except KeyError:
                        validation_errors.append(
                            f"Row {index}: Column for period '{period}' not found for node '{node_name}'"
                        )
                    except Exception as e:
                        validation_errors.append(
                            f"Row {index}: Error processing value for node '{node_name}' period '{period}': {e}"
                        )

                if period_values:
                    if graph.has_node(node_name):
                        logger.warning(
                            f"Node '{node_name}' (from Excel item '{item_name_excel}') already exists. Overwriting data is not standard for readers. Consider unique names or merging logic."
                        )
                        # Get existing node and update? Or raise error? For now, log warning.
                        # existing_node = graph.get_node(node_name)
                        # if isinstance(existing_node, FinancialStatementItemNode):
                        #     existing_node.values.update(period_values)
                    else:
                        # Create and add new node
                        new_node = FinancialStatementItemNode(name=node_name, values=period_values)
                        graph.add_node(new_node)
                        nodes_added += 1
                # else: No valid values for this item in the specified periods

            if validation_errors:
                raise ReadError(
                    f"Validation errors occurred while reading {file_path} sheet '{sheet_name}': {'; '.join(validation_errors)}",
                    source=file_path,
                    reader_type="ExcelReader",
                )

            logger.info(
                f"Successfully created graph with {nodes_added} nodes from {file_path} sheet '{sheet_name}'."
            )
            return graph

        except FileNotFoundError:
            raise ReadError(
                f"File not found: {file_path}",
                source=file_path,
                reader_type="ExcelReader",
            )
        except ValueError as ve:
            # Pandas raises ValueError for bad sheet names etc.
            raise ReadError(
                f"Error reading Excel file: {ve}",
                source=file_path,
                reader_type="ExcelReader",
                original_error=ve,
            )
        except KeyError as ke:
            # Raised if essential columns (like item column after mapping) are missing
            raise ReadError(
                f"Missing expected column/item: {ke}. Check items_col ({items_col_0idx + 1}) and sheet structure.",
                source=file_path,
                reader_type="ExcelReader",
                original_error=ke,
            )
        except Exception as e:
            logger.error(
                f"Failed to read Excel file {file_path} sheet '{sheet_name}': {e}",
                exc_info=True,
            )
            raise ReadError(
                message=f"Failed to process Excel file: {e}",
                source=file_path,
                reader_type="ExcelReader",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/io/readers/excel.py ---

# --- START FILE: fin_statement_model/io/readers/fmp.py ---
"""Data reader for the Financial Modeling Prep (FMP) API."""

import logging
import requests
from typing import Optional, ClassVar, Any
import numpy as np
import yaml
import importlib.resources

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataReader
from fin_statement_model.io.registry import register_reader
from fin_statement_model.io.exceptions import ReadError
from fin_statement_model.io.readers.base import normalize_mapping
from fin_statement_model.io.config.models import FmpReaderConfig

logger = logging.getLogger(__name__)


@register_reader("fmp")
class FmpReader(DataReader):
    """Reads financial statement data from the FMP API into a Graph.

    Fetches data for a specific ticker and statement type.
    Requires an API key, either passed directly or via the FMP_API_KEY env var.

    Supports a `mapping_config` constructor parameter for mapping API field names to canonical node names,
    accepting either a flat mapping or a statement-type keyed mapping.

    Configuration (api_key, statement_type, period_type, limit, mapping_config)
    is passed via an `FmpReaderConfig` object during initialization (typically by
    the `read_data` facade). The `.read()` method currently takes no specific
    keyword arguments beyond the `source` (ticker).

    Stateful Use:
        For advanced use cases involving repeated API calls, consider instantiating
        and reusing a single `FmpReader` instance to avoid redundant API key
        validations and improve performance.
    """

    BASE_URL = "https://financialmodelingprep.com/api/v3"

    # Load default mappings from YAML configuration
    DEFAULT_MAPPINGS: ClassVar[dict[str, dict[str, str]]] = {}

    @classmethod
    def _load_default_mappings(cls) -> None:
        """Load default mapping configurations from YAML file into DEFAULT_MAPPINGS."""
        # Load mapping YAML from config directory relative to this file
        try:
            # Use importlib.resources for robust package data loading
            yaml_content = importlib.resources.files("fin_statement_model.io.readers.config").joinpath("fmp_default_mappings.yaml").read_text(encoding="utf-8")
            cls.DEFAULT_MAPPINGS = yaml.safe_load(yaml_content)
        except FileNotFoundError:
            logger.error("Default FMP mapping file not found.", exc_info=True)
            # Keep DEFAULT_MAPPINGS as empty dict if file is missing
        except Exception as e:
            logger.error(f"Error loading default FMP mapping file: {e}", exc_info=True)
            # Keep DEFAULT_MAPPINGS as empty dict on other errors

    def __init__(self, cfg: FmpReaderConfig) -> None:
        """Initialize the FmpReader with validated configuration.

        Args:
            cfg: A validated `FmpReaderConfig` instance containing parameters like
                 `source` (ticker), `api_key`, `statement_type`, `period_type`,
                 `limit`, and `mapping_config`.
        """
        self.cfg = cfg

    def _get_mapping(
        self,
        statement_type: Optional[str],
    ) -> dict[str, str]:
        """Get the appropriate mapping based on statement type and the stored config."""
        # Start with defaults based on statement type loaded from config
        mapping = dict(self.DEFAULT_MAPPINGS.get(statement_type, {}))

        # Use mapping config from the validated Pydantic config object
        config = self.cfg.mapping_config
        # Normalize and overlay user-provided mappings
        user_map = normalize_mapping(config, context_key=statement_type)
        mapping.update(user_map)
        return mapping

    def _validate_api_key(self):
        """Perform a simple check if the API key seems valid."""
        # API key is now guaranteed by FmpReaderConfig validation
        api_key = self.cfg.api_key
        if not api_key: # Should not happen if validation passed, but defensive check
            raise ReadError(
                "FMP API key is required for reading.",
                source="FMP API",
                reader_type="FmpReader",
            )
        try:
            # Use a cheap endpoint for validation
            test_url = f"{self.BASE_URL}/profile/AAPL?apikey={api_key}"  # Example
            response = requests.get(test_url, timeout=10)
            response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
            # Basic check on response content if needed
            if not response.json():
                raise ReadError(
                    "API key validation returned empty response.",
                    source="FMP API",
                    reader_type="FmpReader",
                )
            logger.debug("FMP API key validated successfully.")
        except requests.exceptions.RequestException as e:
            logger.error(f"FMP API key validation failed: {e}", exc_info=True)
            raise ReadError(
                f"FMP API key validation failed: {e}",
                source="FMP API",
                reader_type="FmpReader",
                original_error=e,
            )

    def read(self, source: str, **kwargs: dict[str, Any]) -> Graph:
        """Fetch data from FMP API and return a Graph.

        Args:
            source (str): The stock ticker symbol (e.g., "AAPL").
            **kwargs: Currently unused. Configuration is handled by the `FmpReaderConfig`
                      object passed during initialization.

        Returns:
            A new Graph instance populated with FinancialStatementItemNodes.

        Raises:
            ReadError: If API key is missing/invalid, API request fails, or data format is unexpected.
        """
        ticker = source
        # Parameters now come directly from the validated config
        statement_type = self.cfg.statement_type
        period_type_arg = self.cfg.period_type
        limit = self.cfg.limit
        api_key = self.cfg.api_key

        # --- Validate Inputs ---
        if not ticker or not isinstance(ticker, str):
            raise ReadError(
                "Invalid source (ticker) provided. Expected a non-empty string.",
                source=ticker,
                reader_type="FmpReader",
            )
        # statement_type and period_type are validated by FmpReaderConfig

        self._validate_api_key()  # Ensure API key is usable

        # Determine mapping for this operation, allowing override via kwargs
        # Mapping is now determined solely by the config passed during __init__
        try:
            mapping = self._get_mapping(statement_type)
        except TypeError as te:
            raise ReadError(
                "Invalid mapping_config provided.",
                source=ticker,
                reader_type="FmpReader",
                original_error=te,
            )
        logger.debug(f"Using mapping for {ticker} {statement_type}: {mapping}")

        # --- Fetch API Data ---
        # Correct endpoint construction based on FMP v3 docs
        # e.g., /income-statement/AAPL, not /income_statement-statement/AAPL
        endpoint_path = statement_type.replace("_", "-")
        endpoint = f"{self.BASE_URL}/{endpoint_path}/{ticker}"
        params = {"apikey": api_key, "limit": limit}
        if period_type_arg == "QTR":
            params["period"] = "quarter"

        try:
            logger.info(
                f"Fetching {period_type_arg} {statement_type} for {ticker} from FMP API (limit={limit})."
            )
            response = requests.get(endpoint, params=params, timeout=30)  # Increased timeout
            response.raise_for_status()  # Check for HTTP errors
            api_data = response.json()

            if not isinstance(api_data, list):
                raise ReadError(
                    f"Unexpected API response format. Expected list, got {type(api_data)}. Response: {str(api_data)[:100]}...",
                    source=f"FMP API ({ticker})",
                    reader_type="FmpReader",
                )
            if not api_data:
                logger.warning(f"FMP API returned empty list for {ticker} {statement_type}.")
                # Return empty graph or raise? Returning empty for now.
                return Graph(periods=[])

        except requests.exceptions.RequestException as e:
            logger.error(
                f"FMP API request failed for {ticker} {statement_type}: {e}",
                exc_info=True,
            )
            raise ReadError(
                f"FMP API request failed: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            )
        except Exception as e:
            logger.error(f"Failed to process FMP API response: {e}", exc_info=True)
            raise ReadError(
                f"Failed to process FMP API response: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            )

        # --- Process Data and Populate Graph ---
        try:
            # FMP data is usually newest first, reverse to process chronologically
            api_data.reverse()

            # Extract periods (e.g., 'date' or 'fillingDate')
            # Using 'date' as it usually represents the period end date
            periods = [item.get("date") for item in api_data if item.get("date")]
            if not periods:
                raise ReadError(
                    "Could not extract periods ('date' field) from FMP API response.",
                    source=f"FMP API ({ticker})",
                    reader_type="FmpReader",
                )

            graph = Graph(periods=periods)
            all_item_data: dict[str, dict[str, float]] = {}

            # Collect data for all items across all periods
            for period_data in api_data:
                period = period_data.get("date")
                if not period:
                    continue  # Skip records without a date

                for api_field, value in period_data.items():
                    node_name = mapping.get(api_field, api_field)  # Use mapping or fallback

                    # Initialize node data dict if first time seeing this node
                    if node_name not in all_item_data:
                        all_item_data[node_name] = {p: np.nan for p in periods}  # Pre-fill with NaN

                    # Store value for this period
                    if isinstance(value, (int, float)):
                        all_item_data[node_name][period] = float(value)

            # Create nodes from collected data
            nodes_added = 0
            for node_name, period_values in all_item_data.items():
                # Filter out periods that only have NaN
                valid_period_values = {p: v for p, v in period_values.items() if not np.isnan(v)}
                if valid_period_values:
                    new_node = FinancialStatementItemNode(
                        name=node_name, values=valid_period_values
                    )
                    graph.add_node(new_node)
                    nodes_added += 1

            logger.info(
                f"Successfully created graph with {nodes_added} nodes from FMP API for {ticker} {statement_type}."
            )
            return graph

        except Exception as e:
            logger.error(f"Failed to parse FMP data and build graph: {e}", exc_info=True)
            raise ReadError(
                message=f"Failed to parse FMP data: {e}",
                source=f"FMP API ({ticker})",
                reader_type="FmpReader",
                original_error=e,
            ) from e

# After class definition, load default mappings
FmpReader._load_default_mappings()

# --- END FILE: fin_statement_model/io/readers/fmp.py ---

# --- START FILE: fin_statement_model/io/readers/statement_config_reader.py ---
"""Reader functions for statement configuration files.

Handles locating, reading, and parsing JSON/YAML configuration files
into raw Python dictionaries.
"""

import json
import yaml
import logging
import importlib.resources
import importlib.util # Needed for checking resource type
from pathlib import Path
from typing import Any, Optional

from fin_statement_model.io.exceptions import ReadError

logger = logging.getLogger(__name__)

__all__ = [
    "list_available_builtin_configs",
    "read_builtin_statement_config",
    "read_statement_config_from_path",
    "read_statement_configs_from_directory",
]

def read_statement_config_from_path(config_path: str) -> dict[str, Any]:
    """Reads and parses a statement configuration file from a given path.

    Supports JSON and YAML formats.

    Args:
        config_path: Absolute or relative path to the configuration file.

    Returns:
        The parsed configuration data as a dictionary.

    Raises:
        ReadError: If the file is not found, has an unsupported extension,
                   or cannot be parsed.
    """
    path = Path(config_path)

    if not path.exists() or not path.is_file():
        raise ReadError(
            message="Configuration file not found or is not a file",
            target=config_path,
        )

    extension = path.suffix.lower()
    config_data = {}

    try:
        with open(path, encoding="utf-8") as f:
            if extension == ".json":
                config_data = json.load(f)
            elif extension in [".yaml", ".yml"]:
                config_data = yaml.safe_load(f)
            else:
                raise ReadError(
                    message="Unsupported file extension for statement config",
                    target=config_path,
                    details=f"Use .json, .yaml, or .yml instead of {extension}",
                )
        logger.debug(f"Successfully read and parsed config file: {config_path}")
        return config_data

    except json.JSONDecodeError as e:
        logger.exception(f"Error parsing JSON configuration file {config_path}")
        raise ReadError(
            message="Invalid JSON format in configuration file",
            target=config_path,
            original_error=e,
            details=f"JSON decode error at line {e.lineno}, column {e.colno}: {e.msg}",
        ) from e
    except yaml.YAMLError as e:
        logger.exception(f"Error parsing YAML configuration file {config_path}")
        details = str(e)
        if hasattr(e, "problem_mark"):
            mark = e.problem_mark
            if mark:
                details = (
                    f"YAML parse error near line {mark.line + 1}, column {mark.column + 1}: "
                    f"{getattr(e, 'problem', '')}"
                )
        raise ReadError(
            message="Invalid YAML format in configuration file",
            target=config_path,
            original_error=e,
            details=details,
        ) from e
    except OSError as e:
        logger.exception(f"IO Error reading configuration file {config_path}")
        raise ReadError(
            message="Failed to read configuration file",
            target=config_path,
            original_error=e,
        ) from e
    except Exception as e:
        logger.exception(f"Unexpected error loading configuration from {config_path}")
        raise ReadError(
            message="Unexpected error loading configuration file",
            target=config_path,
            original_error=e,
        ) from e


def read_statement_configs_from_directory(directory_path: str) -> dict[str, dict[str, Any]]:
    """Reads all statement configs (JSON/YAML) from a directory.

    Args:
        directory_path: Path to the directory containing configuration files.

    Returns:
        A dictionary mapping statement identifiers (filename stem) to their
        parsed configuration data (dict).

    Raises:
        ReadError: If the directory doesn't exist or isn't accessible, or if
                   any individual file fails to read/parse (errors are logged
                   but reading continues for other files unless none succeed).
        FileNotFoundError: If the directory_path does not exist.
    """
    path = Path(directory_path)
    if not path.exists():
        raise FileNotFoundError(f"Configuration directory not found: {directory_path}")
    if not path.is_dir():
        raise ReadError(
            message="Provided path is not a directory",
            target=directory_path,
        )

    configs: dict[str, dict[str, Any]] = {}
    errors: list[str] = []

    config_files = list(path.glob("*.json")) + list(path.glob("*.y*ml"))

    if not config_files:
        logger.warning(f"No configuration files (.json, .yaml, .yml) found in {directory_path}")
        return {}

    for file_path in config_files:
        file_path_str = str(file_path)
        try:
            config_data = read_statement_config_from_path(file_path_str)
            statement_id = file_path.stem # Use filename without extension as ID
            if statement_id in configs:
                logger.warning(
                    f"Duplicate statement ID '{statement_id}' found. Overwriting config from {file_path_str}."
                )
            configs[statement_id] = config_data
            logger.debug(f"Successfully loaded statement config '{statement_id}' from {file_path.name}")
        except ReadError as e:
            logger.exception(f"Failed to read/parse config file {file_path.name}:")
            errors.append(f"{file_path.name}: {e.message}")
        except Exception as e:
            logger.exception(f"Unexpected error processing file {file_path.name}")
            errors.append(f"{file_path.name}: Unexpected error - {e!s}")

    # Decision: Raise error only if NO files could be read successfully?
    # Or just log errors and return what was successful?
    # Current approach: Log errors, return successful ones.
    # If no configs loaded AND errors occurred, maybe raise an aggregate error.
    if not configs and errors:
        raise ReadError(
            message=f"Failed to load any valid configurations from directory {directory_path}",
            target=directory_path,
            details="\n".join(errors)
        )
    elif errors:
        # Log that some files failed if others succeeded
        logger.warning(f"Encountered errors while loading configs from {directory_path}: {len(errors)} file(s) failed.")

    return configs


def _get_builtin_config_package() -> str:
    """Return the package path string for built-in statement configurations.

    Direct return, assuming a fixed location within the package structure.
    Environment variable override is removed as importlib.resources relies on package structure.
    """
    return "fin_statement_model.statements.config.mappings"

def list_available_builtin_configs() -> list[str]:
    """List the names of all built-in statement configuration mappings available.

    Returns:
        List[str]: List of mapping names (filename without extension).
    """
    package_path = _get_builtin_config_package()
    try:
        resource_path = importlib.resources.files(package_path)
        # Check if the resource exists and is a container (directory)
        if not resource_path.is_dir():
            logger.warning(f"Built-in config package path is not a directory: {package_path}")
            return []

        names = [
            res.name.split('.')[0] # Get filename stem
            for res in resource_path.iterdir()
            if res.is_file() and res.suffix.lower() in (".yaml", ".yml", ".json")
        ]
        return sorted(names)
    except (ModuleNotFoundError, FileNotFoundError): # Handle case where package/path doesn't exist
        logger.warning(f"Built-in config package path not found: {package_path}")
        return []

def read_builtin_statement_config(name: str) -> dict[str, Any]:
    """Reads and parses a built-in statement configuration by name.

    Searches for <name>.yaml, <name>.yml, or <name>.json in the built-in
    mappings directory.

    Args:
        name: The name of the built-in configuration (filename without extension).

    Returns:
        The parsed configuration data as a dictionary.

    Raises:
        ReadError: If no matching configuration file is found or if reading/parsing fails.
    """
    package_path = _get_builtin_config_package()
    found_resource_name: Optional[str] = None
    resource_content: Optional[str] = None
    file_extension: Optional[str] = None

    for ext in (".yaml", ".yml", ".json"):
        resource_name = f"{name}{ext}"
        try:
            resource_path = importlib.resources.files(package_path).joinpath(resource_name)
            if resource_path.is_file():
                resource_content = resource_path.read_text(encoding="utf-8")
                found_resource_name = resource_name
                file_extension = ext
                break
        except (FileNotFoundError, ModuleNotFoundError):
            continue # Try next extension or handle package not found below
        except Exception as e:
            # Catch other potential errors during resource access
            logger.exception(f"Error accessing resource {resource_name} in {package_path}")
            raise ReadError(
                message=f"Error accessing built-in config resource '{name}'",
                target=f"{package_path}/{resource_name}",
                original_error=e,
            ) from e

    if resource_content is not None and found_resource_name is not None and file_extension is not None:
        logger.debug(f"Found and read built-in config '{name}' from resource: {package_path}/{found_resource_name}")
        try:
            if file_extension == ".json":
                config_data = json.loads(resource_content)
            else: # .yaml or .yml
                config_data = yaml.safe_load(resource_content)
            return config_data
        except json.JSONDecodeError as e:
            logger.exception(f"Error parsing JSON for built-in config '{name}'")
            raise ReadError(
                message="Invalid JSON format in built-in configuration",
                target=f"{package_path}/{found_resource_name}",
                original_error=e,
            ) from e
        except yaml.YAMLError as e:
            logger.exception(f"Error parsing YAML for built-in config '{name}'")
            raise ReadError(
                message="Invalid YAML format in built-in configuration",
                target=f"{package_path}/{found_resource_name}",
                original_error=e,
            ) from e
    else:
        logger.warning(f"Built-in statement config '{name}' not found in package {package_path}")
        raise ReadError(
            message=f"Built-in statement config '{name}' not found",
            target=package_path,
            details=f"No resource found for '{name}' with .yaml, .yml, or .json extension.",
        )

# --- END FILE: fin_statement_model/io/readers/statement_config_reader.py ---

# --- START FILE: fin_statement_model/io/registry.py ---
"""Registry for readers and writers."""

import logging
from typing import Any, Callable, TypeVar

from .base import DataReader, DataWriter
from .exceptions import FormatNotSupportedError, ReadError, WriteError
from pydantic import ValidationError
from fin_statement_model.io.config.models import (
    CsvReaderConfig,
    ExcelReaderConfig,
    FmpReaderConfig,
    DataFrameReaderConfig,
    DictReaderConfig,
    ExcelWriterConfig,
    DataFrameWriterConfig,
    DictWriterConfig,
    MarkdownWriterConfig,
)

logger = logging.getLogger(__name__)

# --- Registry Dicts and Decorators ---

# Type variables for generic registration decorators
R = TypeVar("R", bound=DataReader)
W = TypeVar("W", bound=DataWriter)

# Internal registries
_readers: dict[str, type[DataReader]] = {}
_writers: dict[str, type[DataWriter]] = {}


def register_reader(format_type: str) -> Callable[[type[R]], type[R]]:
    """Decorator to register a DataReader class for a specific format type.

    Args:
        format_type: The string identifier for the format (e.g., 'excel', 'csv').

    Returns:
        A decorator function that registers the class and returns it unmodified.

    Raises:
        ValueError: If the format_type is already registered for a reader.
    """

    def decorator(cls: type[R]) -> type[R]:
        if format_type in _readers:
            # Allow re-registration if it's the exact same class (e.g., during reload)
            if _readers[format_type] is not cls:
                raise ValueError(
                    f"Reader format type '{format_type}' already registered to {_readers[format_type]}."
                )
            # If same class, just log and allow (idempotent registration)
            logger.debug(f"Re-registering reader format type '{format_type}' to {cls.__name__}")
        else:
            logger.debug(f"Registering reader format type '{format_type}' to {cls.__name__}")
        _readers[format_type] = cls
        return cls

    return decorator


def register_writer(format_type: str) -> Callable[[type[W]], type[W]]:
    """Decorator to register a DataWriter class for a specific format type.

    Args:
        format_type: The string identifier for the format (e.g., 'excel', 'json').

    Returns:
        A decorator function that registers the class and returns it unmodified.

    Raises:
        ValueError: If the format_type is already registered for a writer.
    """

    def decorator(cls: type[W]) -> type[W]:
        if format_type in _writers:
            # Allow re-registration if it's the exact same class
            if _writers[format_type] is not cls:
                raise ValueError(
                    f"Writer format type '{format_type}' already registered to {_writers[format_type]}."
                )
            logger.debug(f"Re-registering writer format type '{format_type}' to {cls.__name__}")
        else:
            logger.debug(f"Registering writer format type '{format_type}' to {cls.__name__}")
        _writers[format_type] = cls
        return cls

    return decorator


# --- Registry Access Functions ---

def get_reader(format_type: str, **kwargs: Any) -> DataReader:
    """Get an instance of the registered DataReader for the given format type.

    Args:
        format_type: The string identifier for the format.
        **kwargs: Keyword arguments to pass to the reader's constructor.

    Returns:
        An initialized DataReader instance.

    Raises:
        FormatNotSupportedError: If no reader is registered for the format type.
        ReadError: If validation fails for known reader types.
    """
    if format_type not in _readers:
        raise FormatNotSupportedError(format_type=format_type, operation="read")

    reader_class = _readers[format_type]
    # Validate config for known reader types
    schema_map = {
        "csv": CsvReaderConfig,
        "excel": ExcelReaderConfig,
        "fmp": FmpReaderConfig,
        "dataframe": DataFrameReaderConfig,
        "dict": DictReaderConfig,
    }
    schema = schema_map.get(format_type)
    if schema:
        try:
            cfg = schema.model_validate({**kwargs, "format_type": format_type})
        except ValidationError as ve:
            # Map other pydantic errors to ReadError
            raise ReadError(
                message="Invalid reader configuration",
                source=kwargs.get("source"),
                reader_type=format_type,
                original_error=ve,
            ) from ve
        # Instantiate reader with config object
        try:
            return reader_class(cfg)
        except Exception as e:
            logger.error(
                f"Failed to instantiate reader for format '{format_type}' ({reader_class.__name__}): {e}",
                exc_info=True,
            )
            raise ReadError(
                message="Failed to initialize reader",
                source=kwargs.get("source"),
                reader_type=format_type,
                original_error=e,
            ) from e
    # Fallback for legacy readers without config schema
    try:
        return reader_class(**kwargs)
    except Exception as e:
        logger.error(
            f"Failed to instantiate reader for format '{format_type}' ({reader_class.__name__}): {e}",
            exc_info=True,
        )
        raise ReadError(
            message="Failed to initialize reader",
            source=kwargs.get("source"),
            reader_type=format_type,
            original_error=e,
        ) from e


def get_writer(format_type: str, **kwargs: Any) -> DataWriter:
    """Get an instance of the registered DataWriter for the given format type.

    Args:
        format_type: The string identifier for the format.
        **kwargs: Keyword arguments to pass to the writer's constructor.

    Returns:
        An initialized DataWriter instance.

    Raises:
        FormatNotSupportedError: If no writer is registered for the format type.
        WriteError: If validation fails for known writer types.
    """
    if format_type not in _writers:
        raise FormatNotSupportedError(format_type=format_type, operation="write")

    writer_class = _writers[format_type]
    # Map writer types to their config models
    writer_schema_map = {
        "excel": ExcelWriterConfig,
        "dataframe": DataFrameWriterConfig,
        "dict": DictWriterConfig,
        "markdown": MarkdownWriterConfig,
    }

    schema_cls = writer_schema_map.get(format_type)

    if schema_cls:
        try:
            cfg = schema_cls.model_validate({**kwargs, "format_type": format_type})
        except ValidationError as ve:
            raise WriteError(
                message="Invalid writer configuration",
                target=kwargs.get("target"),
                writer_type=format_type,
                original_error=ve,
            ) from ve
        try:
            return writer_class(cfg)
        except Exception as e:
            logger.error(
                f"Failed to instantiate writer for format '{format_type}': {e}",
                exc_info=True,
            )
            raise WriteError(
                message="Failed to initialize writer",
                target=kwargs.get("target"),
                writer_type=format_type,
                original_error=e,
            ) from e
    # Fallback if no schema defined
    try:
        return writer_class(**kwargs)
    except Exception as e:
        logger.error(
            f"Failed to instantiate writer for format '{format_type}': {e}",
            exc_info=True,
        )
        raise WriteError(
            message="Failed to initialize writer",
            target=kwargs.get("target"),
            writer_type=format_type,
            original_error=e,
        ) from e


def list_readers() -> dict[str, type[DataReader]]:
    """Return a copy of the registered reader classes."""
    return _readers.copy()


def list_writers() -> dict[str, type[DataWriter]]:
    """Return a copy of the registered writer classes."""
    return _writers.copy()

__all__ = [
    "get_reader",
    "get_writer",
    "list_readers",
    "list_writers",
    "register_reader",
    "register_writer",
]

# --- END FILE: fin_statement_model/io/registry.py ---

# --- START FILE: fin_statement_model/io/writers/__init__.py ---
"""Data writers for various formats."""

# Import specific writers to ensure they are registered or expose functions
from . import dict  # noqa: F401
from . import excel  # noqa: F401
from . import dataframe  # noqa: F401
from . import markdown_writer # noqa: F401 # Added import for markdown writer
# Only import functions from statement_writer
from .statement_writer import write_statement_to_excel, write_statement_to_json
from .excel import ExcelWriter
# from .statement_writer import StatementWriter # REMOVED THIS LINE
from .dataframe import DataFrameWriter
from .dict import DictWriter
from .markdown_writer import MarkdownWriter # Added import for markdown writer class

# Note: No CsvWriter was identified/created

__all__ = [
    # Expose writer classes if needed directly
    # "DataFrameWriter",
    # "DictWriter",
    # "ExcelWriter",
    # Expose functions directly
    "write_statement_to_excel",
    "write_statement_to_json",
    # Expose classes
    ExcelWriter,
    # StatementWriter, # REMOVED THIS LINE
    DataFrameWriter,
    DictWriter,
    MarkdownWriter, # Added markdown writer to __all__
    # Explicit exports if using wildcard import
    # "DictWriter",
    # "ExcelWriter",
    # "write_statement_to_excel",
    # "write_statement_to_json",
]

# --- END FILE: fin_statement_model/io/writers/__init__.py ---

# --- START FILE: fin_statement_model/io/writers/dataframe.py ---
"""Data writer for pandas DataFrames."""

import logging
from typing import Optional, Any

import pandas as pd
import numpy as np

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.base import DataWriter
from fin_statement_model.io.registry import register_writer
from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.io.config.models import DataFrameWriterConfig

logger = logging.getLogger(__name__)


@register_writer("dataframe")
class DataFrameWriter(DataWriter):
    """Writes graph data to a pandas DataFrame.

    Converts the graph to a DataFrame with node names as index and periods as columns.

    Configuration options `recalculate` and `include_nodes` are controlled *exclusively*
    by the `DataFrameWriterConfig` object passed during initialization (typically
    via the `write_data` facade). The `.write()` method does not accept keyword
    arguments to override these settings.
    """

    def __init__(self, cfg: Optional[DataFrameWriterConfig] = None) -> None:
        """Initialize the DataFrameWriter.

        Args:
            cfg: Optional validated `DataFrameWriterConfig` instance.
        """
        self.cfg = cfg

    def write(
        self, graph: Graph, target: Any = None, **kwargs: dict[str, object]
    ) -> pd.DataFrame:
        """Convert the graph data to a pandas DataFrame based on instance configuration.

        Args:
            graph (Graph): The Graph instance to export.
            target (Any): Ignored by this writer; the DataFrame is returned directly.
            **kwargs: Currently unused by this method.

        Returns:
            pd.DataFrame: DataFrame with node names as index and periods as columns.

        Raises:
            WriteError: If an error occurs during conversion.
            AssertionError: If the writer was not initialized with a configuration.
        """
        assert self.cfg is not None, "DataFrameWriter must be initialized with a valid configuration."
        # Use configuration directly from self.cfg
        recalculate = self.cfg.recalculate
        include_nodes = self.cfg.include_nodes

        logger.info("Exporting graph to DataFrame format.")

        try:
            if recalculate:
                try:
                    if graph.periods:
                        graph.recalculate_all(periods=graph.periods)
                        logger.info("Recalculated graph before exporting to DataFrame.")
                    else:
                        logger.warning("Graph has no periods defined, skipping recalculation.")
                except Exception as e:
                    logger.error(
                        f"Error during recalculation for DataFrame export: {e}",
                        exc_info=True,
                    )
                    logger.warning(
                        "Proceeding to export DataFrame without successful recalculation."
                    )

            periods = sorted(graph.periods) if graph.periods else []
            data: dict[str, dict[str, float]] = {}

            nodes_to_process = include_nodes if include_nodes else graph.nodes.keys()
            if include_nodes:
                missing_nodes = [n for n in include_nodes if n not in graph.nodes]
                if missing_nodes:
                    logger.warning(
                        f"Nodes specified in include_nodes not found in graph: {missing_nodes}"
                    )
                nodes_to_process = [n for n in include_nodes if n in graph.nodes]

            for node_id in nodes_to_process:
                node = graph.nodes[node_id]
                row: dict[str, float] = {}
                for period in periods:
                    value = np.nan
                    try:
                        if hasattr(node, "calculate") and callable(node.calculate):
                            value = node.calculate(period)
                        elif (
                            hasattr(node, "values")
                            and isinstance(node.values, dict)
                            and period in node.values
                        ):
                            value = node.values.get(period, np.nan)

                        if not isinstance(value, (int, float, np.number)) or not np.isfinite(value):
                            value = np.nan
                    except Exception as e:
                        logger.debug(
                            f"Could not get value for node '{node_id}' period '{period}' for DataFrame export: {e}"
                        )
                        value = np.nan
                    row[period] = float(value)
                data[node_id] = row

            df = pd.DataFrame.from_dict(data, orient="index", columns=periods)
            df.index.name = "node_name"

            logger.info(f"Successfully exported {len(df)} nodes to DataFrame.")
        except Exception as e:
            logger.error(f"Failed to export graph to DataFrame: {e}", exc_info=True)
            raise WriteError(
                message=f"Failed to export graph to DataFrame: {e}",
                target="DataFrame",
                writer_type="DataFrameWriter",
                original_error=e,
            ) from e
        else:
            return df

# --- END FILE: fin_statement_model/io/writers/dataframe.py ---

# --- START FILE: fin_statement_model/io/writers/dict.py ---
"""Data writer for Python dictionaries."""

import logging
from typing import Any, Optional
import numpy as np

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import CalculationError
# Remove specific node import, we handle all nodes now
# from fin_statement_model.core.nodes import FinancialStatementItemNode
from fin_statement_model.io.base import DataWriter
from fin_statement_model.io.registry import register_writer
from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.io.config.models import DictWriterConfig

logger = logging.getLogger(__name__)


@register_writer("dict")
class DictWriter(DataWriter):
    """Writes graph data to a Python dictionary.

    Calculates the value for each node and period in the graph using
    `graph.calculate()` before exporting.

    Initialized via `DictWriterConfig` (typically by the `write_data` facade),
    although the config currently has no options. The `.write()` method takes no
    specific keyword arguments.
    """

    def __init__(self, cfg: Optional[DictWriterConfig] = None) -> None:
        """Initialize the DictWriter.

        Args:
            cfg: Optional validated `DictWriterConfig` instance.
                 Currently unused but kept for registry symmetry.
        """
        self.cfg = cfg

    def write(
        self, graph: Graph, target: Any = None, **kwargs: dict[str, Any]
    ) -> dict[str, dict[str, float]]:
        """Export calculated data from all graph nodes to a dictionary.

        Args:
            graph (Graph): The Graph instance to export data from. It's recommended
                           to ensure the graph is calculated beforehand if needed,
                           as this writer calculates values per node/period.
            target (Any): Ignored by this writer; the dictionary is returned directly.
            **kwargs: Currently unused.

        Returns:
            Dict[str, Dict[str, float]]: Mapping node names to period-value dicts.
                                         Includes values for all nodes in the graph
                                         for all defined periods. NaN represents
                                         uncalculable values.

        Raises:
            WriteError: If an unexpected error occurs during export.
        """
        logger.info(f"Starting export of graph '{graph}' to dictionary format.")
        result: dict[str, dict[str, float]] = {}
        periods = sorted(graph.periods) if graph.periods else []
        if not periods:
            logger.warning("Graph has no periods defined. Exported dictionary will be empty.")
            return {}

        try:
            for node_id in graph.nodes:
                node_values: dict[str, float] = {}
                for period in periods:
                    value = np.nan
                    try:
                        # Use graph.calculate to get the value for the specific node and period
                        calculated_value = graph.calculate(node_id, period=period)
                        if isinstance(calculated_value, (int, float, np.number)) and np.isfinite(calculated_value):
                            value = float(calculated_value)
                        else:
                            # Handle cases where calculation returns non-numeric or infinite results
                            logger.debug(
                                f"Calculation for node '{node_id}' period '{period}' "
                                f"yielded non-finite/non-numeric result: {calculated_value}. Using NaN."
                            )
                    except CalculationError as calc_err:
                        logger.debug(
                            f"Calculation failed for node '{node_id}' period '{period}': {calc_err}. Using NaN."
                        )
                    except Exception as e:
                        # Catch unexpected errors during calculation for a specific period
                        logger.warning(
                            f"Unexpected error calculating node '{node_id}' period '{period}': {e}. Using NaN.",
                            exc_info=True
                        )
                    node_values[period] = value
                result[node_id] = node_values

            logger.info(f"Successfully exported {len(result)} nodes to dictionary.")
        except Exception as e:
            # Catch errors during the overall export process (e.g., iterating nodes)
            logger.error(f"Failed to export graph to dictionary: {e}", exc_info=True)
            raise WriteError(
                message=f"Failed to export graph to dictionary: {e}",
                target="dict",
                writer_type="DictWriter",
                original_error=e,
            ) from e
        else:
            return result

# --- END FILE: fin_statement_model/io/writers/dict.py ---

# --- START FILE: fin_statement_model/io/writers/excel.py ---
"""Data writer for Excel files."""

import logging
from pathlib import Path
from typing import Any, Optional

from fin_statement_model.core.graph import Graph
from fin_statement_model.io.base import DataWriter
from fin_statement_model.io.registry import register_writer
from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.io.writers.dataframe import DataFrameWriter
from fin_statement_model.io.config.models import ExcelWriterConfig

logger = logging.getLogger(__name__)


@register_writer("excel")
class ExcelWriter(DataWriter):
    """Writes graph data to an Excel file.

    Converts the graph data to a pandas DataFrame first (using `DataFrameWriter`),
    then writes that DataFrame to an Excel file using `pandas.to_excel()`.

    Default configuration for options like `sheet_name`, `recalculate`, `include_nodes`,
    and `excel_writer_kwargs` is provided via an `ExcelWriterConfig` object during
    initialization. However, these settings can be overridden for a specific call
    by passing them as keyword arguments to the `.write()` method.
    """

    def __init__(self, cfg: Optional[ExcelWriterConfig] = None) -> None:
        """Initialize the ExcelWriter.

        Args:
            cfg: Optional validated `ExcelWriterConfig` instance.
        """
        self.cfg = cfg

    def write(self, graph: Graph, target: str, **kwargs: dict[str, Any]) -> None:
        """Write graph data to an Excel file, converting via DataFrame first.

        Args:
            graph (Graph): The Graph object containing the data to write.
            target (str): Path to the target Excel file.
            **kwargs: Optional keyword arguments to override settings from the
                `ExcelWriterConfig` provided during initialization, or to provide
                defaults if no config was used.
                - sheet_name (str): Name of the sheet. Defaults to `cfg.sheet_name` or "Sheet1".
                - recalculate (bool): Recalculate before export. Defaults to `cfg.recalculate` or True.
                - include_nodes (list[str], optional): Nodes to include. Defaults to `cfg.include_nodes` or all nodes.
                - excel_writer_kwargs (dict): Additional args for `pandas.to_excel()`. Defaults to `cfg.excel_writer_kwargs` or {}.

        Raises:
            WriteError: If an error occurs during the writing process.
        """
        file_path = target
        # Combine configuration defaults with method overrides
        if self.cfg:
            sheet_name = kwargs.get("sheet_name", self.cfg.sheet_name)
            recalculate = kwargs.get("recalculate", self.cfg.recalculate)
            include_nodes = kwargs.get("include_nodes", self.cfg.include_nodes)
            excel_writer_options = kwargs.get(
                "excel_writer_kwargs", self.cfg.excel_writer_kwargs
            )
        else:
            sheet_name = kwargs.get("sheet_name", "Sheet1")
            recalculate = kwargs.get("recalculate", True)
            include_nodes = kwargs.get("include_nodes")
            excel_writer_options = kwargs.get("excel_writer_kwargs", {})

        logger.info(f"Exporting graph to Excel file: {file_path}, sheet: {sheet_name}")

        try:
            # 1. Convert graph to DataFrame using DataFrameWriter
            df_writer = DataFrameWriter()
            df = df_writer.write(graph=graph, target=None, recalculate=recalculate, include_nodes=include_nodes)

            # 2. Write DataFrame to Excel
            output_path = Path(file_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            df.to_excel(
                output_path,
                sheet_name=sheet_name,
                index=True,  # Keep node names as index column
                **excel_writer_options,
            )
            logger.info(f"Successfully exported graph to {file_path}, sheet '{sheet_name}'")

        except Exception as e:
            logger.error(
                f"Failed to export graph to Excel file '{file_path}': {e}",
                exc_info=True,
            )
            raise WriteError(
                message=f"Failed to export graph to Excel: {e}",
                target=file_path,
                writer_type="ExcelWriter",
                original_error=e,
            ) from e

# --- END FILE: fin_statement_model/io/writers/excel.py ---

# --- START FILE: fin_statement_model/io/writers/markdown_writer.py ---
import logging
import yaml # Added for parsing statement config
from typing import Any, Iterable, Optional, TypedDict, Union

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.nodes import Node  # Assuming nodes have relevant properties
from fin_statement_model.io.base import DataWriter
from fin_statement_model.io.config.models import BaseWriterConfig
from fin_statement_model.io.exceptions import WriteError
from fin_statement_model.io.registry import register_writer

logger = logging.getLogger(__name__)


class MarkdownWriterConfig(BaseWriterConfig):
    """Configuration specific to the Markdown writer."""

    indent_spaces: int = 4  # Number of spaces per indentation level
    # Add other Markdown-specific config options here if needed


# Define a structure for the items to be written
class StatementItem(TypedDict):
    name: str
    # value: Union[float, int, str, None] # Replaced single value
    values: dict[str, Union[float, int, str, None]] # Values per period
    level: int
    is_subtotal: bool # Indicates if the row is a subtotal or section header


@register_writer("markdown")
class MarkdownWriter(DataWriter):
    """Writes a financial statement structure to a Markdown table."""

    def __init__(self, config: Optional[MarkdownWriterConfig] = None):
        """Initializes the MarkdownWriter."""
        self.config = config or MarkdownWriterConfig(format_type="markdown")
        logger.debug(f"Initialized MarkdownWriter with config: {self.config}")

    def _format_value(self, value: Union[float, int, str, None]) -> str:
        """Formats the value for display in the table."""
        if value is None:
            return ""
        if isinstance(value, (float, int)):
            # Basic number formatting, could be enhanced (e.g., commas)
            return f"{value:,.2f}" if isinstance(value, float) else str(value)
        return str(value)

    def _get_statement_items(self, graph: Graph, statement_config_path: str) -> Iterable[StatementItem]:
        """Dynamically extracts and orders statement items based on YAML config and graph data.

        Args:
            graph: The financial statement graph containing node values.
            statement_config_path: Path to the statement definition YAML file.

        Returns:
            An iterable of StatementItem dictionaries.

        Raises:
            WriteError: If the config file is not found, invalid, or nodes are missing.
        """
        try:
            with open(statement_config_path, 'r') as f:
                config_data = yaml.safe_load(f)
            if not config_data or not isinstance(config_data, dict):
                raise WriteError(f"Invalid or empty YAML structure in {statement_config_path}")

        except FileNotFoundError:
            logger.error(f"Statement configuration file not found: {statement_config_path}")
            raise WriteError(f"Statement configuration file not found: {statement_config_path}") from None
        except yaml.YAMLError as e:
            logger.error(f"Error parsing statement configuration YAML '{statement_config_path}': {e}")
            raise WriteError(f"Error parsing statement configuration YAML: {e}") from e

        periods = sorted(list(graph.periods))
        logger.debug(f"Extracting statement items for periods: {periods} using config: {statement_config_path}")

        # Define the recursive processing function within the scope
        def process_level(items_or_sections: list, level: int):
            for config_item in items_or_sections:
                item_type = config_item.get('type')
                item_id = config_item.get('id')
                item_name = config_item.get('name', 'Unknown Item')
                sign_convention = config_item.get('sign_convention', 1)

                if not item_id or not item_type:
                    logger.warning(f"Skipping item due to missing 'id' or 'type' in config: {config_item}")
                    continue

                # Handle sections/subsections recursively
                if item_type == 'section':
                    section_name = config_item.get('name', 'Unnamed Section')
                    # Optionally yield a section header item (adjust level/formatting as needed)
                    # yield StatementItem(name=f"**{section_name}**", values={p: None for p in periods}, level=level, is_subtotal=False)
                    
                    # Process items within this section/subsection
                    yield from process_level(config_item.get('items', []), level + 1)
                    
                    # Process subsections within this section/subsection
                    yield from process_level(config_item.get('subsections', []), level + 1)
                    
                    # Process subtotal for this section/subsection if it exists
                    section_subtotal_config = config_item.get('subtotal')
                    if section_subtotal_config:
                         subtotal_item = process_item(section_subtotal_config, level + 1) # Indent subtotal under section items
                         if subtotal_item:
                            yield subtotal_item
                    continue # Move to the next item in the list

                # Handle regular items (line_item, calculated, subtotal, metric)
                item_data = process_item(config_item, level)
                if item_data:
                    yield item_data

        # Define the process_item function (handles non-section items)
        def process_item(item_config: dict, level: int) -> Union[StatementItem, None]:
            item_id = item_config.get('id')
            item_name = item_config.get('name', 'Unknown Item')
            item_type = item_config.get('type')
            sign_convention = item_config.get('sign_convention', 1)

            if not item_id or not item_type:
                logger.warning(f"Skipping item due to missing 'id' or 'type' in config: {item_config}")
                return None

            values = {}
            is_subtotal = (item_type == 'subtotal')
            node_id = None

            if item_type == 'line_item':
                node_id = item_config.get('node_id')
                if not node_id:
                    logger.warning(f"Skipping line_item '{item_name}' (ID: {item_id}) - missing 'node_id' in config.")
                    return None
            elif item_type in ['calculated', 'subtotal', 'metric']:
                node_id = item_id
            else:
                logger.warning(f"Unsupported item type '{item_type}' for item '{item_name}'. Skipping.")
                return None

            # Check if node_id could be determined (might be None if type wasn't handled)
            if node_id is None:
                 logger.warning(f"Could not determine node ID for item '{item_name}' (Type: {item_type}). Skipping value fetch.")
                 for period in periods:
                    values[period] = None
                 # Still return the item structure but with None values
                 return StatementItem(name=item_name, values=values, level=level, is_subtotal=is_subtotal)

            # Fetch values from the graph node
            try:
                node = graph.get_node(node_id)
                for period in periods:
                    raw_value = None
                    if item_type == 'line_item':
                        # For line items, get the stored value directly
                        raw_value = node.get_value(period)
                    elif item_type in ['calculated', 'subtotal', 'metric']:
                        # For calculated/subtotal/metric items, use the graph's calculation engine
                        try:
                            raw_value = graph.calculate(node_id, period)
                        except Exception as calc_error: # Catch potential calculation errors
                            logger.error(f"Calculation failed for node '{node_id}' period '{period}': {calc_error}")
                            raw_value = "CALC_ERR"
                    else:
                        # Should not happen based on earlier check, but as a safeguard
                        logger.warning(f"Unexpected item type '{item_type}' during value fetch for '{item_name}'.")
                        raw_value = None

                    # Apply sign convention
                    # Handle potential string values from errors
                    if isinstance(raw_value, (int, float)):
                         values[period] = raw_value * sign_convention
                    elif raw_value is None:
                         values[period] = None
                    else: # Keep error strings as is
                        values[period] = raw_value

            except KeyError:
                logger.warning(f"Node '{node_id}' for item '{item_name}' (Type: {item_type}) not found in graph. Values will be missing.")
                for period in periods:
                    values[period] = None # Fill with None if node is missing
            except Exception as e:
                 logger.error(f"Error fetching value for node '{node_id}' (Item: '{item_name}'): {e}", exc_info=True)
                 for period in periods:
                     values[period] = "ERROR" # Indicate error in output

            return StatementItem(name=item_name, values=values, level=level, is_subtotal=is_subtotal)

        # Start processing from the top-level sections
        yield from process_level(config_data.get('sections', []), level=0)

    def write(self, graph: Graph, target: Any = None, **kwargs: Any) -> str:
        """Writes the financial statement graph to a Markdown table string.

        Args:
            graph: The Graph object containing the financial data.
            target: Ignored by this writer, as it returns a string.
            **kwargs: Additional options. Expected args:
                statement_config_path (str): Path to the statement definition YAML.
                historical_periods (list[str], optional): List of historical period names.
                forecast_periods (list[str], optional): List of forecast period names.
                forecast_configs (dict, optional): Maps node IDs to forecast methods/configs for notes.

        Returns:
            A string containing the formatted statement in Markdown.

        Raises:
            WriteError: If statement_config_path is missing/invalid or an error occurs during writing.
        """
        logger.info(f"Writing graph to Markdown format (target ignored: {target}) using kwargs: {kwargs.keys()}")

        statement_config_path = kwargs.get("statement_config_path")
        if not statement_config_path or not isinstance(statement_config_path, str):
            raise WriteError("Missing or invalid 'statement_config_path' keyword argument for MarkdownWriter.")

        try:
            items = list(self._get_statement_items(graph, statement_config_path))
            if not items:
                logger.warning("No statement items generated from config and graph.")
                return "" # Return empty string

            # --- Get periods and determine historical/forecast --- 
            # Get periods from graph, ensuring order
            periods = sorted(list(graph.periods))
            # Default historical/forecast periods if not provided - attempt to infer or use all
            all_periods = set(periods)
            historical_periods = set(kwargs.get("historical_periods", [])) # Get from kwargs if provided
            forecast_periods = set(kwargs.get("forecast_periods", [])) # Get from kwargs if provided

            # Simple inference if not provided fully
            if not historical_periods and not forecast_periods:
                 # Assume all periods are historical if no forecast info given (basic fallback)
                 logger.warning("No historical/forecast period info provided; assuming all periods are historical for formatting.")
                 historical_periods = all_periods
            elif not historical_periods:
                 historical_periods = all_periods - forecast_periods
            elif not forecast_periods:
                 forecast_periods = all_periods - historical_periods

            # --- Calculate dynamic padding --- 
            max_desc_width = 0
            period_max_value_widths: dict[str, int] = {p: 0 for p in periods}
            formatted_lines = []

            # First pass: format data and calculate max widths
            for item in items:
                indent = " " * (item['level'] * self.config.indent_spaces)
                name = f"{indent}{item['name']}"
                is_subtotal = item['is_subtotal']
                values_formatted: dict[str, str] = {}

                if is_subtotal:
                    name = f"**{name}**"

                max_desc_width = max(max_desc_width, len(name))

                for period in periods:
                    raw_value = item['values'].get(period)
                    value_str = self._format_value(raw_value)
                    if is_subtotal:
                        value_str = f"**{value_str}**"
                    values_formatted[period] = value_str
                    period_max_value_widths[period] = max(period_max_value_widths[period], len(value_str))

                formatted_lines.append({"name": name, "values": values_formatted, "is_subtotal": is_subtotal})

            # Add some spacing between columns
            column_gap = 1 # Minimal gap needed within table cells

            # --- Build the final string --- 
            output_lines = []

            # Build header row
            header_parts = ["Description".ljust(max_desc_width)]
            for period in periods:
                period_label = period
                if period in historical_periods:
                    period_label += " (H)"
                elif period in forecast_periods:
                    period_label += " (F)"
                header_parts.append(period_label.rjust(period_max_value_widths[period]))
            # Join with | and add start/end |
            output_lines.append(f"| {(' | ').join(header_parts)} |")

            # Build separator row
            separator_parts = ["-" * max_desc_width]
            for period in periods:
                separator_parts.append("-" * period_max_value_widths[period])
            output_lines.append(f"| {(' | ').join(separator_parts)} |")

            # Build data rows
            for line_data in formatted_lines:
                row_parts = [line_data["name"].ljust(max_desc_width)]
                for period in periods:
                    value = line_data["values"].get(period, "") # Get value or empty string
                    row_parts.append(value.rjust(period_max_value_widths[period]))
                # Join with | and add start/end |
                output_lines.append(f"| {(' | ').join(row_parts)} |")

            # --- Add Forecast Notes --- 
            forecast_configs = kwargs.get("forecast_configs")
            if forecast_configs:
                notes = ["", "## Forecast Notes"] # Add blank line before header
                for node_id, config in forecast_configs.items():
                    method = config.get('method', 'N/A')
                    cfg_details = config.get('config')
                    desc = f"- **{node_id}**: Forecasted using method '{method}'"
                    if method == 'simple' and cfg_details is not None:
                        desc += f" (e.g., fixed growth rate: {cfg_details:.1%})."
                    elif method == 'curve' and cfg_details:
                        rates_str = ", ".join([f"{r:.1%}" for r in cfg_details])
                        desc += f" (e.g., specific growth rates: [{rates_str}])."
                    elif method == 'historical_growth':
                        desc += " (based on average historical growth)."
                    elif method == 'average':
                        desc += " (based on historical average value)."
                    elif method == 'statistical':
                        dist_name = cfg_details.get('distribution', 'unknown')
                        params_dict = cfg_details.get('params', {})
                        params_str = ", ".join([f"{k}={v:.3f}" if isinstance(v, float) else f"{k}={v}" for k, v in params_dict.items()])
                        desc += f" (using '{dist_name}' distribution with params: {params_str})."
                    else:
                        desc += "."
                    notes.append(desc)
                # Append notes if any were generated
                if len(notes) > 2:  # Header + at least one note
                     output_lines.extend(notes)

            return "\n".join(output_lines)

        except NotImplementedError as nie:
             logger.error(f"Markdown write failed: {nie}")
             raise WriteError(
                message=f"Markdown writer requires graph traversal logic: {nie}",
                target=target,
                writer_type="markdown",
                original_error=nie,
            ) from nie
        except Exception as e:
            logger.exception(f"Error writing Markdown for graph: {e}", exc_info=True)
            raise WriteError(
                message=f"Failed to generate Markdown table: {e}",
                target=target,
                writer_type="markdown",
                original_error=e,
            ) from e 
# --- END FILE: fin_statement_model/io/writers/markdown_writer.py ---

# --- START FILE: fin_statement_model/io/writers/statement_writer.py ---
"""Writers for exporting financial statement data (usually DataFrames)."""

import pandas as pd

from fin_statement_model.io.exceptions import WriteError

__all__ = ["write_statement_to_excel", "write_statement_to_json"]


def write_statement_to_excel(
    statement_df: pd.DataFrame,
    file_path: str,
    **kwargs: dict[str, object],
) -> None:
    """Write a statement DataFrame to an Excel file.

    Args:
        statement_df: The pandas DataFrame containing the formatted statement data.
        file_path: Path to save the Excel file.
        **kwargs: Additional arguments passed directly to pandas.DataFrame.to_excel
                 (e.g., sheet_name, index, header).

    Raises:
        WriteError: If writing the file fails.
    """
    try:
        # Default index=False is common for statement exports
        kwargs.setdefault("index", False)
        statement_df.to_excel(file_path, **kwargs)
    except Exception as e:
        # Removed StatementError handling as it's no longer relevant here
        raise WriteError(
            message="Failed to export statement DataFrame to Excel",
            target=file_path,
            format_type="excel", # Corrected parameter name
            original_error=e,
        ) from e


def write_statement_to_json(
    statement_df: pd.DataFrame,
    file_path: str,
    orient: str = "columns",
    **kwargs: dict[str, object],
) -> None:
    """Write a statement DataFrame to a JSON file.

    Args:
        statement_df: The pandas DataFrame containing the formatted statement data.
        file_path: Path to save the JSON file.
        orient: JSON orientation format (passed to pandas.DataFrame.to_json).
        **kwargs: Additional arguments passed directly to pandas.DataFrame.to_json
                 (e.g., indent, date_format).

    Raises:
        WriteError: If writing the file fails.
    """
    try:
        statement_df.to_json(file_path, orient=orient, **kwargs)
    except Exception as e:
        # Removed StatementError handling
        raise WriteError(
            message="Failed to export statement DataFrame to JSON",
            target=file_path,
            format_type="json", # Corrected parameter name
            original_error=e,
        ) from e

# --- END FILE: fin_statement_model/io/writers/statement_writer.py ---

# --- START FILE: fin_statement_model/logging_config.py ---
"""Centralized logging configuration for the fin_statement_model library."""

import logging

# Attach a NullHandler to the base fin_statement_model logger so that
# all child loggers inherit it and avoid 'No handler' warnings by default.
logging.getLogger("fin_statement_model").addHandler(logging.NullHandler())

# --- END FILE: fin_statement_model/logging_config.py ---

# --- START FILE: fin_statement_model/preprocessing/__init__.py ---
"""Export DataTransformer, CompositeTransformer, and TransformerFactory for preprocessing.

This module exposes core transformer interfaces and factory for the preprocessing layer.
"""

from .base_transformer import DataTransformer, CompositeTransformer
from .transformer_factory import TransformerFactory
from .transformation_service import TransformationService

## Trigger transformer discovery on package import
TransformerFactory.discover_transformers("fin_statement_model.preprocessing.transformers")

__all__ = [
    "CompositeTransformer",
    "DataTransformer",
    "TransformationService",
    "TransformerFactory",
]

# --- END FILE: fin_statement_model/preprocessing/__init__.py ---

# --- START FILE: fin_statement_model/preprocessing/base_transformer.py ---
"""Define base DataTransformer interface for preprocessing layer.

This module provides the DataTransformer abstract base class and CompositeTransformer.
"""

from abc import ABC, abstractmethod
import pandas as pd
from typing import Optional
import logging

logger = logging.getLogger(__name__)


class DataTransformer(ABC):
    """Define base class for data transformers.

    Data transformers convert data between formats and apply business rules.

    This separation follows the Single Responsibility Principle for maintainability.
    """

    def __init__(self, config: Optional[dict[str, object]] = None):
        """Initialize the transformer with optional configuration.

        Args:
            config: Optional configuration dictionary for the transformer
        """
        self.config = config or {}
        logger.debug(f"Initialized {self.__class__.__name__} with config: {self.config}")

    @abstractmethod
    def transform(self, data: object) -> object:
        """Transform the input data.

        Args:
            data: The input data to transform

        Returns:
            Transformed data

        Raises:
            ValueError: If the data cannot be transformed
        """

    def validate_input(self, data: object) -> bool:
        """Validate that the input data is a pandas DataFrame by default.

        This performs a basic DataFrame type check and can be overridden by subclasses with more specific validation logic.

        Args:
            data (object): The input data to validate.

        Returns:
            bool: True if data is a pandas.DataFrame, False otherwise.
        """
        return isinstance(data, pd.DataFrame)

    def _pre_transform_hook(self, data: object) -> object:
        """Hook method called before transformation.

        Args:
            data: The input data

        Returns:
            Processed data to be passed to the transform method

        This method can be overridden by subclasses to add pre-processing steps.
        """
        return data

    def _post_transform_hook(self, data: object) -> object:
        """Hook method called after transformation.

        Args:
            data: The transformed data

        Returns:
            Final processed data

        This method can be overridden by subclasses to add post-processing steps.
        """
        return data

    def execute(self, data: object) -> object:
        """Execute the complete transformation pipeline.

        Args:
            data: The input data to transform

        Returns:
            Transformed data

        Raises:
            ValueError: If the data is invalid or cannot be transformed
        """
        if not self.validate_input(data):
            raise ValueError(f"Invalid input data for {self.__class__.__name__}")

        try:
            # Apply pre-transform hook
            processed_data = self._pre_transform_hook(data)

            # Perform transformation
            result = self.transform(processed_data)
            result = self._post_transform_hook(result)
            logger.debug(f"Successfully transformed data with {self.__class__.__name__}")
        except Exception as e:
            logger.exception(f"Error transforming data with {self.__class__.__name__}")
            raise ValueError("Error transforming data") from e
        else:
            return result


class CompositeTransformer(DataTransformer):
    """Compose multiple transformers into a pipeline.

    This allows building complex transformation chains from simple steps.
    """

    def __init__(self, transformers: list[DataTransformer], config: Optional[dict] = None):
        """Initialize with a list of transformers.

        Args:
            transformers: List of transformers to apply in sequence
            config: Optional configuration dictionary
        """
        super().__init__(config)
        self.transformers = transformers

    def transform(self, data: object) -> object:
        """Apply each transformer in sequence.

        Args:
            data: The input data to transform

        Returns:
            Data transformed by the pipeline
        """
        result = data
        for transformer in self.transformers:
            result = transformer.execute(result)
        return result

    def add_transformer(self, transformer: DataTransformer) -> None:
        """Add a transformer to the pipeline.

        Args:
            transformer: The transformer to add
        """
        self.transformers.append(transformer)

    def remove_transformer(self, index: int) -> Optional[DataTransformer]:
        """Remove a transformer from the pipeline.

        Args:
            index: Index of the transformer to remove

        Returns:
            The removed transformer or None if index is invalid
        """
        if 0 <= index < len(self.transformers):
            return self.transformers.pop(index)
        return None

    def validate_input(self, data: object) -> bool:
        """Validate input for the composite transformer.

        If the pipeline is empty, accepts any data; otherwise, delegates validation to the first transformer.

        Args:
            data (object): Input data to validate.

        Returns:
            bool: True if input is valid for the pipeline.
        """
        if not hasattr(self, "transformers") or not self.transformers:
            return True
        return self.transformers[0].validate_input(data)

# --- END FILE: fin_statement_model/preprocessing/base_transformer.py ---

# --- START FILE: fin_statement_model/preprocessing/config/__init__.py ---
"""Package for preprocessing configuration files."""

# Package for preprocessing configuration files

# --- END FILE: fin_statement_model/preprocessing/config/__init__.py ---

# --- START FILE: fin_statement_model/preprocessing/enums.py ---
"""Define Enum classes for preprocessing transformer types.

Centralize transformer type constants as Enums for clarity.
"""

from enum import Enum


class NormalizationType(Enum):
    """Available normalization types for NormalizationTransformer."""

    PERCENT_OF = "percent_of"
    MINMAX = "minmax"
    STANDARD = "standard"
    SCALE_BY = "scale_by"


class TransformationType(Enum):
    """Available transformation types for TimeSeriesTransformer."""

    GROWTH_RATE = "growth_rate"
    MOVING_AVG = "moving_avg"
    CAGR = "cagr"
    YOY = "yoy"
    QOQ = "qoq"


class ConversionType(Enum):
    """Available conversion types for PeriodConversionTransformer."""

    QUARTERLY_TO_ANNUAL = "quarterly_to_annual"
    MONTHLY_TO_QUARTERLY = "monthly_to_quarterly"
    MONTHLY_TO_ANNUAL = "monthly_to_annual"
    ANNUAL_TO_TTM = "annual_to_ttm"


class StatementType(Enum):
    """Available statement types for StatementFormattingTransformer."""

    INCOME_STATEMENT = "income_statement"
    BALANCE_SHEET = "balance_sheet"
    CASH_FLOW = "cash_flow"

# --- END FILE: fin_statement_model/preprocessing/enums.py ---

# --- START FILE: fin_statement_model/preprocessing/transformation_service.py ---
"""Transformation Service for the Financial Statement Model.

This module provides a high-level service for managing and applying data transformations.
"""

from typing import Optional, Union

import pandas as pd
import logging

from .base_transformer import DataTransformer, CompositeTransformer
from .transformer_factory import TransformerFactory

logger = logging.getLogger(__name__)


class TransformationService:
    """Service for managing and applying data transformations.

    This service separates data transformation logic from data processing,
    making it easier to maintain, test, and extend the codebase.

    It provides methods for common financial data transformations and allows
    for composing multiple transformations into pipelines.
    """

    def __init__(self):
        """Initialize the transformation service."""
        logger.info("TransformationService initialized")

    def normalize_data(
        self,
        data: Union[pd.DataFrame, dict],
        normalization_type: str = "percent_of",
        reference: Optional[str] = None,
        scale_factor: Optional[float] = None,
    ) -> Union[pd.DataFrame, dict]:
        """Normalize financial data.

        Args:
            data: The data to normalize (DataFrame or Dict)
            normalization_type: Type of normalization
            reference: Reference field for percent_of normalization
            scale_factor: Scale factor for scale_by normalization

        Returns:
            Normalized data
        """
        transformer = TransformerFactory.create_transformer(
            "normalization",
            normalization_type=normalization_type,
            reference=reference,
            scale_factor=scale_factor,
        )

        return transformer.execute(data)

    def transform_time_series(
        self,
        data: Union[pd.DataFrame, dict],
        transformation_type: str = "growth_rate",
        periods: int = 1,
        window_size: int = 3,
    ) -> Union[pd.DataFrame, dict]:
        """Apply time series transformations to financial data.

        Args:
            data: The time series data to transform
            transformation_type: Type of transformation
            periods: Number of periods for calculations
            window_size: Window size for moving averages

        Returns:
            Transformed data
        """
        transformer = TransformerFactory.create_transformer(
            "time_series",
            transformation_type=transformation_type,
            periods=periods,
            window_size=window_size,
        )

        return transformer.execute(data)

    def convert_periods(
        self, data: pd.DataFrame, conversion_type: str, aggregation: str = "sum"
    ) -> pd.DataFrame:
        """Convert data between different period types.

        Args:
            data: DataFrame with time periods
            conversion_type: Type of period conversion
            aggregation: Aggregation method

        Returns:
            Transformed DataFrame with converted periods
        """
        transformer = TransformerFactory.create_transformer(
            "period_conversion",
            conversion_type=conversion_type,
            aggregation=aggregation,
        )

        return transformer.execute(data)

    def format_statement(
        self,
        data: pd.DataFrame,
        statement_type: str = "income_statement",
        add_subtotals: bool = True,
        apply_sign_convention: bool = True,
    ) -> pd.DataFrame:
        """Format a financial statement DataFrame.

        Args:
            data: Financial statement data
            statement_type: Type of statement
            add_subtotals: Whether to add standard subtotals
            apply_sign_convention: Whether to apply sign conventions

        Returns:
            Formatted financial statement
        """
        transformer = TransformerFactory.create_transformer(
            "statement_formatting",
            statement_type=statement_type,
            add_subtotals=add_subtotals,
            apply_sign_convention=apply_sign_convention,
        )

        return transformer.execute(data)

    def create_transformation_pipeline(
        self, transformers_config: list[dict[str, object]]
    ) -> DataTransformer:
        """Create a composite transformer from a list of transformer configurations.

        Args:
            transformers_config: List of dicts with transformer configurations
                Each dict should have:
                - 'name': Name of the transformer
                - Additional configuration parameters for that transformer

        Returns:
            A composite transformer with the configured pipeline

        Example:
            config = [
                {'name': 'period_conversion', 'conversion_type': 'quarterly_to_annual'},
                {'name': 'normalization', 'normalization_type': 'percent_of', 'reference': 'revenue'}
            ]
            pipeline = service.create_transformation_pipeline(config)
            transformed_data = pipeline.execute(data)
        """
        transformers = []

        for config in transformers_config:
            if "name" not in config:
                raise ValueError("Each transformer configuration must have a 'name' field")

            name = config.pop("name")
            transformer = TransformerFactory.create_transformer(name, **config)
            transformers.append(transformer)

        return CompositeTransformer(transformers)

    def apply_transformation_pipeline(
        self, data: object, transformers_config: list[dict[str, object]]
    ) -> object:
        """Apply a transformation pipeline to data.

        Args:
            data: The data to transform
            transformers_config: List of transformer configurations

        Returns:
            Transformed data
        """
        pipeline = self.create_transformation_pipeline(transformers_config)
        return pipeline.execute(data)

    def register_custom_transformer(
        self, name: str, transformer_class: type[DataTransformer]
    ) -> None:
        """Register a custom transformer with the factory.

        Args:
            name: Name for the transformer
            transformer_class: The transformer class to register
        """
        TransformerFactory.register_transformer(name, transformer_class)
        logger.info(f"Registered custom transformer: {name}")

    def list_available_transformers(self) -> list[str]:
        """List all available transformer types.

        Returns:
            List of transformer names
        """
        return TransformerFactory.list_transformers()

# --- END FILE: fin_statement_model/preprocessing/transformation_service.py ---

# --- START FILE: fin_statement_model/preprocessing/transformer_factory.py ---
"""Provide TransformerFactory to create and manage data transformers.

This module implements a factory for registering and instantiating transformers.
"""

import importlib
import re
import inspect
import pkgutil
import logging
from typing import ClassVar, Any

from .base_transformer import DataTransformer

logger = logging.getLogger(__name__)


class TransformerFactory:
    """Create and manage transformer instances.

    Centralizes transformer registration, discovery, and instantiation.
    """

    # Registry of transformer types
    _transformers: ClassVar[dict[str, type[DataTransformer]]] = {}

    @classmethod
    def register_transformer(cls, name: str, transformer_class: type[DataTransformer]) -> None:
        """Register a transformer class with the factory.

        Args:
            name: Name to register the transformer under
            transformer_class: The transformer class to register

        Raises:
            ValueError: If the name is already registered
            TypeError: If transformer_class is not a subclass of DataTransformer
        """
        if name in cls._transformers:
            raise ValueError(f"Transformer name '{name}' is already registered")

        if not issubclass(transformer_class, DataTransformer):
            raise TypeError("Transformer class must be a subclass of DataTransformer")

        cls._transformers[name] = transformer_class
        logger.info(f"Registered transformer '{name}'")

    @classmethod
    def create_transformer(cls, name: str, **kwargs: dict[str, Any]) -> DataTransformer:
        """Create a transformer instance by name.

        Args:
            name: Name of the registered transformer
            **kwargs: Arguments to pass to the transformer constructor

        Returns:
            DataTransformer: An instance of the requested transformer

        Raises:
            ValueError: If no transformer is registered with the given name
        """
        if name not in cls._transformers:
            raise ValueError(f"No transformer registered with name '{name}'")

        transformer_class = cls._transformers[name]
        transformer = transformer_class(**kwargs)
        logger.debug(f"Created transformer '{name}'")
        return transformer

    @classmethod
    def list_transformers(cls) -> list[str]:
        """List all registered transformer names.

        Returns:
            List[str]: List of registered transformer names
        """
        return list(cls._transformers.keys())

    @classmethod
    def get_transformer_class(cls, name: str) -> type[DataTransformer]:
        """Get a transformer class by name.

        Args:
            name: Name of the registered transformer

        Returns:
            Type[DataTransformer]: The requested transformer class

        Raises:
            ValueError: If no transformer is registered with the given name
        """
        if name not in cls._transformers:
            raise ValueError(f"No transformer registered with name '{name}'")

        return cls._transformers[name]

    @classmethod
    def discover_transformers(cls, package_name: str) -> None:
        """Discover and register all transformers in a package.

        This method imports all modules in the specified package and
        registers any DataTransformer subclasses found.

        Args:
            package_name: Name of the package to search
        """
        try:
            package = importlib.import_module(package_name)
            package_path = package.__path__

            # Import all modules in the package
            for _, module_name, _ in pkgutil.iter_modules(package_path):
                full_module_name = f"{package_name}.{module_name}"
                module = importlib.import_module(full_module_name)

                # Find all DataTransformer subclasses in the module
                for name, obj in inspect.getmembers(module):
                    if (
                        inspect.isclass(obj)
                        and issubclass(obj, DataTransformer)
                        and obj != DataTransformer
                    ):
                        # Register the transformer with its class name
                        cls.register_transformer(name, obj)
                        # Register snake_case alias without '_transformer'
                        snake = re.sub(r"(.)([A-Z][a-z]+)", r"\1_\2", name)
                        snake = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", snake).lower()
                        alias = snake.replace("_transformer", "")
                        if alias not in cls._transformers:
                            cls.register_transformer(alias, obj)

            logger.info(f"Discovered transformers from package '{package_name}'")

        except ImportError:
            logger.exception(f"Error discovering transformers from package '{package_name}'")

    @classmethod
    def create_composite_transformer(
        cls, transformer_names: list[str], **kwargs: dict[str, Any]
    ) -> DataTransformer:
        """Create a composite transformer from a list of transformer names.

        Args:
            transformer_names: List of registered transformer names to include in the pipeline
            **kwargs: Additional arguments to pass to individual transformers

        Returns:
            DataTransformer: A composite transformer containing the specified transformers

        Raises:
            ValueError: If any transformer name is not registered
        """
        from .base_transformer import CompositeTransformer

        # Use list comprehension for PERF401
        transformers = [cls.create_transformer(name, **kwargs) for name in transformer_names]

        return CompositeTransformer(transformers)

# --- END FILE: fin_statement_model/preprocessing/transformer_factory.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/__init__.py ---
"""Package for preprocessing transformers.

This package exports built-in data transformer classes for the preprocessing layer.
"""

from .normalization import NormalizationTransformer
from .time_series import TimeSeriesTransformer
from .period_conversion import PeriodConversionTransformer
from .statement_formatting import StatementFormattingTransformer

__all__ = [
    "NormalizationTransformer",
    "PeriodConversionTransformer",
    "StatementFormattingTransformer",
    "TimeSeriesTransformer",
]

# --- END FILE: fin_statement_model/preprocessing/transformers/__init__.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/normalization.py ---
"""Provide a NormalizationTransformer to normalize financial data.

Transforms data by percent_of, minmax, standard, or scale_by methods.

This module implements the NormalizationTransformer for the preprocessing layer.
"""

from typing import Optional, Union, ClassVar

import pandas as pd

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.enums import NormalizationType
from fin_statement_model.preprocessing.types import NormalizationConfig


class NormalizationTransformer(DataTransformer):
    """Transformer that normalizes financial data.

    This transformer can normalize values by:
    - Dividing by a reference value (e.g. convert to percentages of revenue)
    - Scaling to a specific range (e.g. 0-1)
    - Applying standard normalization ((x - mean) / std)

    It can operate on DataFrames or dictionary data structures.
    """

    NORMALIZATION_TYPES: ClassVar[list[str]] = [t.value for t in NormalizationType]

    def __init__(
        self,
        normalization_type: Union[str, NormalizationType] = NormalizationType.PERCENT_OF,
        reference: Optional[str] = None,
        scale_factor: Optional[float] = None,
        config: Optional[NormalizationConfig] = None,
    ):
        """Initialize the normalizer.

        Args:
            normalization_type: Type of normalization to apply
                - 'percent_of': Divides by a reference value
                - 'minmax': Scales to range [0,1]
                - 'standard': Applies (x - mean) / std
                - 'scale_by': Multiplies by a scale factor
            reference: Reference field for percent_of normalization
            scale_factor: Factor to scale by for scale_by normalization
            config: Additional configuration options
        """
        super().__init__(config)
        # Normalize enum to string
        if isinstance(normalization_type, NormalizationType):
            norm_type = normalization_type.value
        else:
            norm_type = normalization_type
        if norm_type not in self.NORMALIZATION_TYPES:
            raise ValueError(
                f"Invalid normalization type: {norm_type}. "
                f"Must be one of {self.NORMALIZATION_TYPES}"
            )
        self.normalization_type = norm_type

        self.reference = reference
        self.scale_factor = scale_factor

        # Validation
        if self.normalization_type == NormalizationType.PERCENT_OF.value and not reference:
            raise ValueError("Reference field must be provided for percent_of normalization")

        if self.normalization_type == NormalizationType.SCALE_BY.value and scale_factor is None:
            raise ValueError("Scale factor must be provided for scale_by normalization")

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Normalize the data based on the configured normalization type.

        Args:
            data: pd.DataFrame containing financial data

        Returns:
            pd.DataFrame: Normalized DataFrame
        """
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f"Unsupported data type: {type(data)}. Expected pandas.DataFrame")
        return self._transform_dataframe(data)

    def _transform_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform a DataFrame."""
        result = df.copy()

        if self.normalization_type == NormalizationType.PERCENT_OF.value:
            if self.reference not in df.columns:
                raise ValueError(f"Reference column '{self.reference}' not found in DataFrame")

            for col in df.columns:
                if col != self.reference:
                    result[col] = df[col] / df[self.reference] * 100

        elif self.normalization_type == NormalizationType.MINMAX.value:  # pragma: no cover
            for col in df.columns:
                min_val = df[col].min()
                max_val = df[col].max()

                if max_val > min_val:
                    result[col] = (df[col] - min_val) / (max_val - min_val)  # pragma: no cover

        elif self.normalization_type == NormalizationType.STANDARD.value:
            for col in df.columns:
                mean = df[col].mean()
                std = df[col].std()

                if std > 0:
                    result[col] = (df[col] - mean) / std

        elif self.normalization_type == NormalizationType.SCALE_BY.value:
            for col in df.columns:
                result[col] = df[col] * self.scale_factor

        return result

# --- END FILE: fin_statement_model/preprocessing/transformers/normalization.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/period_conversion.py ---
"""Financial data transformers for the Financial Statement Model.

This module provides the PeriodConversionTransformer for converting between period types:
quarterly_to_annual, monthly_to_quarterly, monthly_to_annual, and annual_to_ttm.
"""

import pandas as pd
from typing import Optional, Union, ClassVar

from fin_statement_model.preprocessing.base_transformer import DataTransformer
from fin_statement_model.preprocessing.types import PeriodConversionConfig
from fin_statement_model.preprocessing.enums import ConversionType

# Configure logging


class PeriodConversionTransformer(DataTransformer):
    """Transformer for converting between different period types.

    This transformer can convert:
    - Quarterly data to annual
    - Monthly data to quarterly or annual
    - Annual data to trailing twelve months (TTM)
    """

    # All valid conversion types
    CONVERSION_TYPES: ClassVar[list[str]] = [t.value for t in ConversionType]

    def __init__(
        self,
        conversion_type: Union[str, ConversionType] = ConversionType.QUARTERLY_TO_ANNUAL,
        aggregation: str = "sum",
        config: Optional[PeriodConversionConfig] = None,
    ):
        """Initialize the period conversion transformer.

        Args:
            conversion_type: Type of period conversion to apply (enum or string)
            aggregation: How to aggregate data (sum, mean, last, etc.)
            config: Optional transformer configuration
        """
        super().__init__(config)
        # Normalize enum to string
        if isinstance(conversion_type, ConversionType):
            ctype = conversion_type.value
        else:
            ctype = conversion_type
        if ctype not in self.CONVERSION_TYPES:
            raise ValueError(
                f"Invalid conversion type: {ctype}. Must be one of {self.CONVERSION_TYPES}"
            )
        self.conversion_type = ctype
        self.aggregation = aggregation

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform data by converting between period types.

        Args:
            data: DataFrame with DatetimeIndex or period labels in the index

        Returns:
            DataFrame with transformed periods
        """
        # Ensure we have a DataFrame
        if not isinstance(data, pd.DataFrame):
            raise TypeError("Period conversion requires a pandas DataFrame")

        # Try to convert index to datetime if it's not already
        if not isinstance(data.index, pd.DatetimeIndex):
            try:
                data = data.copy()
                data.index = pd.to_datetime(data.index, format="%Y-%m-%d")
            except Exception:
                raise ValueError("Index must be convertible to datetime for period conversion")

        if self.conversion_type == ConversionType.QUARTERLY_TO_ANNUAL.value:
            # Group by year and aggregate
            return data.groupby(data.index.year).agg(self.aggregation)

        elif self.conversion_type == ConversionType.MONTHLY_TO_QUARTERLY.value:
            # Group by year and quarter
            return data.groupby([data.index.year, data.index.quarter]).agg(self.aggregation)

        elif self.conversion_type == ConversionType.MONTHLY_TO_ANNUAL.value:
            # Group by year
            return data.groupby(data.index.year).agg(self.aggregation)

        elif self.conversion_type == ConversionType.ANNUAL_TO_TTM.value:
            # Implement TTM as rolling sum with window=4 for quarterly data
            if self.aggregation == "sum":
                return data.rolling(window=4).sum()
            else:
                # For other aggregation methods, we need custom logic
                raise ValueError("annual_to_ttm conversion only supports 'sum' aggregation")

        return data  # pragma: no cover

# --- END FILE: fin_statement_model/preprocessing/transformers/period_conversion.py ---

# --- START FILE: fin_statement_model/preprocessing/transformers/time_series.py ---
"""Financial data transformers for the Financial Statement Model.

This module provides the TimeSeriesTransformer which applies growth rates,
moving averages, CAGR, year-over-year, and quarter-over-quarter conversions.
"""

import pandas as pd
from typing import Union, Optional, ClassVar

from fin_statement_model.preprocessing.types import TimeSeriesConfig
from fin_statement_model.preprocessing.enums import TransformationType
from fin_statement_model.preprocessing.base_transformer import DataTransformer


class TimeSeriesTransformer(DataTransformer):
    """Transformer for time series financial data.

    This transformer can apply common time series transformations like:
    - Calculating growth rates
    - Calculating moving averages
    - Computing compound annual growth rate (CAGR)
    - Converting to year-over-year or quarter-over-quarter comparisons
    """

    TRANSFORMATION_TYPES: ClassVar[list[str]] = [t.value for t in TransformationType]

    def __init__(
        self,
        transformation_type: Union[str, TransformationType] = TransformationType.GROWTH_RATE,
        periods: int = 1,
        window_size: int = 3,
        config: Optional[TimeSeriesConfig] = None,
    ):
        """Initialize the time series transformer.

        Args:
            transformation_type: Type of transformation to apply
                - 'growth_rate': Calculate period-to-period growth rates
                - 'moving_avg': Calculate moving average
                - 'cagr': Calculate compound annual growth rate
                - 'yoy': Year-over-year comparison
                - 'qoq': Quarter-over-quarter comparison
            periods: Number of periods to use in calculations
            window_size: Size of the moving average window
            config: Additional configuration options
        """
        super().__init__(config)
        # Normalize to string
        if isinstance(transformation_type, TransformationType):
            ttype = transformation_type.value
        else:
            ttype = transformation_type
        if ttype not in self.TRANSFORMATION_TYPES:
            raise ValueError(
                f"Invalid transformation type: {ttype}. Must be one of {self.TRANSFORMATION_TYPES}"
            )
        self.transformation_type = ttype

        self.periods = periods
        self.window_size = window_size

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform time series data based on the configured transformation type.

        Args:
            data: pd.DataFrame containing time series financial data

        Returns:
            pd.DataFrame: Transformed DataFrame
        """
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f"Unsupported data type: {type(data)}. Expected pandas.DataFrame")
        return self._transform_dataframe(data)

    def _transform_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform a DataFrame with time series data."""
        result = df.copy()

        if self.transformation_type == "growth_rate":
            for col in df.columns:
                result[f"{col}_growth"] = df[col].pct_change(periods=self.periods) * 100

        elif self.transformation_type == "moving_avg":
            for col in df.columns:
                result[f"{col}_ma{self.window_size}"] = (
                    df[col].rolling(window=self.window_size).mean()
                )

        elif self.transformation_type == "cagr":
            # Assuming the index represents time periods
            n_periods = len(df) - 1
            for col in df.columns:
                start = df[col].iloc[0]
                end = df[col].iloc[-1]
                if start > 0:
                    result[f"{col}_cagr"] = ((end / start) ** (1 / n_periods) - 1) * 100

        elif self.transformation_type == "yoy":
            for col in df.columns:
                result[f"{col}_yoy"] = df[col].pct_change(periods=12) * 100

        elif self.transformation_type == "qoq":
            for col in df.columns:
                result[f"{col}_qoq"] = df[col].pct_change(periods=3) * 100

        return result

# --- END FILE: fin_statement_model/preprocessing/transformers/time_series.py ---

# --- START FILE: fin_statement_model/preprocessing/types.py ---
"""Define types and TypedDicts for preprocessing transformers.

This module provides a TabularData alias (pd.DataFrame only) and configuration TypedDicts.
"""

from typing import Optional
from pydantic import BaseModel
import pandas as pd

# Alias for tabular data inputs (DataFrame-only) accepted by transformers
TabularData = pd.DataFrame


class NormalizationConfig(BaseModel):
    """Configuration for normalization transformations.

    Attributes:
        normalization_type: 'percent_of', 'minmax', 'standard', or 'scale_by'
        reference: reference field name for 'percent_of' normalization
        scale_factor: factor to apply for 'scale_by' normalization
    """

    normalization_type: Optional[str] = None
    reference: Optional[str] = None
    scale_factor: Optional[float] = None


class TimeSeriesConfig(BaseModel):
    """Configuration for time series transformations.

    Attributes:
        transformation_type: 'growth_rate', 'moving_avg', 'cagr', 'yoy', or 'qoq'
        periods: number of periods for percentage change or other transformations
        window_size: window size for rolling calculations
    """

    transformation_type: Optional[str] = None
    periods: Optional[int] = None
    window_size: Optional[int] = None


class PeriodConversionConfig(BaseModel):
    """Configuration for period conversion transformations.

    Attributes:
        conversion_type: 'quarterly_to_annual', 'monthly_to_quarterly', etc.
        aggregation: aggregation method: 'sum', 'mean', 'last', etc.
    """

    conversion_type: Optional[str] = None
    aggregation: Optional[str] = None


class StatementFormattingConfig(BaseModel):
    """Configuration for formatting statement output.

    Attributes:
        statement_type: 'income_statement', 'balance_sheet', 'cash_flow'
        add_subtotals: whether to insert computed subtotals
        apply_sign_convention: whether to apply sign rules to values
    """

    statement_type: Optional[str] = None
    add_subtotals: Optional[bool] = None
    apply_sign_convention: Optional[bool] = None

# --- END FILE: fin_statement_model/preprocessing/types.py ---

# --- START FILE: fin_statement_model/statements/__init__.py ---
"""Financial Statements Layer (`fin_statement_model.statements`).

This package provides domain-specific abstractions for defining, building,
managing, and presenting financial statements (like Income Statement,
Balance Sheet, Cash Flow Statement) based on underlying configurations.

It sits above the `core` layer and orchestrates the use of core components
(like `Graph`, `Node`) within the context of financial statement structures.
It utilizes configurations (often YAML files) to define the layout, items,
and calculations of a statement.

Key functionalities include:
  - Defining statement structure (`StatementStructure`, `Section`, `LineItem` etc.)
  - Loading and validating statement configurations (`StatementConfig`).
  - Building `StatementStructure` objects from configurations
    (`StatementStructureBuilder`).
  - Managing multiple loaded statements (`StatementRegistry`).
  - Populating a `core.graph.Graph` with calculation nodes based on statement
    definitions (`populate_graph_from_statement`).
  - Formatting statement data retrieved from a graph into user-friendly formats,
    primarily pandas DataFrames (`StatementFormatter`).
  - High-level functions to streamline common workflows like generating a
    statement DataFrame or exporting statements to files (`create_statement_dataframe`,
    `export_statements_to_excel`).

This package imports from `core` and `io` (indirectly via `factory`), but should
not be imported by `core`.
"""

# Core statement structure components
from .structure import (
    StatementStructure,
    Section,
    LineItem,
    CalculatedLineItem,
    SubtotalLineItem,
    StatementItemType,
    StatementItem, # Added base item type if needed
)
# Configuration related classes
from .config.config import StatementConfig
# Building
from .builder import StatementStructureBuilder
# Registry
from .registry import StatementRegistry
# Populator
from .populator import populate_graph_from_statement
# Formatting
# Ensure formatter is imported correctly if it's in a sub-package
try:
    from .formatter import StatementFormatter # If __init__.py exists in formatter
except ImportError:
    from .formatter.formatter import StatementFormatter # Direct import
# High-level Orchestration functions (previously Factory)
from .factory import (
    create_statement_dataframe,
    export_statements_to_excel,
    export_statements_to_json,
)
# Errors specific to statements
from .errors import StatementError, ConfigurationError

# Public API definition
__all__ = [
    "CalculatedLineItem",
    "ConfigurationError",
    "LineItem",
    "Section",
    "StatementConfig",
    "StatementError",
    "StatementFormatter",
    "StatementItem",
    "StatementItemType",
    "StatementRegistry",
    "StatementStructure",
    "StatementStructureBuilder",
    "SubtotalLineItem",
    "create_statement_dataframe",
    "export_statements_to_excel",
    "export_statements_to_json",
    "populate_graph_from_statement",
    # --- Removed --- #
    # "FinancialStatementGraph", (unless reintroduced)
    # "StatementFactory", (class)
    # "StatementManager",
]

# Note: FinancialStatementGraph removed as part of refactor, assuming its
# responsibilities are covered by core Graph and statement-specific components.

# --- END FILE: fin_statement_model/statements/__init__.py ---

# --- START FILE: fin_statement_model/statements/builder.py ---
"""Builds StatementStructure objects from validated StatementConfig models.

This module provides the `StatementStructureBuilder`, which translates the
deserialized and validated configuration (represented by `StatementConfig`
containing Pydantic models) into the hierarchical `StatementStructure` object
used internally for representing the layout and components of a financial
statement.
"""

import logging
from typing import Union

# Assuming config and structure modules are accessible
from .config.config import StatementConfig
from .config.models import (
    SectionModel,
    BaseItemModel,
    LineItemModel,
    CalculatedItemModel,
    MetricItemModel,
    SubtotalModel,
)
from .structure import (
    StatementStructure,
    Section,
    LineItem,
    MetricLineItem,
    CalculatedLineItem,
    SubtotalLineItem,
)
from .errors import ConfigurationError

logger = logging.getLogger(__name__)

__all__ = ["StatementStructureBuilder"]


class StatementStructureBuilder:
    """Constructs a `StatementStructure` object from a validated configuration.

    Takes a `StatementConfig` instance (which should have successfully passed
    validation, populating its `.model` attribute) and recursively builds the
    corresponding `StatementStructure`, including its sections, line items,
    calculated items, subtotals, and nested sections.
    """

    def build(self, config: StatementConfig) -> StatementStructure:
        """Build a `StatementStructure` from a validated `StatementConfig`.

        This is the main public method of the builder. It orchestrates the
        conversion process, calling internal helper methods to build sections
        and items.

        Args:
            config: A `StatementConfig` instance whose `.validate_config()`
                method has been successfully called, populating `config.model`.

        Returns:
            The fully constructed `StatementStructure` object, ready to be
            registered or used.

        Raises:
            ValueError: If the provided `config` object has not been validated
                (i.e., `config.model` is `None`).
            ConfigurationError: If an unexpected error occurs during the building
                process, potentially indicating an issue not caught by the
                initial Pydantic validation or an internal inconsistency.
        """
        if config.model is None:
             # Ensure validation has run successfully before building
             raise ValueError(
                 "StatementConfig must be validated (config.model must be set) "
                 "before building the structure."
             )

        # Build from the validated Pydantic model stored in config.model
        try:
            stmt_model = config.model # Use validated model from config
            statement = StatementStructure(
                id=stmt_model.id,
                name=stmt_model.name,
                description=stmt_model.description,
                metadata=stmt_model.metadata,
            )
            for sec_model in stmt_model.sections:
                section = self._build_section_model(sec_model)
                statement.add_section(section)
            logger.info(f"Successfully built StatementStructure for ID '{statement.id}'")
            return statement
        except Exception as e:
             # Catch potential errors during the building process itself
             logger.exception(f"Error building statement structure from validated model for ID '{config.model.id}'")
             raise ConfigurationError(
                 message=f"Failed to build statement structure from validated config: {e}",
                 errors=[str(e)]
             ) from e

    def _build_section_model(self, section_model: SectionModel) -> Section:
        """Build a `Section` object from a `SectionModel`.

        Recursively builds the items and subsections within this section.

        Args:
            section_model: The Pydantic model representing the section configuration.

        Returns:
            A `Section` instance corresponding to the model.
        """
        section = Section(
            id=section_model.id,
            name=section_model.name,
            description=section_model.description,
            metadata=section_model.metadata,
        )
        for item in section_model.items:
            section.add_item(self._build_item_model(item))
        for sub in section_model.subsections:
            # Recursively build subsections
            section.add_item(self._build_section_model(sub))
        if section_model.subtotal:
            section.subtotal = self._build_subtotal_model(section_model.subtotal)
        return section

    def _build_item_model(
        self, item_model: BaseItemModel
    ) -> Union[LineItem, CalculatedLineItem, MetricLineItem, SubtotalLineItem, Section]:
        """Build a statement item object from its corresponding Pydantic model.

        Dispatches the building process based on the specific type of the input
        model (`LineItemModel`, `CalculatedItemModel`, `MetricItemModel`,
        `SubtotalModel`, or `SectionModel` for nested sections).

        Args:
            item_model: The Pydantic model representing a line item, calculated
                item, metric item, subtotal, or nested section.

        Returns:
            The corresponding `StatementStructure` component (`LineItem`,
            `CalculatedLineItem`, `MetricLineItem`, `SubtotalLineItem`, or `Section`).

        Raises:
            ConfigurationError: If an unknown or unexpected model type is
                encountered.
        """
        # Dispatch by model instance type
        if isinstance(item_model, SectionModel):
            # Handle nested sections directly
            return self._build_section_model(item_model)
        if isinstance(item_model, LineItemModel):
            return LineItem(
                id=item_model.id,
                name=item_model.name,
                node_id=item_model.node_id,
                description=item_model.description,
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
            )
        if isinstance(item_model, CalculatedItemModel):
            # Pass the calculation model directly or its dict representation
            return CalculatedLineItem(
                id=item_model.id,
                name=item_model.name,
                # Pass the nested Pydantic model if structure expects it, else dump
                calculation=item_model.calculation.model_dump(), # Assuming structure expects dict
                description=item_model.description,
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
            )
        if isinstance(item_model, MetricItemModel):
            return MetricLineItem(
                id=item_model.id,
                name=item_model.name,
                metric_id=item_model.metric_id,
                inputs=item_model.inputs,
                description=item_model.description,
                sign_convention=item_model.sign_convention,
                metadata=item_model.metadata,
            )
        if isinstance(item_model, SubtotalModel):
            return self._build_subtotal_model(item_model)

        # Should be unreachable if Pydantic validation works
        logger.error(f"Encountered unknown item model type during build: {type(item_model).__name__}")
        raise ConfigurationError(
            message=f"Unknown item model type: {type(item_model).__name__}",
            errors=[f"Item '{getattr(item_model, 'id', '<unknown>')}' has invalid model type."],
        )

    def _build_subtotal_model(self, subtotal_model: SubtotalModel) -> SubtotalLineItem:
        """Build a `SubtotalLineItem` object from a `SubtotalModel`.

        Extracts the relevant item IDs to be summed, either from the explicit
        `items_to_sum` list or from the `calculation.inputs` if provided.

        Args:
            subtotal_model: The Pydantic model representing the subtotal configuration.

        Returns:
            A `SubtotalLineItem` instance.
        """
        # Consolidate logic for getting item IDs
        item_ids = (
            subtotal_model.calculation.inputs
            if subtotal_model.calculation and subtotal_model.calculation.inputs
            else subtotal_model.items_to_sum
        )
        if not item_ids:
             logger.warning(f"Subtotal '{subtotal_model.id}' has no items_to_sum or calculation inputs defined.")
             # Decide handling: error or allow empty subtotal?
             # Allowing for now, may need adjustment based on desired behavior.

        return SubtotalLineItem(
            id=subtotal_model.id,
            name=subtotal_model.name,
            item_ids=item_ids or [], # Ensure it's a list
            description=subtotal_model.description,
            sign_convention=subtotal_model.sign_convention,
            metadata=subtotal_model.metadata,
        )

# --- END FILE: fin_statement_model/statements/builder.py ---

# --- START FILE: fin_statement_model/statements/config/config.py ---
"""Statement configuration handling for Financial Statement Model.

This module provides utilities for parsing and validating statement configuration data
(provided as a dictionary) and building StatementStructure objects.
"""

# Removed json, yaml, Path imports as file loading moved to IO
import logging
from typing import Any, Optional

# Use absolute imports
# Import Pydantic models for building from validated configuration
from fin_statement_model.statements.config.models import (
    StatementModel,
)
from pydantic import ValidationError # Import directly

# Configure logging
logger = logging.getLogger(__name__)


class StatementConfig:
    """Manages configuration parsing and building for financial statement structures.

    This class handles validating statement configuration data (provided as a dictionary)
    and building StatementStructure objects from these configurations.
    It does NOT handle file loading.
    """

    def __init__(self, config_data: dict[str, Any]):
        """Initialize a statement configuration processor.

        Args:
            config_data: Dictionary containing the raw configuration data.

        Raises:
            ValueError: If config_data is not a non-empty dictionary.
        """
        if not config_data or not isinstance(config_data, dict):
             raise ValueError("config_data must be a non-empty dictionary.")
        self.config_data = config_data
        # Remove config_path attribute
        # self.config_path = None # No longer needed
        self.model: Optional[StatementModel] = None # Store validated model

    # Removed load_config method
    # def load_config(self, config_path: str) -> None:
    #     ...

    def validate_config(self) -> list[str]:
        """Validate the configuration data using Pydantic models.

        Returns:
            list[str]: List of validation errors, or empty list if valid.
                     Stores the validated model in self.model on success.
        """
        try:
            # Validate against Pydantic StatementModel
            # Removed redundant import from inside method
            self.model = StatementModel.model_validate(self.config_data)
            return []
        except ValidationError as ve:
            # Convert Pydantic errors to list of strings
            errors: list[str] = []
            for err in ve.errors():
                loc = ".".join(str(x) for x in err.get("loc", []))
                msg = err.get("msg", "")
                errors.append(f"{loc}: {msg}")
            self.model = None # Ensure model is not set on validation error
            return errors
        except Exception as e:
            # Catch other potential validation issues
            logger.exception("Unexpected error during configuration validation")
            self.model = None
            return [f"Unexpected validation error: {e}"]

    # Removed build_statement_structure and helper methods (_build_section_model, etc.)
    # This logic is now moved to StatementStructureBuilder

    # def build_statement_structure(self) -> StatementStructure:
    #     ...
    #
    # def _build_section_model(self, section_model: SectionModel) -> Section:
    #     ...
    #
    # def _build_item_model(...):
    #     ...
    #
    # def _build_subtotal_model(...):
    #     ...

# --- END FILE: fin_statement_model/statements/config/config.py ---

# --- START FILE: fin_statement_model/statements/config/mappings/__init__.py ---
"""Configuration mappings for statement importers and extensions."""

# --- END FILE: fin_statement_model/statements/config/mappings/__init__.py ---

# --- START FILE: fin_statement_model/statements/config/models.py ---
"""Define Pydantic models for statement configuration.

This module defines Pydantic models for validating statement configuration data,
including statements, sections, line items, calculations, and subtotals.
"""

from __future__ import annotations

from typing import Any, Optional, Union, Literal

from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator


class CalculationSpec(BaseModel):
    """Define a calculation specification.

    Args:
        type: Type identifier for the calculation (e.g., 'addition', 'subtraction').
        inputs: List of input node or line item IDs referenced by this calculation.
    """
    type: str = Field(
        ...,
        description="Type identifier for the calculation (e.g., 'addition', 'subtraction').",
    )
    inputs: list[str] = Field(
        ...,
        description="List of input node or line item IDs referenced by this calculation.",
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class BaseItemModel(BaseModel):
    """Define common fields for all statement items.

    Args:
        id: Unique identifier for the item. Must not contain spaces.
        name: Human-readable name of the item.
        description: Optional description for the item.
        metadata: Optional metadata dictionary for the item.
        sign_convention: Sign convention for the item (1 or -1).
    """
    id: str = Field(
        ...,
        description="Unique identifier for the item. Must not contain spaces.",
    )
    name: str = Field(..., description="Human-readable name of the item.")
    description: Optional[str] = Field(
        "", description="Optional description for the item."
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Optional metadata for the item."
    )
    sign_convention: int = Field(
        1, description="Sign convention for the item (1 or -1)."
    )

    @field_validator("id", mode="before")
    def id_must_not_contain_spaces(cls, value: str) -> str:
        """Ensure that 'id' does not contain spaces."""
        if " " in value:
            raise ValueError("must not contain spaces")
        return value

    model_config = ConfigDict(extra="forbid", frozen=True)


class LineItemModel(BaseItemModel):
    """Define a basic line item configuration model.

    Args:
        type: Must be 'line_item' for this model.
        node_id: ID of the core node this line item maps to.
    """
    type: Literal["line_item"] = Field(
        "line_item", description="Discriminator for basic line items."
    )
    node_id: str = Field(
        ..., description="ID of the core node this line item maps to."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class MetricItemModel(BaseItemModel):
    """Define a metric-based line item configuration model.

    Args:
        type: Must be 'metric' for this model.
        metric_id: ID of the metric in the core registry.
        inputs: Mapping of metric input names to statement item IDs.
    """
    type: Literal["metric"] = Field(
        "metric", description="Discriminator for metric-based items."
    )
    metric_id: str = Field(
        ..., description="ID of the metric in the core.metrics.registry."
    )
    inputs: dict[str, str] = Field(
        ...,
        description="Mapping of metric input names to statement item IDs."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class CalculatedItemModel(BaseItemModel):
    """Define a calculated line item configuration model.

    Args:
        type: Must be 'calculated' for this model.
        calculation: Calculation specification for the calculated item.
    """
    type: Literal["calculated"] = Field(
        "calculated", description="Discriminator for calculated items."
    )
    calculation: CalculationSpec = Field(
        ..., description="Calculation specification for the calculated item."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)


class SubtotalModel(BaseItemModel):
    """Define a subtotal configuration model.

    Args:
        type: Must be 'subtotal' for this model.
        calculation: Optional calculation specification for the subtotal.
        items_to_sum: Optional list of item IDs to sum for the subtotal.
    """
    type: Literal["subtotal"] = Field(
        "subtotal", description="Discriminator for subtotal items."
    )
    calculation: Optional[CalculationSpec] = Field(
        None, description="Calculation specification for the subtotal."
    )
    items_to_sum: Optional[list[str]] = Field(
        None, description="List of item IDs to sum for the subtotal."
    )

    @model_validator(mode="before")
    def exactly_one_of_calculation_or_items(
        cls, values: dict[str, Any]
    ) -> dict[str, Any]:
        """Ensure exactly one of 'calculation' or 'items_to_sum' is provided."""
        calc, items = values.get("calculation"), values.get("items_to_sum")
        if bool(calc) == bool(items):
            raise ValueError(
                "must provide exactly one of 'calculation' or 'items_to_sum'"
            )
        return values

    model_config = ConfigDict(extra="forbid", frozen=True)


class SectionModel(BaseItemModel):
    """Define a nested section within the statement configuration.

    Args:
        type: Must be 'section' for this model.
        items: List of line items, calculated items, subtotals, or nested sections.
        subsections: List of nested sections.
        subtotal: Optional subtotal configuration for this section.
    """
    type: Literal["section"] = Field(
        "section", description="Discriminator for nested sections."
    )
    items: list[
        Union[
            LineItemModel,
            CalculatedItemModel,
            MetricItemModel,
            SubtotalModel,
            SectionModel,
        ]
    ] = Field(
        default_factory=list,
        description=(
            "List of line items, calculated items, subtotals, "
            "or nested sections."
        ),
    )
    subsections: list[SectionModel] = Field(
        default_factory=list,
        description="List of nested sections.",
    )
    subtotal: Optional[SubtotalModel] = Field(
        None, description="Optional subtotal configuration for this section."
    )

    model_config = ConfigDict(extra="forbid", frozen=True)

    @model_validator(mode="after")
    def check_unique_item_ids(cls, section: SectionModel) -> SectionModel:
        """Ensure that item and subsection IDs within a section are unique and subtotal refs valid."""
        ids = [item.id for item in section.items] + [sub.id for sub in section.subsections]
        duplicates = {item_id for item_id in ids if ids.count(item_id) > 1}
        if duplicates:
            raise ValueError(
                f"Duplicate item id(s) in section '{section.id}': {', '.join(duplicates)}"
            )
        if section.subtotal and section.subtotal.items_to_sum is not None:
            valid_ids = [item.id for item in section.items]
            missing = [i for i in section.subtotal.items_to_sum if i not in valid_ids]
            if missing:
                raise ValueError(
                    f"Section '{section.id}' subtotal references undefined ids: {', '.join(missing)}"
                )
        return section


SectionModel.model_rebuild(force=True)


class StatementModel(BaseModel):
    """Define the top-level statement configuration model.

    Args:
        id: Unique identifier for the statement. Must not contain spaces.
        name: Human-readable name of the statement.
        description: Optional description of the statement.
        metadata: Optional metadata dictionary.
        sections: List of top-level sections in the statement.
    """
    id: str = Field(
        ..., description="Unique statement identifier. Must not contain spaces."
    )
    name: str = Field(..., description="Human-readable statement name.")
    description: Optional[str] = Field(
        "", description="Optional statement description."
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Optional metadata dictionary."
    )
    sections: list[SectionModel] = Field(
        ..., description="List of top-level sections in the statement."
    )

    @field_validator("id", mode="before")
    def id_must_not_contain_spaces(cls, value: str) -> str:
        """Ensure that statement 'id' does not contain spaces."""
        if " " in value:
            raise ValueError("must not contain spaces")
        return value

    model_config = ConfigDict(extra="forbid", frozen=True)

    @model_validator(mode="after")
    def check_unique_section_ids(cls, model: StatementModel) -> StatementModel:
        """Ensure that top-level section IDs are unique."""
        ids = [section.id for section in model.sections]
        duplicates = {sec_id for sec_id in ids if ids.count(sec_id) > 1}
        if duplicates:
            raise ValueError(f"Duplicate section id(s): {', '.join(duplicates)}")
        return model

# --- END FILE: fin_statement_model/statements/config/models.py ---

# --- START FILE: fin_statement_model/statements/errors.py ---
"""Custom Exception classes for the `fin_statement_model.statements` package.

These exceptions provide more specific error information related to statement
definition, configuration, building, and processing, inheriting from the base
`FinancialModelError` defined in `fin_statement_model.core.errors`.
"""

from typing import Optional
from fin_statement_model.core.errors import FinancialModelError

__all__ = ["ConfigurationError", "StatementError"]

class StatementError(FinancialModelError):
    """Base exception for errors specific to the statements package.

    Indicates a general issue related to statement structure, processing, or
    management (e.g., duplicate registration, invalid item type).
    """


class ConfigurationError(StatementError):
    """Exception raised for errors during statement configuration processing.

    This includes errors encountered while loading, parsing, validating, or
    building statement structures from configuration files (e.g., YAML).

    Attributes:
        message (str): The main error message summarizing the issue.
        config_path (Optional[str]): The path to the configuration file that
            caused the error, if applicable.
        errors (List[str]): A list of specific validation errors or details
            related to the configuration issue.
    """

    def __init__(
        self,
        message: str,
        config_path: Optional[str] = None,
        errors: Optional[list[str]] = None,
    ):
        """Initialize a ConfigurationError.

        Args:
            message: The primary error message describing the configuration issue.
            config_path: Optional path to the configuration file involved.
            errors: Optional list of specific validation errors or related details.
        """
        self.message = message
        self.config_path = config_path
        self.errors = errors or []

        # Build detailed error message
        details = []
        if config_path:
            details.append(f"Config file: {config_path}")
        if errors:
            details.append("Validation errors:")
            details.extend([f"  - {error}" for error in errors])

        full_message = message
        if details:
            full_message = f"{message}\n" + "\n".join(details)

        super().__init__(full_message)

# --- END FILE: fin_statement_model/statements/errors.py ---

# --- START FILE: fin_statement_model/statements/factory.py ---
"""Statement Factory/Processor module.

Provides high-level functions to orchestrate the process of loading statement
configurations, building structures, populating a financial model graph, and
generating formatted outputs (like DataFrames or files).

This module acts as a primary entry point for users wanting to work with
financial statements defined by configuration files.
"""

import logging
from pathlib import Path
from typing import Any, Union, Optional

import pandas as pd

# Core components
from ..core.graph import Graph
from ..core.errors import FinancialModelError, StatementError, ConfigurationError

# IO Layer components
from fin_statement_model.io.readers import statement_config_reader
from fin_statement_model.io.writers import statement_writer
from fin_statement_model.io.exceptions import ReadError, WriteError

# Statements layer components
from .structure import StatementStructure, Section, StatementItem, SubtotalLineItem, CalculatedLineItem
from .config.models import StatementModel # Import the Pydantic model
from .config.config import StatementConfig
from fin_statement_model.statements.registry import StatementRegistry
from fin_statement_model.statements.builder import StatementStructureBuilder
from fin_statement_model.statements.populator import populate_graph_from_statement
from fin_statement_model.statements.formatter import StatementFormatter

logger = logging.getLogger(__name__)

__all__ = [
    "create_statement_dataframe",
    "export_statements_to_excel",
    "export_statements_to_json",
    # Removed StatementFactory class, using functions now
]


def _load_build_register_statements(
    config_path_or_dir: str,
    registry: StatementRegistry,
    builder: StatementStructureBuilder
) -> list[str]:
    """Load, validate, build, and register statement structures from configs.

    This is an internal helper function orchestrating the first part of the
    statement processing pipeline. It reads configurations, validates them using
    StatementConfig, builds the structure using StatementStructureBuilder, and
    registers them with the provided StatementRegistry.

    Args:
        config_path_or_dir: Path to a single statement config file (e.g.,
            'income_statement.yaml') or a directory containing multiple
            config files.
        registry: The StatementRegistry instance to register loaded statements.
        builder: The StatementStructureBuilder instance used to construct
            statement objects from validated configurations.

    Returns:
        A list of statement IDs that were successfully loaded and registered.

    Raises:
        ConfigurationError: If reading or validation of any configuration fails.
        FileNotFoundError: If the `config_path_or_dir` does not exist.
        StatementError: If registration fails (e.g., duplicate ID).
    """
    loaded_statement_ids = []
    errors = []

    try:
        if Path(config_path_or_dir).is_dir():
            raw_configs = statement_config_reader.read_statement_configs_from_directory(config_path_or_dir)
        elif Path(config_path_or_dir).is_file():
            stmt_id = Path(config_path_or_dir).stem
            raw_config = statement_config_reader.read_statement_config_from_path(config_path_or_dir)
            raw_configs = {stmt_id: raw_config}
        else:
            raise FileNotFoundError(f"Config path is not a valid file or directory: {config_path_or_dir}")

    except (ReadError, FileNotFoundError) as e:
        logger.exception(f"Failed to read configuration from {config_path_or_dir}:")
        # Re-raise as a ConfigurationError maybe?
        raise ConfigurationError(message=f"Failed to read config: {e}", config_path=config_path_or_dir) from e

    if not raw_configs:
        logger.warning(f"No statement configurations found at {config_path_or_dir}")
        return []

    for stmt_id, raw_data in raw_configs.items():
        try:
            config = StatementConfig(raw_data)
            validation_errors = config.validate_config()
            if validation_errors:
                raise ConfigurationError(
                    f"Invalid configuration for statement '{stmt_id}'",
                    config_path=f"{config_path_or_dir}/{stmt_id}.ext", # Placeholder path
                    errors=validation_errors
                )

            statement = builder.build(config)
            registry.register(statement) # Raises StatementError on conflict
            loaded_statement_ids.append(statement.id)

        except (ConfigurationError, StatementError, ValueError) as e:
            logger.exception(f"Failed to process/register statement '{stmt_id}':")
            errors.append((stmt_id, str(e)))
        except Exception as e:
            logger.exception(f"Unexpected error processing statement '{stmt_id}' from {config_path_or_dir}")
            errors.append((stmt_id, f"Unexpected error: {e!s}"))

    # Handle errors - maybe raise an aggregate error if any occurred?
    if errors:
        # For now, just log a warning, processing continues with successfully
        # loaded statements
        error_details = "; ".join([f"{sid}: {msg}" for sid, msg in errors])
        logger.warning(
            f"Encountered {len(errors)} errors during statement loading/building "
            f"from {config_path_or_dir}: {error_details}"
        )
        # Consider raising an aggregated error if needed for stricter handling

    return loaded_statement_ids


def _populate_graph(registry: StatementRegistry, graph: Graph) -> list[tuple[str, str]]:
    """Populate the graph with nodes based on registered statements.

    Internal helper function that iterates through all statements registered
    in the `registry` and uses `populate_graph_from_statement` to add the
    corresponding nodes and relationships to the `graph`.

    Args:
        registry: The StatementRegistry containing the statements to process.
        graph: The Graph instance to be populated.

    Returns:
        A list of tuples, where each tuple contains (item_id, error_message)
        for any items that failed during population. Returns an empty list if
        population was successful for all items.
    """
    all_populator_errors = []
    statements = registry.get_all_statements()
    if not statements:
        logger.warning("No statements registered to populate the graph.")
        return []

    for statement in statements:
        populator_errors = populate_graph_from_statement(statement, graph)
        if populator_errors:
            all_populator_errors.extend([(statement.id, item_id, msg) for item_id, msg in populator_errors])

    if all_populator_errors:
        logger.warning(f"Encountered {len(all_populator_errors)} errors during graph population.")
        # Log details if needed: logger.warning(f"Population errors: {all_populator_errors}")

    return [(item_id, msg) for stmt_id, item_id, msg in all_populator_errors] # Return simplified list


def create_statement_dataframe(
    graph: Graph,
    config_path_or_dir: str,
    format_kwargs: Optional[dict[str, Any]] = None,
) -> Union[pd.DataFrame, dict[str, pd.DataFrame]]:
    r"""Load config(s), build structure(s), populate graph, format as DataFrame(s).

    This function orchestrates the entire process of turning statement
    configuration files into pandas DataFrames containing the calculated or
    retrieved financial data.

    It performs the following steps:
    1. Loads configuration(s) from the specified path or directory.
    2. Validates the configuration(s).
    3. Builds the internal statement structure(s).
    4. Registers the structure(s).
    5. Populates the provided `graph` with nodes based on the statement(s).
       (Assumes the graph might already contain necessary data nodes or will
       fetch them).
    6. Formats the statement data from the graph into pandas DataFrame(s).

    Args:
        graph: The core.graph.Graph instance to use and populate. This graph
            should ideally contain the necessary base data nodes (e.g.,
            actuals) before calling this function, or nodes should be capable
            of fetching their data.
        config_path_or_dir: Path to a single statement config file (e.g.,
            './configs/income_statement.yaml') or a directory containing
            multiple config files (e.g., './configs/').
        format_kwargs: Optional dictionary of keyword arguments passed directly
            to the `StatementFormatter.generate_dataframe` method. This can
            be used to control aspects like date ranges, periods, or number
            formatting. See `StatementFormatter` documentation for details.

    Returns:
        If `config_path_or_dir` points to a single file, returns a single
        pandas DataFrame representing that statement.
        If `config_path_or_dir` points to a directory, returns a dictionary
        mapping statement IDs (derived from filenames) to their corresponding
        pandas DataFrames.

    Raises:
        ConfigurationError: If loading or validating configurations fails.
        StatementError: If registering statements fails or if no valid
            statements can be processed.
        FileNotFoundError: If `config_path_or_dir` does not exist or is not a
            valid file or directory.
        FinancialModelError: Potentially other errors from graph operations
            during population or formatting.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> # Assume 'my_graph' is a pre-populated Graph instance
        >>> # Assume 'configs/income_stmt.yaml' defines an income statement
        >>> try:
        ...     income_df = create_statement_dataframe(
        ...         graph=my_graph,
        ...         config_path_or_dir='configs/income_stmt.yaml',
        ...         format_kwargs={'periods': ['2023Q1', '2023Q2']}
        ...     )
        ...     # In real code, use logger.debug or logger.info
        ...     logger.debug(f"Income DataFrame head:\n{income_df.head()}")
        ... except FileNotFoundError:
        ...     # Use logger.error or logger.warning
        ...     logger.error("Config file not found.")
        ... except (ConfigurationError, StatementError) as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Error processing statement: {e}")

        >>> # Process all configs in a directory
        >>> try:
        ...     all_statements = create_statement_dataframe(
        ...         graph=my_graph,
        ...         config_path_or_dir='configs/'
        ...     )
        ...     balance_sheet_df = all_statements.get('balance_sheet')
        ...     if balance_sheet_df is not None:
        ...         # Use logger.info
        ...         logger.info("Balance Sheet DataFrame created.")
        ... except FileNotFoundError:
        ...     # Use logger.error or logger.warning
        ...     logger.error("Config directory not found.")
        ... except StatementError as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Error processing statements: {e}")
    """
    registry = StatementRegistry()
    builder = StatementStructureBuilder()
    format_kwargs = format_kwargs or {}

    # Step 1: Load, Build, Register
    loaded_ids = _load_build_register_statements(config_path_or_dir, registry, builder)
    if not loaded_ids:
        raise StatementError(f"No valid statements could be loaded from {config_path_or_dir}")

    # Step 2: Populate Graph (handles errors internally, logs warnings)
    _populate_graph(registry, graph)

    # Step 3: Format results
    results: dict[str, pd.DataFrame] = {}
    formatting_errors = []
    for stmt_id in loaded_ids:
        statement = registry.get(stmt_id)
        if not statement:
             logger.error(f"Internal error: Statement '{stmt_id}' was loaded but not found in registry.")
             formatting_errors.append((stmt_id, "Statement not found in registry after loading"))
             continue
        try:
            formatter = StatementFormatter(statement)
            df = formatter.generate_dataframe(graph, **format_kwargs)
            results[stmt_id] = df
        except Exception as e:
            logger.exception(f"Failed to format statement '{stmt_id}'")
            formatting_errors.append((stmt_id, f"Formatting error: {e!s}"))

    if formatting_errors:
        # Decide policy: raise error, or return partial results?
        # For now, log warning and return what succeeded.
        logger.warning(f"Encountered {len(formatting_errors)} errors during formatting.")

    # Return single DF or Dict based on input type
    is_single_file = Path(config_path_or_dir).is_file()
    if is_single_file and len(results) == 1:
        return next(iter(results.values()))
    elif is_single_file and not results:
         raise StatementError(f"Failed to generate DataFrame for statement from file: {config_path_or_dir}")
    else:
        # Return dict for directory input, or if multiple results came from single file (unexpected)
        return results


def _export_statements(
    graph: Graph,
    config_path_or_dir: str,
    output_dir: str,
    format_kwargs: Optional[dict[str, Any]],
    writer_func: callable, # Function to call for writing (e.g., write_to_excel)
    writer_kwargs: Optional[dict[str, Any]],
    file_suffix: str, # e.g., ".xlsx" or ".json"
) -> None:
    """Generate and export statement DataFrames using a specific writer function.

    Internal helper function that takes generated DataFrames (or generates them
    if needed via `create_statement_dataframe`) and uses the provided
    `writer_func` to save them to disk.

    Args:
        graph: The core.graph.Graph instance.
        config_path_or_dir: Path to config file or directory.
        output_dir: Directory where output files will be saved.
        format_kwargs: Optional arguments for `create_statement_dataframe`.
        writer_func: The function responsible for writing a DataFrame to a file
            (e.g., `statement_writer.write_statement_to_excel`).
        writer_kwargs: Optional arguments passed directly to the `writer_func`.
        file_suffix: The file extension to use for output files (e.g., ".xlsx").

    Raises:
        WriteError: If any errors occur during the file writing process.
        FinancialModelError: If errors occur during DataFrame generation.
        FileNotFoundError: If config path doesn't exist.
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    try:
        dfs = create_statement_dataframe(graph, config_path_or_dir, format_kwargs)
    except FinancialModelError:
         logger.exception("Failed to generate statement dataframes for export:")
         raise # Re-raise critical errors from generation step

    if not dfs:
        logger.warning(f"No DataFrames generated, nothing to export to {file_suffix} files.")
        return

    # Standardize to dictionary format
    if isinstance(dfs, pd.DataFrame):
        # Try to get a meaningful name if it was a single file
        stmt_id = Path(config_path_or_dir).stem if Path(config_path_or_dir).is_file() else "statement"
        dfs_dict = {stmt_id: dfs}
    else:
        dfs_dict = dfs

    export_errors = []
    for stmt_id, df in dfs_dict.items():
        # Ensure stmt_id is filename-safe (basic replacement)
        safe_stmt_id = stmt_id.replace("/", "_").replace("\\", "_")
        file_path = output_path / f"{safe_stmt_id}{file_suffix}"
        try:
            writer_func(df, str(file_path), **writer_kwargs)
            logger.info(f"Successfully exported statement '{stmt_id}' to {file_path}")
        except WriteError as e:
            logger.exception(f"Failed to write {file_suffix} file for statement '{stmt_id}':")
            export_errors.append((stmt_id, str(e)))
        except Exception as e:
             logger.exception(f"Unexpected error exporting statement '{stmt_id}' to {file_suffix}.")
             export_errors.append((stmt_id, f"Unexpected export error: {e!s}"))

    if export_errors:
        error_summary = "; ".join([f"{sid}: {err}" for sid, err in export_errors])
        raise WriteError(f"Encountered {len(export_errors)} errors during {file_suffix} export: {error_summary}")


def export_statements_to_excel(
    graph: Graph,
    config_path_or_dir: str,
    output_dir: str,
    format_kwargs: Optional[dict[str, Any]] = None,
    writer_kwargs: Optional[dict[str, Any]] = None,
) -> None:
    """Generate statement DataFrames and export them to individual Excel files.

    Loads configurations, builds statements, populates the graph (if necessary),
    generates DataFrames for each statement, and saves each DataFrame to a
    separate `.xlsx` file in the specified `output_dir`.

    Args:
        graph: The core.graph.Graph instance containing necessary data.
        config_path_or_dir: Path to a single statement config file or a
            directory containing multiple config files.
        output_dir: The directory where the resulting Excel files will be saved.
            File names will be derived from the statement IDs (e.g.,
            'income_statement.xlsx').
        format_kwargs: Optional dictionary of arguments passed to
            `StatementFormatter.generate_dataframe` when creating the DataFrames.
        writer_kwargs: Optional dictionary of arguments passed to the underlying
            Excel writer (`io.writers.write_statement_to_excel`), such as
            `sheet_name` or engine options.

    Raises:
        ConfigurationError: If loading or validating configurations fails.
        StatementError: If processing statements fails critically.
        FileNotFoundError: If `config_path_or_dir` is not found.
        WriteError: If writing any of the Excel files fails.
        FinancialModelError: Potentially other errors from graph operations.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> # Assume 'my_graph' is a pre-populated Graph instance
        >>> # Assume configs exist in './statement_configs/'
        >>> try:
        ...     export_statements_to_excel(
        ...         graph=my_graph,
        ...         config_path_or_dir='./statement_configs/',
        ...         output_dir='./output_excel/',
        ...         writer_kwargs={'freeze_panes': (1, 1)} # Freeze header row/col
        ...     )
        ...     # Use logger.info
        ...     logger.info("Statements exported to ./output_excel/")
        ... except (FileNotFoundError, ConfigurationError, StatementError, WriteError) as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Export failed: {e}")
    """
    _export_statements(
        graph=graph,
        config_path_or_dir=config_path_or_dir,
        output_dir=output_dir,
        format_kwargs=format_kwargs or {},
        writer_func=statement_writer.write_statement_to_excel,
        writer_kwargs=writer_kwargs or {},
        file_suffix=".xlsx",
    )


def export_statements_to_json(
    graph: Graph,
    config_path_or_dir: str,
    output_dir: str,
    format_kwargs: Optional[dict[str, Any]] = None,
    writer_kwargs: Optional[dict[str, Any]] = None,
) -> None:
    """Generate statement DataFrames and export them to individual JSON files.

    Loads configurations, builds statements, populates the graph (if necessary),
    generates DataFrames for each statement, and saves each DataFrame to a
    separate `.json` file in the specified `output_dir`.

    Args:
        graph: The core.graph.Graph instance containing necessary data.
        config_path_or_dir: Path to a single statement config file or a
            directory containing multiple config files.
        output_dir: The directory where the resulting JSON files will be saved.
            File names will be derived from the statement IDs (e.g.,
            'balance_sheet.json').
        format_kwargs: Optional dictionary of arguments passed to
            `StatementFormatter.generate_dataframe` when creating the DataFrames.
        writer_kwargs: Optional dictionary of arguments passed to the underlying
            JSON writer (`io.writers.write_statement_to_json`). Common options
            include `orient` (e.g., 'records', 'columns', 'split') and `indent`.
            Defaults to 'records' orient and indent=2 if not provided.

    Raises:
        ConfigurationError: If loading or validating configurations fails.
        StatementError: If processing statements fails critically.
        FileNotFoundError: If `config_path_or_dir` is not found.
        WriteError: If writing any of the JSON files fails.
        FinancialModelError: Potentially other errors from graph operations.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> # Assume 'my_graph' is a pre-populated Graph instance
        >>> # Assume configs exist in './statement_configs/'
        >>> try:
        ...     export_statements_to_json(
        ...         graph=my_graph,
        ...         config_path_or_dir='./statement_configs/',
        ...         output_dir='./output_json/',
        ...         writer_kwargs={'orient': 'split', 'indent': 4}
        ...     )
        ...     # Use logger.info
        ...     logger.info("Statements exported to ./output_json/")
        ... except (FileNotFoundError, ConfigurationError, StatementError, WriteError) as e:
        ...     # Use logger.error or logger.exception
        ...     logger.error(f"Export failed: {e}")
    """
    final_writer_kwargs = writer_kwargs or {}
    # Set JSON specific defaults if not provided
    final_writer_kwargs.setdefault("orient", "records")
    final_writer_kwargs.setdefault("indent", 2)

    _export_statements(
        graph=graph,
        config_path_or_dir=config_path_or_dir,
        output_dir=output_dir,
        format_kwargs=format_kwargs or {},
        writer_func=statement_writer.write_statement_to_json,
        writer_kwargs=final_writer_kwargs,
        file_suffix=".json",
    )

# Removed create_statement_json as DataFrame approach is used for consistency now.
# If direct dict export is needed, add a separate function.
# def create_statement_json(...):
#     ... # Logic to fetch raw data dict using formatter._fetch_data_from_graph

# --- END FILE: fin_statement_model/statements/factory.py ---

# --- START FILE: fin_statement_model/statements/formatter/__init__.py ---
"""Formatter package for financial statements.

This package provides tools for formatting financial statements for display
and reporting, with controls for formatting, styling, and presentation.
"""

from .formatter import StatementFormatter

__all__ = ["StatementFormatter"]

# --- END FILE: fin_statement_model/statements/formatter/__init__.py ---

# --- START FILE: fin_statement_model/statements/formatter/_formatting_utils.py ---
"""Utility functions for formatting statement DataFrames."""

import pandas as pd
from typing import Optional, Any # Keep necessary imports
from pandas.api.types import is_numeric_dtype

def apply_sign_convention(df: pd.DataFrame, period_columns: list[str]) -> pd.DataFrame:
    """Apply sign conventions to the statement values across periods."""
    result = df.copy()
    if "sign_convention" in result.columns:
        for col in period_columns:
            if col in result.columns and is_numeric_dtype(result[col]):
                mask = result[col].notna()
                # Ensure sign_convention is treated as numeric if needed
                sign_col = pd.to_numeric(result.loc[mask, "sign_convention"], errors="coerce").fillna(1)
                result.loc[mask, col] = (
                    result.loc[mask, col] * sign_col
                )
    return result

def format_numbers(
    df: pd.DataFrame,
    default_formats: dict[str, Any], # Pass defaults needed
    number_format: Optional[str] = None,
    period_columns: Optional[list[str]] = None
) -> pd.DataFrame:
    """Format numeric values in the statement.

    Args:
        df: DataFrame to format numbers in
        default_formats: Dictionary containing default formatting options
                         (e.g., 'precision', 'use_thousands_separator').
        number_format: Optional format string
        period_columns: List of columns containing period data to format.
                        If None, attempts to format all numeric columns
                        except metadata/indicators.

    Returns:
        pd.DataFrame: DataFrame with formatted numbers
    """
    result = df.copy()

    if period_columns:
        numeric_cols = [col for col in period_columns if col in result.columns and is_numeric_dtype(result[col])]
    else:
         # Original logic if period_columns not specified
        numeric_cols = [
            col
            for col in result.columns
            if is_numeric_dtype(result[col])
            and col not in ("sign_convention", "depth", "ID") # Added ID
            and not col.startswith("meta_")
            and col != "Line Item" # Ensure Line Item name is not formatted
        ]

    # Get defaults from the passed dictionary
    precision = default_formats.get("precision", 2) # Provide fallback default
    use_thousands = default_formats.get("use_thousands_separator", True)

    if number_format:
        # Use provided format string
        for col in numeric_cols:
            # Check if column exists before applying format
            if col in result.columns:
                result[col] = result[col].apply(
                    lambda x: f"{x:{number_format}}" if pd.notna(x) else ""
                )
    else:
        # Use default formatting based on passed defaults
        for col in numeric_cols:
             # Check if column exists before applying format
            if col in result.columns:
                result[col] = result[col].apply(
                    lambda x: (
                        (f"{x:,.{precision}f}" if pd.notna(x) else "")
                        if use_thousands
                        else (f"{x:.{precision}f}" if pd.notna(x) else "")
                    )
                )

    return result

# --- END FILE: fin_statement_model/statements/formatter/_formatting_utils.py ---

# --- START FILE: fin_statement_model/statements/formatter/formatter.py ---
"""Formatter for financial statements.

This module provides functionality for formatting financial statements
for display or reporting, including applying formatting rules, adding subtotals,
and applying sign conventions.
"""

import pandas as pd
import numpy as np # Added numpy for NaN handling
from typing import Optional, Any, Union, Callable # Updated imports, added Callable
import logging

from fin_statement_model.statements.structure import StatementStructure
from fin_statement_model.statements.structure import (
    Section,
    LineItem, # Added LineItem
    CalculatedLineItem,
    SubtotalLineItem,
    StatementItem,
)

# Add core Graph and errors
from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import NodeError, CalculationError

# Import the new formatting utils
from ._formatting_utils import format_numbers
from ._formatting_utils import apply_sign_convention as apply_sign_convention_func

# Configure logging
logger = logging.getLogger(__name__)


def _fetch_data_from_graph(statement: StatementStructure, graph: Graph) -> dict[str, dict[str, float]]:
    """Fetch necessary node data from the graph for the statement structure."""
    data: dict[str, dict[str, float]] = {}
    all_items = statement.get_all_items() # Method to get all relevant items
    periods = graph.periods

    if not periods:
        logger.warning(f"Graph has no periods defined. Cannot fetch data for statement '{statement.id}'.")
        return {}

    logger.debug(f"Fetching data for statement '{statement.id}' across periods: {periods}")

    processed_node_ids = set()

    for item in all_items:
        node_id = None
        if isinstance(item, LineItem):
            node_id = item.node_id
        elif isinstance(item, (CalculatedLineItem, SubtotalLineItem)):
            # Calculation/Subtotal items also represent nodes in the graph
            node_id = item.id

        if node_id and node_id not in processed_node_ids:
            processed_node_ids.add(node_id)
            # Check if node exists in graph before trying to calculate
            if graph.has_node(node_id):
                values = {}
                for period in periods:
                    try:
                        # Use graph.calculate which handles caching and errors
                        value = graph.calculate(node_id, period)
                        # Ensure value is float or NaN, handle potential None/other types if necessary
                        values[period] = float(value) if pd.notna(value) else np.nan
                    except (NodeError, CalculationError) as e:
                        logger.warning(f"Error calculating node '{node_id}' for period '{period}': {e}. Setting value to NaN.")
                        values[period] = np.nan
                    except Exception as e:
                        logger.error(f"Unexpected error calculating node '{node_id}' for period '{period}': {e}. Setting value to NaN.", exc_info=True)
                        values[period] = np.nan
                data[node_id] = values
            else:
                logger.warning(f"Node '{node_id}' defined in statement structure '{statement.id}' but not found in graph. Skipping data fetch.")
                # Optionally fill with NaNs if needed for formatting
                # data[node_id] = {period: np.nan for period in periods}

    logger.debug(f"Finished fetching data for {len(data)} nodes for statement '{statement.id}'.")
    return data


# Helper class for processing the statement structure
class _StructureProcessor:
    def __init__(
        self,
        statement: StatementStructure,
        all_data: dict[str, dict[str, float]],
        periods: list[str],
        include_empty_items: bool,
        indent_char: str,
        get_item_type_func: Callable, # Pass the type getter func
    ):
        self.statement = statement
        self.all_data = all_data
        self.periods = periods
        self.include_empty_items = include_empty_items
        self.indent_char = indent_char
        self._get_item_type = get_item_type_func
        self.rows: list[dict[str, Any]] = []

    def process(self) -> list[dict[str, Any]]:
        """Processes the statement structure and returns the list of rows."""
        self._process_recursive(self.statement.sections, 0)
        return self.rows

    def _calculate_and_append_subtotal(
        self,
        subtotal_item: SubtotalLineItem,
        depth: int,
    ) -> None:
        """Calculate subtotal values across periods and append the row."""
        subtotal_values: dict[str, float] = {}
        item_ids_to_sum = subtotal_item.input_ids # From CalculatedLineItem base

        if not item_ids_to_sum:
             logger.warning(f"Subtotal item '{subtotal_item.id}' has no item IDs to sum.")
             return

        for period in self.periods:
            period_sum = 0.0
            sum_contributors = 0
            for item_id in item_ids_to_sum:
                value = self.all_data.get(item_id, {}).get(period, np.nan)
                if pd.notna(value):
                     period_sum += value
                     sum_contributors += 1

            subtotal_values[period] = period_sum if sum_contributors > 0 else np.nan

        subtotal_row = {
            "Line Item": self.indent_char * depth + subtotal_item.name,
            "ID": subtotal_item.id,
            **subtotal_values,
            # Metadata
            "line_type": "subtotal",
            "node_id": subtotal_item.id,
            "sign_convention": subtotal_item.sign_convention,
            "is_subtotal": True,
            "is_calculated": True, # Subtotals are implicitly calculated
        }
        self.rows.append(subtotal_row)
        logger.debug(f"Appended subtotal row for ID: {subtotal_item.id}")

    def _process_recursive(
        self,
        items_or_sections: list[Union[Section, StatementItem]],
        current_depth: int,
    ) -> None:
        """Recursively processes structure, calculates, and appends rows."""
        for item in items_or_sections:
            if isinstance(item, Section):
                # Recurse into the section's items
                self._process_recursive(item.items, current_depth + 1)
                # After processing items, add the section's subtotal if it exists
                if hasattr(item, "subtotal") and item.subtotal:
                    self._calculate_and_append_subtotal(
                        item.subtotal, current_depth + 1
                    )
            elif isinstance(item, SubtotalLineItem):
                 # This handles subtotals defined directly within items list
                 self._calculate_and_append_subtotal(
                     item, current_depth
                 )
            elif isinstance(item, (LineItem, CalculatedLineItem)):
                node_id = getattr(item, "node_id", item.id)
                item_data = self.all_data.get(node_id, {})
                row_values = {period: item_data.get(period, np.nan) for period in self.periods}

                if self.include_empty_items or any(pd.notna(v) for v in row_values.values()):
                    row = {
                        "Line Item": self.indent_char * current_depth + item.name,
                        "ID": item.id,
                        **row_values,
                        # Metadata
                        "line_type": self._get_item_type(item), # Use passed func
                        "node_id": node_id,
                        "sign_convention": getattr(item, "sign_convention", 1),
                        "is_subtotal": isinstance(item, SubtotalLineItem),
                        "is_calculated": isinstance(item, CalculatedLineItem),
                    }
                    self.rows.append(row)


class StatementFormatter:
    """Formats financial statements for display or reporting.

    This class provides methods to transform raw financial data into
    formatted financial statements with proper headers, indentation,
    subtotals, and sign conventions.
    """

    def __init__(self, statement: StatementStructure):
        """Initialize a statement formatter.

        Args:
            statement: The statement structure to format
        """
        self.statement = statement
        self.config = {} # TODO: Consider how config is passed/used
        self.default_formats = {
            "precision": 2,
            "use_thousands_separator": True,
            "show_zero_values": True, # TODO: Check if used
            "show_negative_sign": True, # TODO: Check if used
            "indent_character": "  ",
            "subtotal_style": "bold", # TODO: Check if used
            "total_style": "bold", # TODO: Check if used
            "header_style": "bold", # TODO: Check if used
        }

    def generate_dataframe(
        self,
        graph: Graph,
        should_apply_signs: bool = True, # Renamed arg
        include_empty_items: bool = False,
        number_format: Optional[str] = None,
        include_metadata_cols: bool = False,
    ) -> pd.DataFrame:
        """Generate a formatted DataFrame of the statement including subtotals.

        Queries the graph for data based on the statement structure,
        calculates subtotals, and formats the result.

        Args:
            graph: The core.graph.Graph instance containing the data.
            should_apply_signs: Whether to apply sign conventions after calculation.
            include_empty_items: Whether to include items with no data rows.
            number_format: Optional Python format string for numbers (e.g., ',.2f').
            include_metadata_cols: If True, includes hidden metadata columns
                                   (like sign_convention, node_id) in the output.

        Returns:
            pd.DataFrame: Formatted statement DataFrame with subtotals.
        """
        data = _fetch_data_from_graph(self.statement, graph)
        all_periods = graph.periods

        processor = _StructureProcessor(
            statement=self.statement,
            all_data=data,
            periods=all_periods,
            include_empty_items=include_empty_items,
            indent_char=self.default_formats["indent_character"],
            get_item_type_func=self._get_item_type,
        )
        rows = processor.process()

        if not rows:
            return pd.DataFrame(columns=["Line Item", "ID", *all_periods])

        df = pd.DataFrame(rows)

        base_cols = ["Line Item", "ID"]
        metadata_cols = ["line_type", "node_id", "sign_convention", "is_subtotal", "is_calculated"]
        final_cols = base_cols + all_periods
        if include_metadata_cols:
             final_cols += metadata_cols

        for col in final_cols:
             if col not in df.columns:
                 df[col] = np.nan if col in all_periods else ("" if col == "Line Item" else None)

        df = df[final_cols]

        # Use imported function and renamed argument
        if should_apply_signs: # Check the renamed argument
            # Call the imported function
            df = apply_sign_convention_func(df, all_periods)

        # Use imported function for number formatting
        df = format_numbers(
            df,
            default_formats=self.default_formats,
            number_format=number_format,
            period_columns=all_periods
        )

        return df

    def _get_item_type(self, item: StatementItem) -> str:
        """Get the type of a statement item.

        Args:
            item: Statement item to get type for

        Returns:
            str: Item type identifier
        """
        if isinstance(item, Section):
            return "section"
        elif isinstance(item, SubtotalLineItem):
            return "subtotal"
        elif isinstance(item, CalculatedLineItem):
            return "calculated"
        else:
            return "item"

    def format_html(
        self,
        graph: Graph,
        should_apply_signs: bool = True, # Use consistent arg name
        include_empty_items: bool = False,
        css_styles: Optional[dict[str, str]] = None,
    ) -> str:
        """Format the statement data as HTML.

        Args:
            graph: The core.graph.Graph instance containing the data.
            should_apply_signs: Whether to apply sign conventions.
            include_empty_items: Whether to include items with no data.
            css_styles: Optional dict of CSS styles for the HTML.

        Returns:
            str: HTML string representing the statement.
        """
        df = self.generate_dataframe(
            graph=graph,
            should_apply_signs=should_apply_signs,
            include_empty_items=include_empty_items
            # number_format is applied internally by generate_dataframe
        )
        html = df.to_html(index=False)
        if css_styles:
            style_str = "<style>\n"
            for selector, style in css_styles.items():
                style_str += f"{selector} {{ {style} }}\n"
            style_str += "</style>\n"
            html = style_str + html
        return html

# --- END FILE: fin_statement_model/statements/formatter/formatter.py ---

# --- START FILE: fin_statement_model/statements/populator.py ---
"""Populates a `fin_statement_model.core.graph.Graph` with calculation nodes.

This module provides the function `populate_graph_from_statement`, which is
responsible for translating the calculation logic defined within a
`StatementStructure` (specifically `CalculatedLineItem` and `SubtotalLineItem`)
into actual calculation nodes within a `Graph` instance. It bridges the gap
between the static definition of a statement and the dynamic calculation graph.
"""

import logging
from typing import Union

from fin_statement_model.core.graph import Graph
from fin_statement_model.core.errors import (
    NodeError,
    CircularDependencyError,
    CalculationError,
    ConfigurationError,
    MetricError,
)
# Import the MetricRegistry CLASS, not an instance
from fin_statement_model.core.metrics.registry import MetricRegistry
from fin_statement_model.statements.structure import (
    StatementStructure,
    CalculatedLineItem,
    SubtotalLineItem,
    MetricLineItem,
    LineItem,
)

logger = logging.getLogger(__name__)

__all__ = ["populate_graph_from_statement"]


def populate_graph_from_statement(statement: StatementStructure, graph: Graph) -> list[tuple[str, str]]:
    """Add calculation nodes defined in a StatementStructure to a Graph.

    Iterates through `CalculatedLineItem` and `SubtotalLineItem` instances
    found within the provided `statement` structure. For each item, it attempts
    to add a corresponding calculation node to the `graph` using the
    `graph.add_calculation` method. This method implicitly handles dependency
    checking and cycle detection within the graph.

    This function is designed to be idempotent: if a node corresponding to a
    statement item already exists in the graph, it will be skipped.

    Args:
        statement: The `StatementStructure` object containing the definitions
            of calculated items and subtotals (e.g., built by
            `StatementStructureBuilder`).
        graph: The `core.graph.Graph` instance that will be populated with
            the calculation nodes.

    Returns:
        A list of tuples, where each tuple contains `(item_id, error_message)`
        for any items that could not be successfully added to the graph. An
        empty list indicates that all applicable items were added (or already
        existed) without critical errors.

    Raises:
        TypeError: If `statement` is not a `StatementStructure` or `graph` is
            not a `Graph` instance.

    Example:
        >>> from fin_statement_model.core.graph import Graph
        >>> from fin_statement_model.statements.structure import StatementStructure # and items
        >>> # Assume 'my_statement' is a valid StatementStructure instance
        >>> # Assume 'my_graph' is a Graph, potentially with data nodes
        >>> my_graph = Graph()
        >>> my_statement = StatementStructure(id="IS", name="Income Statement")
        >>> # ... add sections and items to my_statement ...
        >>> # Example item: CalculatedLineItem(id="gross_profit", ..., calculation=...)
        >>>
        >>> errors = populate_graph_from_statement(my_statement, my_graph)
        >>> if errors:
        ...     # Example of handling errors - use logging in real code
        ...     logger.warning(f"Errors encountered during population: {errors}")
        ... else:
        ...     logger.info("Graph populated successfully.")
        >>> # 'my_graph' now contains a calculation node for 'gross_profit' (if defined)
    """
    if not isinstance(statement, StatementStructure):
        raise TypeError("statement must be a StatementStructure instance")
    if not isinstance(graph, Graph):
        raise TypeError("graph must be a Graph instance")

    items_to_process = statement.get_calculation_items() # Gets CalculatedLineItem and SubtotalLineItem
    metric_items_to_process = statement.get_metric_items() # Assume this method exists or add it
    all_items_to_process = items_to_process + metric_items_to_process

    errors_encountered: list[tuple[str, str]] = []
    nodes_added_count = 0
    items_to_retry: list[Union[CalculatedLineItem, SubtotalLineItem, MetricLineItem]] = [] # Update Union

    # Instantiate the registry here
    try:
        metric_registry = MetricRegistry() # Instantiate the class
        # TODO: Consider where/how to load metric definitions into this instance.
        # For now, it assumes metrics are loaded elsewhere or not needed immediately.
        # A better approach might be to pass a pre-loaded registry instance
        # into populate_graph_from_statement.
    except Exception as e:
        logger.exception("Failed to initialize MetricRegistry.")
        raise ConfigurationError(f"MetricRegistry initialization failed: {e}") from e

    logger.info(f"Starting graph population for statement '{statement.id}'. Processing {len(all_items_to_process)} calculation/metric items.")

    def _process_item(item: Union[CalculatedLineItem, SubtotalLineItem, MetricLineItem], is_retry: bool = False) -> bool: # Update Union
        """Process a single calculation/subtotal/metric item. Return True if successful, False otherwise."""
        nonlocal nodes_added_count # Allow modification of outer scope variable
        item_id = item.id
        try:
            # Check if node already exists (idempotency)
            if graph.has_node(item_id):
                # logger.debug(f"Node '{item_id}' already exists in graph. Skipping addition.") # Reduced logging
                return True # Consider existing node as success

            # --- Handle MetricLineItem --- #
            if isinstance(item, MetricLineItem):
                try:
                    # Use the instantiated registry object
                    metric = metric_registry.get(item.metric_id)
                except MetricError as e:
                    logger.error(f"Cannot populate item '{item_id}': Metric '{item.metric_id}' not found in registry: {e}")
                    errors_encountered.append((item_id, f"Metric '{item.metric_id}' not found: {e}"))
                    return False # Failed processing

                # Metric found, now resolve its inputs based on the item's mapping
                metric_input_names = metric.inputs # Expected input names by the metric
                item_input_map = item.inputs # Mapping: metric_input_name -> statement_item_id

                resolved_metric_node_ids = []
                missing_metric_input_details = []

                # Verify item_input_map provides all required metric inputs
                provided_metric_inputs = set(item_input_map.keys())
                required_metric_inputs = set(metric_input_names)
                if provided_metric_inputs != required_metric_inputs:
                    missing_req = required_metric_inputs - provided_metric_inputs
                    extra_prov = provided_metric_inputs - required_metric_inputs
                    error_msg = f"Input mapping mismatch for metric '{item.metric_id}' in item '{item_id}'."
                    if missing_req:
                        error_msg += f" Missing required metric inputs: {missing_req}."
                    if extra_prov:
                        error_msg += f" Unexpected inputs provided: {extra_prov}."
                    logger.error(error_msg)
                    errors_encountered.append((item_id, error_msg))
                    return False # Failed processing

                # Resolve statement item IDs to graph node IDs
                for metric_input_name in metric.inputs:
                    input_map_value = item_input_map[metric_input_name] # ID from the YAML mapping value
                    target_node_id = None

                    # 1. Check if the mapping value refers to another statement item ID
                    found_item = statement.find_item_by_id(input_map_value)
                    if found_item:
                        if isinstance(found_item, (CalculatedLineItem, MetricLineItem)): # Includes Subtotal
                            target_node_id = found_item.id # The item ID itself is the node ID
                        elif isinstance(found_item, LineItem):
                            target_node_id = found_item.node_id # Map to the underlying node_id
                    
                    # 2. If not found as a statement item, check if it's a direct graph node ID
                    elif graph.has_node(input_map_value):
                        target_node_id = input_map_value # The mapping value IS the node ID

                    # 3. Now check if the resolved target_node_id exists and add it
                    if target_node_id:
                        if graph.has_node(target_node_id):
                            resolved_metric_node_ids.append(target_node_id)
                        else:
                            # The target node (either from item.node_id or direct mapping) doesn't exist in the graph
                            missing_metric_input_details.append((input_map_value, target_node_id))
                    else:
                        # The input_map_value wasn't found as a statement item OR a direct graph node
                        logger.error(
                            f"Mapped input '{input_map_value}' (for metric input '{metric_input_name}') "
                            f"required by metric item '{item.id}' could not be resolved to a statement item or a graph node in statement '{statement.id}'."
                        )
                        missing_metric_input_details.append((input_map_value, None))

                if missing_metric_input_details:
                    missing_summary = [
                        f"statement item '{i_id}' needs node '{n_id}'"
                        if n_id else f"statement item '{i_id}' not found/mappable"
                        for i_id, n_id in missing_metric_input_details
                    ]
                    if is_retry:
                        logger.error(
                            f"Retry failed for metric item '{item.id}' (metric '{item.metric_id}') for statement '{statement.id}': "
                            f"missing required input nodes: {'; '.join(missing_summary)}"
                        )
                        errors_encountered.append((item_id, f"Missing inputs for metric on retry: {missing_summary}"))
                    return False # Failed due to missing inputs

                # Add the calculation node using details from the metric
                graph.add_calculation(
                    name=item_id, # Use the metric item's ID as the node name
                    input_names=resolved_metric_node_ids, # Pass the resolved graph node IDs
                    operation_type="custom_formula", # Use the correct type for formula strings
                    formula=metric.formula, # Pass the formula string as a parameter
                    # Pass the original variable names expected by the formula
                    formula_variable_names=metric.inputs
                )
                nodes_added_count += 1
                # logger.debug(f"Successfully added calculation node '{item_id}' from MetricLineItem '{item.metric_id}'.")
            
            # --- Handle CalculatedLineItem (existing logic) --- #
            elif isinstance(item, CalculatedLineItem):
                # Get calculation details from the item properties
                calc_type = item.calculation_type
                input_item_ids = item.input_ids # These are ITEM IDs from the config
                params = item.parameters
                
                resolved_node_ids = []
                missing_input_details = [] # Store tuples of (item_id, node_id)

                for input_item_id in input_item_ids:
                    found_item = statement.find_item_by_id(input_item_id)
                    target_node_id = None
                    if found_item:
                        if isinstance(found_item, CalculatedLineItem): # Includes SubtotalLineItem
                            target_node_id = found_item.id
                        elif isinstance(found_item, LineItem):
                            target_node_id = found_item.node_id
                        
                    if target_node_id:
                        if graph.has_node(target_node_id):
                            resolved_node_ids.append(target_node_id)
                        else:
                            # Input node is missing
                            missing_input_details.append((input_item_id, target_node_id))
                    else:
                        logger.error(
                            f"Input item ID '{input_item_id}' required by calculation '{item.id}' not found "
                            f"or does not correspond to a graph node in statement '{statement.id}'."
                        )
                        missing_input_details.append((input_item_id, None))

                if missing_input_details:
                    missing_summary = [
                        f"item '{i_id}' needs node '{n_id}'"
                        if n_id else f"item '{i_id}' not found/mappable"
                        for i_id, n_id in missing_input_details
                    ]
                    # Don't log error immediately on first pass, just fail processing
                    # Log only if it fails on retry
                    if is_retry:
                        logger.error(
                            f"Retry failed for calculated item '{item.id}' for statement '{statement.id}': "
                            f"missing required inputs: {'; '.join(missing_summary)}"
                        )
                        errors_encountered.append((item_id, f"Missing inputs on retry: {missing_summary}"))
                    return False # Failed due to missing inputs
                
                # Add the calculation node
                graph.add_calculation(
                    name=item_id, # Use the calculated item's ID as the node name
                    input_names=resolved_node_ids, # Pass the list of actual node IDs
                    operation_type=calc_type,
                    **params
                )
                nodes_added_count += 1
                # logger.debug(f"Successfully added calculation node '{item_id}' from CalculatedLineItem.")
            
            elif isinstance(item, SubtotalLineItem):
                input_item_ids = item.item_ids # These are ITEM IDs from the config
                if not input_item_ids:
                    # logger.warning(f"Subtotal item '{item_id}' has no input item IDs. Skipping node creation.")
                    return True # Empty subtotal is not an error
                
                resolved_node_ids_sub = []
                missing_input_details_sub = []

                for input_item_id_sub in input_item_ids:
                    found_item_sub = statement.find_item_by_id(input_item_id_sub)
                    target_node_id_sub = None
                    if found_item_sub:
                        if isinstance(found_item_sub, CalculatedLineItem): # Includes SubtotalLineItem
                            target_node_id_sub = found_item_sub.id
                        elif isinstance(found_item_sub, LineItem):
                            target_node_id_sub = found_item_sub.node_id

                    if target_node_id_sub:
                        if graph.has_node(target_node_id_sub):
                            resolved_node_ids_sub.append(target_node_id_sub)
                        else:
                            missing_input_details_sub.append((input_item_id_sub, target_node_id_sub))
                    else:
                        logger.error(
                            f"Input item ID '{input_item_id_sub}' required by subtotal '{item.id}' not found "
                            f"or does not correspond to a graph node in statement '{statement.id}'."
                        )
                        missing_input_details_sub.append((input_item_id_sub, None))

                if missing_input_details_sub:
                    missing_summary_sub = [
                        f"item '{i_id}' needs node '{n_id}'"
                        if n_id else f"item '{i_id}' not found/mappable"
                        for i_id, n_id in missing_input_details_sub
                    ]
                    # Log only if it fails on retry
                    if is_retry:
                        logger.error(
                            f"Retry failed for subtotal item '{item.id}' for statement '{statement.id}': "
                            f"missing required inputs: {'; '.join(missing_summary_sub)}"
                        )
                        errors_encountered.append((item_id, f"Missing inputs for subtotal on retry: {missing_summary_sub}"))
                    return False # Failed due to missing inputs
                
                # Add the calculation node for subtotal
                graph.add_calculation(
                    name=item_id, # Use the subtotal item's ID as the node name
                    input_names=resolved_node_ids_sub, # Pass the RESOLVED node IDs
                    operation_type="addition", # Subtotals are always additions
                )
                nodes_added_count += 1
                # logger.debug(f"Successfully added calculation node '{item_id}' from SubtotalLineItem.")

            else:
                # Should not happen
                logger.warning(f"Skipping unexpected item type during population: {type(item).__name__} for ID '{item_id}'")
                return True # Don't treat as error, but skip
            
            return True # Item processed successfully

        except (NodeError, CircularDependencyError, CalculationError, ConfigurationError) as e:
            # Catch errors from graph.add_calculation or item definition issues
            error_msg = f"Failed to add node '{item_id}': {e!s}"
            if not is_retry: # Only log full exception on first pass if it's not a missing input NodeError
                if not (isinstance(e, NodeError) and "missing input nodes" in str(e).lower()):
                     logger.exception(error_msg)
            else: # Log exception if it fails on retry
                 logger.exception(error_msg)
                 errors_encountered.append((item_id, str(e))) # Store ID and error message
            return False # Failed processing
        except Exception as e:
            # Catch any other unexpected errors during processing of this item
            error_msg = f"Unexpected error processing item '{item_id}': {e!s}"
            logger.exception(error_msg) # Log with stack trace
            errors_encountered.append((item_id, f"Unexpected error: {e!s}"))
            return False # Failed processing

    # --- Initial Processing Pass ---
    items_to_process_loop = list(all_items_to_process) # Start with all items
    processed_in_pass = -1 # Flag to check if progress is being made

    while items_to_process_loop and processed_in_pass != 0: # Loop until no items left or no progress
        items_failed_this_pass = []
        processed_in_pass = 0

        logger.info(f"Population loop: Processing {len(items_to_process_loop)} items...")

        for item in items_to_process_loop:
            # Use is_retry=True only if it's not the very first pass for THIS item
            # Check if item was in the *previous* iteration's failed list if needed,
            # but for simplicity, let's treat subsequent loops as retries for logging.
            # A simple check: len(items_to_process_loop) != len(all_items_to_process) indicates a retry loop.
            is_retry_log = len(items_to_process_loop) < len(all_items_to_process)
            success = _process_item(item, is_retry=is_retry_log)

            if success:
                processed_in_pass += 1
            else:
                items_failed_this_pass.append(item)

        # Prepare for the next loop with only the failed items
        items_to_process_loop = items_failed_this_pass

        if processed_in_pass == 0 and items_to_process_loop:
            # No progress made, but items still failed - indicates persistent issue (e.g., circular dep or truly missing node)
            logger.warning(f"Population loop stalled. {len(items_to_process_loop)} items could not be processed: {[item.id for item in items_to_process_loop]}")
            # Add these persistent failures to the main error list
            for item in items_to_process_loop:
                 # Attempt to add a generic error if not already added by _process_item
                 if not any(err[0] == item.id for err in errors_encountered):
                     errors_encountered.append((item.id, "Failed to process due to unresolved dependencies or circular reference."))
            break # Exit the loop

    if errors_encountered:
        logger.warning(f"Graph population for statement '{statement.id}' completed with {len(errors_encountered)} persistent errors.")
    else:
        log_level = logging.INFO if nodes_added_count > 0 else logging.DEBUG
        logger.log(log_level, f"Graph population for statement '{statement.id}' completed. Added {nodes_added_count} new nodes.")

    return errors_encountered

# --- END FILE: fin_statement_model/statements/populator.py ---

# --- START FILE: fin_statement_model/statements/registry.py ---
"""Registry for managing loaded and validated financial statement structures.

This module provides the `StatementRegistry` class, which acts as a central
store for `StatementStructure` objects after they have been loaded from
configurations and built. It ensures uniqueness of statement IDs and provides
methods for retrieving registered statements.
"""

import logging
from typing import Optional

# Assuming StatementStructure is defined here or imported appropriately
# We might need to adjust this import based on the actual location
try:
    from .structure import StatementStructure
except ImportError:
    # Handle cases where structure might be in a different sub-package later if needed
    # For now, assume it's available via relative import
    from fin_statement_model.statements.structure import StatementStructure

from .errors import StatementError # Assuming StatementError is in statements/errors.py

logger = logging.getLogger(__name__)

__all__ = ["StatementRegistry"]


class StatementRegistry:
    """Manages a collection of loaded financial statement structures.

    This registry holds instances of `StatementStructure`, keyed by their unique
    IDs. It prevents duplicate registrations and provides methods to access
    registered statements individually or collectively.

    Attributes:
        _statements: A dictionary mapping statement IDs (str) to their
                     corresponding `StatementStructure` objects.
    """

    def __init__(self):
        """Initialize an empty statement registry."""
        self._statements: dict[str, StatementStructure] = {}
        logger.debug("StatementRegistry initialized.")

    def register(self, statement: StatementStructure) -> None:
        """Register a statement structure with the registry.

        Ensures the provided object is a `StatementStructure` with a valid ID
        and that the ID is not already present in the registry.

        Args:
            statement: The `StatementStructure` instance to register.

        Raises:
            TypeError: If the `statement` argument is not an instance of
                `StatementStructure`.
            ValueError: If the `statement` has an invalid or empty ID.
            StatementError: If a statement with the same ID (`statement.id`) is
                already registered.
        """
        if not isinstance(statement, StatementStructure):
             raise TypeError("Only StatementStructure objects can be registered.")

        statement_id = statement.id
        if not statement_id:
             raise ValueError("StatementStructure must have a valid non-empty id to be registered.")

        if statement_id in self._statements:
            # Policy: Raise error on conflict
            logger.error(f"Attempted to register duplicate statement ID: '{statement_id}'")
            raise StatementError(
                message=f"Statement with ID '{statement_id}' is already registered.",
                # statement_id=statement_id # Add if StatementError accepts this arg
            )

        self._statements[statement_id] = statement
        logger.info(f"Registered statement '{statement.name}' with ID '{statement_id}'")

    def get(self, statement_id: str) -> Optional[StatementStructure]:
        """Get a registered statement by its ID.

        Returns:
            The `StatementStructure` instance associated with the given ID if
            it exists, otherwise returns `None`.

        Example:
            >>> registry = StatementRegistry()
            >>> # Assume 'income_statement' is a valid StatementStructure instance
            >>> # registry.register(income_statement)
            >>> retrieved_statement = registry.get("income_statement_id")
            >>> if retrieved_statement:
            ...     logger.info(f"Found: {retrieved_statement.name}")
            ... else:
            ...     logger.info("Statement not found.")
        """
        return self._statements.get(statement_id)

    def get_all_ids(self) -> list[str]:
        """Get the IDs of all registered statements.

        Returns:
            A list containing the unique IDs of all statements currently held
            in the registry.
        """
        return list(self._statements.keys())

    def get_all_statements(self) -> list[StatementStructure]:
        """Get all registered statement structure objects.

        Returns:
            A list containing all `StatementStructure` objects currently held
            in the registry.
        """
        return list(self._statements.values())

    def clear(self) -> None:
        """Remove all statement structures from the registry.

        Resets the registry to an empty state.
        """
        self._statements = {}
        logger.info("StatementRegistry cleared.")

    def __len__(self) -> int:
        """Return the number of registered statements."""
        return len(self._statements)

    def __contains__(self, statement_id: str) -> bool:
        """Check if a statement ID exists in the registry.

        Allows using the `in` operator with the registry.

        Args:
            statement_id: The statement ID to check for.

        Returns:
            `True` if a statement with the given ID is registered, `False` otherwise.

        Example:
            >>> registry = StatementRegistry()
            >>> # Assume 'income_statement' is registered with ID 'IS_2023'
            >>> # registry.register(income_statement)
            >>> print("IS_2023" in registry)  # Output: True
            >>> print("BS_2023" in registry)  # Output: False
        """
        return statement_id in self._statements

# --- END FILE: fin_statement_model/statements/registry.py ---

# --- START FILE: fin_statement_model/statements/structure/__init__.py ---
"""Statement structure package.

Re-export domain model classes from submodules.
"""

from .items import (
    StatementItem,
    StatementItemType,
    LineItem,
    CalculatedLineItem,
    MetricLineItem,
    SubtotalLineItem,
)
from .containers import Section, StatementStructure

__all__ = [
    "CalculatedLineItem",
    "LineItem",
    "MetricLineItem",
    "Section",
    "StatementItem",
    "StatementItemType",
    "StatementStructure",
    "SubtotalLineItem",
]

# --- END FILE: fin_statement_model/statements/structure/__init__.py ---

# --- START FILE: fin_statement_model/statements/structure/containers.py ---
"""Container classes for defining hierarchical financial statement structures.

This module provides Section and StatementStructure, which organize
LineItem and CalculatedLineItem objects into nested groups.
"""

from typing import Any, Optional, Union

from fin_statement_model.core.errors import StatementError
from fin_statement_model.statements.structure.items import (
    StatementItem,
    StatementItemType,
    CalculatedLineItem,
    MetricLineItem,
    SubtotalLineItem,
)


__all__ = ["Section", "StatementStructure"]


class Section:
    """Represents a section in a financial statement.

    Sections group related items and subsections into a hierarchical container.
    """

    def __init__(
        self,
        id: str,
        name: str,
        description: str = "",
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a section.

        Args:
            id: Unique identifier for the section.
            name: Display name for the section.
            description: Optional description text.
            metadata: Optional additional metadata.

        Raises:
            StatementError: If id or name is invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(message=f"Invalid section ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(message=f"Invalid section name: {name} for ID: {id}")

        self._id = id
        self._name = name
        self._description = description
        self._metadata = metadata or {}
        self._items: list[Union[Section, StatementItem]] = []

    @property
    def id(self) -> str:
        """Get the section identifier."""
        return self._id

    @property
    def name(self) -> str:
        """Get the section display name."""
        return self._name

    @property
    def description(self) -> str:
        """Get the section description."""
        return self._description

    @property
    def metadata(self) -> dict[str, Any]:
        """Get the section metadata."""
        return self._metadata

    @property
    def items(self) -> list[Union["Section", StatementItem]]:
        """Get the child items and subsections."""
        return list(self._items)

    @property
    def item_type(self) -> StatementItemType:
        """Get the item type (SECTION)."""
        return StatementItemType.SECTION

    def add_item(self, item: Union["Section", StatementItem]) -> None:
        """Add a child item or subsection to this section.

        Args:
            item: The Section or StatementItem to add.

        Raises:
            StatementError: If an item with the same id already exists.
        """
        if any(existing.id == item.id for existing in self._items):
            raise StatementError(message=f"Duplicate item ID: {item.id} in section: {self.id}")
        self._items.append(item)

    def find_item_by_id(self, item_id: str) -> Optional[Union["Section", StatementItem]]:
        """Recursively find an item by its identifier within this section.

        Args:
            item_id: Identifier of the item to search for.

        Returns:
            The found Section or StatementItem, or None if not found.
        """
        if self.id == item_id:
            return self
        for child in self._items:
            if child.id == item_id:
                return child
            if isinstance(child, Section):
                found = child.find_item_by_id(item_id)
                if found:
                    return found
        if hasattr(self, 'subtotal') and self.subtotal and self.subtotal.id == item_id:
            return self.subtotal
        return None


class StatementStructure:
    """Top-level container for a financial statement structure.

    Manages a hierarchy of Section objects.
    """

    def __init__(
        self,
        id: str,
        name: str,
        description: str = "",
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a statement structure.

        Args:
            id: Unique identifier for the statement.
            name: Display name for the statement.
            description: Optional description text.
            metadata: Optional additional metadata.

        Raises:
            StatementError: If id or name is invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(message=f"Invalid statement ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(message=f"Invalid statement name: {name} for ID: {id}")

        self._id = id
        self._name = name
        self._description = description
        self._metadata = metadata or {}
        self._sections: list[Section] = []

    @property
    def id(self) -> str:
        """Get the statement identifier."""
        return self._id

    @property
    def name(self) -> str:
        """Get the statement display name."""
        return self._name

    @property
    def description(self) -> str:
        """Get the statement description."""
        return self._description

    @property
    def metadata(self) -> dict[str, Any]:
        """Get the statement metadata."""
        return self._metadata

    @property
    def sections(self) -> list[Section]:
        """Get the top-level sections."""
        return list(self._sections)

    @property
    def items(self) -> list[Section]:
        """Alias for sections to ease iteration."""
        return self.sections

    def add_section(self, section: Section) -> None:
        """Add a section to the statement.

        Args:
            section: Section to add.

        Raises:
            StatementError: If a section with the same id already exists.
        """
        if any(s.id == section.id for s in self._sections):
            raise StatementError(
                message=f"Duplicate section ID: {section.id} in statement: {self.id}"
            )
        self._sections.append(section)

    def find_item_by_id(self, item_id: str) -> Optional[Union[Section, StatementItem]]:
        """Find an item by its ID in the statement structure.

        Args:
            item_id: The ID of the item to find.

        Returns:
            Optional[Union[Section, StatementItem]]: The found item or None if not found.
        """
        for section in self._sections:
            item = section.find_item_by_id(item_id)
            if item:
                return item
        return None

    def get_calculation_items(
        self,
    ) -> list[Union[CalculatedLineItem, SubtotalLineItem]]:
        """Get all calculation items from the statement structure.

        Returns:
            List[Union[CalculatedLineItem, SubtotalLineItem]]: List of calculation items.
        """
        calculation_items = []

        def collect_calculation_items(items: list[Union["Section", "StatementItem"]]):
            for item in items:
                if isinstance(item, (CalculatedLineItem, SubtotalLineItem)):
                    calculation_items.append(item)
                elif isinstance(item, Section):
                    collect_calculation_items(item.items)
                    if hasattr(item, 'subtotal') and item.subtotal:
                        if isinstance(item.subtotal, SubtotalLineItem):
                            calculation_items.append(item.subtotal)
                        else:
                            pass

        collect_calculation_items(self._sections)
        return calculation_items

    def get_metric_items(self) -> list[MetricLineItem]:
        """Get all metric items from the statement structure.

        Returns:
            List[MetricLineItem]: List of metric items.
        """
        metric_items = []

        def collect_metric_items(items: list[Union["Section", "StatementItem"]]):
            for item in items:
                if isinstance(item, MetricLineItem):
                    metric_items.append(item)
                elif isinstance(item, Section):
                    collect_metric_items(item.items)
                    # Subtotals are handled by get_calculation_items, not relevant here

        collect_metric_items(self._sections)
        return metric_items

    def get_all_items(self) -> list[StatementItem]:
        """Get all StatementItem instances recursively from the structure.

        Traverses all sections and nested sections, collecting only objects that
        are subclasses of StatementItem (e.g., LineItem, CalculatedLineItem),
        excluding Section objects themselves.

        Returns:
            List[StatementItem]: A flat list of all statement items found.
        """
        all_statement_items: list[StatementItem] = []

        def _collect_items_recursive(items_or_sections: list[Union[Section, StatementItem]]) -> None:
            for item in items_or_sections:
                if isinstance(item, Section):
                    _collect_items_recursive(item.items)
                elif isinstance(item, StatementItem):
                    all_statement_items.append(item)

        _collect_items_recursive(self._sections)

        return all_statement_items

# --- END FILE: fin_statement_model/statements/structure/containers.py ---

# --- START FILE: fin_statement_model/statements/structure/items.py ---
"""Statement structure items module defining line items, calculated items, and subtotals."""

from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Optional

from fin_statement_model.core.errors import StatementError

__all__ = [
    "CalculatedLineItem",
    "LineItem",
    "MetricLineItem",
    "StatementItem",
    "StatementItemType",
    "SubtotalLineItem",
]


class StatementItemType(Enum):
    """Types of statement structure items.

    Attributes:
      SECTION: Section container
      LINE_ITEM: Basic financial line item
      SUBTOTAL: Subtotal of multiple items
      CALCULATED: Derived calculation item
      METRIC: Derived metric item from registry
    """

    SECTION = "section"
    LINE_ITEM = "line_item"
    SUBTOTAL = "subtotal"
    CALCULATED = "calculated"
    METRIC = "metric"


class StatementItem(ABC):
    """Abstract base class for all statement structure items.

    Defines a common interface: id, name, and item_type.
    """

    @property
    @abstractmethod
    def id(self) -> str:
        """Get the unique identifier of the item."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Get the display name of the item."""

    @property
    @abstractmethod
    def item_type(self) -> StatementItemType:
        """Get the type of this statement item."""


class LineItem(StatementItem):
    """Represents a basic line item in a financial statement.

    Args:
      id: Unique ID for the line item
      name: Display name for the line item
      node_id: ID of the core graph node that holds values
      description: Optional explanatory text
      sign_convention: 1 for normal values, -1 for inverted
      metadata: Optional additional attributes

    Raises:
      StatementError: If inputs are invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        node_id: str,
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a basic LineItem.

        Args:
            id: Unique ID for the line item.
            name: Display name for the line item.
            node_id: ID of the core graph node holding values.
            description: Optional explanatory text.
            sign_convention: Sign convention (1 for positive, -1 for negative).
            metadata: Optional additional attributes.

        Raises:
            StatementError: If inputs are invalid.
        """
        if not id or not isinstance(id, str):
            raise StatementError(message=f"Invalid line item ID: {id}")
        if not name or not isinstance(name, str):
            raise StatementError(message=f"Invalid line item name: {name} for ID: {id}")
        if not node_id or not isinstance(node_id, str):
            raise StatementError(message=f"Invalid node ID for line item: {id}")
        if sign_convention not in (1, -1):
            raise StatementError(
                message=f"Invalid sign convention {sign_convention} for item: {id}"
            )
        self._id = id
        self._name = name
        self._node_id = node_id
        self._description = description
        self._sign_convention = sign_convention
        self._metadata = metadata or {}

    @property
    def id(self) -> str:
        """Get the unique identifier of the line item."""
        return self._id

    @property
    def name(self) -> str:
        """Get the display name of the line item."""
        return self._name

    @property
    def node_id(self) -> str:
        """Get the core graph node ID for this line item."""
        return self._node_id

    @property
    def description(self) -> str:
        """Get the description for this line item."""
        return self._description

    @property
    def sign_convention(self) -> int:
        """Get the sign convention (1 or -1)."""
        return self._sign_convention

    @property
    def metadata(self) -> dict[str, Any]:
        """Get custom metadata associated with this item."""
        return self._metadata

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (LINE_ITEM)."""
        return StatementItemType.LINE_ITEM


class MetricLineItem(LineItem):
    """Represents a line item whose calculation is defined by a core metric.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      metric_id: ID of the metric in the core.metrics.registry
      inputs: Dict mapping metric input names to statement item IDs
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata

    Raises:
      StatementError: If metric_id or inputs are invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        metric_id: str,
        inputs: dict[str, str],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a MetricLineItem referencing a registered metric.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            metric_id: ID of the metric in the core.metrics.registry.
            inputs: Dict mapping metric input names to statement item IDs.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.

        Raises:
            StatementError: If metric_id or inputs are invalid.
        """
        super().__init__(
            id=id,
            name=name,
            node_id=id,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
        )
        if not metric_id or not isinstance(metric_id, str):
            raise StatementError(message=f"Invalid metric_id '{metric_id}' for item: {id}")
        if not isinstance(inputs, dict) or not inputs:
            raise StatementError(message=f"Metric inputs must be a non-empty dictionary for item: {id}")
        if not all(isinstance(k, str) and isinstance(v, str) for k, v in inputs.items()):
            raise StatementError(message=f"Metric input keys and values must be strings for item: {id}")

        self._metric_id = metric_id
        self._inputs = inputs

    @property
    def metric_id(self) -> str:
        """Get the ID of the metric referenced from the core registry."""
        return self._metric_id

    @property
    def inputs(self) -> dict[str, str]:
        """Get the mapping from metric input names to statement item IDs."""
        return self._inputs

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (METRIC)."""
        return StatementItemType.METRIC


class CalculatedLineItem(LineItem):
    """Represents a calculated line item whose values come from graph calculations.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      calculation: Dict with 'type', 'inputs', optional 'parameters'
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata

    Raises:
      StatementError: If calculation dictionary is invalid
    """

    def __init__(
        self,
        id: str,
        name: str,
        calculation: dict[str, Any],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a CalculatedLineItem based on calculation specification.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            calculation: Calculation spec dict with 'type', 'inputs', optional 'parameters'.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.

        Raises:
            StatementError: If calculation dictionary is invalid.
        """
        super().__init__(
            id=id,
            name=name,
            node_id=id,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
        )
        if not isinstance(calculation, dict):
            raise StatementError(message=f"Invalid calculation spec for item: {id}")
        if "type" not in calculation:
            raise StatementError(message=f"Missing calculation type for item: {id}")
        inputs = calculation.get("inputs")
        if not isinstance(inputs, list) or not inputs:
            raise StatementError(
                message=f"Calculation inputs must be a non-empty list for item: {id}"
            )
        self._calculation = calculation

    @property
    def calculation_type(self) -> str:
        """Get the calculation operation type (e.g., 'addition')."""
        return self._calculation["type"]

    @property
    def input_ids(self) -> list[str]:
        """Get the list of input item IDs for this calculation."""
        return self._calculation["inputs"]

    @property
    def parameters(self) -> dict[str, Any]:
        """Get optional parameters for the calculation."""
        return self._calculation.get("parameters", {})

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (CALCULATED)."""
        return StatementItemType.CALCULATED


class SubtotalLineItem(CalculatedLineItem):
    """Represents a subtotal line item summing multiple other items.

    Args:
      id: Unique ID (also used as node_id)
      name: Display name
      item_ids: List of IDs to sum
      description: Optional description
      sign_convention: 1 or -1
      metadata: Optional metadata

    Raises:
      StatementError: If item_ids is empty or not a list
    """

    def __init__(
        self,
        id: str,
        name: str,
        item_ids: list[str],
        description: str = "",
        sign_convention: int = 1,
        metadata: Optional[dict[str, Any]] = None,
    ):
        """Initialize a SubtotalLineItem summing multiple items.

        Args:
            id: Unique ID (also used as node_id).
            name: Display name.
            item_ids: List of IDs to sum.
            description: Optional description.
            sign_convention: Sign convention (1 or -1).
            metadata: Optional metadata.

        Raises:
            StatementError: If item_ids is empty or not a list.
        """
        if not isinstance(item_ids, list) or not item_ids:
            raise StatementError(message=f"Invalid or empty item IDs for subtotal: {id}")
        calculation = {"type": "addition", "inputs": item_ids, "parameters": {}}
        super().__init__(
            id=id,
            name=name,
            calculation=calculation,
            description=description,
            sign_convention=sign_convention,
            metadata=metadata,
        )
        self._item_ids = item_ids

    @property
    def item_ids(self) -> list[str]:
        """Get the IDs of items summed by this subtotal."""
        return self._item_ids

    @property
    def item_type(self) -> StatementItemType:
        """Get the type of this item (SUBTOTAL)."""
        return StatementItemType.SUBTOTAL

# --- END FILE: fin_statement_model/statements/structure/items.py ---

